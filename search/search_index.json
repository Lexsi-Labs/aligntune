{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AlignTune is a production-ready fine-tuning library designed to simplify training and fine-tuning of Large Language Models (LLMs) with both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) methods. It provides a high-level, unified API that abstracts away the complexities of backend selection, algorithm configuration, and training loops, letting you focus on delivering results. Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced RLHF algorithms, AlignTune streamlines your workflow for LLM fine-tuning. Core Features \u00b6 Multi-Backend Architecture : Choose between TRL (reliable, battle-tested) and Unsloth (faster) backends with intelligent auto-selection. Complete RLHF Coverage : RL algorithms including DPO, PPO, GRPO, GSPO, DAPO, and Dr. GRPO. 27+ Reward Functions : Built-in reward functions for quality, safety, style, and task-specific metrics. Production-Ready : No mock code, comprehensive error handling, extensive testing, and robust validation. Quick Start \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) # Train the model trainer . train () # Evaluate metrics = trainer . evaluate () print ( metrics ) Get Started \u2192 Supported Algorithms \u00b6 Algorithm TRL Backend Unsloth Backend Description SFT Yes Yes Supervised Fine-Tuning DPO Yes Yes Direct Preference Optimization PPO Yes Yes Proximal Policy Optimization GRPO Yes Yes Group Relative Policy Optimization GSPO Yes No Group Sequential Policy Optimization (TRL only) DAPO Yes Yes Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes GRPO Done Right (unbiased variant) Key Capabilities \u00b6 Multiple Training Paradigms : Supports SFT, DPO, PPO, GRPO, and advanced RL algorithms Backend Flexibility : TRL and Unsloth backends with automatic fallback Reward Model Training : Train custom reward models from rule-based functions Comprehensive Evaluation : Multi-level evaluation with lm-eval integration Production Ready : Model serialization, reproducible training, and deployment-ready pipelines Extensible Architecture : Modular design for easy integration of custom algorithms and backends Demo Notebooks \u00b6 Interactive Colab notebooks demonstrating various AlignTune workflows: Supervised Fine-Tuning (SFT) \u00b6 Backend Model Dataset Link Unsloth Qwen/Qwen2.5-0.5B-Instruct bebechien/MobileGameNPC Open in Colab TRL google/txgemma-2b-predict trialbench_adverse-event-rate-prediction Open in Colab Unsloth Qwen/Qwen2.5-0.5B-Instruct bebechien/MobileGameNP Open in Colab Reinforcement Learning (RL) \u00b6 Backend Algorithm Model Dataset Link Unsloth PPO Qwen/Qwen2.5-0.5B-Instruct HuggingFaceH4/ultrachat_200k Open in Colab TRL PPO EleutherAI/pythia-1.4b CarperAI/openai_summarize_tldr Open in Colab TRL GRPO (Coding) Qwen/Qwen3-4B google-research-datasets/mbpp Open in Colab Unsloth GRPO (Math) meta-llama/Llama-3.2-3B-Instruct openai/gsm8k Open in Colab TRL GRPO meta-llama/Llama-3.2-3B-Instruct openai/gsm8k Open in Colab Unsloth DRGRPO Qwen/Qwen2.5-3B-Instruct yahma/alpaca-cleaned Open in Colab TRL DRGRPO Qwen/Qwen2-0.5B-Instruct AI-MO/NuminaMath-TIR Open in Colab Unsloth GSPO Qwen/Qwen3-1.7B CyberNative/Code_Vulnerability_Security_DPO Open in Colab TRL GSPO meta-llama/Llama-3.2-3B-Instruct HuggingFaceH4/ultrachat_200k Open in Colab Unsloth DAPO microsoft/Phi-3.5-mini-instruct HuggingFaceH4/ultrachat_200k Open in Colab TRL DAPO meta-llama/Llama-3.2-3B-Instruct google-research-datasets/mbpp Open in Colab Unsloth DPO microsoft/phi-2 argilla/distilabel-intel-orca-dpo-pairs Open in Colab TRL DPO google/gemma-2-2b-it Anthropic/hh-rlhf Open in Colab Explore the Documentation \u00b6 Getting Started : Installation, setup, and basic usage User Guide : In-depth tutorials for SFT and RL training API Reference : Complete Python API and class/method details Examples : End-to-end code examples Advanced Topics : Architecture, custom backends, and performance optimization Notebooks : Interactive Colab notebooks and local Jupyter notebooks Architecture \u00b6 AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend: SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend: SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO See Architecture for details. Why AlignTune? \u00b6 No Boilerplate : Avoids repetitive code for model-specific data loading, training, and inference Consistent Results : Automates best practices for RLHF research and model selection Fast Iteration : Easily compare different algorithms and backends with the same consistent API Production Ready : Model and config serialization for robust deployment and reproducibility Community-Driven : Extensible design and open contribution policy Contributing \u00b6 We welcome contributions! See Contributing Guide for details. License \u00b6 This project is licensed under the AlignTune Source Available License (ASAL) v1.0 - see the LICENSE file for details. Citation \u00b6 If you use AlignTune in your research, please cite: BibTeX: @software { alignTune2025 , title = {AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training} , author = {Chawla, Chirag and Lyngkhoi, Zera and Seth, Pratinav and Avaiya, Utsav and Bhattacharjee, Soham and Khandoga, Mykola and Yuan, Rui and Sankarapu, Vinay Kumar} , year = {2025} , note = {Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth} , organization = {Lexsi Labs} , url = {https://github.com/Lexsi-Labs/aligntune} , version = {0.0.0} } Plain Text: Chawla, C., Lyngkhoi, Z., Seth, P., Avaiya, U., Bhattacharjee, S., Khandoga, M., Yuan, R., & Sankarapu, V. K. (2025). AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training. Lexsi Labs. https://github.com/Lexsi-Labs/aligntune *Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth Acknowledgments \u00b6 AlignTune is built upon the excellent work of the following projects: HuggingFace Transformers - Model architectures and tokenizers TRL - Transformer Reinforcement Learning library Unsloth - Fast and memory-efficient training HuggingFace Datasets - Dataset loading and processing Support \u00b6 Documentation : docs.aligntune.io GitHub Issues : github.com/Lexsi-Labs/aligntune/issues Discussions : github.com/Lexsi-Labs/aligntune/discussions Get started with AlignTune and accelerate your LLM fine-tuning workflows today!","title":"Home"},{"location":"#core-features","text":"Multi-Backend Architecture : Choose between TRL (reliable, battle-tested) and Unsloth (faster) backends with intelligent auto-selection. Complete RLHF Coverage : RL algorithms including DPO, PPO, GRPO, GSPO, DAPO, and Dr. GRPO. 27+ Reward Functions : Built-in reward functions for quality, safety, style, and task-specific metrics. Production-Ready : No mock code, comprehensive error handling, extensive testing, and robust validation.","title":"Core Features"},{"location":"#quick-start","text":"from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) # Train the model trainer . train () # Evaluate metrics = trainer . evaluate () print ( metrics ) Get Started \u2192","title":"Quick Start"},{"location":"#supported-algorithms","text":"Algorithm TRL Backend Unsloth Backend Description SFT Yes Yes Supervised Fine-Tuning DPO Yes Yes Direct Preference Optimization PPO Yes Yes Proximal Policy Optimization GRPO Yes Yes Group Relative Policy Optimization GSPO Yes No Group Sequential Policy Optimization (TRL only) DAPO Yes Yes Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes GRPO Done Right (unbiased variant)","title":"Supported Algorithms"},{"location":"#key-capabilities","text":"Multiple Training Paradigms : Supports SFT, DPO, PPO, GRPO, and advanced RL algorithms Backend Flexibility : TRL and Unsloth backends with automatic fallback Reward Model Training : Train custom reward models from rule-based functions Comprehensive Evaluation : Multi-level evaluation with lm-eval integration Production Ready : Model serialization, reproducible training, and deployment-ready pipelines Extensible Architecture : Modular design for easy integration of custom algorithms and backends","title":"Key Capabilities"},{"location":"#demo-notebooks","text":"Interactive Colab notebooks demonstrating various AlignTune workflows:","title":"Demo Notebooks"},{"location":"#supervised-fine-tuning-sft","text":"Backend Model Dataset Link Unsloth Qwen/Qwen2.5-0.5B-Instruct bebechien/MobileGameNPC Open in Colab TRL google/txgemma-2b-predict trialbench_adverse-event-rate-prediction Open in Colab Unsloth Qwen/Qwen2.5-0.5B-Instruct bebechien/MobileGameNP Open in Colab","title":"Supervised Fine-Tuning (SFT)"},{"location":"#reinforcement-learning-rl","text":"Backend Algorithm Model Dataset Link Unsloth PPO Qwen/Qwen2.5-0.5B-Instruct HuggingFaceH4/ultrachat_200k Open in Colab TRL PPO EleutherAI/pythia-1.4b CarperAI/openai_summarize_tldr Open in Colab TRL GRPO (Coding) Qwen/Qwen3-4B google-research-datasets/mbpp Open in Colab Unsloth GRPO (Math) meta-llama/Llama-3.2-3B-Instruct openai/gsm8k Open in Colab TRL GRPO meta-llama/Llama-3.2-3B-Instruct openai/gsm8k Open in Colab Unsloth DRGRPO Qwen/Qwen2.5-3B-Instruct yahma/alpaca-cleaned Open in Colab TRL DRGRPO Qwen/Qwen2-0.5B-Instruct AI-MO/NuminaMath-TIR Open in Colab Unsloth GSPO Qwen/Qwen3-1.7B CyberNative/Code_Vulnerability_Security_DPO Open in Colab TRL GSPO meta-llama/Llama-3.2-3B-Instruct HuggingFaceH4/ultrachat_200k Open in Colab Unsloth DAPO microsoft/Phi-3.5-mini-instruct HuggingFaceH4/ultrachat_200k Open in Colab TRL DAPO meta-llama/Llama-3.2-3B-Instruct google-research-datasets/mbpp Open in Colab Unsloth DPO microsoft/phi-2 argilla/distilabel-intel-orca-dpo-pairs Open in Colab TRL DPO google/gemma-2-2b-it Anthropic/hh-rlhf Open in Colab","title":"Reinforcement Learning (RL)"},{"location":"#explore-the-documentation","text":"Getting Started : Installation, setup, and basic usage User Guide : In-depth tutorials for SFT and RL training API Reference : Complete Python API and class/method details Examples : End-to-end code examples Advanced Topics : Architecture, custom backends, and performance optimization Notebooks : Interactive Colab notebooks and local Jupyter notebooks","title":"Explore the Documentation"},{"location":"#architecture","text":"AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend: SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend: SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO See Architecture for details.","title":"Architecture"},{"location":"#why-aligntune","text":"No Boilerplate : Avoids repetitive code for model-specific data loading, training, and inference Consistent Results : Automates best practices for RLHF research and model selection Fast Iteration : Easily compare different algorithms and backends with the same consistent API Production Ready : Model and config serialization for robust deployment and reproducibility Community-Driven : Extensible design and open contribution policy","title":"Why AlignTune?"},{"location":"#contributing","text":"We welcome contributions! See Contributing Guide for details.","title":"Contributing"},{"location":"#license","text":"This project is licensed under the AlignTune Source Available License (ASAL) v1.0 - see the LICENSE file for details.","title":"License"},{"location":"#citation","text":"If you use AlignTune in your research, please cite: BibTeX: @software { alignTune2025 , title = {AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training} , author = {Chawla, Chirag and Lyngkhoi, Zera and Seth, Pratinav and Avaiya, Utsav and Bhattacharjee, Soham and Khandoga, Mykola and Yuan, Rui and Sankarapu, Vinay Kumar} , year = {2025} , note = {Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth} , organization = {Lexsi Labs} , url = {https://github.com/Lexsi-Labs/aligntune} , version = {0.0.0} } Plain Text: Chawla, C., Lyngkhoi, Z., Seth, P., Avaiya, U., Bhattacharjee, S., Khandoga, M., Yuan, R., & Sankarapu, V. K. (2025). AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training. Lexsi Labs. https://github.com/Lexsi-Labs/aligntune *Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth","title":"Citation"},{"location":"#acknowledgments","text":"AlignTune is built upon the excellent work of the following projects: HuggingFace Transformers - Model architectures and tokenizers TRL - Transformer Reinforcement Learning library Unsloth - Fast and memory-efficient training HuggingFace Datasets - Dataset loading and processing","title":"Acknowledgments"},{"location":"#support","text":"Documentation : docs.aligntune.io GitHub Issues : github.com/Lexsi-Labs/aligntune/issues Discussions : github.com/Lexsi-Labs/aligntune/discussions Get started with AlignTune and accelerate your LLM fine-tuning workflows today!","title":"Support"},{"location":"ISSUES/","text":"Known Issues and Troubleshooting \u00b6 This document tracks known issues, common problems, and their solutions. Reporting Issues \u00b6 If you encounter an issue not listed here, please report it: GitHub Issues : Open an issue Discussions : GitHub Discussions Common Issues \u00b6 Import Errors \u00b6 Problem : ImportError: cannot import name 'create_sft_trainer' Solution : Ensure you're using the correct import path: from aligntune.core.backend_factory import create_sft_trainer Backend Availability \u00b6 Problem : Unsloth backend not available Solution : - Check GPU compatibility - Install Unsloth: pip install unsloth - Use TRL backend as fallback: backend=\"trl\" Memory Issues \u00b6 Problem : Out of memory errors during training Solution : - Reduce batch_size - Enable PEFT/LoRA: peft_enabled=True - Use gradient checkpointing - Reduce max_seq_length - Use quantization: quantization={\"load_in_4bit\": True} Backend-Specific Issues \u00b6 TRL Backend \u00b6 See TRL Backend Documentation for TRL-specific issues Unsloth Backend \u00b6 See Unsloth Compatibility Guide for detailed troubleshooting Getting Help \u00b6 For more detailed troubleshooting, see: - Troubleshooting Guide - Unsloth Compatibility - Backend Support Matrix","title":"Known Issues and Troubleshooting"},{"location":"ISSUES/#known-issues-and-troubleshooting","text":"This document tracks known issues, common problems, and their solutions.","title":"Known Issues and Troubleshooting"},{"location":"ISSUES/#reporting-issues","text":"If you encounter an issue not listed here, please report it: GitHub Issues : Open an issue Discussions : GitHub Discussions","title":"Reporting Issues"},{"location":"ISSUES/#common-issues","text":"","title":"Common Issues"},{"location":"ISSUES/#import-errors","text":"Problem : ImportError: cannot import name 'create_sft_trainer' Solution : Ensure you're using the correct import path: from aligntune.core.backend_factory import create_sft_trainer","title":"Import Errors"},{"location":"ISSUES/#backend-availability","text":"Problem : Unsloth backend not available Solution : - Check GPU compatibility - Install Unsloth: pip install unsloth - Use TRL backend as fallback: backend=\"trl\"","title":"Backend Availability"},{"location":"ISSUES/#memory-issues","text":"Problem : Out of memory errors during training Solution : - Reduce batch_size - Enable PEFT/LoRA: peft_enabled=True - Use gradient checkpointing - Reduce max_seq_length - Use quantization: quantization={\"load_in_4bit\": True}","title":"Memory Issues"},{"location":"ISSUES/#backend-specific-issues","text":"","title":"Backend-Specific Issues"},{"location":"ISSUES/#trl-backend","text":"See TRL Backend Documentation for TRL-specific issues","title":"TRL Backend"},{"location":"ISSUES/#unsloth-backend","text":"See Unsloth Compatibility Guide for detailed troubleshooting","title":"Unsloth Backend"},{"location":"ISSUES/#getting-help","text":"For more detailed troubleshooting, see: - Troubleshooting Guide - Unsloth Compatibility - Backend Support Matrix","title":"Getting Help"},{"location":"OVERALL_SUMMARY/","text":"AlignTune Summary \u00b6 Backend Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Quick Start \u00b6 Supervised Fine-Tuning (SFT) \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"gsm8k\" , backend = \"trl\" , output_dir = \"./output/sft_model\" , max_seq_length = 512 , use_unsloth = False , peft_enabled = False , batch_size = 2 , learning_rate = 1e-5 , # Data Configuration subset = \"main\" , task_type = \"instruction_following\" , column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }, # Training Configuration max_steps = 5 , num_epochs = 1 , # Logging run_name = \"sft_training_run\" ) # Start training trainer . train () Reinforcement Learning Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , ) # Start training trainer . train () Detailed Usage \u00b6 1. Basic RL Training \u00b6 The simplest way to start training with any RL algorithm: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , # Options: dpo, ppo, grpo, gspo, dapo, etc. backend = \"trl\" , # Options: trl, unsloth output_dir = \"./output/my_model\" , batch_size = 8 , ) 2. Using Configuration Files \u00b6 You can organize your training settings in YAML or JSON configuration files for better reproducibility and version control: YAML Configuration Example \u00b6 config/grpo_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" config_name : \"main\" algorithm : \"grpo\" backend : \"trl\" output_dir : \"./output/grpo_model\" batch_size : 8 max_steps : 100 learning_rate : 2e-4 max_seq_length : 512 # Column mapping for dataset column_mapping : question : \"prompt\" answer : \"response\" # Reward configuration rewards : - type : \"accuracy\" weight : 1.0 Using the config file: from aligntune.core.backend_factory import create_rl_trainer # Method 1: Pass config file path trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" ) # Method 2: Override specific parameters trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" , batch_size = 16 , # Override the config value max_steps = 200 # Override the config value ) trainer . train () Python Dictionary Configuration \u00b6 from aligntune.core.backend_factory import create_rl_trainer config = { \"model_name\" : \"Qwen/Qwen3-0.6B\" , \"dataset_name\" : \"openai/gsm8k\" , \"config_name\" : \"main\" , \"algorithm\" : \"dpo\" , \"backend\" : \"trl\" , \"output_dir\" : \"./output/dpo_model\" , \"batch_size\" : 8 , \"max_steps\" : 100 , \"learning_rate\" : 1e-5 , } # Method 1: Pass as config parameter trainer = create_rl_trainer ( config = config ) # Method 2: Unpack as kwargs trainer = create_rl_trainer ( ** config ) # Method 3: Mix config with overrides trainer = create_rl_trainer ( config = config , batch_size = 16 # Override ) trainer . train () SFT Configuration Example \u00b6 config/sft_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" backend : \"trl\" output_dir : \"./output/sft_model\" batch_size : 8 num_epochs : 3 learning_rate : 2e-4 max_seq_length : 512 # Dataset configuration subset : \"main\" task_type : \"instruction_following\" column_mapping : question : \"instruction\" answer : \"response\" # Training settings gradient_accumulation_steps : 4 warmup_ratio : 0.1 Using the SFT config: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( config = \"config/sft_training.yaml\" ) trainer . train () 3. Custom Data Processing \u00b6 Define a custom preprocessing function to transform your data: def preprocess_function ( example ): \"\"\"Transform raw data into the required format\"\"\" return { \"prompt\" : example [ \"question\" ], \"chosen\" : example [ \"answer\" ], \"rejected\" : \"None, 0\" } trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/dpo_model\" , batch_size = 8 , processing_fn = preprocess_function , ) 4. Column Mapping \u00b6 Map dataset columns to expected format without writing a preprocessing function: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"google-research-datasets/mbpp\" , algorithm = \"grpo\" , column_mapping = { \"text\" : \"prompt\" , \"code\" : \"response\" }, backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , num_generations = 4 , ) 5. Custom Reward Functions \u00b6 For algorithms like GRPO, GSPO, and Neural Mirror GRPO: def my_math_reward ( text , reference = None , ** kwargs ): \"\"\" Custom reward function for math problems. Returns 1.0 if the answer contains numbers, 0.0 otherwise. \"\"\" has_numbers = any ( char . isdigit () for char in text ) return 1.0 if has_numbers else 0.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"unsloth\" , output_dir = \"./output/grpo_custom_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 1.0 , \"params\" : { \"reward_function\" : my_math_reward } } ] ) Multiple Reward Functions \u00b6 You can combine multiple reward functions with different weights: def accuracy_reward ( text , reference = None , ** kwargs ): \"\"\"Check if output matches reference\"\"\" if reference and text . strip () == reference . strip (): return 1.0 return 0.0 def length_penalty ( text , reference = None , ** kwargs ): \"\"\"Penalize overly long responses\"\"\" max_length = 200 return max ( 0.0 , 1.0 - ( len ( text ) - max_length ) / max_length ) if len ( text ) > max_length else 1.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_multi_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 0.7 , \"params\" : { \"reward_function\" : accuracy_reward } }, { \"type\" : \"custom\" , \"weight\" : 0.3 , \"params\" : { \"reward_function\" : length_penalty } } ] ) Configuration \u00b6 Common Parameters \u00b6 Parameter Type Description Default model_name str HuggingFace model identifier Required dataset_name str HuggingFace dataset identifier Required algorithm str RL algorithm to use Required backend str Training backend: \"trl\" or \"unsloth\" \"trl\" output_dir str Directory for saving outputs Required batch_size int Training batch size 8 learning_rate float Learning rate 1e-5 max_seq_length int Maximum sequence length 512 num_epochs int Number of training epochs 1 max_steps int Maximum training steps (overrides epochs) None config str/dict Path to config file or config dictionary None Configuration File Formats \u00b6 AlignTune supports two configuration file formats: YAML : .yaml or .yml files JSON : .json files Both formats support the same parameters and can be used interchangeably. Advanced Parameters \u00b6 For a complete list of available parameters, see PARAMETERS.md . Best Practices \u00b6 1. Dataset Configuration \u00b6 When working with datasets that have multiple subsets: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , # Important: specify the subset algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/model\" , ) 2. Dataset Loading Changes \u00b6 Note: Newer versions of the datasets library no longer support .py scripts in the data loader. Make sure to: - Use datasets with proper configuration files - Update your dataset loading code accordingly - Use config_name parameter for dataset subsets 3. Column Mapping Strategy \u00b6 Always map your dataset columns to match the expected prompt format: # Example: Mapping code dataset columns column_mapping = { \"text\" : \"prompt\" , # Input text/question \"code\" : \"response\" # Expected output/code } # Example: Mapping Q&A dataset columns column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" } 4. Configuration File Best Practices \u00b6 Version Control : Keep config files in version control for reproducibility Environment-Specific Configs : Create separate configs for dev/prod environments Parameter Overrides : Use command-line/code overrides for quick experiments Documentation : Add comments to YAML configs to document parameter choices Example directory structure: project/ configs/ base_grpo.yaml # Base configuration grpo_small_model.yaml # Small model variant grpo_production.yaml # Production settings train.py evaluate.py Evaluation \u00b6 For unified evaluation and custom metrics, see the Evaluation Guide . Troubleshooting \u00b6 Common Issues \u00b6 Import Errors # Ensure all dependencies are installed pip install -r requirements.txt Dataset Loading Issues Check that config_name matches your dataset's subset name Verify dataset exists on HuggingFace Hub Ensure proper column mapping Memory Issues Reduce batch_size Enable peft_enabled=True for parameter-efficient training Use gradient checkpointing Reduce max_seq_length Backend Compatibility Check the Backend Support Matrix for algorithm-backend compatibility GSPO is not currently supported with Unsloth backend Configuration File Issues Ensure YAML syntax is correct (proper indentation) Verify file path is correct when using config= parameter Check that all required parameters are present in the config Getting Help \u00b6 Check Examples for working examples Review PARAMETERS.md for parameter documentation Open an issue on GitHub with: Error message Code snippet Environment details (Python version, GPU info, etc.) Additional Resources \u00b6 Evaluation Guide : # Parameters Reference : PARAMETERS.md Examples : examples/overview.md - Working examples","title":"AlignTune Summary"},{"location":"OVERALL_SUMMARY/#aligntune-summary","text":"","title":"AlignTune Summary"},{"location":"OVERALL_SUMMARY/#backend-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Backend Support Matrix"},{"location":"OVERALL_SUMMARY/#quick-start","text":"","title":"Quick Start"},{"location":"OVERALL_SUMMARY/#supervised-fine-tuning-sft","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"gsm8k\" , backend = \"trl\" , output_dir = \"./output/sft_model\" , max_seq_length = 512 , use_unsloth = False , peft_enabled = False , batch_size = 2 , learning_rate = 1e-5 , # Data Configuration subset = \"main\" , task_type = \"instruction_following\" , column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }, # Training Configuration max_steps = 5 , num_epochs = 1 , # Logging run_name = \"sft_training_run\" ) # Start training trainer . train ()","title":"Supervised Fine-Tuning (SFT)"},{"location":"OVERALL_SUMMARY/#reinforcement-learning-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , ) # Start training trainer . train ()","title":"Reinforcement Learning Training"},{"location":"OVERALL_SUMMARY/#detailed-usage","text":"","title":"Detailed Usage"},{"location":"OVERALL_SUMMARY/#1-basic-rl-training","text":"The simplest way to start training with any RL algorithm: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , # Options: dpo, ppo, grpo, gspo, dapo, etc. backend = \"trl\" , # Options: trl, unsloth output_dir = \"./output/my_model\" , batch_size = 8 , )","title":"1. Basic RL Training"},{"location":"OVERALL_SUMMARY/#2-using-configuration-files","text":"You can organize your training settings in YAML or JSON configuration files for better reproducibility and version control:","title":"2. Using Configuration Files"},{"location":"OVERALL_SUMMARY/#yaml-configuration-example","text":"config/grpo_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" config_name : \"main\" algorithm : \"grpo\" backend : \"trl\" output_dir : \"./output/grpo_model\" batch_size : 8 max_steps : 100 learning_rate : 2e-4 max_seq_length : 512 # Column mapping for dataset column_mapping : question : \"prompt\" answer : \"response\" # Reward configuration rewards : - type : \"accuracy\" weight : 1.0 Using the config file: from aligntune.core.backend_factory import create_rl_trainer # Method 1: Pass config file path trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" ) # Method 2: Override specific parameters trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" , batch_size = 16 , # Override the config value max_steps = 200 # Override the config value ) trainer . train ()","title":"YAML Configuration Example"},{"location":"OVERALL_SUMMARY/#python-dictionary-configuration","text":"from aligntune.core.backend_factory import create_rl_trainer config = { \"model_name\" : \"Qwen/Qwen3-0.6B\" , \"dataset_name\" : \"openai/gsm8k\" , \"config_name\" : \"main\" , \"algorithm\" : \"dpo\" , \"backend\" : \"trl\" , \"output_dir\" : \"./output/dpo_model\" , \"batch_size\" : 8 , \"max_steps\" : 100 , \"learning_rate\" : 1e-5 , } # Method 1: Pass as config parameter trainer = create_rl_trainer ( config = config ) # Method 2: Unpack as kwargs trainer = create_rl_trainer ( ** config ) # Method 3: Mix config with overrides trainer = create_rl_trainer ( config = config , batch_size = 16 # Override ) trainer . train ()","title":"Python Dictionary Configuration"},{"location":"OVERALL_SUMMARY/#sft-configuration-example","text":"config/sft_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" backend : \"trl\" output_dir : \"./output/sft_model\" batch_size : 8 num_epochs : 3 learning_rate : 2e-4 max_seq_length : 512 # Dataset configuration subset : \"main\" task_type : \"instruction_following\" column_mapping : question : \"instruction\" answer : \"response\" # Training settings gradient_accumulation_steps : 4 warmup_ratio : 0.1 Using the SFT config: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( config = \"config/sft_training.yaml\" ) trainer . train ()","title":"SFT Configuration Example"},{"location":"OVERALL_SUMMARY/#3-custom-data-processing","text":"Define a custom preprocessing function to transform your data: def preprocess_function ( example ): \"\"\"Transform raw data into the required format\"\"\" return { \"prompt\" : example [ \"question\" ], \"chosen\" : example [ \"answer\" ], \"rejected\" : \"None, 0\" } trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/dpo_model\" , batch_size = 8 , processing_fn = preprocess_function , )","title":"3. Custom Data Processing"},{"location":"OVERALL_SUMMARY/#4-column-mapping","text":"Map dataset columns to expected format without writing a preprocessing function: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"google-research-datasets/mbpp\" , algorithm = \"grpo\" , column_mapping = { \"text\" : \"prompt\" , \"code\" : \"response\" }, backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , num_generations = 4 , )","title":"4. Column Mapping"},{"location":"OVERALL_SUMMARY/#5-custom-reward-functions","text":"For algorithms like GRPO, GSPO, and Neural Mirror GRPO: def my_math_reward ( text , reference = None , ** kwargs ): \"\"\" Custom reward function for math problems. Returns 1.0 if the answer contains numbers, 0.0 otherwise. \"\"\" has_numbers = any ( char . isdigit () for char in text ) return 1.0 if has_numbers else 0.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"unsloth\" , output_dir = \"./output/grpo_custom_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 1.0 , \"params\" : { \"reward_function\" : my_math_reward } } ] )","title":"5. Custom Reward Functions"},{"location":"OVERALL_SUMMARY/#multiple-reward-functions","text":"You can combine multiple reward functions with different weights: def accuracy_reward ( text , reference = None , ** kwargs ): \"\"\"Check if output matches reference\"\"\" if reference and text . strip () == reference . strip (): return 1.0 return 0.0 def length_penalty ( text , reference = None , ** kwargs ): \"\"\"Penalize overly long responses\"\"\" max_length = 200 return max ( 0.0 , 1.0 - ( len ( text ) - max_length ) / max_length ) if len ( text ) > max_length else 1.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_multi_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 0.7 , \"params\" : { \"reward_function\" : accuracy_reward } }, { \"type\" : \"custom\" , \"weight\" : 0.3 , \"params\" : { \"reward_function\" : length_penalty } } ] )","title":"Multiple Reward Functions"},{"location":"OVERALL_SUMMARY/#configuration","text":"","title":"Configuration"},{"location":"OVERALL_SUMMARY/#common-parameters","text":"Parameter Type Description Default model_name str HuggingFace model identifier Required dataset_name str HuggingFace dataset identifier Required algorithm str RL algorithm to use Required backend str Training backend: \"trl\" or \"unsloth\" \"trl\" output_dir str Directory for saving outputs Required batch_size int Training batch size 8 learning_rate float Learning rate 1e-5 max_seq_length int Maximum sequence length 512 num_epochs int Number of training epochs 1 max_steps int Maximum training steps (overrides epochs) None config str/dict Path to config file or config dictionary None","title":"Common Parameters"},{"location":"OVERALL_SUMMARY/#configuration-file-formats","text":"AlignTune supports two configuration file formats: YAML : .yaml or .yml files JSON : .json files Both formats support the same parameters and can be used interchangeably.","title":"Configuration File Formats"},{"location":"OVERALL_SUMMARY/#advanced-parameters","text":"For a complete list of available parameters, see PARAMETERS.md .","title":"Advanced Parameters"},{"location":"OVERALL_SUMMARY/#best-practices","text":"","title":"Best Practices"},{"location":"OVERALL_SUMMARY/#1-dataset-configuration","text":"When working with datasets that have multiple subsets: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , # Important: specify the subset algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/model\" , )","title":"1. Dataset Configuration"},{"location":"OVERALL_SUMMARY/#2-dataset-loading-changes","text":"Note: Newer versions of the datasets library no longer support .py scripts in the data loader. Make sure to: - Use datasets with proper configuration files - Update your dataset loading code accordingly - Use config_name parameter for dataset subsets","title":"2. Dataset Loading Changes"},{"location":"OVERALL_SUMMARY/#3-column-mapping-strategy","text":"Always map your dataset columns to match the expected prompt format: # Example: Mapping code dataset columns column_mapping = { \"text\" : \"prompt\" , # Input text/question \"code\" : \"response\" # Expected output/code } # Example: Mapping Q&A dataset columns column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }","title":"3. Column Mapping Strategy"},{"location":"OVERALL_SUMMARY/#4-configuration-file-best-practices","text":"Version Control : Keep config files in version control for reproducibility Environment-Specific Configs : Create separate configs for dev/prod environments Parameter Overrides : Use command-line/code overrides for quick experiments Documentation : Add comments to YAML configs to document parameter choices Example directory structure: project/ configs/ base_grpo.yaml # Base configuration grpo_small_model.yaml # Small model variant grpo_production.yaml # Production settings train.py evaluate.py","title":"4. Configuration File Best Practices"},{"location":"OVERALL_SUMMARY/#evaluation","text":"For unified evaluation and custom metrics, see the Evaluation Guide .","title":"Evaluation"},{"location":"OVERALL_SUMMARY/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"OVERALL_SUMMARY/#common-issues","text":"Import Errors # Ensure all dependencies are installed pip install -r requirements.txt Dataset Loading Issues Check that config_name matches your dataset's subset name Verify dataset exists on HuggingFace Hub Ensure proper column mapping Memory Issues Reduce batch_size Enable peft_enabled=True for parameter-efficient training Use gradient checkpointing Reduce max_seq_length Backend Compatibility Check the Backend Support Matrix for algorithm-backend compatibility GSPO is not currently supported with Unsloth backend Configuration File Issues Ensure YAML syntax is correct (proper indentation) Verify file path is correct when using config= parameter Check that all required parameters are present in the config","title":"Common Issues"},{"location":"OVERALL_SUMMARY/#getting-help","text":"Check Examples for working examples Review PARAMETERS.md for parameter documentation Open an issue on GitHub with: Error message Code snippet Environment details (Python version, GPU info, etc.)","title":"Getting Help"},{"location":"OVERALL_SUMMARY/#additional-resources","text":"Evaluation Guide : # Parameters Reference : PARAMETERS.md Examples : examples/overview.md - Working examples","title":"Additional Resources"},{"location":"PARAMETERS/","text":"AlignTune Configuration Parameters \u00b6 This document provides a comprehensive reference for all configuration parameters available in AlignTune, organized by training type (SFT vs RL) and algorithm-specific settings. Table of Contents \u00b6 SFT (Supervised Fine-Tuning) Parameters RL (Reinforcement Learning) Common Parameters Algorithm-Specific RL Parameters PPO (Proximal Policy Optimization) DPO (Direct Preference Optimization) GRPO (Group Relative Policy Optimization) DAPO / DRGRPO GSPO (Generalized Scoring Proximal Objective) Neural Mirror GRPO Meta-ES SFT (Supervised Fine-Tuning) Parameters \u00b6 Model Configuration ( ModelConfig ) \u00b6 Parameter Type Default Description name_or_path str Required HuggingFace model name or local path precision PrecisionType bf16 Model precision ( bf16 , fp16 , fp32 , auto ) quantization Dict {} Quantization config (e.g., {\"load_in_4bit\": True} ) attn_implementation str \"auto\" Attention implementation ( auto , flash_attention_2 , sdpa ) gradient_checkpointing bool True Enable gradient checkpointing for memory efficiency max_memory Dict None Max memory per device (e.g., {\"0\": \"20GB\"} ) device_map str/Dict \"auto\" Device mapping strategy use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length peft_enabled bool False Enable PEFT/LoRA lora_rank int 16 LoRA rank lora_alpha int 32 LoRA alpha scaling lora_dropout float 0.1 LoRA dropout rate target_modules List[str] None LoRA target modules (e.g., [\"q_proj\", \"v_proj\"] ) bias str \"none\" Bias training ( none , all , lora_only ) use_gradient_checkpointing bool True Enable gradient checkpointing (legacy) num_labels int None Number of labels (classification tasks) model_init_kwargs Dict {} Additional model initialization arguments Dataset Configuration ( DatasetConfig ) \u00b6 Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use subset / config str None Dataset subset/config name percent float None Percentage of data to use (0-100) max_samples int None Maximum number of samples column_mapping Dict {} Column name mappings task_type TaskType SUPERVISED_FINE_TUNING Task type enum auto_detect_fields bool False Auto-detect dataset fields format_type str None Dataset format ( alpaca , dolly , etc.) text_column str \"text\" Text column name instruction_column str \"instruction\" Instruction column name response_column str \"response\" Response column name output_column str \"output\" Output column name input_column str \"input\" Input column name context_column str \"context\" Context column name label_column str \"label\" Label column (classification) tokens_column str \"tokens\" Tokens column (token classification) tags_column str \"ner_tags\" Tags column (NER) messages_column str \"messages\" Messages column (chat) dataset_text_field str \"text\" Dataset text field for trainers chat_template str None Chat template string dataset_num_proc int None Number of parallel processes pad_token str None Padding token preserve_columns List[str] None Columns to preserve during processing processing_fn Callable None Custom processing function processing_batched bool False Whether processing is batched processing_fn_kwargs Dict {} Arguments for processing function Training Configuration ( TrainingConfig ) \u00b6 Parameter Type Default Description per_device_batch_size int 1 Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int 3 Number of training epochs (if max_steps not set) learning_rate float 1e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 0 Warmup steps (or calculated from warmup_ratio) warmup_ratio float 0.1 Warmup ratio of total steps eval_interval int 100 Evaluation interval (steps) save_interval int 500 Save checkpoint interval (steps) max_grad_norm float 1.0 Maximum gradient norm for clipping fp16 bool False Use FP16 training bf16 bool False Use BF16 training dataloader_num_workers int 0 Number of dataloader workers remove_unused_columns bool False Remove unused dataset columns optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" Learning rate scheduler group_by_length bool False Group sequences by length dataloader_drop_last bool False Drop last incomplete batch eval_accumulation_steps int None Evaluation accumulation steps label_smoothing_factor float 0.0 Label smoothing factor early_stopping_patience int None Early stopping patience early_stopping_threshold float 0.0 Early stopping threshold load_best_model_at_end bool True Load best model at end metric_for_best_model str \"eval_loss\" Metric for best model selection greater_is_better bool False Whether higher metric is better use_trl bool False Use TRL backend dataset_num_proc int None Dataset processing processes dataset_kwargs Dict {} Additional dataset arguments packing bool False Enable sequence packing packing_strategy str \"bfd\" Packing strategy ( bfd , wrapped ) eval_packing bool None Enable packing for evaluation padding_free bool False Padding-free training pad_to_multiple_of int None Pad sequences to multiple of N completion_only_loss bool None Compute loss only on completions assistant_only_loss bool False Compute loss only on assistant turns loss_type str \"nll\" Loss type ( nll , dft ) activation_offloading bool False Enable activation offloading use_flash_attention_2 bool None Use Flash Attention 2 gradient_checkpointing bool False Enable gradient checkpointing gradient_checkpointing_kwargs Dict {} Gradient checkpointing arguments Evaluation Configuration ( EvaluationConfig ) \u00b6 Parameter Type Default Description compute_perplexity bool True Compute perplexity metric compute_rouge bool True Compute ROUGE scores compute_bleu bool True Compute BLEU scores compute_meteor bool False Compute METEOR scores (requires nltk) compute_bertscore bool False Compute BERTScore (requires bert-score) compute_semantic_similarity bool False Compute semantic similarity compute_codebleu bool False Compute CodeBLEU (for code tasks) custom_metrics List[Callable] None Custom metric functions max_samples_for_quality_metrics int 50 Max samples for quality metrics bertscore_model str \"microsoft/deberta-xlarge-mnli\" Model for BERTScore semantic_similarity_model str \"sentence-transformers/all-MiniLM-L6-v2\" Model for semantic similarity Logging Configuration ( LoggingConfig ) \u00b6 Parameter Type Default Description output_dir str \"./output\" Output directory run_name str None Run name for logging loggers List[str] [\"tensorboard\"] Logger types ( tensorboard , wandb ) log_level str \"INFO\" Logging level log_interval int 10 Logging interval (steps) save_strategy str \"steps\" Save strategy eval_strategy str \"steps\" Evaluation strategy report_to str \"none\" Reporting destination RL (Reinforcement Learning) Common Parameters \u00b6 Model Configuration ( ModelConfig ) \u00b6 Parameter Type Default Description name_or_path str Required Policy model name or path sft_path str None SFT checkpoint path reward_path str None Reward model path reward_model_name str None Separate reward model name reward_model_source RewardModelSourceConfig None Reward model source configuration precision PrecisionType AUTO Model precision quantization Dict {} Quantization config attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool False Enable gradient checkpointing max_memory Dict None Max memory per device device_map str/Dict None Device mapping use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length reward_value_model str \"meta-llama/Llama-3.2-1B-Instruct\" Reward/value model name reward_value_loading_type str None Loading type ( unsloth , standard ) reward_model_quantization Dict {} Reward model quantization value_model_quantization Dict {} Value model quantization use_peft bool True Enable PEFT/LoRA lora_r int 16 LoRA rank lora_alpha int 32 LoRA alpha lora_dropout float 0.05 LoRA dropout lora_target_modules List[str] [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] LoRA target modules trust_remote_code bool True Trust remote code model_init_kwargs Dict {} Model initialization arguments ref_model_init_kwargs Dict {} Reference model init arguments model_adapter_name str None Model adapter name ref_adapter_name str None Reference adapter name force_use_ref_model bool False Force use of reference model disable_dropout bool True Disable dropout use_logits_to_keep bool False Use logits_to_keep optimization reward_device str \"auto\" Reward model device ( auto , cpu , cuda ) Dataset Configuration ( DatasetConfig ) \u00b6 Parameter Type Default Description name str Required Dataset name or path split str \"train\" Dataset split percent float None Percentage of data (0-100) max_samples int None Maximum samples field_mappings Dict {} Field name mappings format_type str None Dataset format type auto_detect_fields bool True Auto-detect fields column_mapping Dict {} Column mappings (legacy) task_type str \"conversation\" Task type weight float 1.0 Dataset weight chat_template str None Chat template system_prompt str None System prompt dataset_num_proc int None Processing processes pad_token str None Padding token label_pad_token_id int -100 Label padding token ID truncation_mode str \"keep_end\" Truncation mode padding_free bool False Padding-free training precompute_ref_log_probs bool False Precompute reference log probs precompute_ref_batch_size int None Batch size for precomputation tools Any None Tools configuration preserve_columns List[str] None Columns to preserve processing_fn Callable None Custom processing function processing_batched bool False Batched processing processing_fn_kwargs Dict {} Processing function arguments config_name str None Dataset config name Training Configuration ( TrainingConfig ) \u00b6 Parameter Type Default Description per_device_batch_size int 1 Batch size per device per_device_eval_batch_size int 1 Eval batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int None Number of epochs eval_interval int 100 Evaluation interval save_interval int 500 Save interval learning_rate float 1e-5 Learning rate max_grad_norm float 1.0 Max gradient norm weight_decay float 0.01 Weight decay optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" LR scheduler type warmup_steps int 0 Warmup steps warmup_ratio float 0.0 Warmup ratio rollout_batch_size int 1 Rollout batch size kl_coef float 0.1 KL divergence coefficient cliprange float 0.2 PPO clip range cliprange_value float 0.2 Value function clip range num_ppo_epochs int None PPO epochs per batch temperature float 0.6 Sampling temperature whiten_rewards bool False Whiten rewards kl_estimator str \"k1\" KL estimator type vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda response_length int 128 Response length stop_token str \"eos\" Stop token missing_eos_penalty float 1.0 Missing EOS penalty ds3_gather_for_generation bool True DeepSpeed Stage 3 gather generation_kwargs Dict None Generation arguments max_length int 1024 Max sequence length max_prompt_length int 512 Max prompt length max_target_length int None Max target length max_completion_length int 256 Max completion length top_p float 0.95 Nucleus sampling parameter padding_free bool False Padding-free training truncation_mode str \"keep_end\" Truncation mode beta float 0.1 DPO beta parameter loss_type str None Loss type loss_weights Dict None Loss weights f_divergence_type str \"reverse_kl\" F-divergence type f_alpha_divergence_coef float 1.0 Alpha divergence coefficient reference_free bool False Reference-free training label_smoothing float 0.0 Label smoothing use_weighting bool False Use importance weighting rpo_alpha float None RPO alpha parameter ld_alpha float None LD alpha parameter discopop_tau float 0.05 DiscoPOP tau parameter sync_ref_model bool False Sync reference model ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Reference sync steps grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter eval_steps int 100 Evaluation steps eval_strategy str \"no\" Evaluation strategy save_steps int 500 Save steps save_strategy str \"steps\" Save strategy save_total_limit int None Max checkpoints to keep load_best_model_at_end bool False Load best model at end metric_for_best_model str None Best model metric greater_is_better bool False Whether higher is better logging_steps int 10 Logging steps logging_strategy str \"steps\" Logging strategy num_generations int None Number of generations per prompt mask_truncated_completions bool True Mask truncated completions scale_rewards str \"group\" Reward scaling ( group , batch ) reward_weights List[float] None Reward function weights enable_thinking bool False Enable Qwen3 thinking mode fast_inference bool False Enable Unsloth vLLM (faster) vllm_gpu_memory_utilization float 0.7 vLLM GPU memory (0.95 for max) seed int 42 Random seed data_seed int 47 Data seed use_liger_kernel bool False Use Liger kernel use_liger_loss bool None Use Liger loss gradient_checkpointing_kwargs Dict {\"use_reentrant\": False} Gradient checkpointing args group_by_length bool True Group sequences by length Sample Logging Configuration ( SampleLoggingConfig ) \u00b6 Parameter Type Default Description enabled bool False Enable sample logging prompts List[str] None Prompts for sample generation interval_steps int None Steps between samples percent_of_max_steps float None Percent of max steps (0-1) max_new_tokens int 80 Max tokens to generate temperature float 0.6 Generation temperature top_p float 0.9 Nucleus sampling parameter num_samples int 3 Number of samples per prompt Logging Configuration ( LoggingConfig ) \u00b6 Parameter Type Default Description loggers List[str] [\"tensorboard\"] Logger types run_name str None Run name output_dir str \"./output\" Output directory log_level str \"INFO\" Logging level sample_logging SampleLoggingConfig See above Sample logging config report_to str \"none\" Reporting destination Reward Configuration ( RewardConfig ) \u00b6 Parameter Type Default Description type str Required Reward function type weight float 1.0 Reward weight params Dict {} Reward function parameters shield bool False Enable safety shield clip float None Clip reward values normalize bool False Normalize rewards Reward Model Training Configuration ( RewardModelTrainingConfig ) \u00b6 Parameter Type Default Description base_model_name str Required Base model for reward training training_texts List[str] Required Training texts (min 10) reward_functions List[str] Required Reward functions to use output_dir str Required Output directory reference_texts List[str] None Reference texts (optional) reward_weights List[float] None Reward function weights num_epochs int 3 Training epochs learning_rate float 1e-5 Learning rate batch_size int 8 Batch size gradient_accumulation_steps int 4 Gradient accumulation max_length int 512 Max sequence length Reward Model Source Configuration ( RewardModelSourceConfig ) \u00b6 Parameter Type Default Description source_type str Required Source type ( pretrained_hf , pretrained_local , custom_trained ) model_name str None HuggingFace model name (for pretrained_hf ) model_path str None Local model path (for pretrained_local ) training_config RewardModelTrainingConfig None Training config (for custom_trained ) fine_tune_with_rewards bool False Fine-tune pretrained with reward functions Algorithm-Specific RL Parameters \u00b6 PPO (Proximal Policy Optimization) \u00b6 PPO uses the common RL parameters plus: Parameter Type Default Description num_ppo_epochs int 4 Number of PPO epochs per batch cliprange float 0.2 PPO policy clip range cliprange_value float 0.2 PPO value function clip range vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor (GAE) lam float 0.95 GAE lambda parameter kl_coef float 0.1 KL penalty coefficient kl_estimator str \"k1\" KL estimator ( k1 , k2 , k3 ) whiten_rewards bool False Whiten advantages/rewards Note : For optimal performance, ensure policy_model, reward_model, and value_model are from the same model family. DPO (Direct Preference Optimization) \u00b6 DPO uses the common RL parameters plus: Parameter Type Default Description beta float 0.1 DPO beta (inverse temperature) loss_type str \"sigmoid\" Loss type ( sigmoid , hinge , ipo , kto ) label_smoothing float 0.0 Label smoothing factor reference_free bool False Reference-free DPO precompute_ref_log_probs bool False Precompute reference log probs sync_ref_model bool False Sync reference model periodically ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Steps between reference syncs DPO Evaluation Parameters: Parameter Type Default Description dpo_eval_enabled bool False Enable DPO evaluation dpo_eval_max_samples int None Max samples for eval dpo_zero_shot_max_samples int 50 Zero-shot eval samples dpo_few_shot_max_samples int 30 Few-shot eval samples dpo_few_shot_examples_text str None Few-shot examples GRPO (Group Relative Policy Optimization) \u00b6 GRPO uses the common RL parameters plus: Parameter Type Default Description grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling ( group , batch ) kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range DAPO / DRGRPO \u00b6 DAPO (Direct Advantage Policy Optimization) and DRGRPO (Distributional Robust GRPO) share identical parameters with GRPO: Parameter Type Default Description grpo_alpha float 0.1 Alpha parameter grpo_beta float 0.1 Beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range GSPO (Generalized Scoring Proximal Objective) \u00b6 GSPO uses scoring-based policy optimization: Note : GSPO is only supported by TRL backend, not Unsloth. Parameter Type Default Description gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling strategy kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range Neural Mirror GRPO \u00b6 Neural Mirror GRPO combines GRPO with neural mirror descent for improved optimization: Base GRPO Parameters plus neural mirror-specific optimizations. Uses standard GRPO parameters with enhanced optimization through mirror descent. No additional unique parameters. Meta-ES \u00b6 Meta-ES uses evolution strategies for meta-learning the reward function: Parameter Type Default Description meta_iterations int 15 Number of meta-iterations patience int 5 Early stopping patience min_delta float 0.001 Minimum improvement for early stopping init_scale float 0.01 Initial parameter scale N int 10 Population size (must be even) T int 100 Training steps per evaluation sigma float 0.01 ES noise standard deviation sigma_decay float 0.99 ES sigma decay per iteration alpha float 0.01 ES learning rate mirror_coefficient float 0.0001 Mirror descent coefficient debug_mode bool False Enable debug logging eval_timeout int 5 Evaluation timeout (seconds) eval_max_tokens int 512 Max tokens for evaluation eval_k int 1 Top-k sampling for evaluation eval_temperature float 0.8 Temperature for evaluation num_workers int 1 Number of parallel workers no_wandb bool False Disable Weights & Biases logging wandb_project str \"neural-mirror-es\" W&B project name resume str None Path to checkpoint for resuming Usage Notes: - Meta-ES optimizes the reward function itself through evolution - Population size N must be even for symmetric perturbations - T controls how long each candidate is trained before evaluation - sigma controls exploration; decay helps convergence - Suitable for scenarios where reward design is critical Backend Selection \u00b6 AlignTune supports two backends with automatic fallback: Backend Description Availability TRL HuggingFace Transformers RL library Standard, widely supported Unsloth Optimized training with memory efficiency Requires Unsloth installation Backend Priority: - SFT: Unsloth \u2192 TRL - RL: Unsloth \u2192 TRL Setting Backend: # Auto-select best available trainer = create_sft_trainer ( ... , backend = \"auto\" ) # Explicit selection trainer = create_sft_trainer ( ... , backend = \"trl\" ) trainer = create_rl_trainer ( ... , backend = \"unsloth\" , algorithm = \"grpo\" ) Note : When TRL is selected, Unsloth is disabled to prevent interference. GSPO algorithm is TRL-only. Task Types (SFT) \u00b6 Task Type Description INSTRUCTION_FOLLOWING Instruction-response pairs SUPERVISED_FINE_TUNING General supervised training TEXT_CLASSIFICATION Text classification tasks TOKEN_CLASSIFICATION Token-level classification (NER) TEXT_GENERATION General text generation CHAT_COMPLETION Multi-turn chat completion Convenience Functions \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" ) Notes \u00b6 All parameters with Required must be explicitly provided Parameters with defaults can be omitted to use default values Enum parameters accept both enum values and strings (e.g., \"bf16\" or PrecisionType.BF16 ) For classification tasks, num_labels must be set in ModelConfig Reward model configuration supports three modes: pretrained HF, pretrained local, or custom trained DPO evaluation is optional and controlled by dpo_eval_enabled Sample logging is optional for monitoring generation quality during training Additional Resources \u00b6 FinetuneHub Documentation Backend Comparison Guide Algorithm Selection Guide","title":"AlignTune Configuration Parameters"},{"location":"PARAMETERS/#aligntune-configuration-parameters","text":"This document provides a comprehensive reference for all configuration parameters available in AlignTune, organized by training type (SFT vs RL) and algorithm-specific settings.","title":"AlignTune Configuration Parameters"},{"location":"PARAMETERS/#table-of-contents","text":"SFT (Supervised Fine-Tuning) Parameters RL (Reinforcement Learning) Common Parameters Algorithm-Specific RL Parameters PPO (Proximal Policy Optimization) DPO (Direct Preference Optimization) GRPO (Group Relative Policy Optimization) DAPO / DRGRPO GSPO (Generalized Scoring Proximal Objective) Neural Mirror GRPO Meta-ES","title":"Table of Contents"},{"location":"PARAMETERS/#sft-supervised-fine-tuning-parameters","text":"","title":"SFT (Supervised Fine-Tuning) Parameters"},{"location":"PARAMETERS/#model-configuration-modelconfig","text":"Parameter Type Default Description name_or_path str Required HuggingFace model name or local path precision PrecisionType bf16 Model precision ( bf16 , fp16 , fp32 , auto ) quantization Dict {} Quantization config (e.g., {\"load_in_4bit\": True} ) attn_implementation str \"auto\" Attention implementation ( auto , flash_attention_2 , sdpa ) gradient_checkpointing bool True Enable gradient checkpointing for memory efficiency max_memory Dict None Max memory per device (e.g., {\"0\": \"20GB\"} ) device_map str/Dict \"auto\" Device mapping strategy use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length peft_enabled bool False Enable PEFT/LoRA lora_rank int 16 LoRA rank lora_alpha int 32 LoRA alpha scaling lora_dropout float 0.1 LoRA dropout rate target_modules List[str] None LoRA target modules (e.g., [\"q_proj\", \"v_proj\"] ) bias str \"none\" Bias training ( none , all , lora_only ) use_gradient_checkpointing bool True Enable gradient checkpointing (legacy) num_labels int None Number of labels (classification tasks) model_init_kwargs Dict {} Additional model initialization arguments","title":"Model Configuration (ModelConfig)"},{"location":"PARAMETERS/#dataset-configuration-datasetconfig","text":"Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use subset / config str None Dataset subset/config name percent float None Percentage of data to use (0-100) max_samples int None Maximum number of samples column_mapping Dict {} Column name mappings task_type TaskType SUPERVISED_FINE_TUNING Task type enum auto_detect_fields bool False Auto-detect dataset fields format_type str None Dataset format ( alpaca , dolly , etc.) text_column str \"text\" Text column name instruction_column str \"instruction\" Instruction column name response_column str \"response\" Response column name output_column str \"output\" Output column name input_column str \"input\" Input column name context_column str \"context\" Context column name label_column str \"label\" Label column (classification) tokens_column str \"tokens\" Tokens column (token classification) tags_column str \"ner_tags\" Tags column (NER) messages_column str \"messages\" Messages column (chat) dataset_text_field str \"text\" Dataset text field for trainers chat_template str None Chat template string dataset_num_proc int None Number of parallel processes pad_token str None Padding token preserve_columns List[str] None Columns to preserve during processing processing_fn Callable None Custom processing function processing_batched bool False Whether processing is batched processing_fn_kwargs Dict {} Arguments for processing function","title":"Dataset Configuration (DatasetConfig)"},{"location":"PARAMETERS/#training-configuration-trainingconfig","text":"Parameter Type Default Description per_device_batch_size int 1 Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int 3 Number of training epochs (if max_steps not set) learning_rate float 1e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 0 Warmup steps (or calculated from warmup_ratio) warmup_ratio float 0.1 Warmup ratio of total steps eval_interval int 100 Evaluation interval (steps) save_interval int 500 Save checkpoint interval (steps) max_grad_norm float 1.0 Maximum gradient norm for clipping fp16 bool False Use FP16 training bf16 bool False Use BF16 training dataloader_num_workers int 0 Number of dataloader workers remove_unused_columns bool False Remove unused dataset columns optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" Learning rate scheduler group_by_length bool False Group sequences by length dataloader_drop_last bool False Drop last incomplete batch eval_accumulation_steps int None Evaluation accumulation steps label_smoothing_factor float 0.0 Label smoothing factor early_stopping_patience int None Early stopping patience early_stopping_threshold float 0.0 Early stopping threshold load_best_model_at_end bool True Load best model at end metric_for_best_model str \"eval_loss\" Metric for best model selection greater_is_better bool False Whether higher metric is better use_trl bool False Use TRL backend dataset_num_proc int None Dataset processing processes dataset_kwargs Dict {} Additional dataset arguments packing bool False Enable sequence packing packing_strategy str \"bfd\" Packing strategy ( bfd , wrapped ) eval_packing bool None Enable packing for evaluation padding_free bool False Padding-free training pad_to_multiple_of int None Pad sequences to multiple of N completion_only_loss bool None Compute loss only on completions assistant_only_loss bool False Compute loss only on assistant turns loss_type str \"nll\" Loss type ( nll , dft ) activation_offloading bool False Enable activation offloading use_flash_attention_2 bool None Use Flash Attention 2 gradient_checkpointing bool False Enable gradient checkpointing gradient_checkpointing_kwargs Dict {} Gradient checkpointing arguments","title":"Training Configuration (TrainingConfig)"},{"location":"PARAMETERS/#evaluation-configuration-evaluationconfig","text":"Parameter Type Default Description compute_perplexity bool True Compute perplexity metric compute_rouge bool True Compute ROUGE scores compute_bleu bool True Compute BLEU scores compute_meteor bool False Compute METEOR scores (requires nltk) compute_bertscore bool False Compute BERTScore (requires bert-score) compute_semantic_similarity bool False Compute semantic similarity compute_codebleu bool False Compute CodeBLEU (for code tasks) custom_metrics List[Callable] None Custom metric functions max_samples_for_quality_metrics int 50 Max samples for quality metrics bertscore_model str \"microsoft/deberta-xlarge-mnli\" Model for BERTScore semantic_similarity_model str \"sentence-transformers/all-MiniLM-L6-v2\" Model for semantic similarity","title":"Evaluation Configuration (EvaluationConfig)"},{"location":"PARAMETERS/#logging-configuration-loggingconfig","text":"Parameter Type Default Description output_dir str \"./output\" Output directory run_name str None Run name for logging loggers List[str] [\"tensorboard\"] Logger types ( tensorboard , wandb ) log_level str \"INFO\" Logging level log_interval int 10 Logging interval (steps) save_strategy str \"steps\" Save strategy eval_strategy str \"steps\" Evaluation strategy report_to str \"none\" Reporting destination","title":"Logging Configuration (LoggingConfig)"},{"location":"PARAMETERS/#rl-reinforcement-learning-common-parameters","text":"","title":"RL (Reinforcement Learning) Common Parameters"},{"location":"PARAMETERS/#model-configuration-modelconfig_1","text":"Parameter Type Default Description name_or_path str Required Policy model name or path sft_path str None SFT checkpoint path reward_path str None Reward model path reward_model_name str None Separate reward model name reward_model_source RewardModelSourceConfig None Reward model source configuration precision PrecisionType AUTO Model precision quantization Dict {} Quantization config attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool False Enable gradient checkpointing max_memory Dict None Max memory per device device_map str/Dict None Device mapping use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length reward_value_model str \"meta-llama/Llama-3.2-1B-Instruct\" Reward/value model name reward_value_loading_type str None Loading type ( unsloth , standard ) reward_model_quantization Dict {} Reward model quantization value_model_quantization Dict {} Value model quantization use_peft bool True Enable PEFT/LoRA lora_r int 16 LoRA rank lora_alpha int 32 LoRA alpha lora_dropout float 0.05 LoRA dropout lora_target_modules List[str] [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] LoRA target modules trust_remote_code bool True Trust remote code model_init_kwargs Dict {} Model initialization arguments ref_model_init_kwargs Dict {} Reference model init arguments model_adapter_name str None Model adapter name ref_adapter_name str None Reference adapter name force_use_ref_model bool False Force use of reference model disable_dropout bool True Disable dropout use_logits_to_keep bool False Use logits_to_keep optimization reward_device str \"auto\" Reward model device ( auto , cpu , cuda )","title":"Model Configuration (ModelConfig)"},{"location":"PARAMETERS/#dataset-configuration-datasetconfig_1","text":"Parameter Type Default Description name str Required Dataset name or path split str \"train\" Dataset split percent float None Percentage of data (0-100) max_samples int None Maximum samples field_mappings Dict {} Field name mappings format_type str None Dataset format type auto_detect_fields bool True Auto-detect fields column_mapping Dict {} Column mappings (legacy) task_type str \"conversation\" Task type weight float 1.0 Dataset weight chat_template str None Chat template system_prompt str None System prompt dataset_num_proc int None Processing processes pad_token str None Padding token label_pad_token_id int -100 Label padding token ID truncation_mode str \"keep_end\" Truncation mode padding_free bool False Padding-free training precompute_ref_log_probs bool False Precompute reference log probs precompute_ref_batch_size int None Batch size for precomputation tools Any None Tools configuration preserve_columns List[str] None Columns to preserve processing_fn Callable None Custom processing function processing_batched bool False Batched processing processing_fn_kwargs Dict {} Processing function arguments config_name str None Dataset config name","title":"Dataset Configuration (DatasetConfig)"},{"location":"PARAMETERS/#training-configuration-trainingconfig_1","text":"Parameter Type Default Description per_device_batch_size int 1 Batch size per device per_device_eval_batch_size int 1 Eval batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int None Number of epochs eval_interval int 100 Evaluation interval save_interval int 500 Save interval learning_rate float 1e-5 Learning rate max_grad_norm float 1.0 Max gradient norm weight_decay float 0.01 Weight decay optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" LR scheduler type warmup_steps int 0 Warmup steps warmup_ratio float 0.0 Warmup ratio rollout_batch_size int 1 Rollout batch size kl_coef float 0.1 KL divergence coefficient cliprange float 0.2 PPO clip range cliprange_value float 0.2 Value function clip range num_ppo_epochs int None PPO epochs per batch temperature float 0.6 Sampling temperature whiten_rewards bool False Whiten rewards kl_estimator str \"k1\" KL estimator type vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda response_length int 128 Response length stop_token str \"eos\" Stop token missing_eos_penalty float 1.0 Missing EOS penalty ds3_gather_for_generation bool True DeepSpeed Stage 3 gather generation_kwargs Dict None Generation arguments max_length int 1024 Max sequence length max_prompt_length int 512 Max prompt length max_target_length int None Max target length max_completion_length int 256 Max completion length top_p float 0.95 Nucleus sampling parameter padding_free bool False Padding-free training truncation_mode str \"keep_end\" Truncation mode beta float 0.1 DPO beta parameter loss_type str None Loss type loss_weights Dict None Loss weights f_divergence_type str \"reverse_kl\" F-divergence type f_alpha_divergence_coef float 1.0 Alpha divergence coefficient reference_free bool False Reference-free training label_smoothing float 0.0 Label smoothing use_weighting bool False Use importance weighting rpo_alpha float None RPO alpha parameter ld_alpha float None LD alpha parameter discopop_tau float 0.05 DiscoPOP tau parameter sync_ref_model bool False Sync reference model ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Reference sync steps grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter eval_steps int 100 Evaluation steps eval_strategy str \"no\" Evaluation strategy save_steps int 500 Save steps save_strategy str \"steps\" Save strategy save_total_limit int None Max checkpoints to keep load_best_model_at_end bool False Load best model at end metric_for_best_model str None Best model metric greater_is_better bool False Whether higher is better logging_steps int 10 Logging steps logging_strategy str \"steps\" Logging strategy num_generations int None Number of generations per prompt mask_truncated_completions bool True Mask truncated completions scale_rewards str \"group\" Reward scaling ( group , batch ) reward_weights List[float] None Reward function weights enable_thinking bool False Enable Qwen3 thinking mode fast_inference bool False Enable Unsloth vLLM (faster) vllm_gpu_memory_utilization float 0.7 vLLM GPU memory (0.95 for max) seed int 42 Random seed data_seed int 47 Data seed use_liger_kernel bool False Use Liger kernel use_liger_loss bool None Use Liger loss gradient_checkpointing_kwargs Dict {\"use_reentrant\": False} Gradient checkpointing args group_by_length bool True Group sequences by length","title":"Training Configuration (TrainingConfig)"},{"location":"PARAMETERS/#sample-logging-configuration-sampleloggingconfig","text":"Parameter Type Default Description enabled bool False Enable sample logging prompts List[str] None Prompts for sample generation interval_steps int None Steps between samples percent_of_max_steps float None Percent of max steps (0-1) max_new_tokens int 80 Max tokens to generate temperature float 0.6 Generation temperature top_p float 0.9 Nucleus sampling parameter num_samples int 3 Number of samples per prompt","title":"Sample Logging Configuration (SampleLoggingConfig)"},{"location":"PARAMETERS/#logging-configuration-loggingconfig_1","text":"Parameter Type Default Description loggers List[str] [\"tensorboard\"] Logger types run_name str None Run name output_dir str \"./output\" Output directory log_level str \"INFO\" Logging level sample_logging SampleLoggingConfig See above Sample logging config report_to str \"none\" Reporting destination","title":"Logging Configuration (LoggingConfig)"},{"location":"PARAMETERS/#reward-configuration-rewardconfig","text":"Parameter Type Default Description type str Required Reward function type weight float 1.0 Reward weight params Dict {} Reward function parameters shield bool False Enable safety shield clip float None Clip reward values normalize bool False Normalize rewards","title":"Reward Configuration (RewardConfig)"},{"location":"PARAMETERS/#reward-model-training-configuration-rewardmodeltrainingconfig","text":"Parameter Type Default Description base_model_name str Required Base model for reward training training_texts List[str] Required Training texts (min 10) reward_functions List[str] Required Reward functions to use output_dir str Required Output directory reference_texts List[str] None Reference texts (optional) reward_weights List[float] None Reward function weights num_epochs int 3 Training epochs learning_rate float 1e-5 Learning rate batch_size int 8 Batch size gradient_accumulation_steps int 4 Gradient accumulation max_length int 512 Max sequence length","title":"Reward Model Training Configuration (RewardModelTrainingConfig)"},{"location":"PARAMETERS/#reward-model-source-configuration-rewardmodelsourceconfig","text":"Parameter Type Default Description source_type str Required Source type ( pretrained_hf , pretrained_local , custom_trained ) model_name str None HuggingFace model name (for pretrained_hf ) model_path str None Local model path (for pretrained_local ) training_config RewardModelTrainingConfig None Training config (for custom_trained ) fine_tune_with_rewards bool False Fine-tune pretrained with reward functions","title":"Reward Model Source Configuration (RewardModelSourceConfig)"},{"location":"PARAMETERS/#algorithm-specific-rl-parameters","text":"","title":"Algorithm-Specific RL Parameters"},{"location":"PARAMETERS/#ppo-proximal-policy-optimization","text":"PPO uses the common RL parameters plus: Parameter Type Default Description num_ppo_epochs int 4 Number of PPO epochs per batch cliprange float 0.2 PPO policy clip range cliprange_value float 0.2 PPO value function clip range vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor (GAE) lam float 0.95 GAE lambda parameter kl_coef float 0.1 KL penalty coefficient kl_estimator str \"k1\" KL estimator ( k1 , k2 , k3 ) whiten_rewards bool False Whiten advantages/rewards Note : For optimal performance, ensure policy_model, reward_model, and value_model are from the same model family.","title":"PPO (Proximal Policy Optimization)"},{"location":"PARAMETERS/#dpo-direct-preference-optimization","text":"DPO uses the common RL parameters plus: Parameter Type Default Description beta float 0.1 DPO beta (inverse temperature) loss_type str \"sigmoid\" Loss type ( sigmoid , hinge , ipo , kto ) label_smoothing float 0.0 Label smoothing factor reference_free bool False Reference-free DPO precompute_ref_log_probs bool False Precompute reference log probs sync_ref_model bool False Sync reference model periodically ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Steps between reference syncs DPO Evaluation Parameters: Parameter Type Default Description dpo_eval_enabled bool False Enable DPO evaluation dpo_eval_max_samples int None Max samples for eval dpo_zero_shot_max_samples int 50 Zero-shot eval samples dpo_few_shot_max_samples int 30 Few-shot eval samples dpo_few_shot_examples_text str None Few-shot examples","title":"DPO (Direct Preference Optimization)"},{"location":"PARAMETERS/#grpo-group-relative-policy-optimization","text":"GRPO uses the common RL parameters plus: Parameter Type Default Description grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling ( group , batch ) kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"GRPO (Group Relative Policy Optimization)"},{"location":"PARAMETERS/#dapo-drgrpo","text":"DAPO (Direct Advantage Policy Optimization) and DRGRPO (Distributional Robust GRPO) share identical parameters with GRPO: Parameter Type Default Description grpo_alpha float 0.1 Alpha parameter grpo_beta float 0.1 Beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"DAPO / DRGRPO"},{"location":"PARAMETERS/#gspo-generalized-scoring-proximal-objective","text":"GSPO uses scoring-based policy optimization: Note : GSPO is only supported by TRL backend, not Unsloth. Parameter Type Default Description gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling strategy kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"GSPO (Generalized Scoring Proximal Objective)"},{"location":"PARAMETERS/#neural-mirror-grpo","text":"Neural Mirror GRPO combines GRPO with neural mirror descent for improved optimization: Base GRPO Parameters plus neural mirror-specific optimizations. Uses standard GRPO parameters with enhanced optimization through mirror descent. No additional unique parameters.","title":"Neural Mirror GRPO"},{"location":"PARAMETERS/#meta-es","text":"Meta-ES uses evolution strategies for meta-learning the reward function: Parameter Type Default Description meta_iterations int 15 Number of meta-iterations patience int 5 Early stopping patience min_delta float 0.001 Minimum improvement for early stopping init_scale float 0.01 Initial parameter scale N int 10 Population size (must be even) T int 100 Training steps per evaluation sigma float 0.01 ES noise standard deviation sigma_decay float 0.99 ES sigma decay per iteration alpha float 0.01 ES learning rate mirror_coefficient float 0.0001 Mirror descent coefficient debug_mode bool False Enable debug logging eval_timeout int 5 Evaluation timeout (seconds) eval_max_tokens int 512 Max tokens for evaluation eval_k int 1 Top-k sampling for evaluation eval_temperature float 0.8 Temperature for evaluation num_workers int 1 Number of parallel workers no_wandb bool False Disable Weights & Biases logging wandb_project str \"neural-mirror-es\" W&B project name resume str None Path to checkpoint for resuming Usage Notes: - Meta-ES optimizes the reward function itself through evolution - Population size N must be even for symmetric perturbations - T controls how long each candidate is trained before evaluation - sigma controls exploration; decay helps convergence - Suitable for scenarios where reward design is critical","title":"Meta-ES"},{"location":"PARAMETERS/#backend-selection","text":"AlignTune supports two backends with automatic fallback: Backend Description Availability TRL HuggingFace Transformers RL library Standard, widely supported Unsloth Optimized training with memory efficiency Requires Unsloth installation Backend Priority: - SFT: Unsloth \u2192 TRL - RL: Unsloth \u2192 TRL Setting Backend: # Auto-select best available trainer = create_sft_trainer ( ... , backend = \"auto\" ) # Explicit selection trainer = create_sft_trainer ( ... , backend = \"trl\" ) trainer = create_rl_trainer ( ... , backend = \"unsloth\" , algorithm = \"grpo\" ) Note : When TRL is selected, Unsloth is disabled to prevent interference. GSPO algorithm is TRL-only.","title":"Backend Selection"},{"location":"PARAMETERS/#task-types-sft","text":"Task Type Description INSTRUCTION_FOLLOWING Instruction-response pairs SUPERVISED_FINE_TUNING General supervised training TEXT_CLASSIFICATION Text classification tasks TOKEN_CLASSIFICATION Token-level classification (NER) TEXT_GENERATION General text generation CHAT_COMPLETION Multi-turn chat completion","title":"Task Types (SFT)"},{"location":"PARAMETERS/#convenience-functions","text":"","title":"Convenience Functions"},{"location":"PARAMETERS/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 )","title":"SFT Training"},{"location":"PARAMETERS/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" )","title":"RL Training"},{"location":"PARAMETERS/#notes","text":"All parameters with Required must be explicitly provided Parameters with defaults can be omitted to use default values Enum parameters accept both enum values and strings (e.g., \"bf16\" or PrecisionType.BF16 ) For classification tasks, num_labels must be set in ModelConfig Reward model configuration supports three modes: pretrained HF, pretrained local, or custom trained DPO evaluation is optional and controlled by dpo_eval_enabled Sample logging is optional for monitoring generation quality during training","title":"Notes"},{"location":"PARAMETERS/#additional-resources","text":"FinetuneHub Documentation Backend Comparison Guide Algorithm Selection Guide","title":"Additional Resources"},{"location":"cli-reference/","text":"CLI Reference \u00b6 AlignTune provides a comprehensive command-line interface for training and managing models. Overview \u00b6 The CLI provides easy access to all AlignTune functionality without writing Python code. Main Commands \u00b6 aligntune info \u00b6 Display system information and compatibility status. aligntune info aligntune list-backends-cmd \u00b6 List all available backends and their status. aligntune list-backends-cmd Training Commands \u00b6 aligntune train \u00b6 Train a model with specified parameters. # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl # PPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth Training Options \u00b6 Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type (sft, dpo, ppo, etc.) --type dpo --backend Backend (trl, unsloth, auto) --backend auto --epochs Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir Output directory --output-dir ./output Configuration Files \u00b6 The CLI supports YAML configuration files for complex setups: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Examples \u00b6 Quick SFT Training \u00b6 aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --backend auto \\ --epochs 3 \\ --batch-size 4 \\ --learning-rate 5e-5 See Also \u00b6 Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference","title":"CLI Reference"},{"location":"cli-reference/#cli-reference","text":"AlignTune provides a comprehensive command-line interface for training and managing models.","title":"CLI Reference"},{"location":"cli-reference/#overview","text":"The CLI provides easy access to all AlignTune functionality without writing Python code.","title":"Overview"},{"location":"cli-reference/#main-commands","text":"","title":"Main Commands"},{"location":"cli-reference/#aligntune-info","text":"Display system information and compatibility status. aligntune info","title":"aligntune info"},{"location":"cli-reference/#aligntune-list-backends-cmd","text":"List all available backends and their status. aligntune list-backends-cmd","title":"aligntune list-backends-cmd"},{"location":"cli-reference/#training-commands","text":"","title":"Training Commands"},{"location":"cli-reference/#aligntune-train","text":"Train a model with specified parameters. # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl # PPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth","title":"aligntune train"},{"location":"cli-reference/#training-options","text":"Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type (sft, dpo, ppo, etc.) --type dpo --backend Backend (trl, unsloth, auto) --backend auto --epochs Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir Output directory --output-dir ./output","title":"Training Options"},{"location":"cli-reference/#configuration-files","text":"The CLI supports YAML configuration files for complex setups: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Configuration Files"},{"location":"cli-reference/#examples","text":"","title":"Examples"},{"location":"cli-reference/#quick-sft-training","text":"aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --backend auto \\ --epochs 3 \\ --batch-size 4 \\ --learning-rate 5e-5","title":"Quick SFT Training"},{"location":"cli-reference/#see-also","text":"Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference","title":"See Also"},{"location":"unsloth_compatibility/","text":"Unsloth Compatibility Guide \u00b6 This document provides comprehensive information about Unsloth compatibility with AlignTune, including supported versions, known issues, and troubleshooting steps. Overview \u00b6 Unsloth provides faster training with memory optimizations, but requires specific environment configurations. AlignTune includes robust detection and fallback mechanisms to ensure training works even when Unsloth is unavailable. Supported Versions \u00b6 Recommended Combinations \u00b6 PyTorch CUDA Unsloth Status 2.0.0-2.7.0 11.8, 12.1, 12.4 latest Optimal 2.8.0+ 12.8+ latest Known Issues Python Requirements \u00b6 Python 3.8-3.12 CUDA-capable GPU (for GPU training) Known Compatibility Issues \u00b6 1. CUDA Symbol Errors \u00b6 Error : undefined symbol: _ZN3c104cuda9SetDeviceEa Cause : Version incompatibility between PyTorch 2.8.0+ and Unsloth's CUDA extensions. Solutions : - Use PyTorch 2.7.0 or earlier - Update Unsloth: pip install --upgrade unsloth - Use TRL backends instead: --backend trl 2. Flash Attention Issues \u00b6 Error : Flash Attention 2 installation broken Cause : CUDA version incompatibility with Flash Attention. Solutions : - Unsloth automatically falls back to Xformers - Update Flash Attention: pip install --upgrade flash-attn - Use TRL backends: --backend trl 3. Version Mismatches \u00b6 Error : Version incompatibility between dependencies Solutions : - Check compatibility matrix above - Update all dependencies: pip install --upgrade torch unsloth - Use TRL backends: --backend trl Environment Setup \u00b6 Optimal Setup \u00b6 # Install PyTorch with CUDA support pip install torch == 2 .7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Install Unsloth pip install unsloth # Install AlignTune pip install -e . Alternative Setup (if CUDA issues persist) \u00b6 # Use TRL backends instead pip install torch transformers trl pip install -e . Troubleshooting \u00b6 1. Check Environment \u00b6 # Run comprehensive diagnostics aligntune diagnose # Check basic info with verbose output aligntune info --verbose 2. Common Issues and Solutions \u00b6 Unsloth Not Detected \u00b6 # Check if Unsloth is installed python -c \"import unsloth; print('Unsloth available')\" # If error occurs, check CUDA compatibility python -c \"import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')\" CUDA Symbol Errors \u00b6 Solution 1 : Downgrade PyTorch to 2.7.0 Solution 2 : Use TRL backends: --backend trl Solution 3 : Update Unsloth: pip install --upgrade unsloth Flash Attention Issues \u00b6 Unsloth automatically falls back to Xformers No action required, but performance may be slightly reduced 3. Fallback to TRL Backends \u00b6 If Unsloth is not available, AlignTune automatically falls back to TRL backends: # Explicitly use TRL backends aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend trl # Or let AlignTune auto-select aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend auto Performance Comparison \u00b6 Backend Speed Memory Compatibility Unsloth faster 80% less Requires specific setup TRL Standard Standard Universal Legacy Standard Standard Universal Diagnostic Commands \u00b6 Basic Information \u00b6 # Show system status aligntune info # Show detailed diagnostics aligntune info --verbose Comprehensive Diagnostics \u00b6 # Run full environment check aligntune diagnose Backend Selection \u00b6 # List available backends aligntune list-backends # Validate model compatibility aligntune validate --model microsoft/DialoGPT-medium --backend auto Error Messages and Solutions \u00b6 \"Unsloth not available: cuda_symbol_error\" \u00b6 Cause : CUDA version incompatibility Solution : Use TRL backends or fix CUDA setup \"Unsloth not available: flash_attention_error\" \u00b6 Cause : Flash Attention compatibility issues Solution : Unsloth will auto-fallback to Xformers \"Unsloth not available: missing_dependency\" \u00b6 Cause : Unsloth not installed Solution : pip install unsloth or use TRL backends Best Practices \u00b6 Always test with aligntune diagnose before training Use --backend auto to let AlignTune choose the best available backend Check logs for detailed error information Fallback to TRL if Unsloth has issues Update dependencies regularly for best compatibility Getting Help \u00b6 If you encounter issues not covered in this guide: Run aligntune diagnose and share the output Check the logs for detailed error messages Try using TRL backends as a fallback Report issues with full diagnostic output Version History \u00b6 v0.2.0 : Enhanced error detection and diagnostics v0.1.0 : Initial Unsloth support with basic detection For more information, see the main README or run aligntune info --help .","title":"Unsloth Compatibility"},{"location":"unsloth_compatibility/#unsloth-compatibility-guide","text":"This document provides comprehensive information about Unsloth compatibility with AlignTune, including supported versions, known issues, and troubleshooting steps.","title":"Unsloth Compatibility Guide"},{"location":"unsloth_compatibility/#overview","text":"Unsloth provides faster training with memory optimizations, but requires specific environment configurations. AlignTune includes robust detection and fallback mechanisms to ensure training works even when Unsloth is unavailable.","title":"Overview"},{"location":"unsloth_compatibility/#supported-versions","text":"","title":"Supported Versions"},{"location":"unsloth_compatibility/#recommended-combinations","text":"PyTorch CUDA Unsloth Status 2.0.0-2.7.0 11.8, 12.1, 12.4 latest Optimal 2.8.0+ 12.8+ latest Known Issues","title":"Recommended Combinations"},{"location":"unsloth_compatibility/#python-requirements","text":"Python 3.8-3.12 CUDA-capable GPU (for GPU training)","title":"Python Requirements"},{"location":"unsloth_compatibility/#known-compatibility-issues","text":"","title":"Known Compatibility Issues"},{"location":"unsloth_compatibility/#1-cuda-symbol-errors","text":"Error : undefined symbol: _ZN3c104cuda9SetDeviceEa Cause : Version incompatibility between PyTorch 2.8.0+ and Unsloth's CUDA extensions. Solutions : - Use PyTorch 2.7.0 or earlier - Update Unsloth: pip install --upgrade unsloth - Use TRL backends instead: --backend trl","title":"1. CUDA Symbol Errors"},{"location":"unsloth_compatibility/#2-flash-attention-issues","text":"Error : Flash Attention 2 installation broken Cause : CUDA version incompatibility with Flash Attention. Solutions : - Unsloth automatically falls back to Xformers - Update Flash Attention: pip install --upgrade flash-attn - Use TRL backends: --backend trl","title":"2. Flash Attention Issues"},{"location":"unsloth_compatibility/#3-version-mismatches","text":"Error : Version incompatibility between dependencies Solutions : - Check compatibility matrix above - Update all dependencies: pip install --upgrade torch unsloth - Use TRL backends: --backend trl","title":"3. Version Mismatches"},{"location":"unsloth_compatibility/#environment-setup","text":"","title":"Environment Setup"},{"location":"unsloth_compatibility/#optimal-setup","text":"# Install PyTorch with CUDA support pip install torch == 2 .7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Install Unsloth pip install unsloth # Install AlignTune pip install -e .","title":"Optimal Setup"},{"location":"unsloth_compatibility/#alternative-setup-if-cuda-issues-persist","text":"# Use TRL backends instead pip install torch transformers trl pip install -e .","title":"Alternative Setup (if CUDA issues persist)"},{"location":"unsloth_compatibility/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"unsloth_compatibility/#1-check-environment","text":"# Run comprehensive diagnostics aligntune diagnose # Check basic info with verbose output aligntune info --verbose","title":"1. Check Environment"},{"location":"unsloth_compatibility/#2-common-issues-and-solutions","text":"","title":"2. Common Issues and Solutions"},{"location":"unsloth_compatibility/#unsloth-not-detected","text":"# Check if Unsloth is installed python -c \"import unsloth; print('Unsloth available')\" # If error occurs, check CUDA compatibility python -c \"import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')\"","title":"Unsloth Not Detected"},{"location":"unsloth_compatibility/#cuda-symbol-errors","text":"Solution 1 : Downgrade PyTorch to 2.7.0 Solution 2 : Use TRL backends: --backend trl Solution 3 : Update Unsloth: pip install --upgrade unsloth","title":"CUDA Symbol Errors"},{"location":"unsloth_compatibility/#flash-attention-issues","text":"Unsloth automatically falls back to Xformers No action required, but performance may be slightly reduced","title":"Flash Attention Issues"},{"location":"unsloth_compatibility/#3-fallback-to-trl-backends","text":"If Unsloth is not available, AlignTune automatically falls back to TRL backends: # Explicitly use TRL backends aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend trl # Or let AlignTune auto-select aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend auto","title":"3. Fallback to TRL Backends"},{"location":"unsloth_compatibility/#performance-comparison","text":"Backend Speed Memory Compatibility Unsloth faster 80% less Requires specific setup TRL Standard Standard Universal Legacy Standard Standard Universal","title":"Performance Comparison"},{"location":"unsloth_compatibility/#diagnostic-commands","text":"","title":"Diagnostic Commands"},{"location":"unsloth_compatibility/#basic-information","text":"# Show system status aligntune info # Show detailed diagnostics aligntune info --verbose","title":"Basic Information"},{"location":"unsloth_compatibility/#comprehensive-diagnostics","text":"# Run full environment check aligntune diagnose","title":"Comprehensive Diagnostics"},{"location":"unsloth_compatibility/#backend-selection","text":"# List available backends aligntune list-backends # Validate model compatibility aligntune validate --model microsoft/DialoGPT-medium --backend auto","title":"Backend Selection"},{"location":"unsloth_compatibility/#error-messages-and-solutions","text":"","title":"Error Messages and Solutions"},{"location":"unsloth_compatibility/#unsloth-not-available-cuda_symbol_error","text":"Cause : CUDA version incompatibility Solution : Use TRL backends or fix CUDA setup","title":"\"Unsloth not available: cuda_symbol_error\""},{"location":"unsloth_compatibility/#unsloth-not-available-flash_attention_error","text":"Cause : Flash Attention compatibility issues Solution : Unsloth will auto-fallback to Xformers","title":"\"Unsloth not available: flash_attention_error\""},{"location":"unsloth_compatibility/#unsloth-not-available-missing_dependency","text":"Cause : Unsloth not installed Solution : pip install unsloth or use TRL backends","title":"\"Unsloth not available: missing_dependency\""},{"location":"unsloth_compatibility/#best-practices","text":"Always test with aligntune diagnose before training Use --backend auto to let AlignTune choose the best available backend Check logs for detailed error information Fallback to TRL if Unsloth has issues Update dependencies regularly for best compatibility","title":"Best Practices"},{"location":"unsloth_compatibility/#getting-help","text":"If you encounter issues not covered in this guide: Run aligntune diagnose and share the output Check the logs for detailed error messages Try using TRL backends as a fallback Report issues with full diagnostic output","title":"Getting Help"},{"location":"unsloth_compatibility/#version-history","text":"v0.2.0 : Enhanced error detection and diagnostics v0.1.0 : Initial Unsloth support with basic detection For more information, see the main README or run aligntune info --help .","title":"Version History"},{"location":"advanced/architecture/","text":"Architecture Overview \u00b6 Complete architecture documentation for AlignTune. System Architecture \u00b6 High-Level Overview \u00b6 AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend supports: - SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend supports: - SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO Note: GSPO is a TRL-only algorithm. Complete System Architecture \u00b6 flowchart TD UI[User Interface] --> Factory[Backend Factory Layer] Factory --> TRL[TRL Backends] Factory --> Unsloth[Unsloth Backends] TRL --> Core[Core Training Components] Unsloth --> Core Core --> Rewards[Reward System] Core --> Eval[Evaluation System] Core Components \u00b6 1. Backend Factory \u00b6 The backend factory provides a unified interface for creating trainers: Backend Selection : Automatic or manual backend selection Configuration Management : Unified configuration system Fallback Handling : Automatic fallback to available backends Key Classes: - BackendFactory : Main factory class - BackendConfig : Backend configuration - create_sft_trainer() : SFT trainer creation - create_rl_trainer() : RL trainer creation 2. Configuration System \u00b6 Unified configuration system with type safety: SFT Configuration : SFTConfig , ModelConfig , DatasetConfig , TrainingConfig RL Configuration : UnifiedConfig , algorithm-specific configs Validation : Comprehensive validation with clear error messages 3. Training Backends \u00b6 TRL Backends \u00b6 Pure TRL implementations Maximum compatibility Battle-tested reliability Supports all core algorithms (DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO) Unsloth Backends \u00b6 faster training Memory efficient Optimized kernels Supports DPO, PPO, GRPO, DAPO, Dr. GRPO (not GSPO) 4. Reward System \u00b6 27+ Pre-built Functions : Length, sentiment, safety, etc. Composite Rewards : Combine multiple functions Custom Functions : Extensible reward system Reward Model Training : Train neural reward models 5. Evaluation System \u00b6 Training Evaluation : Automatic during training Standalone Evaluation : Evaluate saved models Benchmark Integration : lm-eval integration Custom Tasks : Task-specific evaluation Design Patterns \u00b6 Factory Pattern \u00b6 Backend selection uses the factory pattern: # Factory creates appropriate trainer trainer = BackendFactory . create_trainer ( config , backend_config ) Strategy Pattern \u00b6 Different backends implement the same interface: # Both backends implement TrainerBase class TRLSFTTrainer ( SFTTrainerBase ): ... class UnslothSFTTrainer ( SFTTrainerBase ): ... Registry Pattern \u00b6 Reward functions use registry pattern: # Register and retrieve functions RewardRegistry . register_reward ( \"custom\" , CustomReward ) func = RewardRegistry . get_reward_function ( \"custom\" ) Data Flow \u00b6 Training Flow \u00b6 1. User creates trainer via factory 2. Factory selects backend 3. Backend loads model and dataset 4. Training loop executes 5. Metrics logged and checkpoints saved 6. Model saved to disk Evaluation Flow \u00b6 1. Load model and evaluation dataset 2. Run inference on dataset 3. Compute metrics 4. Return results Extension Points \u00b6 Custom Backends \u00b6 Create custom backends by implementing trainer base classes: class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): ... def evaluate ( self ): ... Custom Reward Functions \u00b6 Extend reward system with custom functions: class CustomReward ( RewardFunction ): def compute ( self , text : str ) -> float : ... Custom Evaluation Tasks \u00b6 Add custom evaluation tasks: task = EvalTask ( name = \"custom\" , category = TaskCategory . CUSTOM , ... ) Next Steps \u00b6 Custom Backends - Creating custom backends Distributed Training - Distributed training architecture Performance - Performance optimization","title":"Architecture"},{"location":"advanced/architecture/#architecture-overview","text":"Complete architecture documentation for AlignTune.","title":"Architecture Overview"},{"location":"advanced/architecture/#system-architecture","text":"","title":"System Architecture"},{"location":"advanced/architecture/#high-level-overview","text":"AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend supports: - SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend supports: - SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO Note: GSPO is a TRL-only algorithm.","title":"High-Level Overview"},{"location":"advanced/architecture/#complete-system-architecture","text":"flowchart TD UI[User Interface] --> Factory[Backend Factory Layer] Factory --> TRL[TRL Backends] Factory --> Unsloth[Unsloth Backends] TRL --> Core[Core Training Components] Unsloth --> Core Core --> Rewards[Reward System] Core --> Eval[Evaluation System]","title":"Complete System Architecture"},{"location":"advanced/architecture/#core-components","text":"","title":"Core Components"},{"location":"advanced/architecture/#1-backend-factory","text":"The backend factory provides a unified interface for creating trainers: Backend Selection : Automatic or manual backend selection Configuration Management : Unified configuration system Fallback Handling : Automatic fallback to available backends Key Classes: - BackendFactory : Main factory class - BackendConfig : Backend configuration - create_sft_trainer() : SFT trainer creation - create_rl_trainer() : RL trainer creation","title":"1. Backend Factory"},{"location":"advanced/architecture/#2-configuration-system","text":"Unified configuration system with type safety: SFT Configuration : SFTConfig , ModelConfig , DatasetConfig , TrainingConfig RL Configuration : UnifiedConfig , algorithm-specific configs Validation : Comprehensive validation with clear error messages","title":"2. Configuration System"},{"location":"advanced/architecture/#3-training-backends","text":"","title":"3. Training Backends"},{"location":"advanced/architecture/#trl-backends","text":"Pure TRL implementations Maximum compatibility Battle-tested reliability Supports all core algorithms (DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO)","title":"TRL Backends"},{"location":"advanced/architecture/#unsloth-backends","text":"faster training Memory efficient Optimized kernels Supports DPO, PPO, GRPO, DAPO, Dr. GRPO (not GSPO)","title":"Unsloth Backends"},{"location":"advanced/architecture/#4-reward-system","text":"27+ Pre-built Functions : Length, sentiment, safety, etc. Composite Rewards : Combine multiple functions Custom Functions : Extensible reward system Reward Model Training : Train neural reward models","title":"4. Reward System"},{"location":"advanced/architecture/#5-evaluation-system","text":"Training Evaluation : Automatic during training Standalone Evaluation : Evaluate saved models Benchmark Integration : lm-eval integration Custom Tasks : Task-specific evaluation","title":"5. Evaluation System"},{"location":"advanced/architecture/#design-patterns","text":"","title":"Design Patterns"},{"location":"advanced/architecture/#factory-pattern","text":"Backend selection uses the factory pattern: # Factory creates appropriate trainer trainer = BackendFactory . create_trainer ( config , backend_config )","title":"Factory Pattern"},{"location":"advanced/architecture/#strategy-pattern","text":"Different backends implement the same interface: # Both backends implement TrainerBase class TRLSFTTrainer ( SFTTrainerBase ): ... class UnslothSFTTrainer ( SFTTrainerBase ): ...","title":"Strategy Pattern"},{"location":"advanced/architecture/#registry-pattern","text":"Reward functions use registry pattern: # Register and retrieve functions RewardRegistry . register_reward ( \"custom\" , CustomReward ) func = RewardRegistry . get_reward_function ( \"custom\" )","title":"Registry Pattern"},{"location":"advanced/architecture/#data-flow","text":"","title":"Data Flow"},{"location":"advanced/architecture/#training-flow","text":"1. User creates trainer via factory 2. Factory selects backend 3. Backend loads model and dataset 4. Training loop executes 5. Metrics logged and checkpoints saved 6. Model saved to disk","title":"Training Flow"},{"location":"advanced/architecture/#evaluation-flow","text":"1. Load model and evaluation dataset 2. Run inference on dataset 3. Compute metrics 4. Return results","title":"Evaluation Flow"},{"location":"advanced/architecture/#extension-points","text":"","title":"Extension Points"},{"location":"advanced/architecture/#custom-backends","text":"Create custom backends by implementing trainer base classes: class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): ... def evaluate ( self ): ...","title":"Custom Backends"},{"location":"advanced/architecture/#custom-reward-functions","text":"Extend reward system with custom functions: class CustomReward ( RewardFunction ): def compute ( self , text : str ) -> float : ...","title":"Custom Reward Functions"},{"location":"advanced/architecture/#custom-evaluation-tasks","text":"Add custom evaluation tasks: task = EvalTask ( name = \"custom\" , category = TaskCategory . CUSTOM , ... )","title":"Custom Evaluation Tasks"},{"location":"advanced/architecture/#next-steps","text":"Custom Backends - Creating custom backends Distributed Training - Distributed training architecture Performance - Performance optimization","title":"Next Steps"},{"location":"advanced/custom-backends/","text":"Custom Backends \u00b6 Create custom training backends by implementing the trainer base classes. Overview \u00b6 AlignTune uses a backend factory and base trainer classes so you can add new backends (e.g., other frameworks) while keeping the same configuration and API. Implementing a Custom SFT Backend \u00b6 Subclass SFTTrainerBase and implement the required methods: from aligntune.core.sft.trainer_base import SFTTrainerBase class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): # Your training loop ... def evaluate ( self , eval_dataset = None , ** kwargs ): # Your evaluation logic ... Implementing a Custom RL Backend \u00b6 Subclass the RL trainer base and implement the lifecycle: from aligntune.core.rl.trainer_base import TrainerBase class CustomRLTrainer ( TrainerBase ): def train ( self ): ... def evaluate ( self , eval_dataset = None , ** kwargs ): ... Registration \u00b6 Register your backend with the factory so it can be selected by name (e.g., in config or backend=\"custom\" ). See the source for BackendFactory and backend registration patterns. Next Steps \u00b6 Architecture - System design overview Distributed Training - Multi-GPU and distributed setups Performance - Optimization tips","title":"Custom Backends"},{"location":"advanced/custom-backends/#custom-backends","text":"Create custom training backends by implementing the trainer base classes.","title":"Custom Backends"},{"location":"advanced/custom-backends/#overview","text":"AlignTune uses a backend factory and base trainer classes so you can add new backends (e.g., other frameworks) while keeping the same configuration and API.","title":"Overview"},{"location":"advanced/custom-backends/#implementing-a-custom-sft-backend","text":"Subclass SFTTrainerBase and implement the required methods: from aligntune.core.sft.trainer_base import SFTTrainerBase class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): # Your training loop ... def evaluate ( self , eval_dataset = None , ** kwargs ): # Your evaluation logic ...","title":"Implementing a Custom SFT Backend"},{"location":"advanced/custom-backends/#implementing-a-custom-rl-backend","text":"Subclass the RL trainer base and implement the lifecycle: from aligntune.core.rl.trainer_base import TrainerBase class CustomRLTrainer ( TrainerBase ): def train ( self ): ... def evaluate ( self , eval_dataset = None , ** kwargs ): ...","title":"Implementing a Custom RL Backend"},{"location":"advanced/custom-backends/#registration","text":"Register your backend with the factory so it can be selected by name (e.g., in config or backend=\"custom\" ). See the source for BackendFactory and backend registration patterns.","title":"Registration"},{"location":"advanced/custom-backends/#next-steps","text":"Architecture - System design overview Distributed Training - Multi-GPU and distributed setups Performance - Optimization tips","title":"Next Steps"},{"location":"advanced/distributed/","text":"Distributed Training \u00b6 Run AlignTune across multiple GPUs and nodes. Overview \u00b6 AlignTune supports distributed training via the underlying backends (TRL, Unsloth) and accelerate . Configuration is passed through the unified config so you can use the same YAML/code for single-GPU and multi-GPU runs. Using Accelerate \u00b6 Configure with accelerate config (multi-GPU, multi-node, etc.). Launch with accelerate launch : accelerate launch --num_processes 4 aligntune train config.yaml Set distributed options in your config under distributed (e.g., rank, world size, backend) when supported by the loader. Backend Behavior \u00b6 TRL : Uses Hugging Face Trainer / Accelerate; supports DDP, DeepSpeed, FSDP when configured. Unsloth : Follow Unsloth\u2019s documentation for multi-GPU; use the same accelerate launch pattern where applicable. Next Steps \u00b6 Architecture - System design Custom Backends - Adding new backends Performance - Optimization","title":"Distributed Training"},{"location":"advanced/distributed/#distributed-training","text":"Run AlignTune across multiple GPUs and nodes.","title":"Distributed Training"},{"location":"advanced/distributed/#overview","text":"AlignTune supports distributed training via the underlying backends (TRL, Unsloth) and accelerate . Configuration is passed through the unified config so you can use the same YAML/code for single-GPU and multi-GPU runs.","title":"Overview"},{"location":"advanced/distributed/#using-accelerate","text":"Configure with accelerate config (multi-GPU, multi-node, etc.). Launch with accelerate launch : accelerate launch --num_processes 4 aligntune train config.yaml Set distributed options in your config under distributed (e.g., rank, world size, backend) when supported by the loader.","title":"Using Accelerate"},{"location":"advanced/distributed/#backend-behavior","text":"TRL : Uses Hugging Face Trainer / Accelerate; supports DDP, DeepSpeed, FSDP when configured. Unsloth : Follow Unsloth\u2019s documentation for multi-GPU; use the same accelerate launch pattern where applicable.","title":"Backend Behavior"},{"location":"advanced/distributed/#next-steps","text":"Architecture - System design Custom Backends - Adding new backends Performance - Optimization","title":"Next Steps"},{"location":"advanced/performance/","text":"Performance Optimization \u00b6 Tips for faster training and lower memory use with AlignTune. Backend Choice \u00b6 Unsloth : Generally faster and more memory-efficient for supported models (SFT, DPO, PPO, GRPO, etc.). Prefer when you have a compatible GPU and model. TRL : Use when you need GSPO or maximum compatibility; still supports mixed precision, LoRA, and gradient checkpointing. Memory \u00b6 Enable gradient checkpointing to trade compute for memory. Use LoRA/QLoRA (e.g., use_peft=True , 4-bit via Unsloth) to reduce VRAM. Reduce batch size and increase gradient_accumulation_steps to keep effective batch size. Reduce max sequence length when possible. Speed \u00b6 Use mixed precision (e.g., bf16/fp16) when your hardware supports it. Prefer Unsloth for supported setups. Increase batch size (within memory limits) and tune gradient_accumulation_steps . Next Steps \u00b6 Backend Support Matrix - Algorithm and backend support Troubleshooting - Common issues Architecture - System design","title":"Performance Optimization"},{"location":"advanced/performance/#performance-optimization","text":"Tips for faster training and lower memory use with AlignTune.","title":"Performance Optimization"},{"location":"advanced/performance/#backend-choice","text":"Unsloth : Generally faster and more memory-efficient for supported models (SFT, DPO, PPO, GRPO, etc.). Prefer when you have a compatible GPU and model. TRL : Use when you need GSPO or maximum compatibility; still supports mixed precision, LoRA, and gradient checkpointing.","title":"Backend Choice"},{"location":"advanced/performance/#memory","text":"Enable gradient checkpointing to trade compute for memory. Use LoRA/QLoRA (e.g., use_peft=True , 4-bit via Unsloth) to reduce VRAM. Reduce batch size and increase gradient_accumulation_steps to keep effective batch size. Reduce max sequence length when possible.","title":"Memory"},{"location":"advanced/performance/#speed","text":"Use mixed precision (e.g., bf16/fp16) when your hardware supports it. Prefer Unsloth for supported setups. Increase batch size (within memory limits) and tune gradient_accumulation_steps .","title":"Speed"},{"location":"advanced/performance/#next-steps","text":"Backend Support Matrix - Algorithm and backend support Troubleshooting - Common issues Architecture - System design","title":"Next Steps"},{"location":"algorithms/dapo/","text":"DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) \u00b6 DAPO is an advanced RLHF algorithm that decouples clipping and dynamic sampling for improved training stability. Overview \u00b6 Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) addresses limitations in traditional PPO by separating the clipping mechanism from dynamic sampling, providing more stable and efficient training. Key Features \u00b6 Decoupled optimization : Separate clipping and sampling mechanisms Dynamic sampling : Adaptive sample generation based on training progress Improved stability : Better gradient behavior than standard PPO Multi-backend support : Available in both TRL and Unsloth backends Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 DAPO improves upon PPO by: Decoupled clipping : Separate value and policy clipping Dynamic sampling : Adjusts sample distribution during training Adaptive optimization : Better handling of reward scaling Configuration Options \u00b6 Parameter Type Default Description clip_range float 0.2 Policy clipping range value_clip_range float 0.2 Value function clipping range dynamic_sampling bool true Enable dynamic sampling sampling_temperature float 1.0 Sampling temperature Advantages \u00b6 More stable training than standard PPO Better performance on complex tasks Reduced hyperparameter sensitivity See Also \u00b6 PPO Algorithm - Base policy optimization GRPO Algorithm - Group-based optimization Algorithms Overview - All supported algorithms","title":"DAPO"},{"location":"algorithms/dapo/#dapo-decouple-clip-and-dynamic-sampling-policy-optimization","text":"DAPO is an advanced RLHF algorithm that decouples clipping and dynamic sampling for improved training stability.","title":"DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization)"},{"location":"algorithms/dapo/#overview","text":"Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) addresses limitations in traditional PPO by separating the clipping mechanism from dynamic sampling, providing more stable and efficient training.","title":"Overview"},{"location":"algorithms/dapo/#key-features","text":"Decoupled optimization : Separate clipping and sampling mechanisms Dynamic sampling : Adaptive sample generation based on training progress Improved stability : Better gradient behavior than standard PPO Multi-backend support : Available in both TRL and Unsloth backends","title":"Key Features"},{"location":"algorithms/dapo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/dapo/#algorithm-details","text":"DAPO improves upon PPO by: Decoupled clipping : Separate value and policy clipping Dynamic sampling : Adjusts sample distribution during training Adaptive optimization : Better handling of reward scaling","title":"Algorithm Details"},{"location":"algorithms/dapo/#configuration-options","text":"Parameter Type Default Description clip_range float 0.2 Policy clipping range value_clip_range float 0.2 Value function clipping range dynamic_sampling bool true Enable dynamic sampling sampling_temperature float 1.0 Sampling temperature","title":"Configuration Options"},{"location":"algorithms/dapo/#advantages","text":"More stable training than standard PPO Better performance on complex tasks Reduced hyperparameter sensitivity","title":"Advantages"},{"location":"algorithms/dapo/#see-also","text":"PPO Algorithm - Base policy optimization GRPO Algorithm - Group-based optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/dpo/","text":"DPO (Direct Preference Optimization) \u00b6 Direct Preference Optimization (DPO) is a popular RLHF algorithm that optimizes language models directly on preference data without requiring a separate reward model. Overview \u00b6 DPO eliminates the need for a reward model by directly optimizing the policy on preference pairs (chosen vs rejected responses). This makes it simpler and faster than PPO-based approaches. Key Features \u00b6 No Reward Model : Works directly with preference pairs Fast Training : Simpler optimization than PPO Both Backends : Supported by TRL and Unsloth Widely Used : Industry-standard for preference learning How It Works \u00b6 DPO optimizes the policy to increase the likelihood of chosen responses while decreasing the likelihood of rejected responses, using a KL divergence constraint to prevent the policy from deviating too far from the reference model. Mathematical Formulation \u00b6 The DPO objective maximizes: L_DPO = E[log \u03c3(\u03b2 * (log \u03c0_\u03b8(y_w | x) - log \u03c0_ref(y_w | x) - log \u03c0_\u03b8(y_l | x) + log \u03c0_ref(y_l | x)))] Where: - \u03c0_\u03b8 : Policy model - \u03c0_ref : Reference model - y_w : Chosen response - y_l : Rejected response - \u03b2 : Temperature parameter Usage \u00b6 Basic DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 1e-6 , beta = 0.1 # DPO temperature parameter ) trainer . train () DPO with Unsloth Backend \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 ) trainer . train () DPO with Custom Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-6 , beta = 0.1 , max_seq_length = 512 , reference_free = False , # Use reference model label_smoothing = 0.0 , loss_type = \"sigmoid\" # DPO loss type ) Configuration Parameters \u00b6 DPO-Specific Parameters \u00b6 Parameter Type Default Description beta float 0.1 DPO temperature parameter (controls KL penalty) reference_free bool False Whether to use reference-free DPO label_smoothing float 0.0 Label smoothing factor loss_type str \"sigmoid\" Loss function type General RL Parameters \u00b6 Parameter Type Default Description num_epochs int 1 Number of training epochs batch_size int 4 Per-device batch size learning_rate float 1e-6 Learning rate max_seq_length int 512 Maximum sequence length Dataset Format \u00b6 DPO requires preference pairs in the following format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know about that topic.\" } Supported Datasets \u00b6 Anthropic/hh-rlhf : Human preference dataset OpenAssistant/oasst1 : OpenAssistant conversations Custom datasets with prompt , chosen , rejected columns Best Practices \u00b6 1. Beta Parameter Tuning \u00b6 The beta parameter controls the strength of the KL penalty: - Low beta (0.01-0.1) : Stronger preference learning, risk of overfitting - Medium beta (0.1-0.5) : Balanced learning - High beta (0.5-1.0) : More conservative, stays closer to reference # Conservative training beta = 0.5 # Aggressive training beta = 0.01 2. Learning Rate \u00b6 DPO typically requires lower learning rates than SFT: - Start with 1e-6 to 5e-6 - Adjust based on loss convergence 3. Batch Size \u00b6 Use batch size 1-4 for smaller models Use batch size 4-8 for larger models Consider gradient accumulation for effective larger batches 4. Number of Epochs \u00b6 Usually 1-3 epochs are sufficient Monitor validation loss to avoid overfitting Evaluation \u00b6 DPO evaluation includes: from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also Common Issues \u00b6 Loss Not Decreasing \u00b6 Solutions : - Reduce learning rate - Increase beta (stronger KL penalty) - Check dataset quality - Verify preference pairs are correct Overfitting \u00b6 Solutions : - Increase beta - Reduce number of epochs - Use more regularization - Increase dataset size Comparison with Other Algorithms \u00b6 Feature DPO PPO GRPO Reward Model No Yes No Training Speed Fast Slow Medium Setup Complexity Low High Medium Preference Data Required Optional Required References \u00b6 Direct Preference Optimization Paper TRL DPO Documentation Next Steps \u00b6 PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"DPO"},{"location":"algorithms/dpo/#dpo-direct-preference-optimization","text":"Direct Preference Optimization (DPO) is a popular RLHF algorithm that optimizes language models directly on preference data without requiring a separate reward model.","title":"DPO (Direct Preference Optimization)"},{"location":"algorithms/dpo/#overview","text":"DPO eliminates the need for a reward model by directly optimizing the policy on preference pairs (chosen vs rejected responses). This makes it simpler and faster than PPO-based approaches.","title":"Overview"},{"location":"algorithms/dpo/#key-features","text":"No Reward Model : Works directly with preference pairs Fast Training : Simpler optimization than PPO Both Backends : Supported by TRL and Unsloth Widely Used : Industry-standard for preference learning","title":"Key Features"},{"location":"algorithms/dpo/#how-it-works","text":"DPO optimizes the policy to increase the likelihood of chosen responses while decreasing the likelihood of rejected responses, using a KL divergence constraint to prevent the policy from deviating too far from the reference model.","title":"How It Works"},{"location":"algorithms/dpo/#mathematical-formulation","text":"The DPO objective maximizes: L_DPO = E[log \u03c3(\u03b2 * (log \u03c0_\u03b8(y_w | x) - log \u03c0_ref(y_w | x) - log \u03c0_\u03b8(y_l | x) + log \u03c0_ref(y_l | x)))] Where: - \u03c0_\u03b8 : Policy model - \u03c0_ref : Reference model - y_w : Chosen response - y_l : Rejected response - \u03b2 : Temperature parameter","title":"Mathematical Formulation"},{"location":"algorithms/dpo/#usage","text":"","title":"Usage"},{"location":"algorithms/dpo/#basic-dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 1e-6 , beta = 0.1 # DPO temperature parameter ) trainer . train ()","title":"Basic DPO Training"},{"location":"algorithms/dpo/#dpo-with-unsloth-backend","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 ) trainer . train ()","title":"DPO with Unsloth Backend"},{"location":"algorithms/dpo/#dpo-with-custom-configuration","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-6 , beta = 0.1 , max_seq_length = 512 , reference_free = False , # Use reference model label_smoothing = 0.0 , loss_type = \"sigmoid\" # DPO loss type )","title":"DPO with Custom Configuration"},{"location":"algorithms/dpo/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"algorithms/dpo/#dpo-specific-parameters","text":"Parameter Type Default Description beta float 0.1 DPO temperature parameter (controls KL penalty) reference_free bool False Whether to use reference-free DPO label_smoothing float 0.0 Label smoothing factor loss_type str \"sigmoid\" Loss function type","title":"DPO-Specific Parameters"},{"location":"algorithms/dpo/#general-rl-parameters","text":"Parameter Type Default Description num_epochs int 1 Number of training epochs batch_size int 4 Per-device batch size learning_rate float 1e-6 Learning rate max_seq_length int 512 Maximum sequence length","title":"General RL Parameters"},{"location":"algorithms/dpo/#dataset-format","text":"DPO requires preference pairs in the following format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know about that topic.\" }","title":"Dataset Format"},{"location":"algorithms/dpo/#supported-datasets","text":"Anthropic/hh-rlhf : Human preference dataset OpenAssistant/oasst1 : OpenAssistant conversations Custom datasets with prompt , chosen , rejected columns","title":"Supported Datasets"},{"location":"algorithms/dpo/#best-practices","text":"","title":"Best Practices"},{"location":"algorithms/dpo/#1-beta-parameter-tuning","text":"The beta parameter controls the strength of the KL penalty: - Low beta (0.01-0.1) : Stronger preference learning, risk of overfitting - Medium beta (0.1-0.5) : Balanced learning - High beta (0.5-1.0) : More conservative, stays closer to reference # Conservative training beta = 0.5 # Aggressive training beta = 0.01","title":"1. Beta Parameter Tuning"},{"location":"algorithms/dpo/#2-learning-rate","text":"DPO typically requires lower learning rates than SFT: - Start with 1e-6 to 5e-6 - Adjust based on loss convergence","title":"2. Learning Rate"},{"location":"algorithms/dpo/#3-batch-size","text":"Use batch size 1-4 for smaller models Use batch size 4-8 for larger models Consider gradient accumulation for effective larger batches","title":"3. Batch Size"},{"location":"algorithms/dpo/#4-number-of-epochs","text":"Usually 1-3 epochs are sufficient Monitor validation loss to avoid overfitting","title":"4. Number of Epochs"},{"location":"algorithms/dpo/#evaluation","text":"DPO evaluation includes: from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also","title":"Evaluation"},{"location":"algorithms/dpo/#common-issues","text":"","title":"Common Issues"},{"location":"algorithms/dpo/#loss-not-decreasing","text":"Solutions : - Reduce learning rate - Increase beta (stronger KL penalty) - Check dataset quality - Verify preference pairs are correct","title":"Loss Not Decreasing"},{"location":"algorithms/dpo/#overfitting","text":"Solutions : - Increase beta - Reduce number of epochs - Use more regularization - Increase dataset size","title":"Overfitting"},{"location":"algorithms/dpo/#comparison-with-other-algorithms","text":"Feature DPO PPO GRPO Reward Model No Yes No Training Speed Fast Slow Medium Setup Complexity Low High Medium Preference Data Required Optional Required","title":"Comparison with Other Algorithms"},{"location":"algorithms/dpo/#references","text":"Direct Preference Optimization Paper TRL DPO Documentation","title":"References"},{"location":"algorithms/dpo/#next-steps","text":"PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"algorithms/dr-grpo/","text":"Dr. GRPO (GRPO Done Right) \u00b6 Dr. GRPO is a corrected and improved version of the original GRPO algorithm. Overview \u00b6 GRPO Done Right (Dr. GRPO) addresses optimization biases identified in the original GRPO implementation, providing more accurate and stable training. Key Features \u00b6 Bias correction : Fixes optimization biases in original GRPO Improved stability : More reliable convergence Enhanced performance : Better results on benchmark tasks Multi-backend support : Available in both TRL and Unsloth Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dr-grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 Dr. GRPO corrects issues in the original GRPO by: Bias correction : Proper normalization of group advantages Improved clipping : Better handling of extreme values Enhanced sampling : More efficient sample utilization Configuration Options \u00b6 Parameter Type Default Description group_size int 8 Number of samples per group bias_correction bool true Enable bias correction clip_range float 0.2 Advantage clipping range Benefits \u00b6 More accurate optimization than original GRPO Better convergence properties Improved performance on complex tasks See Also \u00b6 GRPO Algorithm - Original group optimization Algorithms Overview - All supported algorithms","title":"Dr. GRPO"},{"location":"algorithms/dr-grpo/#dr-grpo-grpo-done-right","text":"Dr. GRPO is a corrected and improved version of the original GRPO algorithm.","title":"Dr. GRPO (GRPO Done Right)"},{"location":"algorithms/dr-grpo/#overview","text":"GRPO Done Right (Dr. GRPO) addresses optimization biases identified in the original GRPO implementation, providing more accurate and stable training.","title":"Overview"},{"location":"algorithms/dr-grpo/#key-features","text":"Bias correction : Fixes optimization biases in original GRPO Improved stability : More reliable convergence Enhanced performance : Better results on benchmark tasks Multi-backend support : Available in both TRL and Unsloth","title":"Key Features"},{"location":"algorithms/dr-grpo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dr-grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/dr-grpo/#algorithm-details","text":"Dr. GRPO corrects issues in the original GRPO by: Bias correction : Proper normalization of group advantages Improved clipping : Better handling of extreme values Enhanced sampling : More efficient sample utilization","title":"Algorithm Details"},{"location":"algorithms/dr-grpo/#configuration-options","text":"Parameter Type Default Description group_size int 8 Number of samples per group bias_correction bool true Enable bias correction clip_range float 0.2 Advantage clipping range","title":"Configuration Options"},{"location":"algorithms/dr-grpo/#benefits","text":"More accurate optimization than original GRPO Better convergence properties Improved performance on complex tasks","title":"Benefits"},{"location":"algorithms/dr-grpo/#see-also","text":"GRPO Algorithm - Original group optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/grpo/","text":"GRPO (Group Relative Policy Optimization) \u00b6 GRPO is an advanced RLHF algorithm that optimizes policies using group-based relative comparisons. Overview \u00b6 Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that performs policy optimization by comparing groups of generated samples rather than individual samples. This approach provides more stable training and better performance on complex tasks. Key Features \u00b6 Group-based optimization : Compares groups of samples for more stable gradients Relative comparisons : Uses relative rewards within groups rather than absolute values Memory efficient : Reduces variance through group normalization Scalable : Works well with large models and complex tasks Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 GRPO optimizes policies by: Group Generation : Generate multiple samples for each prompt Group Scoring : Score all samples in a group Relative Normalization : Normalize rewards within each group Policy Update : Update policy based on group-relative advantages Configuration Options \u00b6 Parameter Type Default Description group_size int 8 Number of samples per group beta float 0.01 KL penalty coefficient cliprange float 0.2 PPO-style clipping range temperature float 0.7 Sampling temperature Best Practices \u00b6 Use larger group sizes (8-16) for better performance Combine with diverse reward functions Monitor group diversity to ensure stable training See Also \u00b6 DPO Algorithm - Direct Preference Optimization PPO Algorithm - Proximal Policy Optimization Algorithms Overview - All supported algorithms","title":"GRPO"},{"location":"algorithms/grpo/#grpo-group-relative-policy-optimization","text":"GRPO is an advanced RLHF algorithm that optimizes policies using group-based relative comparisons.","title":"GRPO (Group Relative Policy Optimization)"},{"location":"algorithms/grpo/#overview","text":"Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that performs policy optimization by comparing groups of generated samples rather than individual samples. This approach provides more stable training and better performance on complex tasks.","title":"Overview"},{"location":"algorithms/grpo/#key-features","text":"Group-based optimization : Compares groups of samples for more stable gradients Relative comparisons : Uses relative rewards within groups rather than absolute values Memory efficient : Reduces variance through group normalization Scalable : Works well with large models and complex tasks","title":"Key Features"},{"location":"algorithms/grpo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/grpo/#algorithm-details","text":"GRPO optimizes policies by: Group Generation : Generate multiple samples for each prompt Group Scoring : Score all samples in a group Relative Normalization : Normalize rewards within each group Policy Update : Update policy based on group-relative advantages","title":"Algorithm Details"},{"location":"algorithms/grpo/#configuration-options","text":"Parameter Type Default Description group_size int 8 Number of samples per group beta float 0.01 KL penalty coefficient cliprange float 0.2 PPO-style clipping range temperature float 0.7 Sampling temperature","title":"Configuration Options"},{"location":"algorithms/grpo/#best-practices","text":"Use larger group sizes (8-16) for better performance Combine with diverse reward functions Monitor group diversity to ensure stable training","title":"Best Practices"},{"location":"algorithms/grpo/#see-also","text":"DPO Algorithm - Direct Preference Optimization PPO Algorithm - Proximal Policy Optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/gspo/","text":"GSPO (Group Sequential Policy Optimization) \u00b6 GSPO is a variant of GRPO that performs sequential optimization within groups. Overview \u00b6 Group Sequential Policy Optimization (GSPO) extends GRPO by performing sequential policy updates within each group, allowing for more fine-grained optimization of complex behaviors. Key Features \u00b6 Sequential updates : Multiple optimization steps per group Fine-grained control : Better handling of complex reward landscapes TRL-only : Currently only available in TRL backend Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only available in TRL num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 GSPO performs multiple sequential policy updates within each group, allowing for: Progressive refinement : Each step improves upon the previous Complex optimization : Better handling of multi-objective rewards Stability : Reduced variance through sequential updates Configuration Options \u00b6 Parameter Type Default Description group_size int 4 Number of samples per group seq_steps int 3 Number of sequential steps beta float 0.01 KL penalty coefficient Limitations \u00b6 Currently only available in TRL backend Higher computational cost than standard GRPO May require tuning of sequential parameters See Also \u00b6 GRPO Algorithm - Base group optimization Algorithms Overview - All supported algorithms","title":"GSPO"},{"location":"algorithms/gspo/#gspo-group-sequential-policy-optimization","text":"GSPO is a variant of GRPO that performs sequential optimization within groups.","title":"GSPO (Group Sequential Policy Optimization)"},{"location":"algorithms/gspo/#overview","text":"Group Sequential Policy Optimization (GSPO) extends GRPO by performing sequential policy updates within each group, allowing for more fine-grained optimization of complex behaviors.","title":"Overview"},{"location":"algorithms/gspo/#key-features","text":"Sequential updates : Multiple optimization steps per group Fine-grained control : Better handling of complex reward landscapes TRL-only : Currently only available in TRL backend","title":"Key Features"},{"location":"algorithms/gspo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only available in TRL num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/gspo/#algorithm-details","text":"GSPO performs multiple sequential policy updates within each group, allowing for: Progressive refinement : Each step improves upon the previous Complex optimization : Better handling of multi-objective rewards Stability : Reduced variance through sequential updates","title":"Algorithm Details"},{"location":"algorithms/gspo/#configuration-options","text":"Parameter Type Default Description group_size int 4 Number of samples per group seq_steps int 3 Number of sequential steps beta float 0.01 KL penalty coefficient","title":"Configuration Options"},{"location":"algorithms/gspo/#limitations","text":"Currently only available in TRL backend Higher computational cost than standard GRPO May require tuning of sequential parameters","title":"Limitations"},{"location":"algorithms/gspo/#see-also","text":"GRPO Algorithm - Base group optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/overview/","text":"RL Algorithms Overview \u00b6 AlignTune supports a comprehensive set of Reinforcement Learning algorithms for aligning Large Language Models with human preferences and optimizing for specific objectives. Algorithm Comparison \u00b6 Algorithm TRL Backend Unsloth Backend Reward Model Description DPO Yes Yes No Direct Preference Optimization - No reward model needed PPO Yes Yes Yes Proximal Policy Optimization - Requires reward model GRPO Yes Yes No Group Relative Policy Optimization - Multi-criteria optimization GSPO Yes No No Group Sequential Policy Optimization - Sequential group learning (TRL only) DAPO Yes Yes No Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes No GRPO Done Right - Unbiased GRPO variant Algorithm Selection Guide \u00b6 When to Use DPO \u00b6 Use DPO when: - You have preference data (chosen vs rejected pairs) - You want to avoid training a separate reward model - You need fast training with minimal setup - You're working with preference datasets like Anthropic/hh-rlhf Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) When to Use PPO \u00b6 Use PPO when: - You have a reward model (or want to train one) - You need fine-grained control over reward signals - You're doing online learning with environment interaction - You want to optimize for complex, multi-objective rewards Example : trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" ) When to Use GRPO \u00b6 Use GRPO when: - You want multi-criteria optimization - You're working with group-based preferences - You need to optimize for multiple objectives simultaneously - You want to avoid reward model training Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" ) When to Use GSPO \u00b6 Use GSPO when: - You need sequential group learning - You're working with sequential decision-making tasks - You want TRL backend (Unsloth not supported) Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" # Only TRL supports GSPO ) When to Use DAPO \u00b6 Use DAPO when: - You want to address GRPO limitations - You need decoupled clipping and dynamic sampling - You're working with large-scale RLHF Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" ) When to Use Dr. GRPO \u00b6 Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" ) When to Use Dr. GRPO \u00b6 Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" ) Algorithm Details \u00b6 DPO - Direct Preference Optimization PPO - Proximal Policy Optimization GRPO - Group Relative Policy Optimization GSPO - Group Sequential Policy Optimization DAPO - Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO - GRPO Done Right Quick Reference \u00b6 Algorithm Requirements \u00b6 Algorithm Dataset Format Reward Model Special Requirements DPO Preference pairs Chosen/rejected pairs PPO Prompts Reward model required GRPO Group preferences Group structure GSPO Sequential groups Sequential structure DAPO Group preferences Group structure Dr. GRPO Group preferences Group structure Next Steps \u00b6 DPO Guide - Learn about Direct Preference Optimization PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Overview"},{"location":"algorithms/overview/#rl-algorithms-overview","text":"AlignTune supports a comprehensive set of Reinforcement Learning algorithms for aligning Large Language Models with human preferences and optimizing for specific objectives.","title":"RL Algorithms Overview"},{"location":"algorithms/overview/#algorithm-comparison","text":"Algorithm TRL Backend Unsloth Backend Reward Model Description DPO Yes Yes No Direct Preference Optimization - No reward model needed PPO Yes Yes Yes Proximal Policy Optimization - Requires reward model GRPO Yes Yes No Group Relative Policy Optimization - Multi-criteria optimization GSPO Yes No No Group Sequential Policy Optimization - Sequential group learning (TRL only) DAPO Yes Yes No Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes No GRPO Done Right - Unbiased GRPO variant","title":"Algorithm Comparison"},{"location":"algorithms/overview/#algorithm-selection-guide","text":"","title":"Algorithm Selection Guide"},{"location":"algorithms/overview/#when-to-use-dpo","text":"Use DPO when: - You have preference data (chosen vs rejected pairs) - You want to avoid training a separate reward model - You need fast training with minimal setup - You're working with preference datasets like Anthropic/hh-rlhf Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"When to Use DPO"},{"location":"algorithms/overview/#when-to-use-ppo","text":"Use PPO when: - You have a reward model (or want to train one) - You need fine-grained control over reward signals - You're doing online learning with environment interaction - You want to optimize for complex, multi-objective rewards Example : trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" )","title":"When to Use PPO"},{"location":"algorithms/overview/#when-to-use-grpo","text":"Use GRPO when: - You want multi-criteria optimization - You're working with group-based preferences - You need to optimize for multiple objectives simultaneously - You want to avoid reward model training Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" )","title":"When to Use GRPO"},{"location":"algorithms/overview/#when-to-use-gspo","text":"Use GSPO when: - You need sequential group learning - You're working with sequential decision-making tasks - You want TRL backend (Unsloth not supported) Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" # Only TRL supports GSPO )","title":"When to Use GSPO"},{"location":"algorithms/overview/#when-to-use-dapo","text":"Use DAPO when: - You want to address GRPO limitations - You need decoupled clipping and dynamic sampling - You're working with large-scale RLHF Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" )","title":"When to Use DAPO"},{"location":"algorithms/overview/#when-to-use-dr-grpo","text":"Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" )","title":"When to Use Dr. GRPO"},{"location":"algorithms/overview/#when-to-use-dr-grpo_1","text":"Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" )","title":"When to Use Dr. GRPO"},{"location":"algorithms/overview/#algorithm-details","text":"DPO - Direct Preference Optimization PPO - Proximal Policy Optimization GRPO - Group Relative Policy Optimization GSPO - Group Sequential Policy Optimization DAPO - Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO - GRPO Done Right","title":"Algorithm Details"},{"location":"algorithms/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"algorithms/overview/#algorithm-requirements","text":"Algorithm Dataset Format Reward Model Special Requirements DPO Preference pairs Chosen/rejected pairs PPO Prompts Reward model required GRPO Group preferences Group structure GSPO Sequential groups Sequential structure DAPO Group preferences Group structure Dr. GRPO Group preferences Group structure","title":"Algorithm Requirements"},{"location":"algorithms/overview/#next-steps","text":"DPO Guide - Learn about Direct Preference Optimization PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"algorithms/ppo/","text":"PPO (Proximal Policy Optimization) \u00b6 Proximal Policy Optimization (PPO) is a powerful RLHF algorithm that uses a reward model to optimize language model behavior through policy gradient methods. Overview \u00b6 PPO optimizes the policy by generating responses, evaluating them with a reward model, and updating the policy to maximize expected reward while staying close to the reference policy. Key Features \u00b6 Reward Model : Uses neural reward models for fine-grained control Online Learning : Generates and evaluates responses during training Both Backends : Supported by TRL and Unsloth Flexible : Works with custom reward models and functions How It Works \u00b6 PPO alternates between: 1. Rollout Phase : Generate responses from the current policy 2. Evaluation Phase : Score responses with reward model 3. Update Phase : Update policy to maximize reward (with KL constraint) Mathematical Formulation \u00b6 The PPO objective maximizes: L_PPO = E[min(r_t * A_t, clip(r_t, 1-\u03b5, 1+\u03b5) * A_t)] Where: - r_t : Probability ratio between policy and reference - A_t : Advantage estimate - \u03b5 : Clipping parameter Usage \u00b6 Basic PPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , # Required num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO with Unsloth Backend \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO with Custom Reward Model \u00b6 # Train custom reward model first from aligntune.rewards.training import RewardModelTrainer reward_trainer = RewardModelTrainer ( base_model_name = \"Qwen/Qwen3-0.6B\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], composite_weights = [ 0.4 , 0.3 , 0.3 ] ) reward_model_path = reward_trainer . train () # Use in PPO training ppo_trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = reward_model_path , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) PPO with Reward Functions \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_functions = [ \"helpfulness\" , \"safety\" ], reward_function_weights = [ 0.6 , 0.4 ], num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) Configuration Parameters \u00b6 PPO-Specific Parameters \u00b6 Parameter Type Default Description cliprange float 0.2 PPO clipping range kl_coef float 0.1 KL penalty coefficient vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda parameter num_generations int 8 Number of generations per prompt num_ppo_epochs int 4 Number of PPO update epochs Reward Model Parameters \u00b6 Parameter Type Default Description reward_model_name str None HuggingFace reward model ID reward_model_path str None Local reward model path reward_functions list [] List of reward function names reward_function_weights list [] Weights for reward functions Dataset Format \u00b6 PPO requires prompts (not preference pairs): { \"prompt\" : \"What is machine learning?\" , # Responses are generated during training } Supported Datasets \u00b6 Anthropic/hh-rlhf : Can extract prompts OpenAssistant/oasst1 : Conversation prompts Custom datasets with prompt column Best Practices \u00b6 1. Reward Model Selection \u00b6 Use pre-trained models : HuggingFace reward models Train custom models : For domain-specific tasks Use reward functions : For rule-based rewards 2. Clipping Range \u00b6 Conservative (0.1-0.2) : Stable training, slower updates Aggressive (0.2-0.3) : Faster updates, risk of instability 3. KL Coefficient \u00b6 Low (0.01-0.1) : More exploration, risk of divergence High (0.1-0.5) : More conservative, stays close to reference 4. Number of Generations \u00b6 Few (4-8) : Faster training, less exploration Many (8-16) : More exploration, slower training Evaluation \u00b6 PPO evaluation includes: config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also Common Issues \u00b6 Reward Model Not Found \u00b6 Solutions : - Verify reward model name/path - Check HuggingFace authentication - Train custom reward model Training Instability \u00b6 Solutions : - Reduce learning rate - Increase KL coefficient - Reduce clipping range - Use smaller batch size Low Rewards \u00b6 Solutions : - Check reward model quality - Verify reward model is appropriate for task - Adjust reward function weights - Increase number of generations Comparison with Other Algorithms \u00b6 Feature PPO DPO GRPO Reward Model Required Not needed Not needed Training Speed Slow Fast Medium Setup Complexity High Low Medium Reward Control Fine-grained Coarse Medium References \u00b6 Proximal Policy Optimization Paper TRL PPO Documentation Next Steps \u00b6 DPO Guide - Learn about Direct Preference Optimization GRPO Guide - Learn about Group Relative Policy Optimization Reward Model Training - Train custom reward models","title":"PPO"},{"location":"algorithms/ppo/#ppo-proximal-policy-optimization","text":"Proximal Policy Optimization (PPO) is a powerful RLHF algorithm that uses a reward model to optimize language model behavior through policy gradient methods.","title":"PPO (Proximal Policy Optimization)"},{"location":"algorithms/ppo/#overview","text":"PPO optimizes the policy by generating responses, evaluating them with a reward model, and updating the policy to maximize expected reward while staying close to the reference policy.","title":"Overview"},{"location":"algorithms/ppo/#key-features","text":"Reward Model : Uses neural reward models for fine-grained control Online Learning : Generates and evaluates responses during training Both Backends : Supported by TRL and Unsloth Flexible : Works with custom reward models and functions","title":"Key Features"},{"location":"algorithms/ppo/#how-it-works","text":"PPO alternates between: 1. Rollout Phase : Generate responses from the current policy 2. Evaluation Phase : Score responses with reward model 3. Update Phase : Update policy to maximize reward (with KL constraint)","title":"How It Works"},{"location":"algorithms/ppo/#mathematical-formulation","text":"The PPO objective maximizes: L_PPO = E[min(r_t * A_t, clip(r_t, 1-\u03b5, 1+\u03b5) * A_t)] Where: - r_t : Probability ratio between policy and reference - A_t : Advantage estimate - \u03b5 : Clipping parameter","title":"Mathematical Formulation"},{"location":"algorithms/ppo/#usage","text":"","title":"Usage"},{"location":"algorithms/ppo/#basic-ppo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , # Required num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"Basic PPO Training"},{"location":"algorithms/ppo/#ppo-with-unsloth-backend","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO with Unsloth Backend"},{"location":"algorithms/ppo/#ppo-with-custom-reward-model","text":"# Train custom reward model first from aligntune.rewards.training import RewardModelTrainer reward_trainer = RewardModelTrainer ( base_model_name = \"Qwen/Qwen3-0.6B\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], composite_weights = [ 0.4 , 0.3 , 0.3 ] ) reward_model_path = reward_trainer . train () # Use in PPO training ppo_trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = reward_model_path , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 )","title":"PPO with Custom Reward Model"},{"location":"algorithms/ppo/#ppo-with-reward-functions","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_functions = [ \"helpfulness\" , \"safety\" ], reward_function_weights = [ 0.6 , 0.4 ], num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 )","title":"PPO with Reward Functions"},{"location":"algorithms/ppo/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"algorithms/ppo/#ppo-specific-parameters","text":"Parameter Type Default Description cliprange float 0.2 PPO clipping range kl_coef float 0.1 KL penalty coefficient vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda parameter num_generations int 8 Number of generations per prompt num_ppo_epochs int 4 Number of PPO update epochs","title":"PPO-Specific Parameters"},{"location":"algorithms/ppo/#reward-model-parameters","text":"Parameter Type Default Description reward_model_name str None HuggingFace reward model ID reward_model_path str None Local reward model path reward_functions list [] List of reward function names reward_function_weights list [] Weights for reward functions","title":"Reward Model Parameters"},{"location":"algorithms/ppo/#dataset-format","text":"PPO requires prompts (not preference pairs): { \"prompt\" : \"What is machine learning?\" , # Responses are generated during training }","title":"Dataset Format"},{"location":"algorithms/ppo/#supported-datasets","text":"Anthropic/hh-rlhf : Can extract prompts OpenAssistant/oasst1 : Conversation prompts Custom datasets with prompt column","title":"Supported Datasets"},{"location":"algorithms/ppo/#best-practices","text":"","title":"Best Practices"},{"location":"algorithms/ppo/#1-reward-model-selection","text":"Use pre-trained models : HuggingFace reward models Train custom models : For domain-specific tasks Use reward functions : For rule-based rewards","title":"1. Reward Model Selection"},{"location":"algorithms/ppo/#2-clipping-range","text":"Conservative (0.1-0.2) : Stable training, slower updates Aggressive (0.2-0.3) : Faster updates, risk of instability","title":"2. Clipping Range"},{"location":"algorithms/ppo/#3-kl-coefficient","text":"Low (0.01-0.1) : More exploration, risk of divergence High (0.1-0.5) : More conservative, stays close to reference","title":"3. KL Coefficient"},{"location":"algorithms/ppo/#4-number-of-generations","text":"Few (4-8) : Faster training, less exploration Many (8-16) : More exploration, slower training","title":"4. Number of Generations"},{"location":"algorithms/ppo/#evaluation","text":"PPO evaluation includes: config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also","title":"Evaluation"},{"location":"algorithms/ppo/#common-issues","text":"","title":"Common Issues"},{"location":"algorithms/ppo/#reward-model-not-found","text":"Solutions : - Verify reward model name/path - Check HuggingFace authentication - Train custom reward model","title":"Reward Model Not Found"},{"location":"algorithms/ppo/#training-instability","text":"Solutions : - Reduce learning rate - Increase KL coefficient - Reduce clipping range - Use smaller batch size","title":"Training Instability"},{"location":"algorithms/ppo/#low-rewards","text":"Solutions : - Check reward model quality - Verify reward model is appropriate for task - Adjust reward function weights - Increase number of generations","title":"Low Rewards"},{"location":"algorithms/ppo/#comparison-with-other-algorithms","text":"Feature PPO DPO GRPO Reward Model Required Not needed Not needed Training Speed Slow Fast Medium Setup Complexity High Low Medium Reward Control Fine-grained Coarse Medium","title":"Comparison with Other Algorithms"},{"location":"algorithms/ppo/#references","text":"Proximal Policy Optimization Paper TRL PPO Documentation","title":"References"},{"location":"algorithms/ppo/#next-steps","text":"DPO Guide - Learn about Direct Preference Optimization GRPO Guide - Learn about Group Relative Policy Optimization Reward Model Training - Train custom reward models","title":"Next Steps"},{"location":"api-reference/api-reference/","text":"AlignTune API Reference \u00b6 Overview \u00b6 This document provides comprehensive documentation for all AlignTune APIs, including detailed parameter descriptions, configuration options, and usage examples. Core Configuration Classes \u00b6 UnifiedConfig \u00b6 The main configuration class that unifies all training parameters. from aligntune import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , # Required: Training algorithm model = ModelConfig ( ... ), # Required: Model configuration datasets = [ DatasetConfig ( ... )], # Required: Dataset configuration(s) train = TrainingConfig ( ... ), # Optional: Training parameters rewards = [ RewardConfig ( ... )], # Optional: Reward functions tasks = [ ... ], # Optional: Task definitions distributed = DistributedConfig ( ... ), # Optional: Distributed training logging = LoggingConfig ( ... ), # Optional: Logging configuration chat_template = \"auto\" , # Optional: Chat template caching = {}, # Optional: Caching configuration ) AlgorithmType \u00b6 Supported training algorithms. from aligntune import AlgorithmType # Available algorithms AlgorithmType . PPO # Proximal Policy Optimization AlgorithmType . DPO # Direct Preference Optimization AlgorithmType . GRPO # Group Relative Policy Optimization AlgorithmType . GSPO # Group Sequential Policy Optimization Model Configuration \u00b6 ModelConfig \u00b6 from aligntune import ModelConfig , PrecisionType model_config = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , # Required: Model name or path sft_path = None , # Optional: Path to SFT model reward_path = None , # Optional: Path to reward model precision = PrecisionType . BF16 , # Optional: Model precision quantization = {}, # Optional: Quantization config attn_implementation = \"auto\" , # Optional: Attention implementation gradient_checkpointing = True , # Optional: Enable gradient checkpointing max_memory = None , # Optional: Memory mapping use_unsloth = False , # Optional: Enable Unsloth max_seq_length = 2048 # Optional: Maximum sequence length ) ModelConfig Parameters \u00b6 Parameter Type Default Description name_or_path str Required HuggingFace model name or local path sft_path str None Path to pre-trained SFT model reward_path str None Path to reward model for PPO precision PrecisionType BF16 Model precision (BF16, FP16, FP32) quantization Dict {} Quantization configuration attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool True Enable gradient checkpointing max_memory Dict None Memory mapping configuration use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length PrecisionType \u00b6 from aligntune import PrecisionType PrecisionType . BF16 # Brain Float 16 (recommended for modern GPUs) PrecisionType . FP16 # Float 16 (good for older GPUs) PrecisionType . FP32 # Float 32 (CPU training) Dataset Configuration \u00b6 DatasetConfig \u00b6 from aligntune import DatasetConfig dataset_config = DatasetConfig ( name = \"Anthropic/hh-rlhf\" , # Required: Dataset name split = \"train\" , # Optional: Dataset split percent = 100 , # Optional: Percentage to use max_samples = None , # Optional: Maximum samples task_type = \"conversation\" , # Optional: Task type weight = 1.0 , # Optional: Dataset weight column_mapping = {}, # Optional: Column mapping preprocessing = {} # Optional: Preprocessing config ) DatasetConfig Parameters \u00b6 Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use percent int 100 Percentage of dataset to use (1-100) max_samples int None Maximum number of samples task_type str \"conversation\" Type of task weight float 1.0 Weight for multi-dataset training column_mapping Dict {} Map dataset columns to expected names preprocessing Dict {} Preprocessing configuration Training Configuration \u00b6 TrainingConfig \u00b6 from aligntune import TrainingConfig training_config = TrainingConfig ( per_device_batch_size = 1 , # Required: Batch size per device gradient_accumulation_steps = 1 , # Optional: Gradient accumulation max_steps = 1000 , # Optional: Maximum training steps eval_interval = 100 , # Optional: Evaluation interval save_interval = 500 , # Optional: Save interval learning_rate = 5e-5 , # Optional: Learning rate weight_decay = 0.01 , # Optional: Weight decay warmup_steps = 100 , # Optional: Warmup steps max_length = 512 , # Optional: Maximum sequence length max_prompt_length = 256 , # Optional: Maximum prompt length temperature = 0.7 , # Optional: Generation temperature beta = 0.1 , # Optional: DPO beta parameter kl_coef = 0.1 , # Optional: PPO KL coefficient cliprange = 0.2 , # Optional: PPO clip range rollout_batch_size = 1 , # Optional: PPO rollout batch size ppo_epochs = 4 , # Optional: PPO epochs vf_coef = 0.1 , # Optional: PPO value function coefficient ent_coef = 0.0 , # Optional: PPO entropy coefficient target_kl = 0.01 , # Optional: PPO target KL init_kl_coef = 0.2 , # Optional: PPO initial KL coefficient adap_kl_ctrl = True , # Optional: PPO adaptive KL control gamma = 1.0 , # Optional: PPO discount factor lam = 0.95 , # Optional: PPO GAE lambda whiten_rewards = False # Optional: PPO reward whitening ) TrainingConfig Parameters \u00b6 Parameter Type Default Description per_device_batch_size int Required Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int 1000 Maximum training steps eval_interval int 100 Steps between evaluations save_interval int 500 Steps between saves learning_rate float 5e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 100 Warmup steps max_length int 512 Maximum sequence length max_prompt_length int 256 Maximum prompt length temperature float 0.7 Generation temperature beta float 0.1 DPO beta parameter kl_coef float 0.1 PPO KL coefficient cliprange float 0.2 PPO clip range rollout_batch_size int 1 PPO rollout batch size ppo_epochs int 4 PPO epochs per update vf_coef float 0.1 PPO value function coefficient ent_coef float 0.0 PPO entropy coefficient target_kl float 0.01 PPO target KL divergence init_kl_coef float 0.2 PPO initial KL coefficient adap_kl_ctrl bool True PPO adaptive KL control gamma float 1.0 PPO discount factor lam float 0.95 PPO GAE lambda whiten_rewards bool False PPO reward whitening Reward Configuration \u00b6 RewardConfig \u00b6 from aligntune import RewardConfig , RewardType reward_config = RewardConfig ( reward_type = RewardType . SAFETY , # Required: Reward type weight = 1.0 , # Optional: Reward weight params = {}, # Optional: Reward parameters model_name = None , # Optional: Model for reward device = \"auto\" , # Optional: Device for reward cache_dir = None # Optional: Cache directory ) RewardConfig Parameters \u00b6 Parameter Type Default Description reward_type RewardType Required Type of reward function weight float 1.0 Weight of this reward params Dict {} Parameters for reward function model_name str None Model name for model-based rewards device str \"auto\" Device for reward computation cache_dir str None Cache directory for models Distributed Configuration \u00b6 DistributedConfig \u00b6 from aligntune import DistributedConfig , BackendType distributed_config = DistributedConfig ( backend = BackendType . SINGLE , # Required: Distributed backend num_processes = 1 , # Optional: Number of processes master_addr = \"localhost\" , # Optional: Master address master_port = 29500 , # Optional: Master port seed = 42 , # Optional: Random seed deepspeed_config = {}, # Optional: DeepSpeed config fsdp_config = {} # Optional: FSDP config ) DistributedConfig Parameters \u00b6 Parameter Type Default Description backend BackendType Required Distributed backend type num_processes int 1 Number of processes master_addr str \"localhost\" Master node address master_port int 29500 Master node port seed int 42 Random seed deepspeed_config Dict {} DeepSpeed configuration fsdp_config Dict {} FSDP configuration BackendType \u00b6 from aligntune import BackendType BackendType . SINGLE # Single GPU/CPU training BackendType . DDP # Distributed Data Parallel BackendType . FSDP # Fully Sharded Data Parallel BackendType . DEEPSPEED # DeepSpeed ZeRO Logging Configuration \u00b6 LoggingConfig \u00b6 from aligntune import LoggingConfig logging_config = LoggingConfig ( output_dir = \"./output\" , # Required: Output directory run_name = \"my_experiment\" , # Optional: Run name loggers = [ \"tensorboard\" ], # Optional: Logging backends level = \"INFO\" , # Optional: Log level save_steps = 500 , # Optional: Save interval eval_steps = 100 , # Optional: Eval interval report_to = [ \"tensorboard\" ], # Optional: Reporting backends logging_dir = \"./logs\" , # Optional: Logging directory disable_tqdm = False , # Optional: Disable progress bars push_to_hub = False , # Optional: Push to HuggingFace Hub hub_model_id = None , # Optional: Hub model ID hub_token = None # Optional: Hub token ) LoggingConfig Parameters \u00b6 Parameter Type Default Description output_dir str Required Output directory for checkpoints run_name str None Name for this training run loggers List[str] [\"tensorboard\"] Logging backends level str \"INFO\" Logging level save_steps int 500 Steps between saves eval_steps int 100 Steps between evaluations report_to List[str] [\"tensorboard\"] Reporting backends logging_dir str \"./logs\" Logging directory disable_tqdm bool False Disable progress bars push_to_hub bool False Push to HuggingFace Hub hub_model_id str None Hub model ID hub_token str None Hub authentication token Backend Factory \u00b6 BackendFactory \u00b6 from aligntune import BackendFactory , BackendConfig , TrainingType , BackendType , RLAlgorithm # Create trainer with specific backend backend_config = BackendConfig ( training_type = TrainingType . RL , backend = BackendType . TRL , algorithm = RLAlgorithm . DPO ) trainer = BackendFactory . create_trainer ( config , backend_config ) Convenience Functions \u00b6 from aligntune import create_sft_trainer , create_rl_trainer # Create SFT trainer sft_trainer = create_sft_trainer ( config , backend = \"trl\" ) # Create RL trainer rl_trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) Configuration Loading \u00b6 ConfigLoader \u00b6 from aligntune import ConfigLoader # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Load from dictionary config = ConfigLoader . load_from_dict ( config_dict ) # Validate configuration ConfigLoader . validate_config ( config ) # Save resolved configuration ConfigLoader . save_resolved_config ( config , output_dir ) Complete Example \u00b6 from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , RewardConfig , RewardType , DistributedConfig , BackendType , LoggingConfig , create_rl_trainer ) # Create complete configuration config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , precision = PrecisionType . BF16 , use_unsloth = True , max_seq_length = 2048 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , split = \"train\" , percent = 10 , max_samples = 1000 ) ], train = TrainingConfig ( per_device_batch_size = 2 , gradient_accumulation_steps = 4 , max_steps = 1000 , learning_rate = 5e-5 , beta = 0.1 , temperature = 0.7 ), rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ) ], distributed = DistributedConfig ( backend = BackendType . SINGLE , seed = 42 ), logging = LoggingConfig ( output_dir = \"./output/dpo_experiment\" , run_name = \"dpo_hh_rlhf\" , loggers = [ \"tensorboard\" , \"wandb\" ] ) ) # Create and train trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) trainer . train () For more examples and use cases, see the Examples Documentation and Reward Functions Reference .","title":"AlignTune API Reference"},{"location":"api-reference/api-reference/#aligntune-api-reference","text":"","title":"AlignTune API Reference"},{"location":"api-reference/api-reference/#overview","text":"This document provides comprehensive documentation for all AlignTune APIs, including detailed parameter descriptions, configuration options, and usage examples.","title":"Overview"},{"location":"api-reference/api-reference/#core-configuration-classes","text":"","title":"Core Configuration Classes"},{"location":"api-reference/api-reference/#unifiedconfig","text":"The main configuration class that unifies all training parameters. from aligntune import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , # Required: Training algorithm model = ModelConfig ( ... ), # Required: Model configuration datasets = [ DatasetConfig ( ... )], # Required: Dataset configuration(s) train = TrainingConfig ( ... ), # Optional: Training parameters rewards = [ RewardConfig ( ... )], # Optional: Reward functions tasks = [ ... ], # Optional: Task definitions distributed = DistributedConfig ( ... ), # Optional: Distributed training logging = LoggingConfig ( ... ), # Optional: Logging configuration chat_template = \"auto\" , # Optional: Chat template caching = {}, # Optional: Caching configuration )","title":"UnifiedConfig"},{"location":"api-reference/api-reference/#algorithmtype","text":"Supported training algorithms. from aligntune import AlgorithmType # Available algorithms AlgorithmType . PPO # Proximal Policy Optimization AlgorithmType . DPO # Direct Preference Optimization AlgorithmType . GRPO # Group Relative Policy Optimization AlgorithmType . GSPO # Group Sequential Policy Optimization","title":"AlgorithmType"},{"location":"api-reference/api-reference/#model-configuration","text":"","title":"Model Configuration"},{"location":"api-reference/api-reference/#modelconfig","text":"from aligntune import ModelConfig , PrecisionType model_config = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , # Required: Model name or path sft_path = None , # Optional: Path to SFT model reward_path = None , # Optional: Path to reward model precision = PrecisionType . BF16 , # Optional: Model precision quantization = {}, # Optional: Quantization config attn_implementation = \"auto\" , # Optional: Attention implementation gradient_checkpointing = True , # Optional: Enable gradient checkpointing max_memory = None , # Optional: Memory mapping use_unsloth = False , # Optional: Enable Unsloth max_seq_length = 2048 # Optional: Maximum sequence length )","title":"ModelConfig"},{"location":"api-reference/api-reference/#modelconfig-parameters","text":"Parameter Type Default Description name_or_path str Required HuggingFace model name or local path sft_path str None Path to pre-trained SFT model reward_path str None Path to reward model for PPO precision PrecisionType BF16 Model precision (BF16, FP16, FP32) quantization Dict {} Quantization configuration attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool True Enable gradient checkpointing max_memory Dict None Memory mapping configuration use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length","title":"ModelConfig Parameters"},{"location":"api-reference/api-reference/#precisiontype","text":"from aligntune import PrecisionType PrecisionType . BF16 # Brain Float 16 (recommended for modern GPUs) PrecisionType . FP16 # Float 16 (good for older GPUs) PrecisionType . FP32 # Float 32 (CPU training)","title":"PrecisionType"},{"location":"api-reference/api-reference/#dataset-configuration","text":"","title":"Dataset Configuration"},{"location":"api-reference/api-reference/#datasetconfig","text":"from aligntune import DatasetConfig dataset_config = DatasetConfig ( name = \"Anthropic/hh-rlhf\" , # Required: Dataset name split = \"train\" , # Optional: Dataset split percent = 100 , # Optional: Percentage to use max_samples = None , # Optional: Maximum samples task_type = \"conversation\" , # Optional: Task type weight = 1.0 , # Optional: Dataset weight column_mapping = {}, # Optional: Column mapping preprocessing = {} # Optional: Preprocessing config )","title":"DatasetConfig"},{"location":"api-reference/api-reference/#datasetconfig-parameters","text":"Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use percent int 100 Percentage of dataset to use (1-100) max_samples int None Maximum number of samples task_type str \"conversation\" Type of task weight float 1.0 Weight for multi-dataset training column_mapping Dict {} Map dataset columns to expected names preprocessing Dict {} Preprocessing configuration","title":"DatasetConfig Parameters"},{"location":"api-reference/api-reference/#training-configuration","text":"","title":"Training Configuration"},{"location":"api-reference/api-reference/#trainingconfig","text":"from aligntune import TrainingConfig training_config = TrainingConfig ( per_device_batch_size = 1 , # Required: Batch size per device gradient_accumulation_steps = 1 , # Optional: Gradient accumulation max_steps = 1000 , # Optional: Maximum training steps eval_interval = 100 , # Optional: Evaluation interval save_interval = 500 , # Optional: Save interval learning_rate = 5e-5 , # Optional: Learning rate weight_decay = 0.01 , # Optional: Weight decay warmup_steps = 100 , # Optional: Warmup steps max_length = 512 , # Optional: Maximum sequence length max_prompt_length = 256 , # Optional: Maximum prompt length temperature = 0.7 , # Optional: Generation temperature beta = 0.1 , # Optional: DPO beta parameter kl_coef = 0.1 , # Optional: PPO KL coefficient cliprange = 0.2 , # Optional: PPO clip range rollout_batch_size = 1 , # Optional: PPO rollout batch size ppo_epochs = 4 , # Optional: PPO epochs vf_coef = 0.1 , # Optional: PPO value function coefficient ent_coef = 0.0 , # Optional: PPO entropy coefficient target_kl = 0.01 , # Optional: PPO target KL init_kl_coef = 0.2 , # Optional: PPO initial KL coefficient adap_kl_ctrl = True , # Optional: PPO adaptive KL control gamma = 1.0 , # Optional: PPO discount factor lam = 0.95 , # Optional: PPO GAE lambda whiten_rewards = False # Optional: PPO reward whitening )","title":"TrainingConfig"},{"location":"api-reference/api-reference/#trainingconfig-parameters","text":"Parameter Type Default Description per_device_batch_size int Required Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int 1000 Maximum training steps eval_interval int 100 Steps between evaluations save_interval int 500 Steps between saves learning_rate float 5e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 100 Warmup steps max_length int 512 Maximum sequence length max_prompt_length int 256 Maximum prompt length temperature float 0.7 Generation temperature beta float 0.1 DPO beta parameter kl_coef float 0.1 PPO KL coefficient cliprange float 0.2 PPO clip range rollout_batch_size int 1 PPO rollout batch size ppo_epochs int 4 PPO epochs per update vf_coef float 0.1 PPO value function coefficient ent_coef float 0.0 PPO entropy coefficient target_kl float 0.01 PPO target KL divergence init_kl_coef float 0.2 PPO initial KL coefficient adap_kl_ctrl bool True PPO adaptive KL control gamma float 1.0 PPO discount factor lam float 0.95 PPO GAE lambda whiten_rewards bool False PPO reward whitening","title":"TrainingConfig Parameters"},{"location":"api-reference/api-reference/#reward-configuration","text":"","title":"Reward Configuration"},{"location":"api-reference/api-reference/#rewardconfig","text":"from aligntune import RewardConfig , RewardType reward_config = RewardConfig ( reward_type = RewardType . SAFETY , # Required: Reward type weight = 1.0 , # Optional: Reward weight params = {}, # Optional: Reward parameters model_name = None , # Optional: Model for reward device = \"auto\" , # Optional: Device for reward cache_dir = None # Optional: Cache directory )","title":"RewardConfig"},{"location":"api-reference/api-reference/#rewardconfig-parameters","text":"Parameter Type Default Description reward_type RewardType Required Type of reward function weight float 1.0 Weight of this reward params Dict {} Parameters for reward function model_name str None Model name for model-based rewards device str \"auto\" Device for reward computation cache_dir str None Cache directory for models","title":"RewardConfig Parameters"},{"location":"api-reference/api-reference/#distributed-configuration","text":"","title":"Distributed Configuration"},{"location":"api-reference/api-reference/#distributedconfig","text":"from aligntune import DistributedConfig , BackendType distributed_config = DistributedConfig ( backend = BackendType . SINGLE , # Required: Distributed backend num_processes = 1 , # Optional: Number of processes master_addr = \"localhost\" , # Optional: Master address master_port = 29500 , # Optional: Master port seed = 42 , # Optional: Random seed deepspeed_config = {}, # Optional: DeepSpeed config fsdp_config = {} # Optional: FSDP config )","title":"DistributedConfig"},{"location":"api-reference/api-reference/#distributedconfig-parameters","text":"Parameter Type Default Description backend BackendType Required Distributed backend type num_processes int 1 Number of processes master_addr str \"localhost\" Master node address master_port int 29500 Master node port seed int 42 Random seed deepspeed_config Dict {} DeepSpeed configuration fsdp_config Dict {} FSDP configuration","title":"DistributedConfig Parameters"},{"location":"api-reference/api-reference/#backendtype","text":"from aligntune import BackendType BackendType . SINGLE # Single GPU/CPU training BackendType . DDP # Distributed Data Parallel BackendType . FSDP # Fully Sharded Data Parallel BackendType . DEEPSPEED # DeepSpeed ZeRO","title":"BackendType"},{"location":"api-reference/api-reference/#logging-configuration","text":"","title":"Logging Configuration"},{"location":"api-reference/api-reference/#loggingconfig","text":"from aligntune import LoggingConfig logging_config = LoggingConfig ( output_dir = \"./output\" , # Required: Output directory run_name = \"my_experiment\" , # Optional: Run name loggers = [ \"tensorboard\" ], # Optional: Logging backends level = \"INFO\" , # Optional: Log level save_steps = 500 , # Optional: Save interval eval_steps = 100 , # Optional: Eval interval report_to = [ \"tensorboard\" ], # Optional: Reporting backends logging_dir = \"./logs\" , # Optional: Logging directory disable_tqdm = False , # Optional: Disable progress bars push_to_hub = False , # Optional: Push to HuggingFace Hub hub_model_id = None , # Optional: Hub model ID hub_token = None # Optional: Hub token )","title":"LoggingConfig"},{"location":"api-reference/api-reference/#loggingconfig-parameters","text":"Parameter Type Default Description output_dir str Required Output directory for checkpoints run_name str None Name for this training run loggers List[str] [\"tensorboard\"] Logging backends level str \"INFO\" Logging level save_steps int 500 Steps between saves eval_steps int 100 Steps between evaluations report_to List[str] [\"tensorboard\"] Reporting backends logging_dir str \"./logs\" Logging directory disable_tqdm bool False Disable progress bars push_to_hub bool False Push to HuggingFace Hub hub_model_id str None Hub model ID hub_token str None Hub authentication token","title":"LoggingConfig Parameters"},{"location":"api-reference/api-reference/#backend-factory","text":"","title":"Backend Factory"},{"location":"api-reference/api-reference/#backendfactory","text":"from aligntune import BackendFactory , BackendConfig , TrainingType , BackendType , RLAlgorithm # Create trainer with specific backend backend_config = BackendConfig ( training_type = TrainingType . RL , backend = BackendType . TRL , algorithm = RLAlgorithm . DPO ) trainer = BackendFactory . create_trainer ( config , backend_config )","title":"BackendFactory"},{"location":"api-reference/api-reference/#convenience-functions","text":"from aligntune import create_sft_trainer , create_rl_trainer # Create SFT trainer sft_trainer = create_sft_trainer ( config , backend = \"trl\" ) # Create RL trainer rl_trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" )","title":"Convenience Functions"},{"location":"api-reference/api-reference/#configuration-loading","text":"","title":"Configuration Loading"},{"location":"api-reference/api-reference/#configloader","text":"from aligntune import ConfigLoader # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Load from dictionary config = ConfigLoader . load_from_dict ( config_dict ) # Validate configuration ConfigLoader . validate_config ( config ) # Save resolved configuration ConfigLoader . save_resolved_config ( config , output_dir )","title":"ConfigLoader"},{"location":"api-reference/api-reference/#complete-example","text":"from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , RewardConfig , RewardType , DistributedConfig , BackendType , LoggingConfig , create_rl_trainer ) # Create complete configuration config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , precision = PrecisionType . BF16 , use_unsloth = True , max_seq_length = 2048 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , split = \"train\" , percent = 10 , max_samples = 1000 ) ], train = TrainingConfig ( per_device_batch_size = 2 , gradient_accumulation_steps = 4 , max_steps = 1000 , learning_rate = 5e-5 , beta = 0.1 , temperature = 0.7 ), rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ) ], distributed = DistributedConfig ( backend = BackendType . SINGLE , seed = 42 ), logging = LoggingConfig ( output_dir = \"./output/dpo_experiment\" , run_name = \"dpo_hh_rlhf\" , loggers = [ \"tensorboard\" , \"wandb\" ] ) ) # Create and train trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) trainer . train () For more examples and use cases, see the Examples Documentation and Reward Functions Reference .","title":"Complete Example"},{"location":"api-reference/backend-factory/","text":"Backend Factory API \u00b6 The Backend Factory is the main entry point for creating trainers in AlignTune. Overview \u00b6 The BackendFactory provides functions to create SFT and RL trainers with automatic backend selection and fallback handling. BackendFactory Class \u00b6 Complete API reference for the BackendFactory class. Factory for creating training backends. Source code in src/aligntune/core/backend_factory.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 class BackendFactory : \"\"\"Factory for creating training backends.\"\"\" # Registry of available backends _backends : Dict [ tuple , Type [ TrainerBase ]] = {} @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) @classmethod def _select_best_backend ( cls , backend_config : BackendConfig ) -> BackendConfig : \"\"\"Select the best available backend with fallback.\"\"\" # Check if requested backend is available if cls . _is_backend_available ( backend_config ): return backend_config # Try fallback backends if backend_config . fallback_enabled : fallback_order = cls . _get_fallback_order ( backend_config ) for fallback_backend in fallback_order : if cls . _is_backend_available ( fallback_backend ): logger . warning ( f \"Requested backend { backend_config . backend } not available, \" f \"falling back to { fallback_backend . backend } \" ) return fallback_backend raise RuntimeError ( f \"No available backend for { backend_config } \" ) @classmethod def _is_backend_available ( cls , backend_config : BackendConfig ) -> bool : \"\"\"Check if a backend is available.\"\"\" key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) trainer_class = cls . _backends . get ( key ) if trainer_class is None : return False # Check if the trainer class can be instantiated try : return trainer_class . is_available () except Exception : return False @classmethod def _get_fallback_order ( cls , backend_config : BackendConfig ) -> list : \"\"\"Get fallback order for backend selection.\"\"\" if backend_config . training_type == TrainingType . SFT : # SFT fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . SFT , BackendType . UNSLOTH ), BackendConfig ( TrainingType . SFT , BackendType . TRL ), ] else : # RL # RL fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . RL , BackendType . UNSLOTH , backend_config . algorithm ), BackendConfig ( TrainingType . RL , BackendType . TRL , backend_config . algorithm ), ] # Remove the original backend from fallbacks fallbacks = [ fb for fb in fallbacks if fb . backend != backend_config . backend ] return fallbacks @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config ) create_trainer ( config , backend_config ) classmethod \u00b6 Create a trainer instance based on backend configuration. Source code in src/aligntune/core/backend_factory.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config ) get_backend_capabilities ( backend_type ) \u00b6 Get capabilities for a specific backend type. Source code in src/aligntune/core/backend_factory.py 426 427 428 429 430 431 432 433 434 435 def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities get_recommended_backend ( training_type , algorithm = None ) classmethod \u00b6 Get recommended backend for given training type and algorithm. Source code in src/aligntune/core/backend_factory.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) is_backend_available ( backend_type ) \u00b6 Check if a specific backend type is available. Source code in src/aligntune/core/backend_factory.py 414 415 416 417 418 419 420 421 422 423 424 def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False list_available_backends () classmethod \u00b6 List all available backends. Source code in src/aligntune/core/backend_factory.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available register_backend ( training_type , backend , algorithm = None , trainer_class = None ) classmethod \u00b6 Register a backend trainer class. Source code in src/aligntune/core/backend_factory.py 325 326 327 328 329 330 331 332 333 334 335 336 @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) options: show_source: true heading_level: 3 Factory Functions \u00b6 create_sft_trainer() \u00b6 Create a Supervised Fine-Tuning trainer. Create SFT trainer with specified backend and parameters. Source code in src/aligntune/core/backend_factory.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def create_sft_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , system_prompt : Optional [ str ] = None , # config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create SFT trainer with specified backend and parameters.\"\"\" if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"sft\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) # Create configuration # Build model config kwargs, only including precision if explicitly provided model_config_kwargs = { \"name_or_path\" : model_name , \"max_seq_length\" : max_seq_length , \"quantization\" : kwargs . get ( 'quantization' , {}), \"use_unsloth\" : kwargs . get ( 'use_unsloth' , False ), \"peft_enabled\" : kwargs . get ( 'use_peft' , kwargs . get ( 'peft_enabled' , False )), \"lora_rank\" : kwargs . get ( 'lora_r' , 16 ), \"lora_alpha\" : kwargs . get ( 'lora_alpha' , 32 ), \"lora_dropout\" : kwargs . get ( 'lora_dropout' , 0.1 ), \"target_modules\" : kwargs . get ( 'lora_target_modules' , None ), \"bias\" : kwargs . get ( 'lora_bias' , 'none' ), \"attn_implementation\" : kwargs . get ( 'attn_implementation' , 'auto' ), \"max_memory\" : kwargs . get ( 'max_memory' ), \"use_gradient_checkpointing\" : kwargs . get ( 'use_gradient_checkpointing' , True ), \"num_labels\" : kwargs . get ( 'num_labels' ), # For classification tasks \"model_init_kwargs\" : kwargs . get ( 'model_init_kwargs' , {}), \"device_map\" : kwargs . get ( 'device_map' , 'auto' ), \"trust_remote_code\" : kwargs . get ( 'trust_remote_code' , False ), } # Only add precision if explicitly provided (otherwise use default from ModelConfig) if 'precision' in kwargs and kwargs . get ( 'precision' ) is not None : # Validate precision value precision_value = kwargs . get ( 'precision' ) if isinstance ( precision_value , str ): # Convert string to PrecisionType enum from .sft.config import PrecisionType try : precision_value = PrecisionType ( precision_value . lower ()) except ValueError : logger . warning ( f \"Invalid precision ' { precision_value } ', using default\" ) precision_value = None if precision_value is not None : model_config_kwargs [ 'precision' ] = precision_value config = SFTConfig ( model = SFTModelConfig ( ** model_config_kwargs ), dataset = SFTDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), subset = kwargs . get ( 'subset' ), # Support dataset config/subset (e.g., for financial_phrasebank) config = kwargs . get ( 'config' ), # Alternative name for subset max_samples = max_samples , percent = kwargs . get ( 'percent' ), column_mapping = kwargs . get ( 'column_mapping' , {}), task_type = kwargs . get ( 'task_type' , 'supervised_fine_tuning' ), system_prompt = system_prompt , # dataset_kwargs=kwargs.get('dataset_kwargs', {}), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), dataset_text_field = kwargs . get ( 'dataset_text_field' , 'text' ), text_column = kwargs . get ( 'text_column' , kwargs . get ( 'dataset_text_field' , 'text' )), # For classification tasks label_column = kwargs . get ( 'label_column' , 'label' ), # For classification tasks # eos_token=kwargs.get('eos_token'), pad_token = kwargs . get ( 'pad_token' ), chat_template = kwargs . get ( 'chat_template' ), preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), ), train = SFTTrainingConfig ( epochs = num_epochs , max_steps = kwargs . get ( 'max_steps' ), per_device_batch_size = batch_size , learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.1 ), weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_steps' , kwargs . get ( 'save_interval' , 500 )), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), fp16 = kwargs . get ( 'fp16' , False ), bf16 = kwargs . get ( 'bf16' , False ), dataloader_num_workers = kwargs . get ( 'dataloader_num_workers' , 0 ), remove_unused_columns = kwargs . get ( 'remove_unused_columns' , False ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), group_by_length = kwargs . get ( 'group_by_length' , True ), dataloader_drop_last = kwargs . get ( 'dataloader_drop_last' , False ), eval_accumulation_steps = kwargs . get ( 'eval_accumulation_steps' ), label_smoothing_factor = kwargs . get ( 'label_smoothing_factor' , 0.0 ), early_stopping_patience = kwargs . get ( 'early_stopping_patience' ), early_stopping_threshold = kwargs . get ( 'early_stopping_threshold' , 0.0 ), load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , True ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' , 'eval_loss' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), use_trl = kwargs . get ( 'use_trl' , False ), dataset_num_proc = kwargs . get ( 'train_dataset_num_proc' ), dataset_kwargs = kwargs . get ( 'train_dataset_kwargs' , {}), packing = kwargs . get ( 'packing' , False ), packing_strategy = kwargs . get ( 'packing_strategy' , 'bfd' ), eval_packing = kwargs . get ( 'eval_packing' ), padding_free = kwargs . get ( 'padding_free' , False ), pad_to_multiple_of = kwargs . get ( 'pad_to_multiple_of' ), completion_only_loss = kwargs . get ( 'completion_only_loss' ), assistant_only_loss = kwargs . get ( 'assistant_only_loss' , False ), loss_type = kwargs . get ( 'loss_type' , 'nll' ), activation_offloading = kwargs . get ( 'activation_offloading' , False ), use_flash_attention_2 = kwargs . get ( 'use_flash_attention_2' ), gradient_checkpointing = kwargs . get ( 'gradient_checkpointing' , False ), gradient_checkpointing_kwargs = kwargs . get ( 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }), extra_params = kwargs , ), logging = SFTLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), log_level = kwargs . get ( 'log_level' , 'INFO' ), log_interval = kwargs . get ( 'logging_steps' , kwargs . get ( 'log_interval' , 10 )), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), eval_strategy = kwargs . get ( 'eval_strategy' , 'steps' ), report_to = kwargs . get ( 'report_to' , \"none\" ), ), evaluation = SFTEvaluationConfig ( compute_perplexity = kwargs . get ( 'compute_perplexity' , True ), compute_rouge = kwargs . get ( 'compute_rouge' , True ), compute_bleu = kwargs . get ( 'compute_bleu' , True ), compute_meteor = kwargs . get ( 'compute_meteor' , False ), compute_bertscore = kwargs . get ( 'compute_bertscore' , False ), compute_semantic_similarity = kwargs . get ( 'compute_semantic_similarity' , False ), compute_codebleu = kwargs . get ( 'compute_codebleu' , False ), max_samples_for_quality_metrics = kwargs . get ( 'max_samples_for_quality_metrics' , 50 ), bertscore_model = kwargs . get ( 'bertscore_model' , 'microsoft/deberta-xlarge-mnli' ), semantic_similarity_model = kwargs . get ( 'semantic_similarity_model' , 'sentence-transformers/all-MiniLM-L6-v2' ) ) ) # Create backend config if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . SFT ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . SFT , backend_type ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 ) trainer . train () create_rl_trainer() \u00b6 Create a Reinforcement Learning trainer. Create RL trainer with specified algorithm and backend. Source code in src/aligntune/core/backend_factory.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def create_rl_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , algorithm : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , max_steps : Optional [ int ] = 100 , # Add max_steps parameter batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , reward_value_model : Optional [ str ] = None , reward_model_name : Optional [ str ] = None , # NEW reward_model_path : Optional [ str ] = None , # NEW: Local reward model path train_custom_reward_model : bool = False , # NEW: Train custom reward model reward_training_texts : Optional [ List [ str ]] = None , # NEW: Training texts for custom model reward_functions : Optional [ List [ str ]] = None , # NEW: Reward functions for custom model reward_function_weights : Optional [ List [ float ]] = None , # NEW: Weights for reward functions reward_training_base_model : Optional [ str ] = None , # NEW: Base model for custom training reward_training_output_dir : Optional [ str ] = None , # NEW: Output dir for custom training reward_value_loading_type : Optional [ str ] = None , reward_model_quantization : Optional [ Dict ] = None , value_model_quantization : Optional [ Dict ] = None , reward_training : Optional [ Dict [ str , Any ]] = None , # NEW: Flexible reward training config reward_device : str = \"auto\" , sample_logging : Optional [ Dict [ str , Any ]] = None , # NEW: Counterfactual GRPO specific parameters boost_factor : float = 2.0 , min_weight : float = 0.5 , max_spans : int = 10 , answer_weight : float = 1.5 , method_name : str = \"counterfactual\" , random_importance : bool = False , invert_importance : bool = False , enable_gradient_conservation : bool = True , weight_debug : bool = False , system_prompt : Optional [ str ] = None , # # NEW: GBMPO-specific parameters gbmpo_divergence_type : Optional [ str ] = None , config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create RL trainer with specified algorithm and backend.\"\"\" # NEW: Load config if provided# Load and parse config if provided if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"rl\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters from merged config model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) algorithm = algorithm or kwargs . pop ( 'algorithm' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) if algorithm is None : raise ValueError ( \"algorithm must be provided either as argument or in config\" ) reward_device = kwargs . pop ( 'reward_device' , reward_device or \"auto\" ) reward_device = reward_device or \"auto\" sample_logging_dict : Dict [ str , Any ] = {} base_sample_logging = sample_logging or kwargs . get ( 'sample_logging_config' ) if base_sample_logging : sample_logging_dict . update ( base_sample_logging ) inline_sample_logging = { \"enabled\" : kwargs . get ( 'enable_sample_logging' ), \"prompts\" : kwargs . get ( 'sample_logging_prompts' ), \"interval_steps\" : kwargs . get ( 'sample_logging_interval_steps' ), \"percent_of_max_steps\" : kwargs . get ( 'sample_logging_percent_of_max_steps' , kwargs . get ( 'sample_logging_percent' )), \"max_new_tokens\" : kwargs . get ( 'sample_logging_max_new_tokens' ), \"temperature\" : kwargs . get ( 'sample_logging_temperature' ), \"top_p\" : kwargs . get ( 'sample_logging_top_p' ), \"num_samples\" : kwargs . get ( 'sample_logging_num_samples' ), } for key , value in inline_sample_logging . items (): if value is not None : sample_logging_dict [ key ] = value cleaned_sample_logging = { k : v for k , v in sample_logging_dict . items () if v is not None } if cleaned_sample_logging : sample_logging_config = SampleLoggingConfig ( ** cleaned_sample_logging ) else : sample_logging_config = SampleLoggingConfig () needs_neural_reward = not any ([ reward_model_path , train_custom_reward_model , reward_functions ]) if reward_model_name is None and needs_neural_reward : reward_model_name = model_name # ============================================ # PPO MODEL CONSISTENCY WARNING # ============================================ if algorithm . lower () == \"ppo\" : print ( \"\u26a0\ufe0f NOTE: For optimal PPO performance, ensure policy_model, reward_model, and value_model are from the same model family\" ) # STRICT VALIDATION: Validate reward model configuration reward_model_source = None source_count = sum ([ reward_model_name is not None , reward_model_path is not None , train_custom_reward_model ]) if source_count > 1 : raise ValueError ( f \"Specify exactly ONE reward source. Found { source_count } : \" f \"reward_model_name= { reward_model_name } , \" f \"reward_model_path= { reward_model_path } , \" f \"train_custom_reward_model= { train_custom_reward_model } \" ) if train_custom_reward_model : # Validate ALL required fields for custom training if not reward_training_texts : raise ValueError ( \"reward_training_texts required when train_custom_reward_model=True\" ) if not reward_functions : raise ValueError ( \"reward_functions required when train_custom_reward_model=True\" ) if not reward_training_base_model : raise ValueError ( \"reward_training_base_model required when train_custom_reward_model=True\" ) if not reward_training_output_dir : raise ValueError ( \"reward_training_output_dir required when train_custom_reward_model=True\" ) # Import required classes from .rl.config import RewardModelTrainingConfig , RewardModelSourceConfig training_config = RewardModelTrainingConfig ( base_model_name = reward_training_base_model , training_texts = reward_training_texts , reward_functions = reward_functions , output_dir = reward_training_output_dir , # Use kwargs for optional training params num_epochs = kwargs . get ( 'reward_training_epochs' , 3 ), learning_rate = kwargs . get ( 'reward_training_lr' , 1e-5 ), batch_size = kwargs . get ( 'reward_training_batch_size' , 8 ), reward_weights = reward_function_weights ) reward_model_source = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) elif reward_model_name : from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = reward_model_name ) elif reward_model_path : # Validate path exists if not Path ( reward_model_path ) . exists (): raise FileNotFoundError ( f \"reward_model_path does not exist: { reward_model_path } \" ) from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = reward_model_path ) # Create reward training config if provided reward_training_config = None if reward_training : from .rl.config import RewardModelTrainingConfig reward_training_config = RewardModelTrainingConfig ( ** reward_training ) # Construct rewards config if reward_functions provided but not in kwargs # rewards_config = kwargs.get('rewards', []) rewards_config = kwargs . get ( 'rewards' ) or [] if not rewards_config and reward_functions and not train_custom_reward_model : weights = reward_function_weights or [ 1.0 ] * len ( reward_functions ) if len ( weights ) != len ( reward_functions ): logger . warning ( \"reward_function_weights length mismatch, using default 1.0\" ) weights = [ 1.0 ] * len ( reward_functions ) for func_name , weight in zip ( reward_functions , weights ): rewards_config . append ({ \"type\" : func_name , \"weight\" : weight , \"params\" : {} }) logger . info ( f \"Constructed rewards config from function list: { len ( rewards_config ) } functions\" ) # FIXED: Added required 'algo' parameter and used 'train' not 'training' if algorithm . lower () == \"counterfact_grpo\" : # Add counterfactual-specific params to kwargs for config creation kwargs . update ({ 'boost_factor' : boost_factor , 'min_weight' : min_weight , 'max_spans' : max_spans , 'answer_weight' : answer_weight , 'method_name' : method_name , 'random_importance' : random_importance , 'invert_importance' : invert_importance , 'enable_gradient_conservation' : enable_gradient_conservation , 'weight_debug' : weight_debug , }) # NEW: Handle GBMPO algorithms - set divergence_type based on algorithm # NEW: Handle GBMPO algorithm variants original_algorithm = algorithm . lower () # Map GBMPO variants to base algorithm and extract divergence type if original_algorithm . startswith ( \"gbmpo\" ): # Extract divergence type from algorithm name if not explicitly provided if gbmpo_divergence_type is None : if original_algorithm == \"gbmpo\" : # Default to l2kl if no variant specified gbmpo_divergence_type = \"l2kl\" else : # Extract from algorithm name: gbmpo_l2kl -> l2kl suffix = original_algorithm . replace ( \"gbmpo_\" , \"\" ) divergence_map = { \"l2\" : \"l2\" , \"l2kl\" : \"l2kl\" , \"probl2\" : \"prob_l2\" , \"probl2kl\" : \"prob_l2kl\" } gbmpo_divergence_type = divergence_map . get ( suffix , \"l2kl\" ) # Normalize to base \"gbmpo\" algorithm algorithm = \"gbmpo\" kwargs [ 'gbmpo_divergence_type' ] = gbmpo_divergence_type logger . info ( f \"GBMPO variant detected: { original_algorithm } -> divergence_type= { gbmpo_divergence_type } \" ) config = UnifiedConfig ( algo = algorithm . lower (), model = RLModelConfig ( name_or_path = model_name , backend = backend if backend != \"auto\" else \"trl\" , max_seq_length = max_seq_length , quantization = kwargs . get ( 'quantization' , {}), gradient_checkpointing = kwargs . get ( 'use_gradient_checkpointing' , False ), precision = kwargs . get ( 'precision' , 'auto' ), reward_value_model = reward_value_model or kwargs . get ( 'reward_value_model' , None ), reward_model_name = reward_model_name , reward_model_source = reward_model_source , reward_value_loading_type = reward_value_loading_type , reward_model_quantization = reward_model_quantization or {}, value_model_quantization = value_model_quantization or {}, use_peft = kwargs . get ( 'use_peft' , True ), lora_r = kwargs . get ( 'lora_r' , 16 ), lora_alpha = kwargs . get ( 'lora_alpha' , 32 ), lora_dropout = kwargs . get ( 'lora_dropout' , 0.05 ), lora_target_modules = kwargs . get ( 'lora_target_modules' , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]), model_init_kwargs = kwargs . get ( 'model_init_kwargs' , {}), ref_model_init_kwargs = kwargs . get ( 'ref_model_init_kwargs' , {}), model_adapter_name = kwargs . get ( 'model_adapter_name' ), ref_adapter_name = kwargs . get ( 'ref_adapter_name' ), force_use_ref_model = kwargs . get ( 'force_use_ref_model' , False ), disable_dropout = kwargs . get ( 'disable_dropout' , True ), use_logits_to_keep = kwargs . get ( 'use_logits_to_keep' , False ), reward_device = reward_device , device_map = kwargs . get ( 'device_map' , 'auto' ), trust_remote_code = kwargs . get ( 'trust_remote_code' , False ), ), datasets = [ RLDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = max_samples , percent = kwargs . get ( 'percent' ), max_eval_samples = kwargs . get ( 'max_eval_samples' , None ), field_mappings = kwargs . get ( 'field_mappings' , {}), column_mapping = kwargs . get ( 'column_mapping' , {}), weight = kwargs . get ( 'dataset_weight' , 1.0 ), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), pad_token = kwargs . get ( 'pad_token' ), label_pad_token_id = kwargs . get ( 'label_pad_token_id' , - 100 ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), padding_free = kwargs . get ( 'padding_free' , False ), precompute_ref_log_probs = kwargs . get ( 'precompute_ref_log_probs' , False ), precompute_ref_batch_size = kwargs . get ( 'precompute_ref_batch_size' ), tools = kwargs . get ( 'tools' ), system_prompt = system_prompt , preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), config_name = kwargs . get ( 'config_name' , None ), ) ], train = RLTrainingConfig ( epochs = num_epochs , max_steps = max_steps , per_device_batch_size = batch_size , per_device_eval_batch_size = kwargs . get ( 'eval_batch_size' , batch_size ), learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), beta = kwargs . get ( 'beta' , 0.1 ), # YAML beta -> config.train.beta kl_coef = kwargs . get ( 'kl_coef' , 0.1 ), num_generations = kwargs . get ( 'num_generations' , batch_size ), cliprange = kwargs . get ( 'cliprange' , 0.2 ), max_length = max_seq_length , use_cache = kwargs . get ( 'use_cache' , True ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 100 ), save_steps = kwargs . get ( 'save_steps' , 500 ), save_total_limit = kwargs . get ( 'save_total_limit' ), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), logging_steps = kwargs . get ( 'logging_steps' , 10 ), eval_steps = kwargs . get ( 'eval_steps' , 100 ), seed = kwargs . get ( 'seed' , 42 ), data_seed = kwargs . get ( 'data_seed' , 47 ), # Match training_script.py mask_truncated_completions = kwargs . get ( 'mask_truncated_completions' , True ), rollout_batch_size = kwargs . get ( 'rollout_batch_size' , 1 ), num_ppo_epochs = kwargs . get ( 'num_ppo_epochs' ), temperature = kwargs . get ( 'temperature' , 0.6 ), top_p = kwargs . get ( 'top_p' , 0.95 ), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), whiten_rewards = kwargs . get ( 'whiten_rewards' , False ), kl_estimator = kwargs . get ( 'kl_estimator' , 'k1' ), vf_coef = kwargs . get ( 'vf_coef' , 0.1 ), cliprange_value = kwargs . get ( 'cliprange_value' , 0.2 ), gamma = kwargs . get ( 'gamma' , 1.0 ), lam = kwargs . get ( 'lam' , 0.95 ), response_length = kwargs . get ( 'response_length' , 128 ), stop_token = kwargs . get ( 'stop_token' , 'eos' ), missing_eos_penalty = kwargs . get ( 'missing_eos_penalty' , 1.0 ), ds3_gather_for_generation = kwargs . get ( 'ds3_gather_for_generation' , True ), generation_kwargs = kwargs . get ( 'generation_kwargs' , {}), max_prompt_length = kwargs . get ( 'max_prompt_length' , 512 ), max_target_length = kwargs . get ( 'max_target_length' ), max_completion_length = kwargs . get ( 'max_completion_length' , 256 ), padding_free = kwargs . get ( 'padding_free' , False ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), loss_type = kwargs . get ( 'loss_type' , 'sigmoid' ), loss_weights = kwargs . get ( 'loss_weights' ), f_divergence_type = kwargs . get ( 'f_divergence_type' , 'reverse_kl' ), f_alpha_divergence_coef = kwargs . get ( 'f_alpha_divergence_coef' , 1.0 ), reference_free = kwargs . get ( 'reference_free' , False ), label_smoothing = kwargs . get ( 'label_smoothing' , 0.0 ), use_weighting = kwargs . get ( 'use_weighting' , False ), rpo_alpha = kwargs . get ( 'rpo_alpha' ), ld_alpha = kwargs . get ( 'ld_alpha' ), discopop_tau = kwargs . get ( 'discopop_tau' , 0.05 ), sync_ref_model = kwargs . get ( 'sync_ref_model' , False ), ref_model_mixup_alpha = kwargs . get ( 'ref_model_mixup_alpha' , 0.6 ), ref_model_sync_steps = kwargs . get ( 'ref_model_sync_steps' , 512 ), use_liger_kernel = kwargs . get ( 'use_liger_kernel' , False ), use_liger_loss = kwargs . get ( 'use_liger_loss' ), # NEW: Counterfactual GRPO specific boost_factor = kwargs . get ( 'boost_factor' , 2.0 ), min_weight = kwargs . get ( 'min_weight' , 0.5 ), max_spans = kwargs . get ( 'max_spans' , 10 ), answer_weight = kwargs . get ( 'answer_weight' , 1.5 ), weighting_mode = kwargs . get ( 'weighting_mode' ), method_name = kwargs . get ( 'method_name' , 'counterfactual' ), random_importance = kwargs . get ( 'random_importance' , False ), invert_importance = kwargs . get ( 'invert_importance' , False ), enable_gradient_conservation = kwargs . get ( 'enable_gradient_conservation' , True ), weight_debug = kwargs . get ( 'weight_debug' , False ), scale_rewards = kwargs . get ( 'scale_rewards' , 'group' ), enable_thinking = kwargs . get ( 'enable_thinking' , False ), fast_inference = kwargs . get ( 'fast_inference' , False ), # Unsloth vLLM fast inference vllm_gpu_memory_utilization = kwargs . get ( 'vllm_gpu_memory_utilization' , 0.7 ), # vLLM GPU memory (0.95 for max speed) gbmpo_l2_coefficient = kwargs . get ( 'gbmpo_l2_coefficient' , 0.0001 ), gbmpo_divergence_type = kwargs . get ( 'gbmpo_divergence_type' ), gbmpo_epsilon = kwargs . get ( 'gbmpo_epsilon' , 0.2 ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.0 ), eval_strategy = kwargs . get ( 'eval_strategy' , 'no' ), logging_strategy = kwargs . get ( 'logging_strategy' , 'steps' ), # BOLT-specific parameters curriculum_enabled = kwargs . get ( 'curriculum_enabled' , False ), curriculum_epsilon = kwargs . get ( 'curriculum_epsilon' , 0.05 ), curriculum_update_freq = kwargs . get ( 'curriculum_update_freq' , 10 ), baseline_enabled = kwargs . get ( 'baseline_enabled' , False ), baseline_rho_min = kwargs . get ( 'baseline_rho_min' , 0.875 ), baseline_rho_max = kwargs . get ( 'baseline_rho_max' , 0.96 ), baseline_D_half = kwargs . get ( 'baseline_D_half' , 0.5 ), baseline_warm_start = kwargs . get ( 'baseline_warm_start' ), use_baseline_advantages = kwargs . get ( 'use_baseline_advantages' , False ), # Meta-ES specific parameters meta_iterations = kwargs . get ( 'meta_iterations' , 15 ), patience = kwargs . get ( 'patience' , 5 ), min_delta = kwargs . get ( 'min_delta' , 0.001 ), init_scale = kwargs . get ( 'init_scale' , 0.01 ), N = kwargs . get ( 'N' , 10 ), T = kwargs . get ( 'T' , 100 ), sigma = kwargs . get ( 'sigma' , 0.01 ), sigma_decay = kwargs . get ( 'sigma_decay' , 0.99 ), alpha = kwargs . get ( 'alpha' , 0.01 ), mirror_coefficient = kwargs . get ( 'mirror_coefficient' , 0.0001 ), debug_mode = kwargs . get ( 'debug_mode' , False ), eval_timeout = kwargs . get ( 'eval_timeout' , 5 ), eval_max_tokens = kwargs . get ( 'eval_max_tokens' , 512 ), eval_k = kwargs . get ( 'eval_k' , 1 ), eval_temperature = kwargs . get ( 'eval_temperature' , 0.8 ), num_workers = kwargs . get ( 'num_workers' , 1 ), no_wandb = kwargs . get ( 'no_wandb' , False ), wandb_project = kwargs . get ( 'wandb_project' , 'neural-mirror-es' ), resume = kwargs . get ( 'resume' ), # DPO evaluation parameters dpo_eval_enabled = kwargs . get ( 'dpo_eval_enabled' , False ), dpo_eval_max_samples = kwargs . get ( 'dpo_eval_max_samples' ), dpo_zero_shot_max_samples = kwargs . get ( 'dpo_zero_shot_max_samples' , 50 ), dpo_few_shot_max_samples = kwargs . get ( 'dpo_few_shot_max_samples' , 30 ), dpo_few_shot_examples_text = kwargs . get ( 'dpo_few_shot_examples_text' ), # Additional checkpoint parameters load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , False ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), # Additional training parameters weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), reward_weights = kwargs . get ( 'reward_weights' ), # GRPO/GSPO specific (if not already present) grpo_alpha = kwargs . get ( 'grpo_alpha' , 0.1 ), grpo_beta = kwargs . get ( 'grpo_beta' , 0.1 ), gspo_gamma = kwargs . get ( 'gspo_gamma' , 0.1 ), gspo_delta = kwargs . get ( 'gspo_delta' , 0.1 ), group_by_length = kwargs . get ( 'group_by_length' , True ), extra_params = kwargs , use_rewards_directly = kwargs . get ( 'use_rewards_directly' , None ), ), logging = RLLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), sample_logging = sample_logging_config , report_to = kwargs . get ( 'report_to' , \"none\" ), ), rewards = rewards_config , chat_template = kwargs . get ( 'chat_template' , 'auto' ), caching = { 'root' : kwargs . get ( 'cache_dir' , 'cache' ), 'enabled' : kwargs . get ( 'caching_enabled' , True ) }, reward_training = reward_training_config ) # Create backend config algorithm_enum = RLAlgorithm ( algorithm . lower ()) if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . RL , algorithm_enum ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . RL , backend_type , algorithm_enum ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () get_backend_status() \u00b6 Get the availability status of all backends. Get current backend status and environment variables. Source code in src/aligntune/core/backend_factory.py 307 308 309 310 311 312 313 314 315 316 def get_backend_status () -> Dict [ str , Any ]: \"\"\"Get current backend status and environment variables.\"\"\" return { \"pure_trl_mode\" : os . environ . get ( 'PURE_TRL_MODE' , '0' ), \"trl_only_mode\" : os . environ . get ( 'TRL_ONLY_MODE' , '0' ), \"disable_unsloth_for_trl\" : os . environ . get ( 'DISABLE_UNSLOTH_FOR_TRL' , '0' ), \"trl_available\" : TRL_AVAILABLE , \"unsloth_available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"current_mode\" : \"TRL-ONLY\" if os . environ . get ( 'PURE_TRL_MODE' ) == '1' else \"UNSLOTH-ENABLED\" } options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl_available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth_available' ] } \" ) list_backends() \u00b6 List all available backends. Source code in src/aligntune/core/backend_factory.py 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 def list_backends () -> Dict [ str , list ]: backends = { \"TRL\" : { \"available\" : TRL_AVAILABLE , \"description\" : \"HuggingFace Transformers Reinforcement Learning library\" , \"status\" : \"\u2705 Available\" if TRL_AVAILABLE else \"\u274c Not Available\" }, \"UNSLOTH\" : { \"available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"description\" : \"Unsloth optimized training with memory efficiency\" , \"status\" : \"\u2705 Available\" if _check_backend_availability ( BackendType . UNSLOTH ) else \"\u274c Not Available\" } } # Print availability status print ( \" \\n \" + \"=\" * 60 ) print ( \"FINETUNEHUB - BACKEND AVAILABILITY\" ) print ( \"=\" * 60 ) for backend_name , info in backends . items (): print ( f \" { backend_name : 10s } : { info [ 'status' ] } \" ) print ( f \" { info [ 'description' ] } \" ) print ( \"=\" * 60 ) print ( \"Note: When TRL is selected, Unsloth is disabled to prevent interference.\" ) print ( \" When Unsloth is selected, TRL-only mode is cleared.\" ) print ( \"=\" * 60 + \" \\n \" ) return backends options: show_source: true heading_level: 3 Enums and Types \u00b6 BackendType Enum \u00b6 Bases: Enum Backend types for training. Source code in src/aligntune/core/backend_factory.py 202 203 204 205 class BackendType ( Enum ): \"\"\"Backend types for training.\"\"\" UNSLOTH = \"unsloth\" # Unsloth backend (fast, memory efficient) TRL = \"trl\" # TRL backend (standard, reliable) options: show_source: true heading_level: 3 TrainingType Enum \u00b6 Bases: Enum Training types supported by AlignTune. Source code in src/aligntune/core/backend_factory.py 195 196 197 198 199 class TrainingType ( Enum ): \"\"\"Training types supported by AlignTune.\"\"\" SFT = \"sft\" # Supervised Fine-Tuning RL = \"rl\" # Reinforcement Learning options: show_source: true heading_level: 3 RLAlgorithm Enum \u00b6 Bases: Enum RL algorithms supported. Source code in src/aligntune/core/backend_factory.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class RLAlgorithm ( Enum ): \"\"\"RL algorithms supported.\"\"\" DPO = \"dpo\" # Direct Preference Optimization PPO = \"ppo\" # Proximal Policy Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective # New COUNTERFACT_GRPO = \"counterfact_grpo\" # NEW: Add GBMPO GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3 BackendConfig Dataclass \u00b6 Configuration for backend selection with task type support. Source code in src/aligntune/core/backend_factory.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 @dataclass class BackendConfig : \"\"\"Configuration for backend selection with task type support.\"\"\" training_type : TrainingType backend : BackendType algorithm : Optional [ RLAlgorithm ] = None # Only for RL task_type : Optional [ TaskType ] = None # NEW: For SFT tasks fallback_enabled : bool = True def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING __post_init__ () \u00b6 Validate configuration. Source code in src/aligntune/core/backend_factory.py 237 238 239 240 241 242 243 244 245 246 247 248 def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING options: show_source: true heading_level: 3 Examples \u00b6 SFT with TRL Backend \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) SFT with Unsloth Backend \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) DPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" ) Error Handling \u00b6 Backend Not Available \u00b6 try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ValueError as e : print ( f \"Backend not available: { e } \" ) # Falls back to TRL automatically Invalid Algorithm \u00b6 try : trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"invalid\" ) except ValueError as e : print ( f \"Invalid algorithm: { e } \" ) Next Steps \u00b6 Configuration Classes - Configuration options Trainers - Trainer methods User Guide - Usage guide","title":"Backend Factory"},{"location":"api-reference/backend-factory/#backend-factory-api","text":"The Backend Factory is the main entry point for creating trainers in AlignTune.","title":"Backend Factory API"},{"location":"api-reference/backend-factory/#overview","text":"The BackendFactory provides functions to create SFT and RL trainers with automatic backend selection and fallback handling.","title":"Overview"},{"location":"api-reference/backend-factory/#backendfactory-class","text":"Complete API reference for the BackendFactory class. Factory for creating training backends. Source code in src/aligntune/core/backend_factory.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 class BackendFactory : \"\"\"Factory for creating training backends.\"\"\" # Registry of available backends _backends : Dict [ tuple , Type [ TrainerBase ]] = {} @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) @classmethod def _select_best_backend ( cls , backend_config : BackendConfig ) -> BackendConfig : \"\"\"Select the best available backend with fallback.\"\"\" # Check if requested backend is available if cls . _is_backend_available ( backend_config ): return backend_config # Try fallback backends if backend_config . fallback_enabled : fallback_order = cls . _get_fallback_order ( backend_config ) for fallback_backend in fallback_order : if cls . _is_backend_available ( fallback_backend ): logger . warning ( f \"Requested backend { backend_config . backend } not available, \" f \"falling back to { fallback_backend . backend } \" ) return fallback_backend raise RuntimeError ( f \"No available backend for { backend_config } \" ) @classmethod def _is_backend_available ( cls , backend_config : BackendConfig ) -> bool : \"\"\"Check if a backend is available.\"\"\" key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) trainer_class = cls . _backends . get ( key ) if trainer_class is None : return False # Check if the trainer class can be instantiated try : return trainer_class . is_available () except Exception : return False @classmethod def _get_fallback_order ( cls , backend_config : BackendConfig ) -> list : \"\"\"Get fallback order for backend selection.\"\"\" if backend_config . training_type == TrainingType . SFT : # SFT fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . SFT , BackendType . UNSLOTH ), BackendConfig ( TrainingType . SFT , BackendType . TRL ), ] else : # RL # RL fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . RL , BackendType . UNSLOTH , backend_config . algorithm ), BackendConfig ( TrainingType . RL , BackendType . TRL , backend_config . algorithm ), ] # Remove the original backend from fallbacks fallbacks = [ fb for fb in fallbacks if fb . backend != backend_config . backend ] return fallbacks @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config )","title":"BackendFactory Class"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.create_trainer","text":"Create a trainer instance based on backend configuration. Source code in src/aligntune/core/backend_factory.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config )","title":"create_trainer"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.get_backend_capabilities","text":"Get capabilities for a specific backend type. Source code in src/aligntune/core/backend_factory.py 426 427 428 429 430 431 432 433 434 435 def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities","title":"get_backend_capabilities"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.get_recommended_backend","text":"Get recommended backend for given training type and algorithm. Source code in src/aligntune/core/backend_factory.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" )","title":"get_recommended_backend"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.is_backend_available","text":"Check if a specific backend type is available. Source code in src/aligntune/core/backend_factory.py 414 415 416 417 418 419 420 421 422 423 424 def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False","title":"is_backend_available"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.list_available_backends","text":"List all available backends. Source code in src/aligntune/core/backend_factory.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available","title":"list_available_backends"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.register_backend","text":"Register a backend trainer class. Source code in src/aligntune/core/backend_factory.py 325 326 327 328 329 330 331 332 333 334 335 336 @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) options: show_source: true heading_level: 3","title":"register_backend"},{"location":"api-reference/backend-factory/#factory-functions","text":"","title":"Factory Functions"},{"location":"api-reference/backend-factory/#create_sft_trainer","text":"Create a Supervised Fine-Tuning trainer. Create SFT trainer with specified backend and parameters. Source code in src/aligntune/core/backend_factory.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def create_sft_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , system_prompt : Optional [ str ] = None , # config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create SFT trainer with specified backend and parameters.\"\"\" if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"sft\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) # Create configuration # Build model config kwargs, only including precision if explicitly provided model_config_kwargs = { \"name_or_path\" : model_name , \"max_seq_length\" : max_seq_length , \"quantization\" : kwargs . get ( 'quantization' , {}), \"use_unsloth\" : kwargs . get ( 'use_unsloth' , False ), \"peft_enabled\" : kwargs . get ( 'use_peft' , kwargs . get ( 'peft_enabled' , False )), \"lora_rank\" : kwargs . get ( 'lora_r' , 16 ), \"lora_alpha\" : kwargs . get ( 'lora_alpha' , 32 ), \"lora_dropout\" : kwargs . get ( 'lora_dropout' , 0.1 ), \"target_modules\" : kwargs . get ( 'lora_target_modules' , None ), \"bias\" : kwargs . get ( 'lora_bias' , 'none' ), \"attn_implementation\" : kwargs . get ( 'attn_implementation' , 'auto' ), \"max_memory\" : kwargs . get ( 'max_memory' ), \"use_gradient_checkpointing\" : kwargs . get ( 'use_gradient_checkpointing' , True ), \"num_labels\" : kwargs . get ( 'num_labels' ), # For classification tasks \"model_init_kwargs\" : kwargs . get ( 'model_init_kwargs' , {}), \"device_map\" : kwargs . get ( 'device_map' , 'auto' ), \"trust_remote_code\" : kwargs . get ( 'trust_remote_code' , False ), } # Only add precision if explicitly provided (otherwise use default from ModelConfig) if 'precision' in kwargs and kwargs . get ( 'precision' ) is not None : # Validate precision value precision_value = kwargs . get ( 'precision' ) if isinstance ( precision_value , str ): # Convert string to PrecisionType enum from .sft.config import PrecisionType try : precision_value = PrecisionType ( precision_value . lower ()) except ValueError : logger . warning ( f \"Invalid precision ' { precision_value } ', using default\" ) precision_value = None if precision_value is not None : model_config_kwargs [ 'precision' ] = precision_value config = SFTConfig ( model = SFTModelConfig ( ** model_config_kwargs ), dataset = SFTDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), subset = kwargs . get ( 'subset' ), # Support dataset config/subset (e.g., for financial_phrasebank) config = kwargs . get ( 'config' ), # Alternative name for subset max_samples = max_samples , percent = kwargs . get ( 'percent' ), column_mapping = kwargs . get ( 'column_mapping' , {}), task_type = kwargs . get ( 'task_type' , 'supervised_fine_tuning' ), system_prompt = system_prompt , # dataset_kwargs=kwargs.get('dataset_kwargs', {}), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), dataset_text_field = kwargs . get ( 'dataset_text_field' , 'text' ), text_column = kwargs . get ( 'text_column' , kwargs . get ( 'dataset_text_field' , 'text' )), # For classification tasks label_column = kwargs . get ( 'label_column' , 'label' ), # For classification tasks # eos_token=kwargs.get('eos_token'), pad_token = kwargs . get ( 'pad_token' ), chat_template = kwargs . get ( 'chat_template' ), preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), ), train = SFTTrainingConfig ( epochs = num_epochs , max_steps = kwargs . get ( 'max_steps' ), per_device_batch_size = batch_size , learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.1 ), weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_steps' , kwargs . get ( 'save_interval' , 500 )), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), fp16 = kwargs . get ( 'fp16' , False ), bf16 = kwargs . get ( 'bf16' , False ), dataloader_num_workers = kwargs . get ( 'dataloader_num_workers' , 0 ), remove_unused_columns = kwargs . get ( 'remove_unused_columns' , False ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), group_by_length = kwargs . get ( 'group_by_length' , True ), dataloader_drop_last = kwargs . get ( 'dataloader_drop_last' , False ), eval_accumulation_steps = kwargs . get ( 'eval_accumulation_steps' ), label_smoothing_factor = kwargs . get ( 'label_smoothing_factor' , 0.0 ), early_stopping_patience = kwargs . get ( 'early_stopping_patience' ), early_stopping_threshold = kwargs . get ( 'early_stopping_threshold' , 0.0 ), load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , True ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' , 'eval_loss' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), use_trl = kwargs . get ( 'use_trl' , False ), dataset_num_proc = kwargs . get ( 'train_dataset_num_proc' ), dataset_kwargs = kwargs . get ( 'train_dataset_kwargs' , {}), packing = kwargs . get ( 'packing' , False ), packing_strategy = kwargs . get ( 'packing_strategy' , 'bfd' ), eval_packing = kwargs . get ( 'eval_packing' ), padding_free = kwargs . get ( 'padding_free' , False ), pad_to_multiple_of = kwargs . get ( 'pad_to_multiple_of' ), completion_only_loss = kwargs . get ( 'completion_only_loss' ), assistant_only_loss = kwargs . get ( 'assistant_only_loss' , False ), loss_type = kwargs . get ( 'loss_type' , 'nll' ), activation_offloading = kwargs . get ( 'activation_offloading' , False ), use_flash_attention_2 = kwargs . get ( 'use_flash_attention_2' ), gradient_checkpointing = kwargs . get ( 'gradient_checkpointing' , False ), gradient_checkpointing_kwargs = kwargs . get ( 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }), extra_params = kwargs , ), logging = SFTLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), log_level = kwargs . get ( 'log_level' , 'INFO' ), log_interval = kwargs . get ( 'logging_steps' , kwargs . get ( 'log_interval' , 10 )), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), eval_strategy = kwargs . get ( 'eval_strategy' , 'steps' ), report_to = kwargs . get ( 'report_to' , \"none\" ), ), evaluation = SFTEvaluationConfig ( compute_perplexity = kwargs . get ( 'compute_perplexity' , True ), compute_rouge = kwargs . get ( 'compute_rouge' , True ), compute_bleu = kwargs . get ( 'compute_bleu' , True ), compute_meteor = kwargs . get ( 'compute_meteor' , False ), compute_bertscore = kwargs . get ( 'compute_bertscore' , False ), compute_semantic_similarity = kwargs . get ( 'compute_semantic_similarity' , False ), compute_codebleu = kwargs . get ( 'compute_codebleu' , False ), max_samples_for_quality_metrics = kwargs . get ( 'max_samples_for_quality_metrics' , 50 ), bertscore_model = kwargs . get ( 'bertscore_model' , 'microsoft/deberta-xlarge-mnli' ), semantic_similarity_model = kwargs . get ( 'semantic_similarity_model' , 'sentence-transformers/all-MiniLM-L6-v2' ) ) ) # Create backend config if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . SFT ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . SFT , backend_type ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 ) trainer . train ()","title":"create_sft_trainer()"},{"location":"api-reference/backend-factory/#create_rl_trainer","text":"Create a Reinforcement Learning trainer. Create RL trainer with specified algorithm and backend. Source code in src/aligntune/core/backend_factory.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def create_rl_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , algorithm : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , max_steps : Optional [ int ] = 100 , # Add max_steps parameter batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , reward_value_model : Optional [ str ] = None , reward_model_name : Optional [ str ] = None , # NEW reward_model_path : Optional [ str ] = None , # NEW: Local reward model path train_custom_reward_model : bool = False , # NEW: Train custom reward model reward_training_texts : Optional [ List [ str ]] = None , # NEW: Training texts for custom model reward_functions : Optional [ List [ str ]] = None , # NEW: Reward functions for custom model reward_function_weights : Optional [ List [ float ]] = None , # NEW: Weights for reward functions reward_training_base_model : Optional [ str ] = None , # NEW: Base model for custom training reward_training_output_dir : Optional [ str ] = None , # NEW: Output dir for custom training reward_value_loading_type : Optional [ str ] = None , reward_model_quantization : Optional [ Dict ] = None , value_model_quantization : Optional [ Dict ] = None , reward_training : Optional [ Dict [ str , Any ]] = None , # NEW: Flexible reward training config reward_device : str = \"auto\" , sample_logging : Optional [ Dict [ str , Any ]] = None , # NEW: Counterfactual GRPO specific parameters boost_factor : float = 2.0 , min_weight : float = 0.5 , max_spans : int = 10 , answer_weight : float = 1.5 , method_name : str = \"counterfactual\" , random_importance : bool = False , invert_importance : bool = False , enable_gradient_conservation : bool = True , weight_debug : bool = False , system_prompt : Optional [ str ] = None , # # NEW: GBMPO-specific parameters gbmpo_divergence_type : Optional [ str ] = None , config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create RL trainer with specified algorithm and backend.\"\"\" # NEW: Load config if provided# Load and parse config if provided if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"rl\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters from merged config model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) algorithm = algorithm or kwargs . pop ( 'algorithm' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) if algorithm is None : raise ValueError ( \"algorithm must be provided either as argument or in config\" ) reward_device = kwargs . pop ( 'reward_device' , reward_device or \"auto\" ) reward_device = reward_device or \"auto\" sample_logging_dict : Dict [ str , Any ] = {} base_sample_logging = sample_logging or kwargs . get ( 'sample_logging_config' ) if base_sample_logging : sample_logging_dict . update ( base_sample_logging ) inline_sample_logging = { \"enabled\" : kwargs . get ( 'enable_sample_logging' ), \"prompts\" : kwargs . get ( 'sample_logging_prompts' ), \"interval_steps\" : kwargs . get ( 'sample_logging_interval_steps' ), \"percent_of_max_steps\" : kwargs . get ( 'sample_logging_percent_of_max_steps' , kwargs . get ( 'sample_logging_percent' )), \"max_new_tokens\" : kwargs . get ( 'sample_logging_max_new_tokens' ), \"temperature\" : kwargs . get ( 'sample_logging_temperature' ), \"top_p\" : kwargs . get ( 'sample_logging_top_p' ), \"num_samples\" : kwargs . get ( 'sample_logging_num_samples' ), } for key , value in inline_sample_logging . items (): if value is not None : sample_logging_dict [ key ] = value cleaned_sample_logging = { k : v for k , v in sample_logging_dict . items () if v is not None } if cleaned_sample_logging : sample_logging_config = SampleLoggingConfig ( ** cleaned_sample_logging ) else : sample_logging_config = SampleLoggingConfig () needs_neural_reward = not any ([ reward_model_path , train_custom_reward_model , reward_functions ]) if reward_model_name is None and needs_neural_reward : reward_model_name = model_name # ============================================ # PPO MODEL CONSISTENCY WARNING # ============================================ if algorithm . lower () == \"ppo\" : print ( \"\u26a0\ufe0f NOTE: For optimal PPO performance, ensure policy_model, reward_model, and value_model are from the same model family\" ) # STRICT VALIDATION: Validate reward model configuration reward_model_source = None source_count = sum ([ reward_model_name is not None , reward_model_path is not None , train_custom_reward_model ]) if source_count > 1 : raise ValueError ( f \"Specify exactly ONE reward source. Found { source_count } : \" f \"reward_model_name= { reward_model_name } , \" f \"reward_model_path= { reward_model_path } , \" f \"train_custom_reward_model= { train_custom_reward_model } \" ) if train_custom_reward_model : # Validate ALL required fields for custom training if not reward_training_texts : raise ValueError ( \"reward_training_texts required when train_custom_reward_model=True\" ) if not reward_functions : raise ValueError ( \"reward_functions required when train_custom_reward_model=True\" ) if not reward_training_base_model : raise ValueError ( \"reward_training_base_model required when train_custom_reward_model=True\" ) if not reward_training_output_dir : raise ValueError ( \"reward_training_output_dir required when train_custom_reward_model=True\" ) # Import required classes from .rl.config import RewardModelTrainingConfig , RewardModelSourceConfig training_config = RewardModelTrainingConfig ( base_model_name = reward_training_base_model , training_texts = reward_training_texts , reward_functions = reward_functions , output_dir = reward_training_output_dir , # Use kwargs for optional training params num_epochs = kwargs . get ( 'reward_training_epochs' , 3 ), learning_rate = kwargs . get ( 'reward_training_lr' , 1e-5 ), batch_size = kwargs . get ( 'reward_training_batch_size' , 8 ), reward_weights = reward_function_weights ) reward_model_source = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) elif reward_model_name : from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = reward_model_name ) elif reward_model_path : # Validate path exists if not Path ( reward_model_path ) . exists (): raise FileNotFoundError ( f \"reward_model_path does not exist: { reward_model_path } \" ) from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = reward_model_path ) # Create reward training config if provided reward_training_config = None if reward_training : from .rl.config import RewardModelTrainingConfig reward_training_config = RewardModelTrainingConfig ( ** reward_training ) # Construct rewards config if reward_functions provided but not in kwargs # rewards_config = kwargs.get('rewards', []) rewards_config = kwargs . get ( 'rewards' ) or [] if not rewards_config and reward_functions and not train_custom_reward_model : weights = reward_function_weights or [ 1.0 ] * len ( reward_functions ) if len ( weights ) != len ( reward_functions ): logger . warning ( \"reward_function_weights length mismatch, using default 1.0\" ) weights = [ 1.0 ] * len ( reward_functions ) for func_name , weight in zip ( reward_functions , weights ): rewards_config . append ({ \"type\" : func_name , \"weight\" : weight , \"params\" : {} }) logger . info ( f \"Constructed rewards config from function list: { len ( rewards_config ) } functions\" ) # FIXED: Added required 'algo' parameter and used 'train' not 'training' if algorithm . lower () == \"counterfact_grpo\" : # Add counterfactual-specific params to kwargs for config creation kwargs . update ({ 'boost_factor' : boost_factor , 'min_weight' : min_weight , 'max_spans' : max_spans , 'answer_weight' : answer_weight , 'method_name' : method_name , 'random_importance' : random_importance , 'invert_importance' : invert_importance , 'enable_gradient_conservation' : enable_gradient_conservation , 'weight_debug' : weight_debug , }) # NEW: Handle GBMPO algorithms - set divergence_type based on algorithm # NEW: Handle GBMPO algorithm variants original_algorithm = algorithm . lower () # Map GBMPO variants to base algorithm and extract divergence type if original_algorithm . startswith ( \"gbmpo\" ): # Extract divergence type from algorithm name if not explicitly provided if gbmpo_divergence_type is None : if original_algorithm == \"gbmpo\" : # Default to l2kl if no variant specified gbmpo_divergence_type = \"l2kl\" else : # Extract from algorithm name: gbmpo_l2kl -> l2kl suffix = original_algorithm . replace ( \"gbmpo_\" , \"\" ) divergence_map = { \"l2\" : \"l2\" , \"l2kl\" : \"l2kl\" , \"probl2\" : \"prob_l2\" , \"probl2kl\" : \"prob_l2kl\" } gbmpo_divergence_type = divergence_map . get ( suffix , \"l2kl\" ) # Normalize to base \"gbmpo\" algorithm algorithm = \"gbmpo\" kwargs [ 'gbmpo_divergence_type' ] = gbmpo_divergence_type logger . info ( f \"GBMPO variant detected: { original_algorithm } -> divergence_type= { gbmpo_divergence_type } \" ) config = UnifiedConfig ( algo = algorithm . lower (), model = RLModelConfig ( name_or_path = model_name , backend = backend if backend != \"auto\" else \"trl\" , max_seq_length = max_seq_length , quantization = kwargs . get ( 'quantization' , {}), gradient_checkpointing = kwargs . get ( 'use_gradient_checkpointing' , False ), precision = kwargs . get ( 'precision' , 'auto' ), reward_value_model = reward_value_model or kwargs . get ( 'reward_value_model' , None ), reward_model_name = reward_model_name , reward_model_source = reward_model_source , reward_value_loading_type = reward_value_loading_type , reward_model_quantization = reward_model_quantization or {}, value_model_quantization = value_model_quantization or {}, use_peft = kwargs . get ( 'use_peft' , True ), lora_r = kwargs . get ( 'lora_r' , 16 ), lora_alpha = kwargs . get ( 'lora_alpha' , 32 ), lora_dropout = kwargs . get ( 'lora_dropout' , 0.05 ), lora_target_modules = kwargs . get ( 'lora_target_modules' , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]), model_init_kwargs = kwargs . get ( 'model_init_kwargs' , {}), ref_model_init_kwargs = kwargs . get ( 'ref_model_init_kwargs' , {}), model_adapter_name = kwargs . get ( 'model_adapter_name' ), ref_adapter_name = kwargs . get ( 'ref_adapter_name' ), force_use_ref_model = kwargs . get ( 'force_use_ref_model' , False ), disable_dropout = kwargs . get ( 'disable_dropout' , True ), use_logits_to_keep = kwargs . get ( 'use_logits_to_keep' , False ), reward_device = reward_device , device_map = kwargs . get ( 'device_map' , 'auto' ), trust_remote_code = kwargs . get ( 'trust_remote_code' , False ), ), datasets = [ RLDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = max_samples , percent = kwargs . get ( 'percent' ), max_eval_samples = kwargs . get ( 'max_eval_samples' , None ), field_mappings = kwargs . get ( 'field_mappings' , {}), column_mapping = kwargs . get ( 'column_mapping' , {}), weight = kwargs . get ( 'dataset_weight' , 1.0 ), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), pad_token = kwargs . get ( 'pad_token' ), label_pad_token_id = kwargs . get ( 'label_pad_token_id' , - 100 ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), padding_free = kwargs . get ( 'padding_free' , False ), precompute_ref_log_probs = kwargs . get ( 'precompute_ref_log_probs' , False ), precompute_ref_batch_size = kwargs . get ( 'precompute_ref_batch_size' ), tools = kwargs . get ( 'tools' ), system_prompt = system_prompt , preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), config_name = kwargs . get ( 'config_name' , None ), ) ], train = RLTrainingConfig ( epochs = num_epochs , max_steps = max_steps , per_device_batch_size = batch_size , per_device_eval_batch_size = kwargs . get ( 'eval_batch_size' , batch_size ), learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), beta = kwargs . get ( 'beta' , 0.1 ), # YAML beta -> config.train.beta kl_coef = kwargs . get ( 'kl_coef' , 0.1 ), num_generations = kwargs . get ( 'num_generations' , batch_size ), cliprange = kwargs . get ( 'cliprange' , 0.2 ), max_length = max_seq_length , use_cache = kwargs . get ( 'use_cache' , True ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 100 ), save_steps = kwargs . get ( 'save_steps' , 500 ), save_total_limit = kwargs . get ( 'save_total_limit' ), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), logging_steps = kwargs . get ( 'logging_steps' , 10 ), eval_steps = kwargs . get ( 'eval_steps' , 100 ), seed = kwargs . get ( 'seed' , 42 ), data_seed = kwargs . get ( 'data_seed' , 47 ), # Match training_script.py mask_truncated_completions = kwargs . get ( 'mask_truncated_completions' , True ), rollout_batch_size = kwargs . get ( 'rollout_batch_size' , 1 ), num_ppo_epochs = kwargs . get ( 'num_ppo_epochs' ), temperature = kwargs . get ( 'temperature' , 0.6 ), top_p = kwargs . get ( 'top_p' , 0.95 ), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), whiten_rewards = kwargs . get ( 'whiten_rewards' , False ), kl_estimator = kwargs . get ( 'kl_estimator' , 'k1' ), vf_coef = kwargs . get ( 'vf_coef' , 0.1 ), cliprange_value = kwargs . get ( 'cliprange_value' , 0.2 ), gamma = kwargs . get ( 'gamma' , 1.0 ), lam = kwargs . get ( 'lam' , 0.95 ), response_length = kwargs . get ( 'response_length' , 128 ), stop_token = kwargs . get ( 'stop_token' , 'eos' ), missing_eos_penalty = kwargs . get ( 'missing_eos_penalty' , 1.0 ), ds3_gather_for_generation = kwargs . get ( 'ds3_gather_for_generation' , True ), generation_kwargs = kwargs . get ( 'generation_kwargs' , {}), max_prompt_length = kwargs . get ( 'max_prompt_length' , 512 ), max_target_length = kwargs . get ( 'max_target_length' ), max_completion_length = kwargs . get ( 'max_completion_length' , 256 ), padding_free = kwargs . get ( 'padding_free' , False ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), loss_type = kwargs . get ( 'loss_type' , 'sigmoid' ), loss_weights = kwargs . get ( 'loss_weights' ), f_divergence_type = kwargs . get ( 'f_divergence_type' , 'reverse_kl' ), f_alpha_divergence_coef = kwargs . get ( 'f_alpha_divergence_coef' , 1.0 ), reference_free = kwargs . get ( 'reference_free' , False ), label_smoothing = kwargs . get ( 'label_smoothing' , 0.0 ), use_weighting = kwargs . get ( 'use_weighting' , False ), rpo_alpha = kwargs . get ( 'rpo_alpha' ), ld_alpha = kwargs . get ( 'ld_alpha' ), discopop_tau = kwargs . get ( 'discopop_tau' , 0.05 ), sync_ref_model = kwargs . get ( 'sync_ref_model' , False ), ref_model_mixup_alpha = kwargs . get ( 'ref_model_mixup_alpha' , 0.6 ), ref_model_sync_steps = kwargs . get ( 'ref_model_sync_steps' , 512 ), use_liger_kernel = kwargs . get ( 'use_liger_kernel' , False ), use_liger_loss = kwargs . get ( 'use_liger_loss' ), # NEW: Counterfactual GRPO specific boost_factor = kwargs . get ( 'boost_factor' , 2.0 ), min_weight = kwargs . get ( 'min_weight' , 0.5 ), max_spans = kwargs . get ( 'max_spans' , 10 ), answer_weight = kwargs . get ( 'answer_weight' , 1.5 ), weighting_mode = kwargs . get ( 'weighting_mode' ), method_name = kwargs . get ( 'method_name' , 'counterfactual' ), random_importance = kwargs . get ( 'random_importance' , False ), invert_importance = kwargs . get ( 'invert_importance' , False ), enable_gradient_conservation = kwargs . get ( 'enable_gradient_conservation' , True ), weight_debug = kwargs . get ( 'weight_debug' , False ), scale_rewards = kwargs . get ( 'scale_rewards' , 'group' ), enable_thinking = kwargs . get ( 'enable_thinking' , False ), fast_inference = kwargs . get ( 'fast_inference' , False ), # Unsloth vLLM fast inference vllm_gpu_memory_utilization = kwargs . get ( 'vllm_gpu_memory_utilization' , 0.7 ), # vLLM GPU memory (0.95 for max speed) gbmpo_l2_coefficient = kwargs . get ( 'gbmpo_l2_coefficient' , 0.0001 ), gbmpo_divergence_type = kwargs . get ( 'gbmpo_divergence_type' ), gbmpo_epsilon = kwargs . get ( 'gbmpo_epsilon' , 0.2 ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.0 ), eval_strategy = kwargs . get ( 'eval_strategy' , 'no' ), logging_strategy = kwargs . get ( 'logging_strategy' , 'steps' ), # BOLT-specific parameters curriculum_enabled = kwargs . get ( 'curriculum_enabled' , False ), curriculum_epsilon = kwargs . get ( 'curriculum_epsilon' , 0.05 ), curriculum_update_freq = kwargs . get ( 'curriculum_update_freq' , 10 ), baseline_enabled = kwargs . get ( 'baseline_enabled' , False ), baseline_rho_min = kwargs . get ( 'baseline_rho_min' , 0.875 ), baseline_rho_max = kwargs . get ( 'baseline_rho_max' , 0.96 ), baseline_D_half = kwargs . get ( 'baseline_D_half' , 0.5 ), baseline_warm_start = kwargs . get ( 'baseline_warm_start' ), use_baseline_advantages = kwargs . get ( 'use_baseline_advantages' , False ), # Meta-ES specific parameters meta_iterations = kwargs . get ( 'meta_iterations' , 15 ), patience = kwargs . get ( 'patience' , 5 ), min_delta = kwargs . get ( 'min_delta' , 0.001 ), init_scale = kwargs . get ( 'init_scale' , 0.01 ), N = kwargs . get ( 'N' , 10 ), T = kwargs . get ( 'T' , 100 ), sigma = kwargs . get ( 'sigma' , 0.01 ), sigma_decay = kwargs . get ( 'sigma_decay' , 0.99 ), alpha = kwargs . get ( 'alpha' , 0.01 ), mirror_coefficient = kwargs . get ( 'mirror_coefficient' , 0.0001 ), debug_mode = kwargs . get ( 'debug_mode' , False ), eval_timeout = kwargs . get ( 'eval_timeout' , 5 ), eval_max_tokens = kwargs . get ( 'eval_max_tokens' , 512 ), eval_k = kwargs . get ( 'eval_k' , 1 ), eval_temperature = kwargs . get ( 'eval_temperature' , 0.8 ), num_workers = kwargs . get ( 'num_workers' , 1 ), no_wandb = kwargs . get ( 'no_wandb' , False ), wandb_project = kwargs . get ( 'wandb_project' , 'neural-mirror-es' ), resume = kwargs . get ( 'resume' ), # DPO evaluation parameters dpo_eval_enabled = kwargs . get ( 'dpo_eval_enabled' , False ), dpo_eval_max_samples = kwargs . get ( 'dpo_eval_max_samples' ), dpo_zero_shot_max_samples = kwargs . get ( 'dpo_zero_shot_max_samples' , 50 ), dpo_few_shot_max_samples = kwargs . get ( 'dpo_few_shot_max_samples' , 30 ), dpo_few_shot_examples_text = kwargs . get ( 'dpo_few_shot_examples_text' ), # Additional checkpoint parameters load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , False ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), # Additional training parameters weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), reward_weights = kwargs . get ( 'reward_weights' ), # GRPO/GSPO specific (if not already present) grpo_alpha = kwargs . get ( 'grpo_alpha' , 0.1 ), grpo_beta = kwargs . get ( 'grpo_beta' , 0.1 ), gspo_gamma = kwargs . get ( 'gspo_gamma' , 0.1 ), gspo_delta = kwargs . get ( 'gspo_delta' , 0.1 ), group_by_length = kwargs . get ( 'group_by_length' , True ), extra_params = kwargs , use_rewards_directly = kwargs . get ( 'use_rewards_directly' , None ), ), logging = RLLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), sample_logging = sample_logging_config , report_to = kwargs . get ( 'report_to' , \"none\" ), ), rewards = rewards_config , chat_template = kwargs . get ( 'chat_template' , 'auto' ), caching = { 'root' : kwargs . get ( 'cache_dir' , 'cache' ), 'enabled' : kwargs . get ( 'caching_enabled' , True ) }, reward_training = reward_training_config ) # Create backend config algorithm_enum = RLAlgorithm ( algorithm . lower ()) if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . RL , algorithm_enum ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . RL , backend_type , algorithm_enum ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train ()","title":"create_rl_trainer()"},{"location":"api-reference/backend-factory/#get_backend_status","text":"Get the availability status of all backends. Get current backend status and environment variables. Source code in src/aligntune/core/backend_factory.py 307 308 309 310 311 312 313 314 315 316 def get_backend_status () -> Dict [ str , Any ]: \"\"\"Get current backend status and environment variables.\"\"\" return { \"pure_trl_mode\" : os . environ . get ( 'PURE_TRL_MODE' , '0' ), \"trl_only_mode\" : os . environ . get ( 'TRL_ONLY_MODE' , '0' ), \"disable_unsloth_for_trl\" : os . environ . get ( 'DISABLE_UNSLOTH_FOR_TRL' , '0' ), \"trl_available\" : TRL_AVAILABLE , \"unsloth_available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"current_mode\" : \"TRL-ONLY\" if os . environ . get ( 'PURE_TRL_MODE' ) == '1' else \"UNSLOTH-ENABLED\" } options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl_available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth_available' ] } \" )","title":"get_backend_status()"},{"location":"api-reference/backend-factory/#list_backends","text":"List all available backends. Source code in src/aligntune/core/backend_factory.py 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 def list_backends () -> Dict [ str , list ]: backends = { \"TRL\" : { \"available\" : TRL_AVAILABLE , \"description\" : \"HuggingFace Transformers Reinforcement Learning library\" , \"status\" : \"\u2705 Available\" if TRL_AVAILABLE else \"\u274c Not Available\" }, \"UNSLOTH\" : { \"available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"description\" : \"Unsloth optimized training with memory efficiency\" , \"status\" : \"\u2705 Available\" if _check_backend_availability ( BackendType . UNSLOTH ) else \"\u274c Not Available\" } } # Print availability status print ( \" \\n \" + \"=\" * 60 ) print ( \"FINETUNEHUB - BACKEND AVAILABILITY\" ) print ( \"=\" * 60 ) for backend_name , info in backends . items (): print ( f \" { backend_name : 10s } : { info [ 'status' ] } \" ) print ( f \" { info [ 'description' ] } \" ) print ( \"=\" * 60 ) print ( \"Note: When TRL is selected, Unsloth is disabled to prevent interference.\" ) print ( \" When Unsloth is selected, TRL-only mode is cleared.\" ) print ( \"=\" * 60 + \" \\n \" ) return backends options: show_source: true heading_level: 3","title":"list_backends()"},{"location":"api-reference/backend-factory/#enums-and-types","text":"","title":"Enums and Types"},{"location":"api-reference/backend-factory/#backendtype-enum","text":"Bases: Enum Backend types for training. Source code in src/aligntune/core/backend_factory.py 202 203 204 205 class BackendType ( Enum ): \"\"\"Backend types for training.\"\"\" UNSLOTH = \"unsloth\" # Unsloth backend (fast, memory efficient) TRL = \"trl\" # TRL backend (standard, reliable) options: show_source: true heading_level: 3","title":"BackendType Enum"},{"location":"api-reference/backend-factory/#trainingtype-enum","text":"Bases: Enum Training types supported by AlignTune. Source code in src/aligntune/core/backend_factory.py 195 196 197 198 199 class TrainingType ( Enum ): \"\"\"Training types supported by AlignTune.\"\"\" SFT = \"sft\" # Supervised Fine-Tuning RL = \"rl\" # Reinforcement Learning options: show_source: true heading_level: 3","title":"TrainingType Enum"},{"location":"api-reference/backend-factory/#rlalgorithm-enum","text":"Bases: Enum RL algorithms supported. Source code in src/aligntune/core/backend_factory.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class RLAlgorithm ( Enum ): \"\"\"RL algorithms supported.\"\"\" DPO = \"dpo\" # Direct Preference Optimization PPO = \"ppo\" # Proximal Policy Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective # New COUNTERFACT_GRPO = \"counterfact_grpo\" # NEW: Add GBMPO GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3","title":"RLAlgorithm Enum"},{"location":"api-reference/backend-factory/#backendconfig-dataclass","text":"Configuration for backend selection with task type support. Source code in src/aligntune/core/backend_factory.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 @dataclass class BackendConfig : \"\"\"Configuration for backend selection with task type support.\"\"\" training_type : TrainingType backend : BackendType algorithm : Optional [ RLAlgorithm ] = None # Only for RL task_type : Optional [ TaskType ] = None # NEW: For SFT tasks fallback_enabled : bool = True def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING","title":"BackendConfig Dataclass"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendConfig.__post_init__","text":"Validate configuration. Source code in src/aligntune/core/backend_factory.py 237 238 239 240 241 242 243 244 245 246 247 248 def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/backend-factory/#examples","text":"","title":"Examples"},{"location":"api-reference/backend-factory/#sft-with-trl-backend","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" )","title":"SFT with TRL Backend"},{"location":"api-reference/backend-factory/#sft-with-unsloth-backend","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" )","title":"SFT with Unsloth Backend"},{"location":"api-reference/backend-factory/#dpo-training","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"DPO Training"},{"location":"api-reference/backend-factory/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" )","title":"PPO Training"},{"location":"api-reference/backend-factory/#error-handling","text":"","title":"Error Handling"},{"location":"api-reference/backend-factory/#backend-not-available","text":"try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ValueError as e : print ( f \"Backend not available: { e } \" ) # Falls back to TRL automatically","title":"Backend Not Available"},{"location":"api-reference/backend-factory/#invalid-algorithm","text":"try : trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"invalid\" ) except ValueError as e : print ( f \"Invalid algorithm: { e } \" )","title":"Invalid Algorithm"},{"location":"api-reference/backend-factory/#next-steps","text":"Configuration Classes - Configuration options Trainers - Trainer methods User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/configuration/","text":"Configuration Classes API Reference \u00b6 Complete API reference for AlignTune configuration classes. SFT Configuration \u00b6 SFTConfig \u00b6 Main configuration class for SFT training. Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion Source code in src/aligntune/core/sft/config.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @dataclass class SFTConfig : \"\"\" Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion \"\"\" model : ModelConfig dataset : DatasetConfig train : TrainingConfig = field ( default_factory = TrainingConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) evaluation : EvaluationConfig = field ( default_factory = EvaluationConfig ) def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () def _apply_task_specific_settings ( self ): \"\"\"Apply task-specific configuration settings.\"\"\" task_type = self . dataset . task_type if task_type == TaskType . TEXT_CLASSIFICATION : # Classification tasks should not use Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for text classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 2 # Binary classification default elif task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification also not ideal for Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for token classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 9 # Common for NER elif task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . CHAT_COMPLETION ]: # These tasks benefit from longer sequences if self . model . max_seq_length < 1024 : import logging logger = logging . getLogger ( __name__ ) logger . info ( f \"Task { task_type . value } typically benefits from longer sequences. \" f \"Consider increasing max_seq_length from { self . model . max_seq_length } .\" ) elif task_type == TaskType . TEXT_GENERATION : # Generation tasks can benefit from Unsloth if not self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . info ( \"Text generation tasks can benefit from Unsloth acceleration. \" \"Consider setting use_unsloth=True.\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks __post_init__ () \u00b6 Validate unified configuration and apply task-specific settings. Source code in src/aligntune/core/sft/config.py 341 342 343 344 345 346 347 348 349 350 351 def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () from_dict ( config_dict ) classmethod \u00b6 Create configuration from dictionary. Source code in src/aligntune/core/sft/config.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) get_recommended_backend () \u00b6 Get recommended backend for this task type. Source code in src/aligntune/core/sft/config.py 477 478 479 480 481 482 def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks get_task_type () \u00b6 Get the task type for this configuration. Source code in src/aligntune/core/sft/config.py 452 453 454 def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type is_classification_task () \u00b6 Check if this is a classification task. Source code in src/aligntune/core/sft/config.py 456 457 458 459 460 461 def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] is_generation_task () \u00b6 Check if this is a generation task. Source code in src/aligntune/core/sft/config.py 463 464 465 466 467 468 469 470 def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] supports_unsloth () \u00b6 Check if Unsloth backend is suitable for this task. Source code in src/aligntune/core/sft/config.py 472 473 474 475 def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () to_dict () \u00b6 Convert configuration to dictionary. Source code in src/aligntune/core/sft/config.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" ), train = TrainingConfig ( epochs = 3 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" ) ) ModelConfig (SFT) \u00b6 Model configuration for SFT training. Model configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ModelConfig : \"\"\"Model configuration with task-aware defaults.\"\"\" name_or_path : str precision : PrecisionType = PrecisionType . BF16 quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = True max_memory : Optional [ Dict [ str , str ]] = None use_unsloth : bool = False max_seq_length : int = 2048 peft_enabled : bool = False lora_rank : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.1 target_modules : Optional [ List [ str ]] = None bias : str = \"none\" use_gradient_checkpointing : bool = True # Classification-specific num_labels : Optional [ int ] = None model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) device_map : Optional [ Union [ str , Dict ]] = \"auto\" # Add this line trust_remote_code : bool = True def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) __post_init__ () \u00b6 Validate model configuration. Source code in src/aligntune/core/sft/config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) options: show_source: true heading_level: 3 DatasetConfig (SFT) \u00b6 Dataset configuration for SFT training. Dataset configuration with task-specific field support. Source code in src/aligntune/core/sft/config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @dataclass class DatasetConfig : \"\"\"Dataset configuration with task-specific field support.\"\"\" name : str split : str = \"train\" subset : Optional [ str ] = None # Add this line for dataset config/subset config : Optional [ str ] = None # Alternative name for subset percent : Optional [ float ] = None max_samples : Optional [ int ] = None column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : TaskType = TaskType . SUPERVISED_FINE_TUNING system_prompt : Optional [ str ] = None # Auto-detection auto_detect_fields : bool = False format_type : Optional [ str ] = None # Common fields for all tasks text_column : str = \"text\" # Instruction/SFT specific instruction_column : str = \"instruction\" response_column : str = \"response\" output_column : str = \"output\" input_column : str = \"input\" context_column : str = \"context\" # Classification specific label_column : str = \"label\" # Token classification specific tokens_column : str = \"tokens\" tags_column : str = \"ner_tags\" # Chat specific messages_column : str = \"messages\" # Dataset text field for trainers dataset_text_field : str = \"text\" # Chat template chat_template : Optional [ str ] = None dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None # Processing and Filtering (Added to match RLDatasetConfig and fix factory error) preserve_columns : Optional [ List [ str ]] = None processing_fn : Optional [ Callable ] = None processing_batched : bool = False processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () def _set_task_defaults ( self ): \"\"\"Set task-specific default field names.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : # Ensure we have instruction and response columns if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TEXT_CLASSIFICATION : # Classification needs text and label if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification needs tokens and tags if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . CHAT_COMPLETION : # Chat needs messages field if not self . column_mapping : self . column_mapping = {} __post_init__ () \u00b6 Validate dataset configuration. Source code in src/aligntune/core/sft/config.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () options: show_source: true heading_level: 3 TrainingConfig (SFT) \u00b6 Training configuration for SFT. Training configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @dataclass class TrainingConfig : \"\"\"Training configuration with task-aware defaults.\"\"\" per_device_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None learning_rate : float = 1e-5 weight_decay : float = 0.01 warmup_steps : int = 0 warmup_ratio : float = 0.1 eval_interval : int = 100 save_interval : int = 500 max_grad_norm : float = 1.0 fp16 : bool = False bf16 : bool = False dataloader_num_workers : int = 0 remove_unused_columns : bool = False # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" # Advanced training settings group_by_length : bool = False dataloader_drop_last : bool = False eval_accumulation_steps : Optional [ int ] = None label_smoothing_factor : float = 0.0 # Early stopping early_stopping_patience : Optional [ int ] = None early_stopping_threshold : float = 0.0 # Evaluation load_best_model_at_end : bool = True metric_for_best_model : str = \"eval_loss\" greater_is_better : bool = False # TRL specific use_trl : bool = False dataset_num_proc : Optional [ int ] = None dataset_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Sequence packing and padding packing : bool = False packing_strategy : str = \"bfd\" eval_packing : Optional [ bool ] = None padding_free : bool = False pad_to_multiple_of : Optional [ int ] = None # Loss and masking controls completion_only_loss : Optional [ bool ] = None assistant_only_loss : bool = False loss_type : str = \"nll\" activation_offloading : bool = False use_flash_attention_2 : Optional [ bool ] = None gradient_checkpointing : bool = False # Added gradient_checkpointing_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Added extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) __post_init__ () \u00b6 Validate training configuration. Source code in src/aligntune/core/sft/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) options: show_source: true heading_level: 3 EvaluationConfig (SFT) \u00b6 Evaluation configuration for SFT. Evaluation configuration for SFT models. Source code in src/aligntune/core/sft/config.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @dataclass class EvaluationConfig : \"\"\"Evaluation configuration for SFT models.\"\"\" # Which metrics to compute compute_perplexity : bool = True compute_rouge : bool = True compute_bleu : bool = True compute_meteor : bool = False # Optional: requires nltk compute_bertscore : bool = False # Optional: requires bert-score compute_semantic_similarity : bool = False # Optional: requires sentence-transformers compute_codebleu : bool = False # Optional: for code tasks, requires codebleu # Custom metrics: list of callables taking (prediction, reference, instruction) -> Dict[str, float] custom_metrics : Optional [ List [ Callable [[ str , str , str ], Dict [ str , float ]]]] = None # Evaluation parameters max_samples_for_quality_metrics : int = 50 # Limit samples for faster evaluation bertscore_model : str = \"microsoft/deberta-xlarge-mnli\" # Model for BERTScore semantic_similarity_model : str = \"sentence-transformers/all-MiniLM-L6-v2\" # For semantic similarity def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) __post_init__ () \u00b6 Validate evaluation configuration. Source code in src/aligntune/core/sft/config.py 297 298 299 300 def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) options: show_source: true heading_level: 3 LoggingConfig (SFT) \u00b6 Logging configuration for SFT. Logging configuration. Source code in src/aligntune/core/sft/config.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" output_dir : str = \"./output\" run_name : Optional [ str ] = None loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) log_level : str = \"INFO\" log_interval : int = 10 save_strategy : str = \"steps\" eval_strategy : str = \"steps\" report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) __post_init__ () \u00b6 Validate logging configuration. Source code in src/aligntune/core/sft/config.py 315 316 317 318 319 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3 TaskType Enum (SFT) \u00b6 Task type enumeration for SFT. Bases: Enum Supported SFT task types. Source code in src/aligntune/core/sft/config.py 16 17 18 19 20 21 22 23 class TaskType ( Enum ): \"\"\"Supported SFT task types.\"\"\" INSTRUCTION_FOLLOWING = \"instruction_following\" SUPERVISED_FINE_TUNING = \"supervised_fine_tuning\" TEXT_CLASSIFICATION = \"text_classification\" TOKEN_CLASSIFICATION = \"token_classification\" TEXT_GENERATION = \"text_generation\" CHAT_COMPLETION = \"chat_completion\" options: show_source: true heading_level: 3 PrecisionType Enum \u00b6 Model precision type enumeration. Bases: Enum Model precision types. Source code in src/aligntune/core/sft/config.py 32 33 34 35 36 37 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3 RL Configuration \u00b6 UnifiedConfig \u00b6 Main configuration class for RL training. Unified configuration for RLHF training with no placeholder defaults. Source code in src/aligntune/core/rl/config.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 @dataclass class UnifiedConfig : \"\"\"Unified configuration for RLHF training with no placeholder defaults.\"\"\" algo : AlgorithmType model : ModelConfig datasets : List [ DatasetConfig ] tasks : List [ Dict [ str , Any ]] = field ( default_factory = list ) rewards : List [ RewardConfig ] = field ( default_factory = list ) train : TrainingConfig = field ( default_factory = TrainingConfig ) distributed : DistributedConfig = field ( default_factory = DistributedConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) chat_template : Optional [ str ] = None caching : Dict [ str , Any ] = field ( default_factory = dict ) reward_training : Optional [ RewardModelTrainingConfig ] = None def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} __post_init__ () \u00b6 Validate unified configuration. Source code in src/aligntune/core/rl/config.py 705 706 707 708 709 710 711 def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) to_dict () \u00b6 Convert configuration to dictionary. Source code in src/aligntune/core/rl/config.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" )], train = TrainingConfig ( epochs = 1 , learning_rate = 1e-6 ), logging = LoggingConfig ( output_dir = \"./output\" ) ) ModelConfig (RL) \u00b6 Model configuration for RL training. Model configuration with no default model names. Source code in src/aligntune/core/rl/config.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @dataclass class ModelConfig : \"\"\"Model configuration with no default model names.\"\"\" name_or_path : str backend : str = \"trl\" # Backend: \"trl\" or \"unsloth\" sft_path : Optional [ str ] = None reward_path : Optional [ str ] = None reward_model_name : Optional [ str ] = None # NEW: Separate reward model reward_model_source : Optional [ 'RewardModelSourceConfig' ] = None # NEW: Reward model source configuration precision : PrecisionType = PrecisionType . AUTO quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = False max_memory : Optional [ Dict [ str , str ]] = None device_map : Optional [ Union [ str , Dict [ str , int ]]] = None # Added device_map use_unsloth : bool = False # Enable Unsloth acceleration max_seq_length : int = 2048 # For Unsloth reward_value_model : str = None # Llama model for reward/value (loaded pre-Unsloth to avoid patches) reward_value_loading_type : Optional [ str ] = None # 'unsloth' or 'standard' reward_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) value_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) use_peft : bool = True # Enable PEFT (LoRA) by default lora_r : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.05 lora_target_modules : List [ str ] = field ( default_factory = lambda : [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]) trust_remote_code : bool = True # Trust remote code for model loading # New parameters from model initialization model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) ref_model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) model_adapter_name : Optional [ str ] = None ref_adapter_name : Optional [ str ] = None force_use_ref_model : bool = False disable_dropout : bool = True use_logits_to_keep : bool = False reward_device : str = \"auto\" def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized __post_init__ () \u00b6 Validate model configuration. Source code in src/aligntune/core/rl/config.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized options: show_source: true heading_level: 3 DatasetConfig (RL) \u00b6 Dataset configuration for RL training. Dataset configuration with flexible field mapping support. Source code in src/aligntune/core/rl/config.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @dataclass class DatasetConfig : \"\"\"Dataset configuration with flexible field mapping support.\"\"\" name : str split : str = \"train\" percent : Optional [ float ] = None max_samples : Optional [ int ] = None max_eval_samples : Optional [ int ] = None # Enhanced field mapping system field_mappings : Dict [ str , str ] = field ( default_factory = dict ) format_type : Optional [ str ] = None # \"alpaca\", \"dolly\", \"ultrachat\", \"custom\" auto_detect_fields : bool = True # Legacy support column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : str = \"conversation\" weight : float = 1.0 chat_template : Optional [ str ] = None system_prompt : Optional [ str ] = None # New parameters from dataset processing dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None label_pad_token_id : int = - 100 truncation_mode : str = 'keep_end' padding_free : bool = False precompute_ref_log_probs : bool = False precompute_ref_batch_size : Optional [ int ] = None tools : Optional [ Any ] = None # ADD THESE NEW PARAMETERS preserve_columns : Optional [ List [ str ]] = None # NEW: Columns to preserve processing_fn : Optional [ Any ] = None # NEW: Custom processing function processing_batched : bool = False # NEW: Whether processing is batched processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # NEW: Args for processing_fn config_name : Optional [ str ] = None def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" ) __post_init__ () \u00b6 Validate dataset configuration. Source code in src/aligntune/core/rl/config.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" ) to_dict () \u00b6 Convert to dictionary. Source code in src/aligntune/core/rl/config.py 157 158 159 160 161 162 163 164 165 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 TrainingConfig (RL) \u00b6 Training configuration for RL. Training configuration. Source code in src/aligntune/core/rl/config.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 @dataclass class TrainingConfig : \"\"\"Training configuration.\"\"\" # Basic training parameters per_device_batch_size : int = 1 per_device_eval_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None eval_interval : int = 100 save_interval : int = 500 learning_rate : float = 1e-5 max_grad_norm : float = 1.0 weight_decay : float = 0.01 use_cache : bool = False , # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" warmup_steps : int = 0 # If 0, calculated from warmup_ratio or defaults to 5% of max_steps warmup_ratio : float = 0.0 # Alternative to warmup_steps # PPO/RL-specific parameters rollout_batch_size : int = 1 kl_coef : float = 0.1 cliprange : float = 0.2 cliprange_value : float = 0.2 num_ppo_epochs : Optional [ int ] = None temperature : float = 0.6 whiten_rewards : bool = False kl_estimator : str = 'k1' vf_coef : float = 0.1 gamma : float = 1.0 lam : float = 0.95 # Generation parameters response_length : int = 128 stop_token : str = 'eos' missing_eos_penalty : float = 1.0 ds3_gather_for_generation : bool = True generation_kwargs : dict = None # Length parameters max_length : int = 1024 max_prompt_length : int = 512 max_target_length : Optional [ int ] = None max_completion_length : int = 256 # Generation sampling parameters top_p : float = 0.95 # Processing parameters padding_free : bool = False truncation_mode : str = 'keep_end' # DPO-specific parameters beta : float = 0.1 loss_type : str = None loss_weights : Optional [ dict ] = None f_divergence_type : str = 'reverse_kl' f_alpha_divergence_coef : float = 1.0 reference_free : bool = False label_smoothing : float = 0.0 use_weighting : bool = False # Algorithm-specific parameters rpo_alpha : Optional [ float ] = None ld_alpha : Optional [ float ] = None discopop_tau : float = 0.05 # Reference model parameters sync_ref_model : bool = False ref_model_mixup_alpha : float = 0.6 ref_model_sync_steps : int = 512 # GRPO-specific parameters grpo_alpha : float = 0.1 grpo_beta : float = 0.1 # GSPO-specific parameters gspo_gamma : float = 0.1 gspo_delta : float = 0.1 # Evaluation parameters eval_steps : Optional [ int ] = 100 eval_strategy : str = \"no\" # DPO-specific evaluation options dpo_eval_enabled : bool = False dpo_eval_max_samples : Optional [ int ] = None dpo_zero_shot_max_samples : int = 50 dpo_few_shot_max_samples : int = 30 dpo_few_shot_examples_text : Optional [ str ] = None # Checkpointing parameters save_steps : int = 500 save_strategy : str = \"steps\" save_total_limit : Optional [ int ] = None load_best_model_at_end : bool = False metric_for_best_model : Optional [ str ] = None greater_is_better : bool = False # Logging parameters logging_steps : int = 10 logging_strategy : str = \"steps\" # Generation parameters (GRPO-specific) num_generations : Optional [ int ] = None mask_truncated_completions : bool = True scale_rewards : str = \"group\" reward_weights : Optional [ List [ float ]] = None enable_thinking : bool = False # Qwen3 thinking mode fast_inference : bool = False # Unsloth vLLM fast inference (2-3x faster generation) vllm_gpu_memory_utilization : float = 0.7 # vLLM GPU memory (0.95 for max speed with spare VRAM) # Seed parameters seed : Optional [ int ] = 42 data_seed : Optional [ int ] = 47 # Match training_script.py for fair comparison # Meta-ES specific meta_iterations : int = 15 patience : int = 5 min_delta : float = 0.001 init_scale : float = 0.01 N : int = 10 # Population size (must be even) T : int = 100 # Training steps per evaluation sigma : float = 0.01 # ES noise std sigma_decay : float = 0.99 # ES sigma decay per iteration alpha : float = 0.01 # ES learning rate mirror_coefficient : float = 0.0001 debug_mode : bool = False eval_timeout : int = 5 eval_max_tokens : int = 512 eval_k : int = 1 eval_temperature : float = 0.8 num_workers : int = 1 no_wandb : bool = False wandb_project : str = \"neural-mirror-es\" resume : Optional [ str ] = None use_rewards_directly : Optional [ List [ callable ]] = None # ============================================================================ # NEW: Counterfactual GRPO-specific parameters # ============================================================================ boost_factor : float = 2.0 \"\"\"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 \"\"\" min_weight : float = 0.5 \"\"\"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 \"\"\" max_spans : int = 10 \"\"\"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 \"\"\" answer_weight : float = 1.5 \"\"\"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 \"\"\" weighting_mode : Optional [ str ] = None \"\"\"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. \"\"\" method_name : str = \"counterfactual\" \"\"\"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) \"\"\" random_importance : bool = False \"\"\"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. \"\"\" invert_importance : bool = False \"\"\"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. \"\"\" enable_gradient_conservation : bool = True \"\"\"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) \"\"\" weight_debug : bool = False \"\"\"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). \"\"\" # ============================================================================ # NEW: GBMPO-specific parameters (Generalized Bregman Mirror Descent) # ============================================================================ gbmpo_l2_coefficient : float = 0.0001 \"\"\"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 \"\"\" gbmpo_divergence_type : Optional [ str ] = None \"\"\"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. \"\"\" gbmpo_epsilon : float = 0.2 \"\"\"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 \"\"\" # Performance optimization use_liger_kernel : bool = False use_liger_loss : Optional [ bool ] = None # BOLT-specific parameters (Baseline-Optimized Learning Technique) curriculum_enabled : bool = False # Enable uncertainty-based sampling curriculum_epsilon : float = 0.05 # Floor for sampling weights curriculum_update_freq : int = 10 # Steps between weight updates baseline_enabled : bool = False # Enable persistent baseline tracking baseline_rho_min : float = 0.875 # Min forgetting factor (fast adaptation) baseline_rho_max : float = 0.96 # Max forgetting factor (slow adaptation) baseline_D_half : float = 0.5 # KL half-life for adaptive forgetting baseline_warm_start : Optional [ str ] = None # Path to JSON/PKL for warm-start use_baseline_advantages : bool = False # Use A = r - v\u0302(x) vs group mean group_by_length : bool = True extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) answer_weight = 1.5 class-attribute instance-attribute \u00b6 Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 boost_factor = 2.0 class-attribute instance-attribute \u00b6 Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 enable_gradient_conservation = True class-attribute instance-attribute \u00b6 Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) gbmpo_divergence_type = None class-attribute instance-attribute \u00b6 Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. gbmpo_epsilon = 0.2 class-attribute instance-attribute \u00b6 PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 gbmpo_l2_coefficient = 0.0001 class-attribute instance-attribute \u00b6 L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 invert_importance = False class-attribute instance-attribute \u00b6 Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. max_spans = 10 class-attribute instance-attribute \u00b6 Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 method_name = 'counterfactual' class-attribute instance-attribute \u00b6 Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) min_weight = 0.5 class-attribute instance-attribute \u00b6 Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 random_importance = False class-attribute instance-attribute \u00b6 Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. weight_debug = False class-attribute instance-attribute \u00b6 Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). weighting_mode = None class-attribute instance-attribute \u00b6 Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. __post_init__ () \u00b6 Validate training configuration. Source code in src/aligntune/core/rl/config.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) options: show_source: true heading_level: 3 RewardConfig \u00b6 Reward function configuration. Reward function configuration. Source code in src/aligntune/core/rl/config.py 182 183 184 185 186 187 188 189 190 @dataclass class RewardConfig : \"\"\"Reward function configuration.\"\"\" type : str weight : float = 1.0 params : Dict [ str , Any ] = field ( default_factory = dict ) shield : bool = False clip : Optional [ float ] = None normalize : bool = False options: show_source: true heading_level: 3 RewardModelSourceConfig \u00b6 Reward model source configuration. Reward model source - EXACTLY ONE must be specified. Source code in src/aligntune/core/rl/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 @dataclass class RewardModelSourceConfig : \"\"\"Reward model source - EXACTLY ONE must be specified.\"\"\" source_type : str # Must be set explicitly model_name : Optional [ str ] = None model_path : Optional [ str ] = None training_config : Optional [ RewardModelTrainingConfig ] = None fine_tune_with_rewards : bool = False # NEW: Enable hybrid mode to fine-tune pretrained with reward functions def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result __post_init__ () \u00b6 Validate source configuration with strict mutual exclusivity. Source code in src/aligntune/core/rl/config.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) to_dict () \u00b6 Convert to dictionary. Source code in src/aligntune/core/rl/config.py 288 289 290 291 292 293 294 295 296 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 RewardModelTrainingConfig \u00b6 Reward model training configuration. Configuration for reward model training - ALL fields validated. Source code in src/aligntune/core/rl/config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @dataclass class RewardModelTrainingConfig : \"\"\"Configuration for reward model training - ALL fields validated.\"\"\" base_model_name : str # NO default - must be explicit training_texts : List [ str ] # NO default - must provide data reward_functions : List [ str ] # NO default - must specify output_dir : str # NO default - must specify where to save # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training params with JUSTIFIED defaults num_epochs : int = 3 # Standard for reward model training learning_rate : float = 1e-5 # Standard LM fine-tuning rate batch_size : int = 8 # Balanced for memory/speed gradient_accumulation_steps : int = 4 # For effective batch size 32 max_length : int = 512 # Standard transformer length def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) __post_init__ () \u00b6 Validate ALL fields with strict validation. Source code in src/aligntune/core/rl/config.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) options: show_source: true heading_level: 3 LoggingConfig (RL) \u00b6 Logging configuration for RL. Logging configuration. Source code in src/aligntune/core/rl/config.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) run_name : Optional [ str ] = None output_dir : str = \"./output\" log_level : str = \"INFO\" sample_logging : SampleLoggingConfig = field ( default_factory = SampleLoggingConfig ) report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) __post_init__ () \u00b6 Validate logging configuration. Source code in src/aligntune/core/rl/config.py 658 659 660 661 662 663 664 665 666 667 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3 SampleLoggingConfig \u00b6 Sample logging configuration for qualitative generation. Configuration for qualitative sample logging. Source code in src/aligntune/core/rl/config.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 @dataclass class SampleLoggingConfig : \"\"\"Configuration for qualitative sample logging.\"\"\" enabled : bool = False prompts : Optional [ List [ str ]] = None interval_steps : Optional [ int ] = None percent_of_max_steps : Optional [ float ] = None max_new_tokens : int = 80 temperature : float = 0.6 top_p : float = 0.9 num_samples : int = 3 def __post_init__ ( self ): if self . interval_steps is not None and self . interval_steps <= 0 : raise ValueError ( \"sample_logging.interval_steps must be positive when set\" ) if self . percent_of_max_steps is not None : if self . percent_of_max_steps <= 0 or self . percent_of_max_steps > 1 : raise ValueError ( \"sample_logging.percent_of_max_steps must be in (0, 1]\" ) if self . max_new_tokens <= 0 : raise ValueError ( \"sample_logging.max_new_tokens must be positive\" ) if not ( 0 < self . temperature ): raise ValueError ( \"sample_logging.temperature must be positive\" ) if not ( 0 < self . top_p <= 1 ): raise ValueError ( \"sample_logging.top_p must be in (0, 1]\" ) if self . num_samples <= 0 : raise ValueError ( \"sample_logging.num_samples must be positive\" ) options: show_source: true heading_level: 3 DistributedConfig \u00b6 Distributed training configuration. Distributed training configuration. Source code in src/aligntune/core/rl/config.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 @dataclass class DistributedConfig : \"\"\"Distributed training configuration.\"\"\" backend : BackendType = BackendType . SINGLE fsdp_config : Dict [ str , Any ] = field ( default_factory = dict ) deepspeed_config : Dict [ str , Any ] = field ( default_factory = dict ) nodes : int = 1 gpus_per_node : int = 1 seed : int = 42 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) __post_init__ () \u00b6 Validate distributed configuration. Source code in src/aligntune/core/rl/config.py 605 606 607 608 609 610 611 612 613 614 615 616 617 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) options: show_source: true heading_level: 3 AlgorithmType Enum \u00b6 RL algorithm type enumeration. Bases: Enum Supported RLHF algorithms. Source code in src/aligntune/core/rl/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 class AlgorithmType ( Enum ): \"\"\"Supported RLHF algorithms.\"\"\" PPO = \"ppo\" DPO = \"dpo\" GRPO = \"grpo\" GSPO = \"gspo\" COUNTERFACT_GRPO = \"counterfact_grpo\" GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3 PrecisionType Enum (RL) \u00b6 Model precision type enumeration for RL. Bases: Enum Model precision types. Source code in src/aligntune/core/rl/config.py 30 31 32 33 34 35 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3 BackendType Enum (RL) \u00b6 Distributed training backend enumeration. Bases: Enum Distributed training backends. Source code in src/aligntune/core/rl/config.py 38 39 40 41 42 43 class BackendType ( Enum ): \"\"\"Distributed training backends.\"\"\" SINGLE = \"single\" DDP = \"ddp\" FSDP = \"fsdp\" DEEPSPEED = \"deepspeed\" options: show_source: true heading_level: 3 Configuration Loaders \u00b6 SFTConfigLoader \u00b6 Configuration loader for SFT configs. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config load_from_yaml ( path ) staticmethod \u00b6 Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config save_config ( config , path ) staticmethod \u00b6 Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) validate_config ( config ) staticmethod \u00b6 Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 ConfigLoader (RL) \u00b6 Configuration loader for RL configs. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) create_config_from_cli_args ( ** kwargs ) staticmethod \u00b6 Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) load_from_yaml ( path ) staticmethod \u00b6 Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) parse_dataset_spec ( spec ) staticmethod \u00b6 Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) parse_reward_spec ( spec ) staticmethod \u00b6 Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) save_resolved_config ( config , output_dir ) staticmethod \u00b6 Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) validate_config ( config ) staticmethod \u00b6 Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Helper Functions \u00b6 create_instruction_following_config() \u00b6 Create configuration for instruction following. Create configuration for instruction following tasks. Source code in src/aligntune/core/sft/config.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def create_instruction_following_config ( model_name : str , dataset_name : str , output_dir : str = \"./output/instruction\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for instruction following tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 1024 ), use_unsloth = kwargs . get ( 'use_unsloth' , True ), peft_enabled = kwargs . get ( 'peft_enabled' , True ), quantization = kwargs . get ( 'quantization' , { \"load_in_4bit\" : True }) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . INSTRUCTION_FOLLOWING , instruction_column = kwargs . get ( 'instruction_column' , 'instruction' ), response_column = kwargs . get ( 'response_column' , 'output' ), context_column = kwargs . get ( 'context_column' , 'input' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 4 ), learning_rate = kwargs . get ( 'learning_rate' , 2e-4 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'instruction_following' ) ) ) options: show_source: true heading_level: 3 create_text_classification_config() \u00b6 Create configuration for text classification. Create configuration for text classification tasks. Source code in src/aligntune/core/sft/config.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def create_text_classification_config ( model_name : str , dataset_name : str , num_labels : int , output_dir : str = \"./output/classification\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for text classification tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 512 ), use_unsloth = False , # Not recommended for classification peft_enabled = kwargs . get ( 'peft_enabled' , True ), num_labels = num_labels , quantization = kwargs . get ( 'quantization' , {}) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . TEXT_CLASSIFICATION , text_column = kwargs . get ( 'text_column' , 'text' ), label_column = kwargs . get ( 'label_column' , 'label' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 8 ), learning_rate = kwargs . get ( 'learning_rate' , 5e-5 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'text_classification' ) ) ) options: show_source: true heading_level: 3 Usage Examples \u00b6 Creating SFT Configuration \u00b6 from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" , precision = \"bf16\" , max_seq_length = 512 ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" , max_samples = 1000 , task_type = \"instruction_following\" ), train = TrainingConfig ( epochs = 3 , per_device_batch_size = 4 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_sft_training\" ) ) Creating RL Configuration \u00b6 from aligntune.core.rl.config import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig ) config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , use_peft = True , lora_r = 16 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , max_samples = 1000 ) ], train = TrainingConfig ( epochs = 1 , per_device_batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 # DPO parameter ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_dpo_training\" ) ) Loading from YAML \u00b6 from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) Next Steps \u00b6 Backend Factory - Trainer creation Trainers - Trainer classes User Guide - Usage guide","title":"Configuration Classes"},{"location":"api-reference/configuration/#configuration-classes-api-reference","text":"Complete API reference for AlignTune configuration classes.","title":"Configuration Classes API Reference"},{"location":"api-reference/configuration/#sft-configuration","text":"","title":"SFT Configuration"},{"location":"api-reference/configuration/#sftconfig","text":"Main configuration class for SFT training. Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion Source code in src/aligntune/core/sft/config.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @dataclass class SFTConfig : \"\"\" Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion \"\"\" model : ModelConfig dataset : DatasetConfig train : TrainingConfig = field ( default_factory = TrainingConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) evaluation : EvaluationConfig = field ( default_factory = EvaluationConfig ) def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () def _apply_task_specific_settings ( self ): \"\"\"Apply task-specific configuration settings.\"\"\" task_type = self . dataset . task_type if task_type == TaskType . TEXT_CLASSIFICATION : # Classification tasks should not use Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for text classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 2 # Binary classification default elif task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification also not ideal for Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for token classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 9 # Common for NER elif task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . CHAT_COMPLETION ]: # These tasks benefit from longer sequences if self . model . max_seq_length < 1024 : import logging logger = logging . getLogger ( __name__ ) logger . info ( f \"Task { task_type . value } typically benefits from longer sequences. \" f \"Consider increasing max_seq_length from { self . model . max_seq_length } .\" ) elif task_type == TaskType . TEXT_GENERATION : # Generation tasks can benefit from Unsloth if not self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . info ( \"Text generation tasks can benefit from Unsloth acceleration. \" \"Consider setting use_unsloth=True.\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks","title":"SFTConfig"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.__post_init__","text":"Validate unified configuration and apply task-specific settings. Source code in src/aligntune/core/sft/config.py 341 342 343 344 345 346 347 348 349 350 351 def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings ()","title":"__post_init__"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.from_dict","text":"Create configuration from dictionary. Source code in src/aligntune/core/sft/config.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config )","title":"from_dict"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.get_recommended_backend","text":"Get recommended backend for this task type. Source code in src/aligntune/core/sft/config.py 477 478 479 480 481 482 def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks","title":"get_recommended_backend"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.get_task_type","text":"Get the task type for this configuration. Source code in src/aligntune/core/sft/config.py 452 453 454 def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type","title":"get_task_type"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.is_classification_task","text":"Check if this is a classification task. Source code in src/aligntune/core/sft/config.py 456 457 458 459 460 461 def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]","title":"is_classification_task"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.is_generation_task","text":"Check if this is a generation task. Source code in src/aligntune/core/sft/config.py 463 464 465 466 467 468 469 470 def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]","title":"is_generation_task"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.supports_unsloth","text":"Check if Unsloth backend is suitable for this task. Source code in src/aligntune/core/sft/config.py 472 473 474 475 def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task ()","title":"supports_unsloth"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.to_dict","text":"Convert configuration to dictionary. Source code in src/aligntune/core/sft/config.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" ), train = TrainingConfig ( epochs = 3 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" ) )","title":"to_dict"},{"location":"api-reference/configuration/#modelconfig-sft","text":"Model configuration for SFT training. Model configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ModelConfig : \"\"\"Model configuration with task-aware defaults.\"\"\" name_or_path : str precision : PrecisionType = PrecisionType . BF16 quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = True max_memory : Optional [ Dict [ str , str ]] = None use_unsloth : bool = False max_seq_length : int = 2048 peft_enabled : bool = False lora_rank : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.1 target_modules : Optional [ List [ str ]] = None bias : str = \"none\" use_gradient_checkpointing : bool = True # Classification-specific num_labels : Optional [ int ] = None model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) device_map : Optional [ Union [ str , Dict ]] = \"auto\" # Add this line trust_remote_code : bool = True def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" )","title":"ModelConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.ModelConfig.__post_init__","text":"Validate model configuration. Source code in src/aligntune/core/sft/config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#datasetconfig-sft","text":"Dataset configuration for SFT training. Dataset configuration with task-specific field support. Source code in src/aligntune/core/sft/config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @dataclass class DatasetConfig : \"\"\"Dataset configuration with task-specific field support.\"\"\" name : str split : str = \"train\" subset : Optional [ str ] = None # Add this line for dataset config/subset config : Optional [ str ] = None # Alternative name for subset percent : Optional [ float ] = None max_samples : Optional [ int ] = None column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : TaskType = TaskType . SUPERVISED_FINE_TUNING system_prompt : Optional [ str ] = None # Auto-detection auto_detect_fields : bool = False format_type : Optional [ str ] = None # Common fields for all tasks text_column : str = \"text\" # Instruction/SFT specific instruction_column : str = \"instruction\" response_column : str = \"response\" output_column : str = \"output\" input_column : str = \"input\" context_column : str = \"context\" # Classification specific label_column : str = \"label\" # Token classification specific tokens_column : str = \"tokens\" tags_column : str = \"ner_tags\" # Chat specific messages_column : str = \"messages\" # Dataset text field for trainers dataset_text_field : str = \"text\" # Chat template chat_template : Optional [ str ] = None dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None # Processing and Filtering (Added to match RLDatasetConfig and fix factory error) preserve_columns : Optional [ List [ str ]] = None processing_fn : Optional [ Callable ] = None processing_batched : bool = False processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () def _set_task_defaults ( self ): \"\"\"Set task-specific default field names.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : # Ensure we have instruction and response columns if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TEXT_CLASSIFICATION : # Classification needs text and label if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification needs tokens and tags if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . CHAT_COMPLETION : # Chat needs messages field if not self . column_mapping : self . column_mapping = {}","title":"DatasetConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.DatasetConfig.__post_init__","text":"Validate dataset configuration. Source code in src/aligntune/core/sft/config.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#trainingconfig-sft","text":"Training configuration for SFT. Training configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @dataclass class TrainingConfig : \"\"\"Training configuration with task-aware defaults.\"\"\" per_device_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None learning_rate : float = 1e-5 weight_decay : float = 0.01 warmup_steps : int = 0 warmup_ratio : float = 0.1 eval_interval : int = 100 save_interval : int = 500 max_grad_norm : float = 1.0 fp16 : bool = False bf16 : bool = False dataloader_num_workers : int = 0 remove_unused_columns : bool = False # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" # Advanced training settings group_by_length : bool = False dataloader_drop_last : bool = False eval_accumulation_steps : Optional [ int ] = None label_smoothing_factor : float = 0.0 # Early stopping early_stopping_patience : Optional [ int ] = None early_stopping_threshold : float = 0.0 # Evaluation load_best_model_at_end : bool = True metric_for_best_model : str = \"eval_loss\" greater_is_better : bool = False # TRL specific use_trl : bool = False dataset_num_proc : Optional [ int ] = None dataset_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Sequence packing and padding packing : bool = False packing_strategy : str = \"bfd\" eval_packing : Optional [ bool ] = None padding_free : bool = False pad_to_multiple_of : Optional [ int ] = None # Loss and masking controls completion_only_loss : Optional [ bool ] = None assistant_only_loss : bool = False loss_type : str = \"nll\" activation_offloading : bool = False use_flash_attention_2 : Optional [ bool ] = None gradient_checkpointing : bool = False # Added gradient_checkpointing_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Added extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" )","title":"TrainingConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.TrainingConfig.__post_init__","text":"Validate training configuration. Source code in src/aligntune/core/sft/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#evaluationconfig-sft","text":"Evaluation configuration for SFT. Evaluation configuration for SFT models. Source code in src/aligntune/core/sft/config.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @dataclass class EvaluationConfig : \"\"\"Evaluation configuration for SFT models.\"\"\" # Which metrics to compute compute_perplexity : bool = True compute_rouge : bool = True compute_bleu : bool = True compute_meteor : bool = False # Optional: requires nltk compute_bertscore : bool = False # Optional: requires bert-score compute_semantic_similarity : bool = False # Optional: requires sentence-transformers compute_codebleu : bool = False # Optional: for code tasks, requires codebleu # Custom metrics: list of callables taking (prediction, reference, instruction) -> Dict[str, float] custom_metrics : Optional [ List [ Callable [[ str , str , str ], Dict [ str , float ]]]] = None # Evaluation parameters max_samples_for_quality_metrics : int = 50 # Limit samples for faster evaluation bertscore_model : str = \"microsoft/deberta-xlarge-mnli\" # Model for BERTScore semantic_similarity_model : str = \"sentence-transformers/all-MiniLM-L6-v2\" # For semantic similarity def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" )","title":"EvaluationConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.EvaluationConfig.__post_init__","text":"Validate evaluation configuration. Source code in src/aligntune/core/sft/config.py 297 298 299 300 def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#loggingconfig-sft","text":"Logging configuration for SFT. Logging configuration. Source code in src/aligntune/core/sft/config.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" output_dir : str = \"./output\" run_name : Optional [ str ] = None loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) log_level : str = \"INFO\" log_interval : int = 10 save_strategy : str = \"steps\" eval_strategy : str = \"steps\" report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" )","title":"LoggingConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.LoggingConfig.__post_init__","text":"Validate logging configuration. Source code in src/aligntune/core/sft/config.py 315 316 317 318 319 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#tasktype-enum-sft","text":"Task type enumeration for SFT. Bases: Enum Supported SFT task types. Source code in src/aligntune/core/sft/config.py 16 17 18 19 20 21 22 23 class TaskType ( Enum ): \"\"\"Supported SFT task types.\"\"\" INSTRUCTION_FOLLOWING = \"instruction_following\" SUPERVISED_FINE_TUNING = \"supervised_fine_tuning\" TEXT_CLASSIFICATION = \"text_classification\" TOKEN_CLASSIFICATION = \"token_classification\" TEXT_GENERATION = \"text_generation\" CHAT_COMPLETION = \"chat_completion\" options: show_source: true heading_level: 3","title":"TaskType Enum (SFT)"},{"location":"api-reference/configuration/#precisiontype-enum","text":"Model precision type enumeration. Bases: Enum Model precision types. Source code in src/aligntune/core/sft/config.py 32 33 34 35 36 37 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3","title":"PrecisionType Enum"},{"location":"api-reference/configuration/#rl-configuration","text":"","title":"RL Configuration"},{"location":"api-reference/configuration/#unifiedconfig","text":"Main configuration class for RL training. Unified configuration for RLHF training with no placeholder defaults. Source code in src/aligntune/core/rl/config.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 @dataclass class UnifiedConfig : \"\"\"Unified configuration for RLHF training with no placeholder defaults.\"\"\" algo : AlgorithmType model : ModelConfig datasets : List [ DatasetConfig ] tasks : List [ Dict [ str , Any ]] = field ( default_factory = list ) rewards : List [ RewardConfig ] = field ( default_factory = list ) train : TrainingConfig = field ( default_factory = TrainingConfig ) distributed : DistributedConfig = field ( default_factory = DistributedConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) chat_template : Optional [ str ] = None caching : Dict [ str , Any ] = field ( default_factory = dict ) reward_training : Optional [ RewardModelTrainingConfig ] = None def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()}","title":"UnifiedConfig"},{"location":"api-reference/configuration/#core.rl.config.UnifiedConfig.__post_init__","text":"Validate unified configuration. Source code in src/aligntune/core/rl/config.py 705 706 707 708 709 710 711 def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.UnifiedConfig.to_dict","text":"Convert configuration to dictionary. Source code in src/aligntune/core/rl/config.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" )], train = TrainingConfig ( epochs = 1 , learning_rate = 1e-6 ), logging = LoggingConfig ( output_dir = \"./output\" ) )","title":"to_dict"},{"location":"api-reference/configuration/#modelconfig-rl","text":"Model configuration for RL training. Model configuration with no default model names. Source code in src/aligntune/core/rl/config.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @dataclass class ModelConfig : \"\"\"Model configuration with no default model names.\"\"\" name_or_path : str backend : str = \"trl\" # Backend: \"trl\" or \"unsloth\" sft_path : Optional [ str ] = None reward_path : Optional [ str ] = None reward_model_name : Optional [ str ] = None # NEW: Separate reward model reward_model_source : Optional [ 'RewardModelSourceConfig' ] = None # NEW: Reward model source configuration precision : PrecisionType = PrecisionType . AUTO quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = False max_memory : Optional [ Dict [ str , str ]] = None device_map : Optional [ Union [ str , Dict [ str , int ]]] = None # Added device_map use_unsloth : bool = False # Enable Unsloth acceleration max_seq_length : int = 2048 # For Unsloth reward_value_model : str = None # Llama model for reward/value (loaded pre-Unsloth to avoid patches) reward_value_loading_type : Optional [ str ] = None # 'unsloth' or 'standard' reward_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) value_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) use_peft : bool = True # Enable PEFT (LoRA) by default lora_r : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.05 lora_target_modules : List [ str ] = field ( default_factory = lambda : [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]) trust_remote_code : bool = True # Trust remote code for model loading # New parameters from model initialization model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) ref_model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) model_adapter_name : Optional [ str ] = None ref_adapter_name : Optional [ str ] = None force_use_ref_model : bool = False disable_dropout : bool = True use_logits_to_keep : bool = False reward_device : str = \"auto\" def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized","title":"ModelConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.ModelConfig.__post_init__","text":"Validate model configuration. Source code in src/aligntune/core/rl/config.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#datasetconfig-rl","text":"Dataset configuration for RL training. Dataset configuration with flexible field mapping support. Source code in src/aligntune/core/rl/config.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @dataclass class DatasetConfig : \"\"\"Dataset configuration with flexible field mapping support.\"\"\" name : str split : str = \"train\" percent : Optional [ float ] = None max_samples : Optional [ int ] = None max_eval_samples : Optional [ int ] = None # Enhanced field mapping system field_mappings : Dict [ str , str ] = field ( default_factory = dict ) format_type : Optional [ str ] = None # \"alpaca\", \"dolly\", \"ultrachat\", \"custom\" auto_detect_fields : bool = True # Legacy support column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : str = \"conversation\" weight : float = 1.0 chat_template : Optional [ str ] = None system_prompt : Optional [ str ] = None # New parameters from dataset processing dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None label_pad_token_id : int = - 100 truncation_mode : str = 'keep_end' padding_free : bool = False precompute_ref_log_probs : bool = False precompute_ref_batch_size : Optional [ int ] = None tools : Optional [ Any ] = None # ADD THESE NEW PARAMETERS preserve_columns : Optional [ List [ str ]] = None # NEW: Columns to preserve processing_fn : Optional [ Any ] = None # NEW: Custom processing function processing_batched : bool = False # NEW: Whether processing is batched processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # NEW: Args for processing_fn config_name : Optional [ str ] = None def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" )","title":"DatasetConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.DatasetConfig.__post_init__","text":"Validate dataset configuration. Source code in src/aligntune/core/rl/config.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.DatasetConfig.to_dict","text":"Convert to dictionary. Source code in src/aligntune/core/rl/config.py 157 158 159 160 161 162 163 164 165 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3","title":"to_dict"},{"location":"api-reference/configuration/#trainingconfig-rl","text":"Training configuration for RL. Training configuration. Source code in src/aligntune/core/rl/config.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 @dataclass class TrainingConfig : \"\"\"Training configuration.\"\"\" # Basic training parameters per_device_batch_size : int = 1 per_device_eval_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None eval_interval : int = 100 save_interval : int = 500 learning_rate : float = 1e-5 max_grad_norm : float = 1.0 weight_decay : float = 0.01 use_cache : bool = False , # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" warmup_steps : int = 0 # If 0, calculated from warmup_ratio or defaults to 5% of max_steps warmup_ratio : float = 0.0 # Alternative to warmup_steps # PPO/RL-specific parameters rollout_batch_size : int = 1 kl_coef : float = 0.1 cliprange : float = 0.2 cliprange_value : float = 0.2 num_ppo_epochs : Optional [ int ] = None temperature : float = 0.6 whiten_rewards : bool = False kl_estimator : str = 'k1' vf_coef : float = 0.1 gamma : float = 1.0 lam : float = 0.95 # Generation parameters response_length : int = 128 stop_token : str = 'eos' missing_eos_penalty : float = 1.0 ds3_gather_for_generation : bool = True generation_kwargs : dict = None # Length parameters max_length : int = 1024 max_prompt_length : int = 512 max_target_length : Optional [ int ] = None max_completion_length : int = 256 # Generation sampling parameters top_p : float = 0.95 # Processing parameters padding_free : bool = False truncation_mode : str = 'keep_end' # DPO-specific parameters beta : float = 0.1 loss_type : str = None loss_weights : Optional [ dict ] = None f_divergence_type : str = 'reverse_kl' f_alpha_divergence_coef : float = 1.0 reference_free : bool = False label_smoothing : float = 0.0 use_weighting : bool = False # Algorithm-specific parameters rpo_alpha : Optional [ float ] = None ld_alpha : Optional [ float ] = None discopop_tau : float = 0.05 # Reference model parameters sync_ref_model : bool = False ref_model_mixup_alpha : float = 0.6 ref_model_sync_steps : int = 512 # GRPO-specific parameters grpo_alpha : float = 0.1 grpo_beta : float = 0.1 # GSPO-specific parameters gspo_gamma : float = 0.1 gspo_delta : float = 0.1 # Evaluation parameters eval_steps : Optional [ int ] = 100 eval_strategy : str = \"no\" # DPO-specific evaluation options dpo_eval_enabled : bool = False dpo_eval_max_samples : Optional [ int ] = None dpo_zero_shot_max_samples : int = 50 dpo_few_shot_max_samples : int = 30 dpo_few_shot_examples_text : Optional [ str ] = None # Checkpointing parameters save_steps : int = 500 save_strategy : str = \"steps\" save_total_limit : Optional [ int ] = None load_best_model_at_end : bool = False metric_for_best_model : Optional [ str ] = None greater_is_better : bool = False # Logging parameters logging_steps : int = 10 logging_strategy : str = \"steps\" # Generation parameters (GRPO-specific) num_generations : Optional [ int ] = None mask_truncated_completions : bool = True scale_rewards : str = \"group\" reward_weights : Optional [ List [ float ]] = None enable_thinking : bool = False # Qwen3 thinking mode fast_inference : bool = False # Unsloth vLLM fast inference (2-3x faster generation) vllm_gpu_memory_utilization : float = 0.7 # vLLM GPU memory (0.95 for max speed with spare VRAM) # Seed parameters seed : Optional [ int ] = 42 data_seed : Optional [ int ] = 47 # Match training_script.py for fair comparison # Meta-ES specific meta_iterations : int = 15 patience : int = 5 min_delta : float = 0.001 init_scale : float = 0.01 N : int = 10 # Population size (must be even) T : int = 100 # Training steps per evaluation sigma : float = 0.01 # ES noise std sigma_decay : float = 0.99 # ES sigma decay per iteration alpha : float = 0.01 # ES learning rate mirror_coefficient : float = 0.0001 debug_mode : bool = False eval_timeout : int = 5 eval_max_tokens : int = 512 eval_k : int = 1 eval_temperature : float = 0.8 num_workers : int = 1 no_wandb : bool = False wandb_project : str = \"neural-mirror-es\" resume : Optional [ str ] = None use_rewards_directly : Optional [ List [ callable ]] = None # ============================================================================ # NEW: Counterfactual GRPO-specific parameters # ============================================================================ boost_factor : float = 2.0 \"\"\"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 \"\"\" min_weight : float = 0.5 \"\"\"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 \"\"\" max_spans : int = 10 \"\"\"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 \"\"\" answer_weight : float = 1.5 \"\"\"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 \"\"\" weighting_mode : Optional [ str ] = None \"\"\"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. \"\"\" method_name : str = \"counterfactual\" \"\"\"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) \"\"\" random_importance : bool = False \"\"\"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. \"\"\" invert_importance : bool = False \"\"\"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. \"\"\" enable_gradient_conservation : bool = True \"\"\"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) \"\"\" weight_debug : bool = False \"\"\"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). \"\"\" # ============================================================================ # NEW: GBMPO-specific parameters (Generalized Bregman Mirror Descent) # ============================================================================ gbmpo_l2_coefficient : float = 0.0001 \"\"\"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 \"\"\" gbmpo_divergence_type : Optional [ str ] = None \"\"\"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. \"\"\" gbmpo_epsilon : float = 0.2 \"\"\"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 \"\"\" # Performance optimization use_liger_kernel : bool = False use_liger_loss : Optional [ bool ] = None # BOLT-specific parameters (Baseline-Optimized Learning Technique) curriculum_enabled : bool = False # Enable uncertainty-based sampling curriculum_epsilon : float = 0.05 # Floor for sampling weights curriculum_update_freq : int = 10 # Steps between weight updates baseline_enabled : bool = False # Enable persistent baseline tracking baseline_rho_min : float = 0.875 # Min forgetting factor (fast adaptation) baseline_rho_max : float = 0.96 # Max forgetting factor (slow adaptation) baseline_D_half : float = 0.5 # KL half-life for adaptive forgetting baseline_warm_start : Optional [ str ] = None # Path to JSON/PKL for warm-start use_baseline_advantages : bool = False # Use A = r - v\u0302(x) vs group mean group_by_length : bool = True extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" )","title":"TrainingConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.answer_weight","text":"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5","title":"answer_weight"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.boost_factor","text":"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0","title":"boost_factor"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.enable_gradient_conservation","text":"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion)","title":"enable_gradient_conservation"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_divergence_type","text":"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms.","title":"gbmpo_divergence_type"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_epsilon","text":"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3","title":"gbmpo_epsilon"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_l2_coefficient","text":"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005","title":"gbmpo_l2_coefficient"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.invert_importance","text":"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters.","title":"invert_importance"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.max_spans","text":"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20","title":"max_spans"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.method_name","text":"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation)","title":"method_name"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.min_weight","text":"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5","title":"min_weight"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.random_importance","text":"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random.","title":"random_importance"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.weight_debug","text":"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs).","title":"weight_debug"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.weighting_mode","text":"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags.","title":"weighting_mode"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.__post_init__","text":"Validate training configuration. Source code in src/aligntune/core/rl/config.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#rewardconfig","text":"Reward function configuration. Reward function configuration. Source code in src/aligntune/core/rl/config.py 182 183 184 185 186 187 188 189 190 @dataclass class RewardConfig : \"\"\"Reward function configuration.\"\"\" type : str weight : float = 1.0 params : Dict [ str , Any ] = field ( default_factory = dict ) shield : bool = False clip : Optional [ float ] = None normalize : bool = False options: show_source: true heading_level: 3","title":"RewardConfig"},{"location":"api-reference/configuration/#rewardmodelsourceconfig","text":"Reward model source configuration. Reward model source - EXACTLY ONE must be specified. Source code in src/aligntune/core/rl/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 @dataclass class RewardModelSourceConfig : \"\"\"Reward model source - EXACTLY ONE must be specified.\"\"\" source_type : str # Must be set explicitly model_name : Optional [ str ] = None model_path : Optional [ str ] = None training_config : Optional [ RewardModelTrainingConfig ] = None fine_tune_with_rewards : bool = False # NEW: Enable hybrid mode to fine-tune pretrained with reward functions def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result","title":"RewardModelSourceConfig"},{"location":"api-reference/configuration/#core.rl.config.RewardModelSourceConfig.__post_init__","text":"Validate source configuration with strict mutual exclusivity. Source code in src/aligntune/core/rl/config.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.RewardModelSourceConfig.to_dict","text":"Convert to dictionary. Source code in src/aligntune/core/rl/config.py 288 289 290 291 292 293 294 295 296 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3","title":"to_dict"},{"location":"api-reference/configuration/#rewardmodeltrainingconfig","text":"Reward model training configuration. Configuration for reward model training - ALL fields validated. Source code in src/aligntune/core/rl/config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @dataclass class RewardModelTrainingConfig : \"\"\"Configuration for reward model training - ALL fields validated.\"\"\" base_model_name : str # NO default - must be explicit training_texts : List [ str ] # NO default - must provide data reward_functions : List [ str ] # NO default - must specify output_dir : str # NO default - must specify where to save # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training params with JUSTIFIED defaults num_epochs : int = 3 # Standard for reward model training learning_rate : float = 1e-5 # Standard LM fine-tuning rate batch_size : int = 8 # Balanced for memory/speed gradient_accumulation_steps : int = 4 # For effective batch size 32 max_length : int = 512 # Standard transformer length def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" )","title":"RewardModelTrainingConfig"},{"location":"api-reference/configuration/#core.rl.config.RewardModelTrainingConfig.__post_init__","text":"Validate ALL fields with strict validation. Source code in src/aligntune/core/rl/config.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#loggingconfig-rl","text":"Logging configuration for RL. Logging configuration. Source code in src/aligntune/core/rl/config.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) run_name : Optional [ str ] = None output_dir : str = \"./output\" log_level : str = \"INFO\" sample_logging : SampleLoggingConfig = field ( default_factory = SampleLoggingConfig ) report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" )","title":"LoggingConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.LoggingConfig.__post_init__","text":"Validate logging configuration. Source code in src/aligntune/core/rl/config.py 658 659 660 661 662 663 664 665 666 667 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#sampleloggingconfig","text":"Sample logging configuration for qualitative generation. Configuration for qualitative sample logging. Source code in src/aligntune/core/rl/config.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 @dataclass class SampleLoggingConfig : \"\"\"Configuration for qualitative sample logging.\"\"\" enabled : bool = False prompts : Optional [ List [ str ]] = None interval_steps : Optional [ int ] = None percent_of_max_steps : Optional [ float ] = None max_new_tokens : int = 80 temperature : float = 0.6 top_p : float = 0.9 num_samples : int = 3 def __post_init__ ( self ): if self . interval_steps is not None and self . interval_steps <= 0 : raise ValueError ( \"sample_logging.interval_steps must be positive when set\" ) if self . percent_of_max_steps is not None : if self . percent_of_max_steps <= 0 or self . percent_of_max_steps > 1 : raise ValueError ( \"sample_logging.percent_of_max_steps must be in (0, 1]\" ) if self . max_new_tokens <= 0 : raise ValueError ( \"sample_logging.max_new_tokens must be positive\" ) if not ( 0 < self . temperature ): raise ValueError ( \"sample_logging.temperature must be positive\" ) if not ( 0 < self . top_p <= 1 ): raise ValueError ( \"sample_logging.top_p must be in (0, 1]\" ) if self . num_samples <= 0 : raise ValueError ( \"sample_logging.num_samples must be positive\" ) options: show_source: true heading_level: 3","title":"SampleLoggingConfig"},{"location":"api-reference/configuration/#distributedconfig","text":"Distributed training configuration. Distributed training configuration. Source code in src/aligntune/core/rl/config.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 @dataclass class DistributedConfig : \"\"\"Distributed training configuration.\"\"\" backend : BackendType = BackendType . SINGLE fsdp_config : Dict [ str , Any ] = field ( default_factory = dict ) deepspeed_config : Dict [ str , Any ] = field ( default_factory = dict ) nodes : int = 1 gpus_per_node : int = 1 seed : int = 42 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" )","title":"DistributedConfig"},{"location":"api-reference/configuration/#core.rl.config.DistributedConfig.__post_init__","text":"Validate distributed configuration. Source code in src/aligntune/core/rl/config.py 605 606 607 608 609 610 611 612 613 614 615 616 617 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#algorithmtype-enum","text":"RL algorithm type enumeration. Bases: Enum Supported RLHF algorithms. Source code in src/aligntune/core/rl/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 class AlgorithmType ( Enum ): \"\"\"Supported RLHF algorithms.\"\"\" PPO = \"ppo\" DPO = \"dpo\" GRPO = \"grpo\" GSPO = \"gspo\" COUNTERFACT_GRPO = \"counterfact_grpo\" GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3","title":"AlgorithmType Enum"},{"location":"api-reference/configuration/#precisiontype-enum-rl","text":"Model precision type enumeration for RL. Bases: Enum Model precision types. Source code in src/aligntune/core/rl/config.py 30 31 32 33 34 35 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3","title":"PrecisionType Enum (RL)"},{"location":"api-reference/configuration/#backendtype-enum-rl","text":"Distributed training backend enumeration. Bases: Enum Distributed training backends. Source code in src/aligntune/core/rl/config.py 38 39 40 41 42 43 class BackendType ( Enum ): \"\"\"Distributed training backends.\"\"\" SINGLE = \"single\" DDP = \"ddp\" FSDP = \"fsdp\" DEEPSPEED = \"deepspeed\" options: show_source: true heading_level: 3","title":"BackendType Enum (RL)"},{"location":"api-reference/configuration/#configuration-loaders","text":"","title":"Configuration Loaders"},{"location":"api-reference/configuration/#sftconfigloader","text":"Configuration loader for SFT configs. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"SFTConfigLoader"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.load_from_dict","text":"Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_dict"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.load_from_yaml","text":"Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_yaml"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.validate_config","text":"Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3","title":"validate_config"},{"location":"api-reference/configuration/#configloader-rl","text":"Configuration loader for RL configs. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"ConfigLoader (RL)"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.create_config_from_cli_args","text":"Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"create_config_from_cli_args"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.load_from_dict","text":"Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_dict"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.load_from_yaml","text":"Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_yaml"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.parse_dataset_spec","text":"Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params )","title":"parse_dataset_spec"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.parse_reward_spec","text":"Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params )","title":"parse_reward_spec"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.save_resolved_config","text":"Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False )","title":"save_resolved_config"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.validate_config","text":"Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3","title":"validate_config"},{"location":"api-reference/configuration/#helper-functions","text":"","title":"Helper Functions"},{"location":"api-reference/configuration/#create_instruction_following_config","text":"Create configuration for instruction following. Create configuration for instruction following tasks. Source code in src/aligntune/core/sft/config.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def create_instruction_following_config ( model_name : str , dataset_name : str , output_dir : str = \"./output/instruction\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for instruction following tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 1024 ), use_unsloth = kwargs . get ( 'use_unsloth' , True ), peft_enabled = kwargs . get ( 'peft_enabled' , True ), quantization = kwargs . get ( 'quantization' , { \"load_in_4bit\" : True }) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . INSTRUCTION_FOLLOWING , instruction_column = kwargs . get ( 'instruction_column' , 'instruction' ), response_column = kwargs . get ( 'response_column' , 'output' ), context_column = kwargs . get ( 'context_column' , 'input' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 4 ), learning_rate = kwargs . get ( 'learning_rate' , 2e-4 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'instruction_following' ) ) ) options: show_source: true heading_level: 3","title":"create_instruction_following_config()"},{"location":"api-reference/configuration/#create_text_classification_config","text":"Create configuration for text classification. Create configuration for text classification tasks. Source code in src/aligntune/core/sft/config.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def create_text_classification_config ( model_name : str , dataset_name : str , num_labels : int , output_dir : str = \"./output/classification\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for text classification tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 512 ), use_unsloth = False , # Not recommended for classification peft_enabled = kwargs . get ( 'peft_enabled' , True ), num_labels = num_labels , quantization = kwargs . get ( 'quantization' , {}) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . TEXT_CLASSIFICATION , text_column = kwargs . get ( 'text_column' , 'text' ), label_column = kwargs . get ( 'label_column' , 'label' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 8 ), learning_rate = kwargs . get ( 'learning_rate' , 5e-5 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'text_classification' ) ) ) options: show_source: true heading_level: 3","title":"create_text_classification_config()"},{"location":"api-reference/configuration/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/configuration/#creating-sft-configuration","text":"from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" , precision = \"bf16\" , max_seq_length = 512 ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" , max_samples = 1000 , task_type = \"instruction_following\" ), train = TrainingConfig ( epochs = 3 , per_device_batch_size = 4 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_sft_training\" ) )","title":"Creating SFT Configuration"},{"location":"api-reference/configuration/#creating-rl-configuration","text":"from aligntune.core.rl.config import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig ) config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , use_peft = True , lora_r = 16 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , max_samples = 1000 ) ], train = TrainingConfig ( epochs = 1 , per_device_batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 # DPO parameter ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_dpo_training\" ) )","title":"Creating RL Configuration"},{"location":"api-reference/configuration/#loading-from-yaml","text":"from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" )","title":"Loading from YAML"},{"location":"api-reference/configuration/#next-steps","text":"Backend Factory - Trainer creation Trainers - Trainer classes User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/core/","text":"Core API Reference \u00b6 Complete API reference for AlignTune core functions and utilities. Overview \u00b6 This page covers core utility functions, error classes, and helper functions. For main API functions, see Backend Factory . Configuration Loaders \u00b6 SFTConfigLoader \u00b6 Load SFT configuration from YAML files. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config load_from_yaml ( path ) staticmethod \u00b6 Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config save_config ( config , path ) staticmethod \u00b6 Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) validate_config ( config ) staticmethod \u00b6 Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config_loader import SFTConfigLoader config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader (RL) \u00b6 Load RL configuration from YAML files. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) create_config_from_cli_args ( ** kwargs ) staticmethod \u00b6 Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) load_from_yaml ( path ) staticmethod \u00b6 Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) parse_dataset_spec ( spec ) staticmethod \u00b6 Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) parse_reward_spec ( spec ) staticmethod \u00b6 Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) save_resolved_config ( config , output_dir ) staticmethod \u00b6 Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) validate_config ( config ) staticmethod \u00b6 Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"ppo_config.yaml\" ) Trainer Factories \u00b6 SFTTrainerFactory \u00b6 Factory for creating SFT trainers. Factory for creating SFT trainers - delegates to Backend Factory. Source code in src/aligntune/core/sft/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class SFTTrainerFactory : \"\"\"Factory for creating SFT trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) create_trainer ( config ) classmethod \u00b6 Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config SFTConfig SFT configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/sft/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) options: show_source: true heading_level: 3 TrainerFactory (RL) \u00b6 Factory for creating RL trainers. Factory for creating RLHF trainers - delegates to Backend Factory. Source code in src/aligntune/core/rl/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class TrainerFactory : \"\"\"Factory for creating RLHF trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) create_trainer ( config ) classmethod \u00b6 Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config UnifiedConfig Unified configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/rl/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) options: show_source: true heading_level: 3 Error Classes \u00b6 AlignTuneError \u00b6 Base exception for AlignTune errors. Bases: Exception Base exception class for AlignTune errors. Source code in src/aligntune/utils/errors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AlignTuneError ( Exception ): \"\"\"Base exception class for AlignTune errors.\"\"\" def __init__ ( self , message : str , error_code : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . message = message self . error_code = error_code self . suggestions = suggestions or [] super () . __init__ ( self . message ) def __str__ ( self ): return self . format_error () def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) format_error () \u00b6 Format error message with suggestions. Source code in src/aligntune/utils/errors.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) options: show_source: true heading_level: 3 ConfigurationError \u00b6 Configuration-related errors. Bases: AlignTuneError Configuration-related errors. Source code in src/aligntune/utils/errors.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class ConfigurationError ( AlignTuneError ): \"\"\"Configuration-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field error_code = \"CONFIG_ERROR\" if not suggestions : suggestions = self . _generate_config_suggestions ( message , field ) super () . __init__ ( message , error_code , suggestions ) def _generate_config_suggestions ( self , message : str , field : Optional [ str ]) -> List [ str ]: \"\"\"Generate configuration-specific suggestions.\"\"\" suggestions = [] if \"optimizer\" in message . lower (): suggestions . extend ([ \"Check available optimizers with 'aligntune recipes list'\" , \"Use 'adamw_torch' for general training\" , \"Use 'adamw_8bit' for memory-efficient training\" ]) if \"scheduler\" in message . lower (): suggestions . extend ([ \"Check available schedulers with 'aligntune recipes list'\" , \"Use 'cosine' for most training scenarios\" , \"Use 'linear' for fine-tuning tasks\" ]) if \"model\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Ensure model name is correct (e.g., 'meta-llama/Meta-Llama-3-8B')\" , \"Run 'huggingface-cli login' if using private models\" , \"Check model availability at https://huggingface.co/models\" ]) if \"dataset\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Check dataset name is correct (e.g., 'tatsu-lab/alpaca')\" , \"Verify dataset exists at https://huggingface.co/datasets\" ]) if \"memory\" in message . lower (): suggestions . extend ([ \"Reduce batch size\" , \"Enable gradient checkpointing\" , \"Use 4-bit quantization\" , \"Reduce model max length\" ]) if field : suggestions . append ( f \"Check the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3 TrainingError \u00b6 Training-related errors. Bases: AlignTuneError Training-related errors. Source code in src/aligntune/utils/errors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TrainingError ( AlignTuneError ): \"\"\"Training-related errors.\"\"\" def __init__ ( self , message : str , stage : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . stage = stage error_code = \"TRAINING_ERROR\" if not suggestions : suggestions = self . _generate_training_suggestions ( message , stage ) super () . __init__ ( message , error_code , suggestions ) def _generate_training_suggestions ( self , message : str , stage : Optional [ str ]) -> List [ str ]: \"\"\"Generate training-specific suggestions.\"\"\" suggestions = [] if \"cuda\" in message . lower () or \"gpu\" in message . lower (): suggestions . extend ([ \"Check GPU memory availability with 'nvidia-smi'\" , \"Reduce batch size or enable gradient accumulation\" , \"Enable gradient checkpointing\" , \"Use mixed precision (bf16/fp16)\" ]) if \"out of memory\" in message . lower (): suggestions . extend ([ \"Reduce per_device_batch_size\" , \"Increase gradient_accumulation_steps\" , \"Enable gradient_checkpointing\" , \"Use 4-bit quantization\" , \"Reduce max_seq_length\" ]) if \"nan\" in message . lower () or \"inf\" in message . lower (): suggestions . extend ([ \"Reduce learning rate\" , \"Enable gradient clipping\" , \"Check input data quality\" , \"Use more stable optimizer (AdamW instead of Lion)\" ]) if \"loss\" in message . lower () and \"not decreasing\" in message . lower (): suggestions . extend ([ \"Increase learning rate\" , \"Check data quality and preprocessing\" , \"Try different optimizer\" , \"Adjust batch size\" ]) if stage : suggestions . append ( f \"Error occurred during { stage } stage\" ) return suggestions options: show_source: true heading_level: 3 EnvironmentError \u00b6 Environment-related errors. Bases: AlignTuneError Environment and dependency errors. Source code in src/aligntune/utils/errors.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class EnvironmentError ( AlignTuneError ): \"\"\"Environment and dependency errors.\"\"\" def __init__ ( self , message : str , dependency : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . dependency = dependency error_code = \"ENV_ERROR\" if not suggestions : suggestions = self . _generate_env_suggestions ( message , dependency ) super () . __init__ ( message , error_code , suggestions ) def _generate_env_suggestions ( self , message : str , dependency : Optional [ str ]) -> List [ str ]: \"\"\"Generate environment-specific suggestions.\"\"\" suggestions = [] if \"torch\" in message . lower () or \"cuda\" in message . lower (): suggestions . extend ([ \"Install PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" , \"Check CUDA version compatibility\" , \"Update GPU drivers\" ]) if \"transformers\" in message . lower (): suggestions . extend ([ \"Install transformers: pip install transformers\" , \"Update to latest version: pip install --upgrade transformers\" ]) if \"trl\" in message . lower (): suggestions . extend ([ \"Install TRL: pip install trl\" , \"Update to latest version: pip install --upgrade trl\" ]) if \"bitsandbytes\" in message . lower (): suggestions . extend ([ \"Install bitsandbytes: pip install bitsandbytes\" , \"For CUDA issues, try: pip install bitsandbytes --index-url https://download.pytorch.org/whl/cu118\" ]) if \"unsloth\" in message . lower (): suggestions . extend ([ \"Install unsloth: pip install unsloth\" , \"Check CUDA compatibility for your GPU\" ]) if dependency : suggestions . append ( f \"Install missing dependency: pip install { dependency } \" ) return suggestions options: show_source: true heading_level: 3 ValidationError \u00b6 Validation-related errors. Bases: AlignTuneError Validation-related errors. Source code in src/aligntune/utils/errors.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class ValidationError ( AlignTuneError ): \"\"\"Validation-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , value : Optional [ Any ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field self . value = value error_code = \"VALIDATION_ERROR\" if not suggestions : suggestions = self . _generate_validation_suggestions ( message , field , value ) super () . __init__ ( message , error_code , suggestions ) def _generate_validation_suggestions ( self , message : str , field : Optional [ str ], value : Optional [ Any ]) -> List [ str ]: \"\"\"Generate validation-specific suggestions.\"\"\" suggestions = [] if \"positive\" in message . lower () and field : suggestions . append ( f \"Set { field } to a positive value (current: { value } )\" ) if \"required\" in message . lower () and field : suggestions . append ( f \"Provide a value for the required field ' { field } '\" ) if \"invalid\" in message . lower (): suggestions . append ( \"Check the allowed values in the documentation\" ) if field : suggestions . append ( f \"Fix the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3 Utility Functions \u00b6 validate_backend_selection() \u00b6 Validate backend selection and provide helpful errors. Validate backend selection and provide helpful error messages. Source code in src/aligntune/core/backend_factory.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def validate_backend_selection ( backend : str , training_type : str = \"RL\" ) -> BackendType : \"\"\"Validate backend selection and provide helpful error messages.\"\"\" try : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) except ValueError : available_backends = [ bt . value for bt in BackendType ] raise ValueError ( f \"Invalid backend ' { backend } '. Available backends: { available_backends } \" ) # Check availability if not _check_backend_availability ( backend_type ): if backend_type == BackendType . TRL : raise ImportError ( \"TRL backend not available. Install with: pip install trl \\n \" \"Alternatively, use Unsloth backend: --backend unsloth\" ) elif backend_type == BackendType . UNSLOTH : raise ImportError ( \"Unsloth backend not available. Install with: pip install unsloth \\n \" \"Alternatively, use TRL backend: --backend trl\" ) return backend_type options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import validate_backend_selection backend_type = validate_backend_selection ( \"trl\" , \"SFT\" ) See Also \u00b6 Backend Factory - Main API for creating trainers Configuration Classes - Configuration API Trainers - Trainer API","title":"Core API"},{"location":"api-reference/core/#core-api-reference","text":"Complete API reference for AlignTune core functions and utilities.","title":"Core API Reference"},{"location":"api-reference/core/#overview","text":"This page covers core utility functions, error classes, and helper functions. For main API functions, see Backend Factory .","title":"Overview"},{"location":"api-reference/core/#configuration-loaders","text":"","title":"Configuration Loaders"},{"location":"api-reference/core/#sftconfigloader","text":"Load SFT configuration from YAML files. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"SFTConfigLoader"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.load_from_dict","text":"Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_dict"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.load_from_yaml","text":"Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_yaml"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.validate_config","text":"Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config_loader import SFTConfigLoader config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" )","title":"validate_config"},{"location":"api-reference/core/#configloader-rl","text":"Load RL configuration from YAML files. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"ConfigLoader (RL)"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.create_config_from_cli_args","text":"Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"create_config_from_cli_args"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.load_from_dict","text":"Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_dict"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.load_from_yaml","text":"Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_yaml"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.parse_dataset_spec","text":"Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split] with optional limit (percent=N or max=N) and optional query [?map.key=value]. Example: \"Anthropic/hh-rlhf:train\" with percent=25 and map.prompt=prompt. \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params )","title":"parse_dataset_spec"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.parse_reward_spec","text":"Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params )","title":"parse_reward_spec"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.save_resolved_config","text":"Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False )","title":"save_resolved_config"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.validate_config","text":"Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"ppo_config.yaml\" )","title":"validate_config"},{"location":"api-reference/core/#trainer-factories","text":"","title":"Trainer Factories"},{"location":"api-reference/core/#sfttrainerfactory","text":"Factory for creating SFT trainers. Factory for creating SFT trainers - delegates to Backend Factory. Source code in src/aligntune/core/sft/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class SFTTrainerFactory : \"\"\"Factory for creating SFT trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , )","title":"SFTTrainerFactory"},{"location":"api-reference/core/#core.sft.trainer_factory.SFTTrainerFactory.create_trainer","text":"Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config SFTConfig SFT configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/sft/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) options: show_source: true heading_level: 3","title":"create_trainer"},{"location":"api-reference/core/#trainerfactory-rl","text":"Factory for creating RL trainers. Factory for creating RLHF trainers - delegates to Backend Factory. Source code in src/aligntune/core/rl/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class TrainerFactory : \"\"\"Factory for creating RLHF trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , )","title":"TrainerFactory (RL)"},{"location":"api-reference/core/#core.rl.trainer_factory.TrainerFactory.create_trainer","text":"Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config UnifiedConfig Unified configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/rl/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) options: show_source: true heading_level: 3","title":"create_trainer"},{"location":"api-reference/core/#error-classes","text":"","title":"Error Classes"},{"location":"api-reference/core/#aligntuneerror","text":"Base exception for AlignTune errors. Bases: Exception Base exception class for AlignTune errors. Source code in src/aligntune/utils/errors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AlignTuneError ( Exception ): \"\"\"Base exception class for AlignTune errors.\"\"\" def __init__ ( self , message : str , error_code : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . message = message self . error_code = error_code self . suggestions = suggestions or [] super () . __init__ ( self . message ) def __str__ ( self ): return self . format_error () def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines )","title":"AlignTuneError"},{"location":"api-reference/core/#utils.errors.AlignTuneError.format_error","text":"Format error message with suggestions. Source code in src/aligntune/utils/errors.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) options: show_source: true heading_level: 3","title":"format_error"},{"location":"api-reference/core/#configurationerror","text":"Configuration-related errors. Bases: AlignTuneError Configuration-related errors. Source code in src/aligntune/utils/errors.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class ConfigurationError ( AlignTuneError ): \"\"\"Configuration-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field error_code = \"CONFIG_ERROR\" if not suggestions : suggestions = self . _generate_config_suggestions ( message , field ) super () . __init__ ( message , error_code , suggestions ) def _generate_config_suggestions ( self , message : str , field : Optional [ str ]) -> List [ str ]: \"\"\"Generate configuration-specific suggestions.\"\"\" suggestions = [] if \"optimizer\" in message . lower (): suggestions . extend ([ \"Check available optimizers with 'aligntune recipes list'\" , \"Use 'adamw_torch' for general training\" , \"Use 'adamw_8bit' for memory-efficient training\" ]) if \"scheduler\" in message . lower (): suggestions . extend ([ \"Check available schedulers with 'aligntune recipes list'\" , \"Use 'cosine' for most training scenarios\" , \"Use 'linear' for fine-tuning tasks\" ]) if \"model\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Ensure model name is correct (e.g., 'meta-llama/Meta-Llama-3-8B')\" , \"Run 'huggingface-cli login' if using private models\" , \"Check model availability at https://huggingface.co/models\" ]) if \"dataset\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Check dataset name is correct (e.g., 'tatsu-lab/alpaca')\" , \"Verify dataset exists at https://huggingface.co/datasets\" ]) if \"memory\" in message . lower (): suggestions . extend ([ \"Reduce batch size\" , \"Enable gradient checkpointing\" , \"Use 4-bit quantization\" , \"Reduce model max length\" ]) if field : suggestions . append ( f \"Check the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3","title":"ConfigurationError"},{"location":"api-reference/core/#trainingerror","text":"Training-related errors. Bases: AlignTuneError Training-related errors. Source code in src/aligntune/utils/errors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TrainingError ( AlignTuneError ): \"\"\"Training-related errors.\"\"\" def __init__ ( self , message : str , stage : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . stage = stage error_code = \"TRAINING_ERROR\" if not suggestions : suggestions = self . _generate_training_suggestions ( message , stage ) super () . __init__ ( message , error_code , suggestions ) def _generate_training_suggestions ( self , message : str , stage : Optional [ str ]) -> List [ str ]: \"\"\"Generate training-specific suggestions.\"\"\" suggestions = [] if \"cuda\" in message . lower () or \"gpu\" in message . lower (): suggestions . extend ([ \"Check GPU memory availability with 'nvidia-smi'\" , \"Reduce batch size or enable gradient accumulation\" , \"Enable gradient checkpointing\" , \"Use mixed precision (bf16/fp16)\" ]) if \"out of memory\" in message . lower (): suggestions . extend ([ \"Reduce per_device_batch_size\" , \"Increase gradient_accumulation_steps\" , \"Enable gradient_checkpointing\" , \"Use 4-bit quantization\" , \"Reduce max_seq_length\" ]) if \"nan\" in message . lower () or \"inf\" in message . lower (): suggestions . extend ([ \"Reduce learning rate\" , \"Enable gradient clipping\" , \"Check input data quality\" , \"Use more stable optimizer (AdamW instead of Lion)\" ]) if \"loss\" in message . lower () and \"not decreasing\" in message . lower (): suggestions . extend ([ \"Increase learning rate\" , \"Check data quality and preprocessing\" , \"Try different optimizer\" , \"Adjust batch size\" ]) if stage : suggestions . append ( f \"Error occurred during { stage } stage\" ) return suggestions options: show_source: true heading_level: 3","title":"TrainingError"},{"location":"api-reference/core/#environmenterror","text":"Environment-related errors. Bases: AlignTuneError Environment and dependency errors. Source code in src/aligntune/utils/errors.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class EnvironmentError ( AlignTuneError ): \"\"\"Environment and dependency errors.\"\"\" def __init__ ( self , message : str , dependency : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . dependency = dependency error_code = \"ENV_ERROR\" if not suggestions : suggestions = self . _generate_env_suggestions ( message , dependency ) super () . __init__ ( message , error_code , suggestions ) def _generate_env_suggestions ( self , message : str , dependency : Optional [ str ]) -> List [ str ]: \"\"\"Generate environment-specific suggestions.\"\"\" suggestions = [] if \"torch\" in message . lower () or \"cuda\" in message . lower (): suggestions . extend ([ \"Install PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" , \"Check CUDA version compatibility\" , \"Update GPU drivers\" ]) if \"transformers\" in message . lower (): suggestions . extend ([ \"Install transformers: pip install transformers\" , \"Update to latest version: pip install --upgrade transformers\" ]) if \"trl\" in message . lower (): suggestions . extend ([ \"Install TRL: pip install trl\" , \"Update to latest version: pip install --upgrade trl\" ]) if \"bitsandbytes\" in message . lower (): suggestions . extend ([ \"Install bitsandbytes: pip install bitsandbytes\" , \"For CUDA issues, try: pip install bitsandbytes --index-url https://download.pytorch.org/whl/cu118\" ]) if \"unsloth\" in message . lower (): suggestions . extend ([ \"Install unsloth: pip install unsloth\" , \"Check CUDA compatibility for your GPU\" ]) if dependency : suggestions . append ( f \"Install missing dependency: pip install { dependency } \" ) return suggestions options: show_source: true heading_level: 3","title":"EnvironmentError"},{"location":"api-reference/core/#validationerror","text":"Validation-related errors. Bases: AlignTuneError Validation-related errors. Source code in src/aligntune/utils/errors.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class ValidationError ( AlignTuneError ): \"\"\"Validation-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , value : Optional [ Any ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field self . value = value error_code = \"VALIDATION_ERROR\" if not suggestions : suggestions = self . _generate_validation_suggestions ( message , field , value ) super () . __init__ ( message , error_code , suggestions ) def _generate_validation_suggestions ( self , message : str , field : Optional [ str ], value : Optional [ Any ]) -> List [ str ]: \"\"\"Generate validation-specific suggestions.\"\"\" suggestions = [] if \"positive\" in message . lower () and field : suggestions . append ( f \"Set { field } to a positive value (current: { value } )\" ) if \"required\" in message . lower () and field : suggestions . append ( f \"Provide a value for the required field ' { field } '\" ) if \"invalid\" in message . lower (): suggestions . append ( \"Check the allowed values in the documentation\" ) if field : suggestions . append ( f \"Fix the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3","title":"ValidationError"},{"location":"api-reference/core/#utility-functions","text":"","title":"Utility Functions"},{"location":"api-reference/core/#validate_backend_selection","text":"Validate backend selection and provide helpful errors. Validate backend selection and provide helpful error messages. Source code in src/aligntune/core/backend_factory.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def validate_backend_selection ( backend : str , training_type : str = \"RL\" ) -> BackendType : \"\"\"Validate backend selection and provide helpful error messages.\"\"\" try : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) except ValueError : available_backends = [ bt . value for bt in BackendType ] raise ValueError ( f \"Invalid backend ' { backend } '. Available backends: { available_backends } \" ) # Check availability if not _check_backend_availability ( backend_type ): if backend_type == BackendType . TRL : raise ImportError ( \"TRL backend not available. Install with: pip install trl \\n \" \"Alternatively, use Unsloth backend: --backend unsloth\" ) elif backend_type == BackendType . UNSLOTH : raise ImportError ( \"Unsloth backend not available. Install with: pip install unsloth \\n \" \"Alternatively, use TRL backend: --backend trl\" ) return backend_type options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import validate_backend_selection backend_type = validate_backend_selection ( \"trl\" , \"SFT\" )","title":"validate_backend_selection()"},{"location":"api-reference/core/#see-also","text":"Backend Factory - Main API for creating trainers Configuration Classes - Configuration API Trainers - Trainer API","title":"See Also"},{"location":"api-reference/overview/","text":"API Reference Overview \u00b6 AlignTune provides a comprehensive API for fine-tuning models. This section covers all available APIs. Core APIs \u00b6 Backend Factory \u00b6 The main entry point for creating trainers: from aligntune.core.backend_factory import ( create_sft_trainer , create_rl_trainer , BackendType , get_backend_status ) Functions : - create_sft_trainer() - Create SFT trainer - create_rl_trainer() - Create RL trainer - get_backend_status() - Check backend availability See Backend Factory for details. Configuration Classes \u00b6 Configuration classes for all training parameters: from aligntune.core.sft.config import SFTConfig , TaskType from aligntune.core.rl.config import UnifiedConfig , AlgorithmType See Configuration Classes for details. Trainers \u00b6 Base trainer classes with common methods: from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.rl.trainer_base import TrainerBase Common Methods : - train() - Start training - evaluate() - Evaluate model - save_model() - Save model - load_checkpoint() - Load checkpoint - predict() - Generate predictions - push_to_hub() - Push to HuggingFace Hub See Trainers for details. Quick Reference \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () API Documentation \u00b6 Unified API - Unified RLHF API system Core API - Core functions and utilities Backend Factory - Trainer creation Configuration Classes - All configuration options Trainers - Trainer classes and methods Reward Functions Reference - Complete reward functions list Reward Model Training Reference - Reward model system architecture Next Steps \u00b6 Getting Started - Quick start guide User Guide - Detailed usage guide Examples - Code examples","title":"Overview"},{"location":"api-reference/overview/#api-reference-overview","text":"AlignTune provides a comprehensive API for fine-tuning models. This section covers all available APIs.","title":"API Reference Overview"},{"location":"api-reference/overview/#core-apis","text":"","title":"Core APIs"},{"location":"api-reference/overview/#backend-factory","text":"The main entry point for creating trainers: from aligntune.core.backend_factory import ( create_sft_trainer , create_rl_trainer , BackendType , get_backend_status ) Functions : - create_sft_trainer() - Create SFT trainer - create_rl_trainer() - Create RL trainer - get_backend_status() - Check backend availability See Backend Factory for details.","title":"Backend Factory"},{"location":"api-reference/overview/#configuration-classes","text":"Configuration classes for all training parameters: from aligntune.core.sft.config import SFTConfig , TaskType from aligntune.core.rl.config import UnifiedConfig , AlgorithmType See Configuration Classes for details.","title":"Configuration Classes"},{"location":"api-reference/overview/#trainers","text":"Base trainer classes with common methods: from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.rl.trainer_base import TrainerBase Common Methods : - train() - Start training - evaluate() - Evaluate model - save_model() - Save model - load_checkpoint() - Load checkpoint - predict() - Generate predictions - push_to_hub() - Push to HuggingFace Hub See Trainers for details.","title":"Trainers"},{"location":"api-reference/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"api-reference/overview/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train ()","title":"SFT Training"},{"location":"api-reference/overview/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train ()","title":"RL Training"},{"location":"api-reference/overview/#api-documentation","text":"Unified API - Unified RLHF API system Core API - Core functions and utilities Backend Factory - Trainer creation Configuration Classes - All configuration options Trainers - Trainer classes and methods Reward Functions Reference - Complete reward functions list Reward Model Training Reference - Reward model system architecture","title":"API Documentation"},{"location":"api-reference/overview/#next-steps","text":"Getting Started - Quick start guide User Guide - Detailed usage guide Examples - Code examples","title":"Next Steps"},{"location":"api-reference/reward-functions-reference/","text":"AlignTune Reward Functions Documentation \u00b6 Overview \u00b6 AlignTune provides 27+ prebuilt reward functions covering all major aspects of text generation quality, safety, and task-specific performance. These reward functions can be used individually or combined to create sophisticated reward landscapes for RLHF training. Available Reward Functions \u00b6 Basic Quality Rewards \u00b6 1. Length Reward ( length ) \u00b6 Purpose : Encourages appropriate response length Parameters : min_length , max_length Use Case : Preventing too short or too long responses Example : {\"type\": \"length\", \"params\": {\"min_length\": 10, \"max_length\": 500}} 2. Coherence Reward ( coherence ) \u00b6 Purpose : Measures logical flow and coherence of text Parameters : None Use Case : Ensuring responses make logical sense Example : {\"type\": \"coherence\", \"weight\": 1.0} 3. Fluency Reward ( fluency ) \u00b6 Purpose : Measures grammatical correctness and fluency Parameters : None Use Case : Ensuring grammatically correct responses Example : {\"type\": \"fluency\", \"weight\": 0.8} Task-Specific Rewards \u00b6 4. Sentiment Reward ( sentiment ) \u00b6 Purpose : Encourages specific sentiment (positive, negative, neutral) Parameters : target_sentiment Use Case : Customer service, emotional support Example : {\"type\": \"sentiment\", \"params\": {\"target_sentiment\": \"positive\"}} 5. Safety Reward ( safety ) \u00b6 Purpose : Detects and penalizes harmful content Parameters : strict (bool) Use Case : Content moderation, safe AI Example : {\"type\": \"safety\", \"params\": {\"strict\": true}} 6. Factuality Reward ( factuality ) \u00b6 Purpose : Measures factual accuracy Parameters : None Use Case : Information retrieval, Q&A systems Example : {\"type\": \"factuality\", \"weight\": 1.2} 7. Bias Reward ( bias ) \u00b6 Purpose : Detects and penalizes biased content Parameters : None Use Case : Fair AI, unbiased responses Example : {\"type\": \"bias\", \"weight\": 0.5} Generation Quality Metrics \u00b6 8. BLEU Reward ( bleu ) \u00b6 Purpose : Measures n-gram overlap with reference Parameters : n_gram , smooth Use Case : Translation, summarization Example : {\"type\": \"bleu\", \"params\": {\"n_gram\": 4}} 9. ROUGE Reward ( rouge ) \u00b6 Purpose : Measures recall-oriented understudy for gisting evaluation Parameters : rouge_type (rouge-1, rouge-2, rouge-l) Use Case : Summarization, text generation Example : {\"type\": \"rouge\", \"params\": {\"rouge_type\": \"rouge-l\"}} 10. METEOR Reward ( meteor ) \u00b6 Purpose : Advanced translation evaluation metric Parameters : None Use Case : Translation quality Example : {\"type\": \"meteor\", \"weight\": 0.7} 11. BERTScore Reward ( bertscore ) \u00b6 Purpose : Contextual embedding-based similarity Parameters : model_name Use Case : Semantic similarity, paraphrase detection Example : {\"type\": \"bertscore\", \"params\": {\"model_name\": \"microsoft/DialoGPT-medium\"}} Code Quality Rewards \u00b6 12. Code Syntax Reward ( code_syntax ) \u00b6 Purpose : Validates code syntax correctness Parameters : language Use Case : Code generation, programming assistance Example : {\"type\": \"code_syntax\", \"params\": {\"language\": \"python\"}} 13. Code Execution Reward ( code_execution ) \u00b6 Purpose : Tests if code runs without errors Parameters : timeout , safe_mode Use Case : Code generation, programming education Example : {\"type\": \"code_execution\", \"params\": {\"timeout\": 5, \"safe_mode\": true}} 14. Code Completeness Reward ( code_completeness ) \u00b6 Purpose : Measures code completeness and structure Parameters : None Use Case : Code generation quality Example : {\"type\": \"code_completeness\", \"weight\": 1.0} Math and Reasoning Rewards \u00b6 15. Math Correctness Reward ( math_correctness ) \u00b6 Purpose : Validates mathematical correctness Parameters : tolerance Use Case : Math problem solving, scientific computation Example : {\"type\": \"math_correctness\", \"params\": {\"tolerance\": 1e-6}} 16. Logical Consistency Reward ( logical_consistency ) \u00b6 Purpose : Measures logical consistency in reasoning Parameters : None Use Case : Reasoning tasks, logical problem solving Example : {\"type\": \"logical_consistency\", \"weight\": 1.0} 17. Commonsense Reward ( commonsense ) \u00b6 Purpose : Evaluates commonsense reasoning Parameters : None Use Case : General reasoning, everyday knowledge Example : {\"type\": \"commonsense\", \"weight\": 0.8} Specialized Rewards \u00b6 18. Hallucination Reward ( hallucination ) \u00b6 Purpose : Detects and penalizes hallucinated information Parameters : None Use Case : Factual accuracy, information retrieval Example : {\"type\": \"hallucination\", \"weight\": 1.5} 19. Toxicity Reward ( toxicity ) \u00b6 Purpose : Detects toxic, harmful, or offensive content Parameters : threshold Use Case : Content moderation, safe AI Example : {\"type\": \"toxicity\", \"params\": {\"threshold\": 0.5}} 20. Politeness Reward ( politeness ) \u00b6 Purpose : Encourages polite and respectful language Parameters : None Use Case : Customer service, professional communication Example : {\"type\": \"politeness\", \"weight\": 0.7} 21. Helpfulness Reward ( helpfulness ) \u00b6 Purpose : Measures how helpful and informative the response is Parameters : None Use Case : Q&A systems, customer support Example : {\"type\": \"helpfulness\", \"weight\": 1.0} 22. Honesty Reward ( honesty ) \u00b6 Purpose : Encourages honest and truthful responses Parameters : None Use Case : Trustworthy AI, factual accuracy Example : {\"type\": \"honesty\", \"weight\": 1.2} Multi-Modal Rewards (Future) \u00b6 23. Image Relevance Reward ( image_relevance ) \u00b6 Purpose : Measures relevance to provided images Parameters : None Use Case : Multi-modal tasks, image captioning Example : {\"type\": \"image_relevance\", \"weight\": 1.0} 24. Audio Quality Reward ( audio_quality ) \u00b6 Purpose : Measures audio quality and clarity Parameters : None Use Case : Speech generation, audio processing Example : {\"type\": \"audio_quality\", \"weight\": 1.0} Usage Examples \u00b6 Basic Configuration \u00b6 # Single reward function rewards : - type : \"length\" weight : 1.0 params : min_length : 10 max_length : 500 Multiple Reward Functions \u00b6 # Multiple reward functions with different weights rewards : - type : \"length\" weight : 0.5 params : min_length : 20 max_length : 300 - type : \"safety\" weight : 2.0 params : strict : true - type : \"helpfulness\" weight : 1.5 - type : \"coherence\" weight : 1.0 Task-Specific Configurations \u00b6 Customer Support Bot \u00b6 rewards : - type : \"politeness\" weight : 1.5 - type : \"helpfulness\" weight : 2.0 - type : \"safety\" weight : 1.0 - type : \"length\" weight : 0.5 params : min_length : 50 max_length : 200 Code Generation Assistant \u00b6 rewards : - type : \"code_syntax\" weight : 2.0 params : language : \"python\" - type : \"code_execution\" weight : 1.5 params : timeout : 5 safe_mode : true - type : \"code_completeness\" weight : 1.0 - type : \"helpfulness\" weight : 0.8 Math Problem Solver \u00b6 rewards : - type : \"math_correctness\" weight : 3.0 params : tolerance : 1e-6 - type : \"logical_consistency\" weight : 1.5 - type : \"coherence\" weight : 1.0 - type : \"length\" weight : 0.3 params : min_length : 10 max_length : 1000 Content Moderation System \u00b6 rewards : - type : \"safety\" weight : 3.0 params : strict : true - type : \"toxicity\" weight : 2.5 params : threshold : 0.3 - type : \"bias\" weight : 2.0 - type : \"factuality\" weight : 1.5 Advanced Configuration \u00b6 Custom Reward Weights \u00b6 from aligntune import RewardConfig , RewardType # Create custom reward configuration rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ), RewardConfig ( reward_type = RewardType . LENGTH , weight = 0.5 , params = { \"min_length\" : 20 , \"max_length\" : 300 } ) ] Dynamic Reward Weighting \u00b6 # Example with dynamic weighting based on task rewards : - type : \"safety\" weight : 2.0 # High priority for safety - type : \"helpfulness\" weight : 1.5 # Medium-high priority - type : \"coherence\" weight : 1.0 # Standard priority - type : \"length\" weight : 0.3 # Low priority Reward Function Performance \u00b6 Computational Efficiency \u00b6 Fast : Length, Coherence, Fluency, Sentiment Medium : Safety, Toxicity, Bias, Politeness Slow : BERTScore, Code Execution, Math Correctness Memory Usage \u00b6 Low : Basic rewards (Length, Coherence, etc.) Medium : Task-specific rewards (Safety, Sentiment, etc.) High : Model-based rewards (BERTScore, Code Execution, etc.) Accuracy vs Speed Trade-offs \u00b6 High Accuracy : BERTScore, Math Correctness, Code Execution Balanced : Safety, Toxicity, Helpfulness Fast Approximation : Length, Coherence, Fluency Best Practices \u00b6 Reward Selection \u00b6 Start Simple : Begin with basic rewards (length, coherence, safety) Add Task-Specific : Include rewards relevant to your use case Balance Weights : Ensure no single reward dominates Monitor Performance : Track reward trends during training Weight Tuning \u00b6 Safety First : Always include safety rewards with high weight Task Relevance : Higher weights for task-specific rewards Quality Balance : Balance between different quality aspects Iterative Tuning : Adjust weights based on training results Performance Optimization \u00b6 Use Fast Rewards : Prefer fast rewards for real-time applications Batch Processing : Process multiple texts together when possible Caching : Cache model outputs for repeated computations Device Selection : Use appropriate devices (CPU/GPU) for each reward Extending Reward Functions \u00b6 Custom Reward Functions \u00b6 from aligntune import RewardFunction , RewardType class CustomReward ( RewardFunction ): def __init__ ( self , config : RewardConfig ): super () . __init__ ( config ) # Initialize your custom reward logic def compute ( self , text : str , reference : Optional [ str ] = None , ** kwargs ) -> float : # Implement your custom reward computation return 0.8 # Return reward score between 0 and 1 # Register custom reward from aligntune import RewardRegistry RewardRegistry . register_reward ( \"custom\" , CustomReward ) Combining Rewards \u00b6 def combined_reward ( text : str , rewards : List [ RewardFunction ]) -> float : \"\"\"Combine multiple reward functions.\"\"\" total_score = 0.0 total_weight = 0.0 for reward_func in rewards : score = reward_func . compute ( text ) weight = reward_func . config . weight total_score += score * weight total_weight += weight return total_score / total_weight if total_weight > 0 else 0.0 Monitoring and Debugging \u00b6 Reward Tracking \u00b6 Monitor individual reward scores during training Track reward trends and convergence Identify which rewards are most influential Debug reward computation issues Common Issues \u00b6 Reward Collapse : All rewards converge to similar values Reward Instability : High variance in reward scores Reward Conflicts : Conflicting objectives between rewards Performance Bottlenecks : Slow reward computation Debugging Tools \u00b6 Reward score logging and visualization Individual reward breakdown analysis Performance profiling for slow rewards A/B testing for reward configurations AlignTune provides a comprehensive suite of reward functions to create sophisticated reward landscapes for any RLHF training scenario. Choose the right combination of rewards for your specific use case and achieve optimal model performance!","title":"AlignTune Reward Functions Documentation"},{"location":"api-reference/reward-functions-reference/#aligntune-reward-functions-documentation","text":"","title":"AlignTune Reward Functions Documentation"},{"location":"api-reference/reward-functions-reference/#overview","text":"AlignTune provides 27+ prebuilt reward functions covering all major aspects of text generation quality, safety, and task-specific performance. These reward functions can be used individually or combined to create sophisticated reward landscapes for RLHF training.","title":"Overview"},{"location":"api-reference/reward-functions-reference/#available-reward-functions","text":"","title":"Available Reward Functions"},{"location":"api-reference/reward-functions-reference/#basic-quality-rewards","text":"","title":"Basic Quality Rewards"},{"location":"api-reference/reward-functions-reference/#1-length-reward-length","text":"Purpose : Encourages appropriate response length Parameters : min_length , max_length Use Case : Preventing too short or too long responses Example : {\"type\": \"length\", \"params\": {\"min_length\": 10, \"max_length\": 500}}","title":"1. Length Reward (length)"},{"location":"api-reference/reward-functions-reference/#2-coherence-reward-coherence","text":"Purpose : Measures logical flow and coherence of text Parameters : None Use Case : Ensuring responses make logical sense Example : {\"type\": \"coherence\", \"weight\": 1.0}","title":"2. Coherence Reward (coherence)"},{"location":"api-reference/reward-functions-reference/#3-fluency-reward-fluency","text":"Purpose : Measures grammatical correctness and fluency Parameters : None Use Case : Ensuring grammatically correct responses Example : {\"type\": \"fluency\", \"weight\": 0.8}","title":"3. Fluency Reward (fluency)"},{"location":"api-reference/reward-functions-reference/#task-specific-rewards","text":"","title":"Task-Specific Rewards"},{"location":"api-reference/reward-functions-reference/#4-sentiment-reward-sentiment","text":"Purpose : Encourages specific sentiment (positive, negative, neutral) Parameters : target_sentiment Use Case : Customer service, emotional support Example : {\"type\": \"sentiment\", \"params\": {\"target_sentiment\": \"positive\"}}","title":"4. Sentiment Reward (sentiment)"},{"location":"api-reference/reward-functions-reference/#5-safety-reward-safety","text":"Purpose : Detects and penalizes harmful content Parameters : strict (bool) Use Case : Content moderation, safe AI Example : {\"type\": \"safety\", \"params\": {\"strict\": true}}","title":"5. Safety Reward (safety)"},{"location":"api-reference/reward-functions-reference/#6-factuality-reward-factuality","text":"Purpose : Measures factual accuracy Parameters : None Use Case : Information retrieval, Q&A systems Example : {\"type\": \"factuality\", \"weight\": 1.2}","title":"6. Factuality Reward (factuality)"},{"location":"api-reference/reward-functions-reference/#7-bias-reward-bias","text":"Purpose : Detects and penalizes biased content Parameters : None Use Case : Fair AI, unbiased responses Example : {\"type\": \"bias\", \"weight\": 0.5}","title":"7. Bias Reward (bias)"},{"location":"api-reference/reward-functions-reference/#generation-quality-metrics","text":"","title":"Generation Quality Metrics"},{"location":"api-reference/reward-functions-reference/#8-bleu-reward-bleu","text":"Purpose : Measures n-gram overlap with reference Parameters : n_gram , smooth Use Case : Translation, summarization Example : {\"type\": \"bleu\", \"params\": {\"n_gram\": 4}}","title":"8. BLEU Reward (bleu)"},{"location":"api-reference/reward-functions-reference/#9-rouge-reward-rouge","text":"Purpose : Measures recall-oriented understudy for gisting evaluation Parameters : rouge_type (rouge-1, rouge-2, rouge-l) Use Case : Summarization, text generation Example : {\"type\": \"rouge\", \"params\": {\"rouge_type\": \"rouge-l\"}}","title":"9. ROUGE Reward (rouge)"},{"location":"api-reference/reward-functions-reference/#10-meteor-reward-meteor","text":"Purpose : Advanced translation evaluation metric Parameters : None Use Case : Translation quality Example : {\"type\": \"meteor\", \"weight\": 0.7}","title":"10. METEOR Reward (meteor)"},{"location":"api-reference/reward-functions-reference/#11-bertscore-reward-bertscore","text":"Purpose : Contextual embedding-based similarity Parameters : model_name Use Case : Semantic similarity, paraphrase detection Example : {\"type\": \"bertscore\", \"params\": {\"model_name\": \"microsoft/DialoGPT-medium\"}}","title":"11. BERTScore Reward (bertscore)"},{"location":"api-reference/reward-functions-reference/#code-quality-rewards","text":"","title":"Code Quality Rewards"},{"location":"api-reference/reward-functions-reference/#12-code-syntax-reward-code_syntax","text":"Purpose : Validates code syntax correctness Parameters : language Use Case : Code generation, programming assistance Example : {\"type\": \"code_syntax\", \"params\": {\"language\": \"python\"}}","title":"12. Code Syntax Reward (code_syntax)"},{"location":"api-reference/reward-functions-reference/#13-code-execution-reward-code_execution","text":"Purpose : Tests if code runs without errors Parameters : timeout , safe_mode Use Case : Code generation, programming education Example : {\"type\": \"code_execution\", \"params\": {\"timeout\": 5, \"safe_mode\": true}}","title":"13. Code Execution Reward (code_execution)"},{"location":"api-reference/reward-functions-reference/#14-code-completeness-reward-code_completeness","text":"Purpose : Measures code completeness and structure Parameters : None Use Case : Code generation quality Example : {\"type\": \"code_completeness\", \"weight\": 1.0}","title":"14. Code Completeness Reward (code_completeness)"},{"location":"api-reference/reward-functions-reference/#math-and-reasoning-rewards","text":"","title":"Math and Reasoning Rewards"},{"location":"api-reference/reward-functions-reference/#15-math-correctness-reward-math_correctness","text":"Purpose : Validates mathematical correctness Parameters : tolerance Use Case : Math problem solving, scientific computation Example : {\"type\": \"math_correctness\", \"params\": {\"tolerance\": 1e-6}}","title":"15. Math Correctness Reward (math_correctness)"},{"location":"api-reference/reward-functions-reference/#16-logical-consistency-reward-logical_consistency","text":"Purpose : Measures logical consistency in reasoning Parameters : None Use Case : Reasoning tasks, logical problem solving Example : {\"type\": \"logical_consistency\", \"weight\": 1.0}","title":"16. Logical Consistency Reward (logical_consistency)"},{"location":"api-reference/reward-functions-reference/#17-commonsense-reward-commonsense","text":"Purpose : Evaluates commonsense reasoning Parameters : None Use Case : General reasoning, everyday knowledge Example : {\"type\": \"commonsense\", \"weight\": 0.8}","title":"17. Commonsense Reward (commonsense)"},{"location":"api-reference/reward-functions-reference/#specialized-rewards","text":"","title":"Specialized Rewards"},{"location":"api-reference/reward-functions-reference/#18-hallucination-reward-hallucination","text":"Purpose : Detects and penalizes hallucinated information Parameters : None Use Case : Factual accuracy, information retrieval Example : {\"type\": \"hallucination\", \"weight\": 1.5}","title":"18. Hallucination Reward (hallucination)"},{"location":"api-reference/reward-functions-reference/#19-toxicity-reward-toxicity","text":"Purpose : Detects toxic, harmful, or offensive content Parameters : threshold Use Case : Content moderation, safe AI Example : {\"type\": \"toxicity\", \"params\": {\"threshold\": 0.5}}","title":"19. Toxicity Reward (toxicity)"},{"location":"api-reference/reward-functions-reference/#20-politeness-reward-politeness","text":"Purpose : Encourages polite and respectful language Parameters : None Use Case : Customer service, professional communication Example : {\"type\": \"politeness\", \"weight\": 0.7}","title":"20. Politeness Reward (politeness)"},{"location":"api-reference/reward-functions-reference/#21-helpfulness-reward-helpfulness","text":"Purpose : Measures how helpful and informative the response is Parameters : None Use Case : Q&A systems, customer support Example : {\"type\": \"helpfulness\", \"weight\": 1.0}","title":"21. Helpfulness Reward (helpfulness)"},{"location":"api-reference/reward-functions-reference/#22-honesty-reward-honesty","text":"Purpose : Encourages honest and truthful responses Parameters : None Use Case : Trustworthy AI, factual accuracy Example : {\"type\": \"honesty\", \"weight\": 1.2}","title":"22. Honesty Reward (honesty)"},{"location":"api-reference/reward-functions-reference/#multi-modal-rewards-future","text":"","title":"Multi-Modal Rewards (Future)"},{"location":"api-reference/reward-functions-reference/#23-image-relevance-reward-image_relevance","text":"Purpose : Measures relevance to provided images Parameters : None Use Case : Multi-modal tasks, image captioning Example : {\"type\": \"image_relevance\", \"weight\": 1.0}","title":"23. Image Relevance Reward (image_relevance)"},{"location":"api-reference/reward-functions-reference/#24-audio-quality-reward-audio_quality","text":"Purpose : Measures audio quality and clarity Parameters : None Use Case : Speech generation, audio processing Example : {\"type\": \"audio_quality\", \"weight\": 1.0}","title":"24. Audio Quality Reward (audio_quality)"},{"location":"api-reference/reward-functions-reference/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/reward-functions-reference/#basic-configuration","text":"# Single reward function rewards : - type : \"length\" weight : 1.0 params : min_length : 10 max_length : 500","title":"Basic Configuration"},{"location":"api-reference/reward-functions-reference/#multiple-reward-functions","text":"# Multiple reward functions with different weights rewards : - type : \"length\" weight : 0.5 params : min_length : 20 max_length : 300 - type : \"safety\" weight : 2.0 params : strict : true - type : \"helpfulness\" weight : 1.5 - type : \"coherence\" weight : 1.0","title":"Multiple Reward Functions"},{"location":"api-reference/reward-functions-reference/#task-specific-configurations","text":"","title":"Task-Specific Configurations"},{"location":"api-reference/reward-functions-reference/#customer-support-bot","text":"rewards : - type : \"politeness\" weight : 1.5 - type : \"helpfulness\" weight : 2.0 - type : \"safety\" weight : 1.0 - type : \"length\" weight : 0.5 params : min_length : 50 max_length : 200","title":"Customer Support Bot"},{"location":"api-reference/reward-functions-reference/#code-generation-assistant","text":"rewards : - type : \"code_syntax\" weight : 2.0 params : language : \"python\" - type : \"code_execution\" weight : 1.5 params : timeout : 5 safe_mode : true - type : \"code_completeness\" weight : 1.0 - type : \"helpfulness\" weight : 0.8","title":"Code Generation Assistant"},{"location":"api-reference/reward-functions-reference/#math-problem-solver","text":"rewards : - type : \"math_correctness\" weight : 3.0 params : tolerance : 1e-6 - type : \"logical_consistency\" weight : 1.5 - type : \"coherence\" weight : 1.0 - type : \"length\" weight : 0.3 params : min_length : 10 max_length : 1000","title":"Math Problem Solver"},{"location":"api-reference/reward-functions-reference/#content-moderation-system","text":"rewards : - type : \"safety\" weight : 3.0 params : strict : true - type : \"toxicity\" weight : 2.5 params : threshold : 0.3 - type : \"bias\" weight : 2.0 - type : \"factuality\" weight : 1.5","title":"Content Moderation System"},{"location":"api-reference/reward-functions-reference/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"api-reference/reward-functions-reference/#custom-reward-weights","text":"from aligntune import RewardConfig , RewardType # Create custom reward configuration rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ), RewardConfig ( reward_type = RewardType . LENGTH , weight = 0.5 , params = { \"min_length\" : 20 , \"max_length\" : 300 } ) ]","title":"Custom Reward Weights"},{"location":"api-reference/reward-functions-reference/#dynamic-reward-weighting","text":"# Example with dynamic weighting based on task rewards : - type : \"safety\" weight : 2.0 # High priority for safety - type : \"helpfulness\" weight : 1.5 # Medium-high priority - type : \"coherence\" weight : 1.0 # Standard priority - type : \"length\" weight : 0.3 # Low priority","title":"Dynamic Reward Weighting"},{"location":"api-reference/reward-functions-reference/#reward-function-performance","text":"","title":"Reward Function Performance"},{"location":"api-reference/reward-functions-reference/#computational-efficiency","text":"Fast : Length, Coherence, Fluency, Sentiment Medium : Safety, Toxicity, Bias, Politeness Slow : BERTScore, Code Execution, Math Correctness","title":"Computational Efficiency"},{"location":"api-reference/reward-functions-reference/#memory-usage","text":"Low : Basic rewards (Length, Coherence, etc.) Medium : Task-specific rewards (Safety, Sentiment, etc.) High : Model-based rewards (BERTScore, Code Execution, etc.)","title":"Memory Usage"},{"location":"api-reference/reward-functions-reference/#accuracy-vs-speed-trade-offs","text":"High Accuracy : BERTScore, Math Correctness, Code Execution Balanced : Safety, Toxicity, Helpfulness Fast Approximation : Length, Coherence, Fluency","title":"Accuracy vs Speed Trade-offs"},{"location":"api-reference/reward-functions-reference/#best-practices","text":"","title":"Best Practices"},{"location":"api-reference/reward-functions-reference/#reward-selection","text":"Start Simple : Begin with basic rewards (length, coherence, safety) Add Task-Specific : Include rewards relevant to your use case Balance Weights : Ensure no single reward dominates Monitor Performance : Track reward trends during training","title":"Reward Selection"},{"location":"api-reference/reward-functions-reference/#weight-tuning","text":"Safety First : Always include safety rewards with high weight Task Relevance : Higher weights for task-specific rewards Quality Balance : Balance between different quality aspects Iterative Tuning : Adjust weights based on training results","title":"Weight Tuning"},{"location":"api-reference/reward-functions-reference/#performance-optimization","text":"Use Fast Rewards : Prefer fast rewards for real-time applications Batch Processing : Process multiple texts together when possible Caching : Cache model outputs for repeated computations Device Selection : Use appropriate devices (CPU/GPU) for each reward","title":"Performance Optimization"},{"location":"api-reference/reward-functions-reference/#extending-reward-functions","text":"","title":"Extending Reward Functions"},{"location":"api-reference/reward-functions-reference/#custom-reward-functions","text":"from aligntune import RewardFunction , RewardType class CustomReward ( RewardFunction ): def __init__ ( self , config : RewardConfig ): super () . __init__ ( config ) # Initialize your custom reward logic def compute ( self , text : str , reference : Optional [ str ] = None , ** kwargs ) -> float : # Implement your custom reward computation return 0.8 # Return reward score between 0 and 1 # Register custom reward from aligntune import RewardRegistry RewardRegistry . register_reward ( \"custom\" , CustomReward )","title":"Custom Reward Functions"},{"location":"api-reference/reward-functions-reference/#combining-rewards","text":"def combined_reward ( text : str , rewards : List [ RewardFunction ]) -> float : \"\"\"Combine multiple reward functions.\"\"\" total_score = 0.0 total_weight = 0.0 for reward_func in rewards : score = reward_func . compute ( text ) weight = reward_func . config . weight total_score += score * weight total_weight += weight return total_score / total_weight if total_weight > 0 else 0.0","title":"Combining Rewards"},{"location":"api-reference/reward-functions-reference/#monitoring-and-debugging","text":"","title":"Monitoring and Debugging"},{"location":"api-reference/reward-functions-reference/#reward-tracking","text":"Monitor individual reward scores during training Track reward trends and convergence Identify which rewards are most influential Debug reward computation issues","title":"Reward Tracking"},{"location":"api-reference/reward-functions-reference/#common-issues","text":"Reward Collapse : All rewards converge to similar values Reward Instability : High variance in reward scores Reward Conflicts : Conflicting objectives between rewards Performance Bottlenecks : Slow reward computation","title":"Common Issues"},{"location":"api-reference/reward-functions-reference/#debugging-tools","text":"Reward score logging and visualization Individual reward breakdown analysis Performance profiling for slow rewards A/B testing for reward configurations AlignTune provides a comprehensive suite of reward functions to create sophisticated reward landscapes for any RLHF training scenario. Choose the right combination of rewards for your specific use case and achieve optimal model performance!","title":"Debugging Tools"},{"location":"api-reference/reward-model-training-reference/","text":"Reward Model Training System \u00b6 Overview \u00b6 The Reward Model Training System provides comprehensive capabilities for training neural reward models from rule-based reward functions and integrating them into PPO training pipelines. This system enables scalable reward model training with strict validation, no fallbacks, and production-ready implementations. Architecture \u00b6 Core Components \u00b6 RewardModelTrainer : Main training class using TRL's RewardTrainer RewardModelDataset : PyTorch Dataset for reward training data RewardModelValidator : Strict validation with no fallbacks RewardModelLoader : Loads HF Hub, local, or custom trained models RewardModelTrainingConfig : Configuration for training parameters RewardModelSourceConfig : Configuration for reward model sources Integration Points \u00b6 PPO Integration : Both Unsloth and TRL PPO backends support reward model loading Reward Functions : Uses existing CompositeReward system for training data generation Configuration : Integrates with existing dataclass-based config system Validation : Strict validation throughout with clear error messages Usage Modes \u00b6 1. Standalone Reward Model Training \u00b6 Train reward models independently without PPO: from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func ], composite_weights = [ 0.5 , 0.5 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) 2. PPO with Custom Reward Model Training \u00b6 Train custom reward models as part of PPO training: from aligntune.core.backend_factory import create_rl_trainer # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , max_samples = 100 ) 3. PPO with Pre-trained Reward Models \u00b6 Use existing reward models from HuggingFace Hub or local paths: # HuggingFace Hub model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" , # ... other config ) # Local model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_path = \"./reward_models/my_custom_model\" , # ... other config ) Configuration \u00b6 RewardModelTrainingConfig \u00b6 Configuration for reward model training with strict validation: @dataclass class RewardModelTrainingConfig : base_model_name : str # Required - no default training_texts : List [ str ] # Required - no default reward_functions : List [ str ] # Required - no default output_dir : str # Required - no default # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training parameters with justified defaults num_epochs : int = 3 learning_rate : float = 1e-5 batch_size : int = 8 gradient_accumulation_steps : int = 4 max_length : int = 512 RewardModelSourceConfig \u00b6 Configuration for reward model source with strict mutual exclusivity: @dataclass class RewardModelSourceConfig : source_type : str # \"pretrained_hf\", \"pretrained_local\", \"custom_trained\" model_name : Optional [ str ] = None # For HF Hub model_path : Optional [ str ] = None # For local training_config : Optional [ RewardModelTrainingConfig ] = None # For custom Validation \u00b6 Strict Validation Rules \u00b6 Exactly One Source : Must specify exactly one reward source (HF Hub, local, or custom) No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Consistent Lengths : Reference texts must match training texts length Positive Weights : All reward weights must be positive Error Handling \u00b6 No Fallbacks : System fails fast with clear error messages No Silent Failures : All errors are logged and raised Comprehensive Validation : Every input is validated before processing Clear Messages : Error messages include actionable information Examples \u00b6 Complete Custom Training Example \u00b6 import logging from aligntune.core.backend_factory import create_rl_trainer logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def load_training_texts (): \"\"\"Load your training texts here.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure about this, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" , # ... more texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # Training parameters num_epochs = 1 , batch_size = 1 , max_samples = 100 , output_dir = \"./output/ppo_custom_reward\" ) # Setup and train trainer . setup_data () trainer . setup_trainer () results = trainer . train () logger . info ( f \"Training completed: { results } \" ) if __name__ == \"__main__\" : main () Standalone Training Example \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry def standalone_training (): # Get reward functions registry = RewardRegistry () reward_functions = [ registry . get_reward_function ( \"length\" ), registry . get_reward_function ( \"sentiment\" ), registry . get_reward_function ( \"safety\" ) ] # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = reward_functions , composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data texts = [ \"Your training texts here...\" ] * 20 training_data = trainer . generate_training_data ( texts ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/standalone\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Model trained and saved to: { model_path } \" ) if __name__ == \"__main__\" : standalone_training () Supported Reward Functions \u00b6 The system supports all reward functions available in the registry: Length : Text length rewards Sentiment : Sentiment analysis rewards Safety : Safety and toxicity rewards Coherence : Text coherence rewards BLEU : BLEU score rewards ROUGE : ROUGE score rewards Math Correctness : Mathematical reasoning rewards Code Syntax : Code quality rewards Toxicity : Toxicity detection rewards Troubleshooting \u00b6 Common Issues \u00b6 \"Need at least 10 training texts\" Solution: Provide at least 10 training texts Check: len(training_texts) >= 10 \"Missing reward functions\" Solution: Use functions available in registry Check: RewardRegistry().list_reward_functions() \"Multiple reward sources specified\" Solution: Specify exactly one source (HF Hub, local, or custom) Check: Only one of reward_model_name , reward_model_path , train_custom_reward_model should be True \"reward_model_path does not exist\" Solution: Ensure local path exists and contains model files Check: Path exists and contains config.json and model files \"Model not accessible on HuggingFace Hub\" Solution: Check model name and internet connection Check: huggingface_hub.model_info(model_name) Debug Mode \u00b6 Enable debug logging for detailed information: import logging logging . basicConfig ( level = logging . DEBUG ) Validation Helpers \u00b6 Use validation helpers to check configuration: from aligntune.rewards.training import RewardModelValidator # Validate training config RewardModelValidator . validate_training_config ( config ) # Validate reward source RewardModelValidator . validate_reward_source ( source_config ) # Validate HF model access RewardModelValidator . validate_hf_model_access ( \"model-name\" ) # Validate local path RewardModelValidator . validate_local_path ( \"/path/to/model\" ) Performance Considerations \u00b6 Memory Usage \u00b6 Batch Processing : Use appropriate batch sizes for your GPU memory Gradient Accumulation : Use gradient accumulation for effective larger batch sizes Model Quantization : Consider 4-bit quantization for memory efficiency Training Speed \u00b6 Batch Size : Larger batches generally train faster Gradient Accumulation : Use gradient accumulation to simulate larger batches Model Size : Smaller base models train faster but may have lower quality Scalability \u00b6 Distributed Training : Use multiple GPUs for large-scale training Data Parallelism : Process multiple samples in parallel Model Parallelism : Split large models across devices Best Practices \u00b6 Data Quality : Use high-quality training texts for better reward models Function Selection : Choose reward functions relevant to your task Weight Tuning : Experiment with different reward function weights Validation : Always validate your configuration before training Monitoring : Monitor training progress and adjust parameters as needed Testing : Test trained models before using in production API Reference \u00b6 RewardModelTrainer \u00b6 class RewardModelTrainer : def __init__ ( self , base_model_name : str , reward_functions : List [ RewardFunction ], composite_weights : List [ float ]) def generate_training_data ( self , texts : List [ str ], references : Optional [ List [ str ]] = None , batch_size : int = 32 ) -> RewardModelDataset def train_reward_model ( self , training_data : RewardModelDataset , output_dir : str , ** kwargs ) -> str def create_scalable_pipeline ( self , training_texts : List [ str ], output_dir : str , ** kwargs ) -> str RewardModelDataset \u00b6 class RewardModelDataset ( Dataset ): def __init__ ( self , texts : List [ str ], reward_scores : List [ float ], tokenizer : AutoTokenizer , max_length : int = 512 ) def __len__ ( self ) -> int def __getitem__ ( self , idx : int ) -> Dict [ str , torch . Tensor ] RewardModelValidator \u00b6 class RewardModelValidator : @staticmethod def validate_reward_source ( source_config : RewardModelSourceConfig ) -> None @staticmethod def validate_training_config ( config : RewardModelTrainingConfig ) -> None @staticmethod def validate_hf_model_access ( model_name : str ) -> None @staticmethod def validate_local_path ( path : str ) -> None RewardModelLoader \u00b6 class RewardModelLoader : def load_from_huggingface ( self , model_name : str , ** kwargs ) -> AutoModelForSequenceClassification def load_from_local ( self , model_path : str , ** kwargs ) -> AutoModelForSequenceClassification Contributing \u00b6 When contributing to the reward model training system: Follow Validation : Always use strict validation with no fallbacks Add Tests : Include comprehensive tests for new features Document Changes : Update documentation for any API changes Error Handling : Provide clear, actionable error messages Performance : Consider performance implications of changes License \u00b6 This reward model training system is part of the AlignTune project and follows the same license terms.","title":"Reward Model Training System"},{"location":"api-reference/reward-model-training-reference/#reward-model-training-system","text":"","title":"Reward Model Training System"},{"location":"api-reference/reward-model-training-reference/#overview","text":"The Reward Model Training System provides comprehensive capabilities for training neural reward models from rule-based reward functions and integrating them into PPO training pipelines. This system enables scalable reward model training with strict validation, no fallbacks, and production-ready implementations.","title":"Overview"},{"location":"api-reference/reward-model-training-reference/#architecture","text":"","title":"Architecture"},{"location":"api-reference/reward-model-training-reference/#core-components","text":"RewardModelTrainer : Main training class using TRL's RewardTrainer RewardModelDataset : PyTorch Dataset for reward training data RewardModelValidator : Strict validation with no fallbacks RewardModelLoader : Loads HF Hub, local, or custom trained models RewardModelTrainingConfig : Configuration for training parameters RewardModelSourceConfig : Configuration for reward model sources","title":"Core Components"},{"location":"api-reference/reward-model-training-reference/#integration-points","text":"PPO Integration : Both Unsloth and TRL PPO backends support reward model loading Reward Functions : Uses existing CompositeReward system for training data generation Configuration : Integrates with existing dataclass-based config system Validation : Strict validation throughout with clear error messages","title":"Integration Points"},{"location":"api-reference/reward-model-training-reference/#usage-modes","text":"","title":"Usage Modes"},{"location":"api-reference/reward-model-training-reference/#1-standalone-reward-model-training","text":"Train reward models independently without PPO: from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func ], composite_weights = [ 0.5 , 0.5 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 )","title":"1. Standalone Reward Model Training"},{"location":"api-reference/reward-model-training-reference/#2-ppo-with-custom-reward-model-training","text":"Train custom reward models as part of PPO training: from aligntune.core.backend_factory import create_rl_trainer # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , max_samples = 100 )","title":"2. PPO with Custom Reward Model Training"},{"location":"api-reference/reward-model-training-reference/#3-ppo-with-pre-trained-reward-models","text":"Use existing reward models from HuggingFace Hub or local paths: # HuggingFace Hub model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" , # ... other config ) # Local model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_path = \"./reward_models/my_custom_model\" , # ... other config )","title":"3. PPO with Pre-trained Reward Models"},{"location":"api-reference/reward-model-training-reference/#configuration","text":"","title":"Configuration"},{"location":"api-reference/reward-model-training-reference/#rewardmodeltrainingconfig","text":"Configuration for reward model training with strict validation: @dataclass class RewardModelTrainingConfig : base_model_name : str # Required - no default training_texts : List [ str ] # Required - no default reward_functions : List [ str ] # Required - no default output_dir : str # Required - no default # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training parameters with justified defaults num_epochs : int = 3 learning_rate : float = 1e-5 batch_size : int = 8 gradient_accumulation_steps : int = 4 max_length : int = 512","title":"RewardModelTrainingConfig"},{"location":"api-reference/reward-model-training-reference/#rewardmodelsourceconfig","text":"Configuration for reward model source with strict mutual exclusivity: @dataclass class RewardModelSourceConfig : source_type : str # \"pretrained_hf\", \"pretrained_local\", \"custom_trained\" model_name : Optional [ str ] = None # For HF Hub model_path : Optional [ str ] = None # For local training_config : Optional [ RewardModelTrainingConfig ] = None # For custom","title":"RewardModelSourceConfig"},{"location":"api-reference/reward-model-training-reference/#validation","text":"","title":"Validation"},{"location":"api-reference/reward-model-training-reference/#strict-validation-rules","text":"Exactly One Source : Must specify exactly one reward source (HF Hub, local, or custom) No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Consistent Lengths : Reference texts must match training texts length Positive Weights : All reward weights must be positive","title":"Strict Validation Rules"},{"location":"api-reference/reward-model-training-reference/#error-handling","text":"No Fallbacks : System fails fast with clear error messages No Silent Failures : All errors are logged and raised Comprehensive Validation : Every input is validated before processing Clear Messages : Error messages include actionable information","title":"Error Handling"},{"location":"api-reference/reward-model-training-reference/#examples","text":"","title":"Examples"},{"location":"api-reference/reward-model-training-reference/#complete-custom-training-example","text":"import logging from aligntune.core.backend_factory import create_rl_trainer logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def load_training_texts (): \"\"\"Load your training texts here.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure about this, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" , # ... more texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # Training parameters num_epochs = 1 , batch_size = 1 , max_samples = 100 , output_dir = \"./output/ppo_custom_reward\" ) # Setup and train trainer . setup_data () trainer . setup_trainer () results = trainer . train () logger . info ( f \"Training completed: { results } \" ) if __name__ == \"__main__\" : main ()","title":"Complete Custom Training Example"},{"location":"api-reference/reward-model-training-reference/#standalone-training-example","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry def standalone_training (): # Get reward functions registry = RewardRegistry () reward_functions = [ registry . get_reward_function ( \"length\" ), registry . get_reward_function ( \"sentiment\" ), registry . get_reward_function ( \"safety\" ) ] # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = reward_functions , composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data texts = [ \"Your training texts here...\" ] * 20 training_data = trainer . generate_training_data ( texts ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/standalone\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Model trained and saved to: { model_path } \" ) if __name__ == \"__main__\" : standalone_training ()","title":"Standalone Training Example"},{"location":"api-reference/reward-model-training-reference/#supported-reward-functions","text":"The system supports all reward functions available in the registry: Length : Text length rewards Sentiment : Sentiment analysis rewards Safety : Safety and toxicity rewards Coherence : Text coherence rewards BLEU : BLEU score rewards ROUGE : ROUGE score rewards Math Correctness : Mathematical reasoning rewards Code Syntax : Code quality rewards Toxicity : Toxicity detection rewards","title":"Supported Reward Functions"},{"location":"api-reference/reward-model-training-reference/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api-reference/reward-model-training-reference/#common-issues","text":"\"Need at least 10 training texts\" Solution: Provide at least 10 training texts Check: len(training_texts) >= 10 \"Missing reward functions\" Solution: Use functions available in registry Check: RewardRegistry().list_reward_functions() \"Multiple reward sources specified\" Solution: Specify exactly one source (HF Hub, local, or custom) Check: Only one of reward_model_name , reward_model_path , train_custom_reward_model should be True \"reward_model_path does not exist\" Solution: Ensure local path exists and contains model files Check: Path exists and contains config.json and model files \"Model not accessible on HuggingFace Hub\" Solution: Check model name and internet connection Check: huggingface_hub.model_info(model_name)","title":"Common Issues"},{"location":"api-reference/reward-model-training-reference/#debug-mode","text":"Enable debug logging for detailed information: import logging logging . basicConfig ( level = logging . DEBUG )","title":"Debug Mode"},{"location":"api-reference/reward-model-training-reference/#validation-helpers","text":"Use validation helpers to check configuration: from aligntune.rewards.training import RewardModelValidator # Validate training config RewardModelValidator . validate_training_config ( config ) # Validate reward source RewardModelValidator . validate_reward_source ( source_config ) # Validate HF model access RewardModelValidator . validate_hf_model_access ( \"model-name\" ) # Validate local path RewardModelValidator . validate_local_path ( \"/path/to/model\" )","title":"Validation Helpers"},{"location":"api-reference/reward-model-training-reference/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"api-reference/reward-model-training-reference/#memory-usage","text":"Batch Processing : Use appropriate batch sizes for your GPU memory Gradient Accumulation : Use gradient accumulation for effective larger batch sizes Model Quantization : Consider 4-bit quantization for memory efficiency","title":"Memory Usage"},{"location":"api-reference/reward-model-training-reference/#training-speed","text":"Batch Size : Larger batches generally train faster Gradient Accumulation : Use gradient accumulation to simulate larger batches Model Size : Smaller base models train faster but may have lower quality","title":"Training Speed"},{"location":"api-reference/reward-model-training-reference/#scalability","text":"Distributed Training : Use multiple GPUs for large-scale training Data Parallelism : Process multiple samples in parallel Model Parallelism : Split large models across devices","title":"Scalability"},{"location":"api-reference/reward-model-training-reference/#best-practices","text":"Data Quality : Use high-quality training texts for better reward models Function Selection : Choose reward functions relevant to your task Weight Tuning : Experiment with different reward function weights Validation : Always validate your configuration before training Monitoring : Monitor training progress and adjust parameters as needed Testing : Test trained models before using in production","title":"Best Practices"},{"location":"api-reference/reward-model-training-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/reward-model-training-reference/#rewardmodeltrainer","text":"class RewardModelTrainer : def __init__ ( self , base_model_name : str , reward_functions : List [ RewardFunction ], composite_weights : List [ float ]) def generate_training_data ( self , texts : List [ str ], references : Optional [ List [ str ]] = None , batch_size : int = 32 ) -> RewardModelDataset def train_reward_model ( self , training_data : RewardModelDataset , output_dir : str , ** kwargs ) -> str def create_scalable_pipeline ( self , training_texts : List [ str ], output_dir : str , ** kwargs ) -> str","title":"RewardModelTrainer"},{"location":"api-reference/reward-model-training-reference/#rewardmodeldataset","text":"class RewardModelDataset ( Dataset ): def __init__ ( self , texts : List [ str ], reward_scores : List [ float ], tokenizer : AutoTokenizer , max_length : int = 512 ) def __len__ ( self ) -> int def __getitem__ ( self , idx : int ) -> Dict [ str , torch . Tensor ]","title":"RewardModelDataset"},{"location":"api-reference/reward-model-training-reference/#rewardmodelvalidator","text":"class RewardModelValidator : @staticmethod def validate_reward_source ( source_config : RewardModelSourceConfig ) -> None @staticmethod def validate_training_config ( config : RewardModelTrainingConfig ) -> None @staticmethod def validate_hf_model_access ( model_name : str ) -> None @staticmethod def validate_local_path ( path : str ) -> None","title":"RewardModelValidator"},{"location":"api-reference/reward-model-training-reference/#rewardmodelloader","text":"class RewardModelLoader : def load_from_huggingface ( self , model_name : str , ** kwargs ) -> AutoModelForSequenceClassification def load_from_local ( self , model_path : str , ** kwargs ) -> AutoModelForSequenceClassification","title":"RewardModelLoader"},{"location":"api-reference/reward-model-training-reference/#contributing","text":"When contributing to the reward model training system: Follow Validation : Always use strict validation with no fallbacks Add Tests : Include comprehensive tests for new features Document Changes : Update documentation for any API changes Error Handling : Provide clear, actionable error messages Performance : Consider performance implications of changes","title":"Contributing"},{"location":"api-reference/reward-model-training-reference/#license","text":"This reward model training system is part of the AlignTune project and follows the same license terms.","title":"License"},{"location":"api-reference/trainers/","text":"Trainers API Reference \u00b6 Complete API reference for AlignTune trainer classes. Base Trainers \u00b6 TrainerBase (RL) \u00b6 Abstract base trainer for RL training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. Source code in src/aligntune/core/rl/trainer_base.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 class TrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. \"\"\" def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = False, # metrics: Optional[List] = None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) # metric_key_prefix: Prefix for metric keys # use_custom_evaluator: If True, use BaseEvaluator/RLEvaluator. If False, use native. # metrics: Metrics to compute (only for custom evaluator, overrides setup) # **kwargs: Additional evaluation arguments (e.g., reference_model, reward_model for RL) # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Route to appropriate evaluator # if use_custom_evaluator: # eval_metrics = self._evaluate_with_custom(eval_dataset, metrics, **kwargs) # else: # eval_metrics = self._evaluate_native(eval_dataset, **kwargs) # # Apply metric key prefix # if metric_key_prefix and eval_metrics: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # if eval_metrics: # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def _evaluate_with_custom( # self, # eval_dataset, # metrics: Optional[List], # **kwargs # ) -> Dict[str, float]: # \"\"\"Evaluate using BaseEvaluator or RLEvaluator.\"\"\" # # Auto-setup if not configured # if self.custom_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\", metrics=metrics) # # Use provided dataset or fall back to configured dataset # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # # RL Evaluation # if isinstance(self.custom_evaluator, RLEvaluator): # # Get reference model (defaults to policy model if not provided) # reference_model = kwargs.pop('reference_model', self.model) # reward_model = kwargs.pop('reward_model', None) # return self.custom_evaluator.evaluate_rl( # policy_model=self.model, # reference_model=reference_model, # tokenizer=self.tokenizer, # dataset=dataset, # reward_model=reward_model, # **kwargs # ) # # Base Evaluation (SFT) # else: # task_name = kwargs.pop('task_name', 'text_generation') # return self.custom_evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # task_name=task_name, # **kwargs # ) # def _evaluate_native( # self, # eval_dataset=None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Use UnifiedEvaluator (native evaluation). # Subclasses can override this to use their trainer-specific evaluation. # \"\"\" # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None def _evaluate_with_custom_evaluator ( self , eval_dataset = None , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Custom evaluation using unified framework (BaseEvaluator + RLEvaluator).\"\"\" dataset = eval_dataset or self . eval_dataset if not dataset : logger . warning ( \"No eval dataset available for custom evaluation\" ) return {} eval_results = {} max_samples = kwargs . get ( 'max_samples' , None ) # 1. BaseEvaluator: Text generation metrics (ROUGE, BLEU, Perplexity) if self . base_evaluator : try : logger . info ( \"Running BaseEvaluator for text generation metrics...\" ) base_metrics = self . base_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = kwargs . get ( 'task_name' , 'text_generation' ), max_samples = max_samples ) eval_results . update ( base_metrics ) logger . info ( f \"BaseEvaluator completed: { list ( base_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"BaseEvaluator failed: { e } \" ) logger . debug ( \"BaseEvaluator error details:\" , exc_info = True ) # 2. RLEvaluator: RL-specific metrics (KL Divergence, Reward Accuracy) if self . rl_evaluator : try : logger . info ( \"Running RLEvaluator for RL-specific metrics...\" ) # Get reference model and reward model from kwargs or use defaults reference_model = kwargs . get ( 'reference_model' , self . model ) reward_model = kwargs . get ( 'reward_model' , getattr ( self , '_combined_reward_function' , None )) # Run RL evaluation rl_metrics = self . rl_evaluator . evaluate_rl ( policy_model = self . model , reference_model = reference_model , tokenizer = self . tokenizer , dataset = dataset , reward_model = reward_model , max_samples = max_samples ) eval_results . update ( rl_metrics ) logger . info ( f \"RLEvaluator completed: { list ( rl_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"RLEvaluator failed: { e } \" ) logger . debug ( \"RLEvaluator error details:\" , exc_info = True ) if not eval_results : logger . warning ( \"No evaluation metrics were computed successfully\" ) return eval_results def _evaluate_native ( self , eval_dataset : Optional [ Any ] = None , ** kwargs : Any ) -> Dict [ str , float ]: \"\"\"Native evaluation - uses UnifiedEvaluator or trainer's built-in evaluate. Subclasses can override this to use their trainer-specific evaluation. \"\"\" dataset = eval_dataset or self . eval_dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Try trainer's built-in evaluate first (if exists) if hasattr ( self , 'trainer' ) and self . trainer and hasattr ( self . trainer , 'evaluate' ): try : return self . trainer . evaluate () except Exception as e : logger . warning ( f \"Trainer evaluation failed: { e } \" ) # Fallback to UnifiedEvaluator # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) return {} def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False __enter__ () \u00b6 Context manager entry. Source code in src/aligntune/core/rl/trainer_base.py 911 912 913 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) \u00b6 Context manager exit. Source code in src/aligntune/core/rl/trainer_base.py 915 916 917 918 919 920 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False __init__ ( config , callbacks = None ) \u00b6 Initialize trainer with configuration. Source code in src/aligntune/core/rl/trainer_base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) cleanup () \u00b6 Cleanup resources. Source code in src/aligntune/core/rl/trainer_base.py 904 905 906 907 908 909 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) create_data_loader () \u00b6 Create data loader (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 699 700 701 def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) evaluate ( eval_dataset = None , metric_key_prefix = 'eval' , use_custom_evaluator = False , metrics = None , ** kwargs ) \u00b6 Evaluate the trained model with flexible evaluation strategies. Parameters: Name Type Description Default eval_dataset Optional [ Any ] Dataset to evaluate on (uses self.eval_dataset if None) None metric_key_prefix str Prefix for metric names in results 'eval' use_custom_evaluator bool If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation False metrics Optional [ List ] Optional list of additional metrics to compute None **kwargs Any Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics with the specified prefix Source code in src/aligntune/core/rl/trainer_base.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results get_next_batch () \u00b6 Get next batch from data loader. Source code in src/aligntune/core/rl/trainer_base.py 687 688 689 690 691 692 693 694 695 696 697 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) get_sample_outputs () \u00b6 Get sample outputs for logging (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 715 716 717 def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] load_checkpoint ( checkpoint_path ) \u00b6 Load checkpoint and broadcast to all ranks. Source code in src/aligntune/core/rl/trainer_base.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) on_eval ( metrics ) \u00b6 Hook for evaluation operations. Source code in src/aligntune/core/rl/trainer_base.py 709 710 711 712 713 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () on_step ( step , metrics ) \u00b6 Hook for step-level operations. Source code in src/aligntune/core/rl/trainer_base.py 703 704 705 706 707 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) predict ( inputs , max_new_tokens = 100 , temperature = 1.0 , top_p = 0.9 , do_sample = True , ** kwargs ) \u00b6 Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Any Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/rl/trainer_base.py 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions push_to_hub ( repo_id , private = False , token = None , commit_message = 'Upload fine-tuned model' , ** kwargs ) \u00b6 Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Any Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/rl/trainer_base.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url save_checkpoint () \u00b6 Save checkpoint (rank-0 only). Source code in src/aligntune/core/rl/trainer_base.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) setup_custom_evaluator ( evaluator_type = 'auto' , metrics = None ) \u00b6 Setup custom evaluators (BaseEvaluator/RLEvaluator). Parameters: Name Type Description Default evaluator_type str \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) 'auto' metrics Optional [ List ] Optional list of metrics to use (overrides defaults) None Source code in src/aligntune/core/rl/trainer_base.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None setup_data () abstractmethod \u00b6 Setup datasets and data loaders. Source code in src/aligntune/core/rl/trainer_base.py 111 112 113 114 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass setup_model () abstractmethod \u00b6 Setup model, tokenizer, and optimization. Source code in src/aligntune/core/rl/trainer_base.py 106 107 108 109 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass setup_rewards () abstractmethod \u00b6 Setup reward functions and evaluators. Source code in src/aligntune/core/rl/trainer_base.py 116 117 118 119 @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass train () \u00b6 Main training loop with hooks. Source code in src/aligntune/core/rl/trainer_base.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) train_step ( batch ) abstractmethod \u00b6 Execute single training step. Source code in src/aligntune/core/rl/trainer_base.py 121 122 123 124 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.rl.trainer_base import TrainerBase from aligntune.core.rl.config import UnifiedConfig # TrainerBase is abstract - use concrete implementations # See Backend Trainers below SFTTrainerBase \u00b6 Abstract base trainer for SFT training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. Source code in src/aligntune/core/sft/trainer_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class SFTTrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. \"\"\" def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics def _evaluate_with_custom ( self , eval_dataset : Optional [ Any ], metrics : Optional [ List ], ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Evaluate using BaseEvaluator (SFT only - no RL metrics).\"\"\" # Auto-setup if not configured if self . custom_evaluator is None : self . setup_custom_evaluator ( evaluator_type = \"base\" , metrics = metrics ) # Use provided dataset or fall back to configured dataset dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Base Evaluation (SFT) task_name = kwargs . pop ( 'task_name' , 'text_generation' ) return self . custom_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = task_name , ** kwargs ) def _evaluate_native ( self , eval_dataset : Optional [ Any ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Use SFTEvaluator (native evaluation).\"\"\" dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} return self . evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , config = self . config , ** kwargs ) def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader __init__ ( config , callbacks = None ) \u00b6 Initialize trainer with configuration. Source code in src/aligntune/core/sft/trainer_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) cleanup () \u00b6 Cleanup resources. Source code in src/aligntune/core/sft/trainer_base.py 592 593 594 595 596 597 598 599 600 601 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader create_data_loader () \u00b6 Create data loader - to be implemented by subclasses. Source code in src/aligntune/core/sft/trainer_base.py 395 396 397 def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) evaluate ( eval_dataset = None , metric_key_prefix = 'eval' , use_custom_evaluator = False , metrics = None , ** kwargs ) \u00b6 Run evaluation and return metrics. Parameters: Name Type Description Default eval_dataset Optional [ Any ] Dataset to evaluate on (defaults to self.eval_dataset) None metric_key_prefix str Prefix for metric keys 'eval' use_custom_evaluator bool If True, use BaseEvaluator. If False, use native SFTEvaluator. False metrics Optional [ List ] Metrics to compute (only for custom evaluator, overrides setup) None **kwargs Any Additional evaluation arguments {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics Source code in src/aligntune/core/sft/trainer_base.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics get_next_batch () \u00b6 Get next batch from data loader. Source code in src/aligntune/core/sft/trainer_base.py 383 384 385 386 387 388 389 390 391 392 393 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) load_checkpoint ( checkpoint_path ) \u00b6 Load checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) on_eval ( metrics ) \u00b6 Hook called after each evaluation. Source code in src/aligntune/core/sft/trainer_base.py 403 404 405 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass on_step ( step , metrics ) \u00b6 Hook called after each training step. Source code in src/aligntune/core/sft/trainer_base.py 399 400 401 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass predict ( inputs , max_new_tokens = 100 , temperature = 1.0 , top_p = 0.9 , do_sample = True , ** kwargs ) \u00b6 Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Any Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/sft/trainer_base.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions push_to_hub ( repo_id , private = False , token = None , commit_message = 'Upload fine-tuned model' , ** kwargs ) \u00b6 Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Any Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/sft/trainer_base.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url save_checkpoint () \u00b6 Save checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) setup_data () abstractmethod \u00b6 Setup datasets and data loaders. Source code in src/aligntune/core/sft/trainer_base.py 99 100 101 102 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass setup_model () abstractmethod \u00b6 Setup model, tokenizer, and optimization. Source code in src/aligntune/core/sft/trainer_base.py 94 95 96 97 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass train () \u00b6 Main training loop with hooks. Source code in src/aligntune/core/sft/trainer_base.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) train_step ( batch ) abstractmethod \u00b6 Execute single training step. Source code in src/aligntune/core/sft/trainer_base.py 104 105 106 107 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.sft.config import SFTConfig # SFTTrainerBase is abstract - use concrete implementations # See Backend Trainers below TrainingState \u00b6 Training state tracking dataclass. Training state tracking. Source code in src/aligntune/core/rl/trainer_base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TrainingState : \"\"\"Training state tracking.\"\"\" step : int = 0 epoch : int = 0 best_metric : float = 0.0 checkpoint_path : Optional [ str ] = None start_time : float = field ( default_factory = time . time ) last_eval_time : float = field ( default_factory = time . time ) last_save_time : float = field ( default_factory = time . time ) def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time get_elapsed_time () \u00b6 Get elapsed training time in seconds. Source code in src/aligntune/core/rl/trainer_base.py 60 61 62 def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time update_best_metric ( metric ) \u00b6 Update best metric if improved. Source code in src/aligntune/core/rl/trainer_base.py 53 54 55 56 57 58 def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False update_epoch ( epoch ) \u00b6 Update current epoch. Source code in src/aligntune/core/rl/trainer_base.py 49 50 51 def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch update_step ( step ) \u00b6 Update current step. Source code in src/aligntune/core/rl/trainer_base.py 45 46 47 def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step options: show_source: true heading_level: 3 Backend Trainers \u00b6 TRL Backends \u00b6 TRLSFTTrainer \u00b6 TRL backend for Supervised Fine-Tuning. Use create_sft_trainer() with backend=\"trl\" instead of instantiating directly. Example : from aligntune.backends.trl.sft.sft import TRLSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = TRLSFTTrainer ( config ) trainer . train () TRLDPOTrainer \u00b6 TRL backend for Direct Preference Optimization. Use create_rl_trainer() with algorithm=\"dpo\" and backend=\"trl\" . Example : from aligntune.backends.trl.rl.dpo.dpo import TRLDPOTrainer from aligntune.core.rl.config import UnifiedConfig , AlgorithmType config = UnifiedConfig ( algo = AlgorithmType . DPO , ... ) trainer = TRLDPOTrainer ( config ) trainer . train () TRLPPOTrainer \u00b6 TRL backend for Proximal Policy Optimization. TRL backend for Proximal Policy Optimization. Use create_rl_trainer() with algorithm=\"ppo\" and backend=\"trl\" . Example : from aligntune.backends.trl.rl.ppo.ppo import TRLPPOTrainer from aligntune.core.rl.config import UnifiedConfig config = UnifiedConfig ( algo = AlgorithmType . PPO , ... ) trainer = TRLPPOTrainer ( config ) trainer . train () TRLGRPOTrainer \u00b6 TRL backend for Group Relative Policy Optimization. TRL backend for Group Relative Policy Optimization. Use create_rl_trainer() with algorithm=\"grpo\" and backend=\"trl\" . Unsloth Backends \u00b6 UnslothSFTTrainer \u00b6 Unsloth backend for Supervised Fine-Tuning (faster). Unsloth backend for Supervised Fine-Tuning (faster). Use create_sft_trainer() with backend=\"unsloth\" . Example : from aligntune.backends.unsloth.sft.sft import UnslothSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = UnslothSFTTrainer ( config ) trainer . train () UnslothDPOTrainer \u00b6 Unsloth backend for Direct Preference Optimization. Unsloth backend for Direct Preference Optimization. Use create_rl_trainer() with algorithm=\"dpo\" and backend=\"unsloth\" . UnslothPPOTrainer \u00b6 Unsloth backend for Proximal Policy Optimization. Unsloth backend for Proximal Policy Optimization. Use create_rl_trainer() with algorithm=\"ppo\" and backend=\"unsloth\" . UnslothGRPOTrainer \u00b6 Unsloth backend for Group Relative Policy Optimization. Unsloth backend for Group Relative Policy Optimization. Use create_rl_trainer() with algorithm=\"grpo\" and backend=\"unsloth\" . Specialized Trainers \u00b6 ClassificationTrainer \u00b6 Specialized trainer for text classification tasks. Specialized trainer for text classification. Use with TRL backend via create_sft_trainer() and task type configuration. Note: Only available for TRL backend. Usage Examples \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Predict result = trainer . predict ( \"What is AI?\" ) RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Push to Hub url = trainer . push_to_hub ( \"username/my-model\" ) Next Steps \u00b6 Backend Factory - Trainer creation functions Configuration Classes - Configuration options User Guide - Usage guide","title":"Trainers"},{"location":"api-reference/trainers/#trainers-api-reference","text":"Complete API reference for AlignTune trainer classes.","title":"Trainers API Reference"},{"location":"api-reference/trainers/#base-trainers","text":"","title":"Base Trainers"},{"location":"api-reference/trainers/#trainerbase-rl","text":"Abstract base trainer for RL training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. Source code in src/aligntune/core/rl/trainer_base.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 class TrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. \"\"\" def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = False, # metrics: Optional[List] = None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) # metric_key_prefix: Prefix for metric keys # use_custom_evaluator: If True, use BaseEvaluator/RLEvaluator. If False, use native. # metrics: Metrics to compute (only for custom evaluator, overrides setup) # **kwargs: Additional evaluation arguments (e.g., reference_model, reward_model for RL) # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Route to appropriate evaluator # if use_custom_evaluator: # eval_metrics = self._evaluate_with_custom(eval_dataset, metrics, **kwargs) # else: # eval_metrics = self._evaluate_native(eval_dataset, **kwargs) # # Apply metric key prefix # if metric_key_prefix and eval_metrics: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # if eval_metrics: # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def _evaluate_with_custom( # self, # eval_dataset, # metrics: Optional[List], # **kwargs # ) -> Dict[str, float]: # \"\"\"Evaluate using BaseEvaluator or RLEvaluator.\"\"\" # # Auto-setup if not configured # if self.custom_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\", metrics=metrics) # # Use provided dataset or fall back to configured dataset # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # # RL Evaluation # if isinstance(self.custom_evaluator, RLEvaluator): # # Get reference model (defaults to policy model if not provided) # reference_model = kwargs.pop('reference_model', self.model) # reward_model = kwargs.pop('reward_model', None) # return self.custom_evaluator.evaluate_rl( # policy_model=self.model, # reference_model=reference_model, # tokenizer=self.tokenizer, # dataset=dataset, # reward_model=reward_model, # **kwargs # ) # # Base Evaluation (SFT) # else: # task_name = kwargs.pop('task_name', 'text_generation') # return self.custom_evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # task_name=task_name, # **kwargs # ) # def _evaluate_native( # self, # eval_dataset=None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Use UnifiedEvaluator (native evaluation). # Subclasses can override this to use their trainer-specific evaluation. # \"\"\" # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None def _evaluate_with_custom_evaluator ( self , eval_dataset = None , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Custom evaluation using unified framework (BaseEvaluator + RLEvaluator).\"\"\" dataset = eval_dataset or self . eval_dataset if not dataset : logger . warning ( \"No eval dataset available for custom evaluation\" ) return {} eval_results = {} max_samples = kwargs . get ( 'max_samples' , None ) # 1. BaseEvaluator: Text generation metrics (ROUGE, BLEU, Perplexity) if self . base_evaluator : try : logger . info ( \"Running BaseEvaluator for text generation metrics...\" ) base_metrics = self . base_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = kwargs . get ( 'task_name' , 'text_generation' ), max_samples = max_samples ) eval_results . update ( base_metrics ) logger . info ( f \"BaseEvaluator completed: { list ( base_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"BaseEvaluator failed: { e } \" ) logger . debug ( \"BaseEvaluator error details:\" , exc_info = True ) # 2. RLEvaluator: RL-specific metrics (KL Divergence, Reward Accuracy) if self . rl_evaluator : try : logger . info ( \"Running RLEvaluator for RL-specific metrics...\" ) # Get reference model and reward model from kwargs or use defaults reference_model = kwargs . get ( 'reference_model' , self . model ) reward_model = kwargs . get ( 'reward_model' , getattr ( self , '_combined_reward_function' , None )) # Run RL evaluation rl_metrics = self . rl_evaluator . evaluate_rl ( policy_model = self . model , reference_model = reference_model , tokenizer = self . tokenizer , dataset = dataset , reward_model = reward_model , max_samples = max_samples ) eval_results . update ( rl_metrics ) logger . info ( f \"RLEvaluator completed: { list ( rl_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"RLEvaluator failed: { e } \" ) logger . debug ( \"RLEvaluator error details:\" , exc_info = True ) if not eval_results : logger . warning ( \"No evaluation metrics were computed successfully\" ) return eval_results def _evaluate_native ( self , eval_dataset : Optional [ Any ] = None , ** kwargs : Any ) -> Dict [ str , float ]: \"\"\"Native evaluation - uses UnifiedEvaluator or trainer's built-in evaluate. Subclasses can override this to use their trainer-specific evaluation. \"\"\" dataset = eval_dataset or self . eval_dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Try trainer's built-in evaluate first (if exists) if hasattr ( self , 'trainer' ) and self . trainer and hasattr ( self . trainer , 'evaluate' ): try : return self . trainer . evaluate () except Exception as e : logger . warning ( f \"Trainer evaluation failed: { e } \" ) # Fallback to UnifiedEvaluator # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) return {} def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False","title":"TrainerBase (RL)"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__enter__","text":"Context manager entry. Source code in src/aligntune/core/rl/trainer_base.py 911 912 913 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self","title":"__enter__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__exit__","text":"Context manager exit. Source code in src/aligntune/core/rl/trainer_base.py 915 916 917 918 919 920 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False","title":"__exit__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__init__","text":"Initialize trainer with configuration. Source code in src/aligntune/core/rl/trainer_base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" )","title":"__init__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.cleanup","text":"Cleanup resources. Source code in src/aligntune/core/rl/trainer_base.py 904 905 906 907 908 909 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" )","title":"cleanup"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.create_data_loader","text":"Create data loader (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 699 700 701 def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" )","title":"create_data_loader"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.evaluate","text":"Evaluate the trained model with flexible evaluation strategies. Parameters: Name Type Description Default eval_dataset Optional [ Any ] Dataset to evaluate on (uses self.eval_dataset if None) None metric_key_prefix str Prefix for metric names in results 'eval' use_custom_evaluator bool If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation False metrics Optional [ List ] Optional list of additional metrics to compute None **kwargs Any Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics with the specified prefix Source code in src/aligntune/core/rl/trainer_base.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results","title":"evaluate"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.get_next_batch","text":"Get next batch from data loader. Source code in src/aligntune/core/rl/trainer_base.py 687 688 689 690 691 692 693 694 695 696 697 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader )","title":"get_next_batch"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.get_sample_outputs","text":"Get sample outputs for logging (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 715 716 717 def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return []","title":"get_sample_outputs"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.load_checkpoint","text":"Load checkpoint and broadcast to all ranks. Source code in src/aligntune/core/rl/trainer_base.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" )","title":"load_checkpoint"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.on_eval","text":"Hook for evaluation operations. Source code in src/aligntune/core/rl/trainer_base.py 709 710 711 712 713 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step ()","title":"on_eval"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.on_step","text":"Hook for step-level operations. Source code in src/aligntune/core/rl/trainer_base.py 703 704 705 706 707 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step )","title":"on_step"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.predict","text":"Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Any Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/rl/trainer_base.py 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions","title":"predict"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.push_to_hub","text":"Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Any Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/rl/trainer_base.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url","title":"push_to_hub"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.save_checkpoint","text":"Save checkpoint (rank-0 only). Source code in src/aligntune/core/rl/trainer_base.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control )","title":"save_checkpoint"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_custom_evaluator","text":"Setup custom evaluators (BaseEvaluator/RLEvaluator). Parameters: Name Type Description Default evaluator_type str \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) 'auto' metrics Optional [ List ] Optional list of metrics to use (overrides defaults) None Source code in src/aligntune/core/rl/trainer_base.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None","title":"setup_custom_evaluator"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_data","text":"Setup datasets and data loaders. Source code in src/aligntune/core/rl/trainer_base.py 111 112 113 114 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass","title":"setup_data"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_model","text":"Setup model, tokenizer, and optimization. Source code in src/aligntune/core/rl/trainer_base.py 106 107 108 109 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass","title":"setup_model"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_rewards","text":"Setup reward functions and evaluators. Source code in src/aligntune/core/rl/trainer_base.py 116 117 118 119 @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass","title":"setup_rewards"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.train","text":"Main training loop with hooks. Source code in src/aligntune/core/rl/trainer_base.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" )","title":"train"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.train_step","text":"Execute single training step. Source code in src/aligntune/core/rl/trainer_base.py 121 122 123 124 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.rl.trainer_base import TrainerBase from aligntune.core.rl.config import UnifiedConfig # TrainerBase is abstract - use concrete implementations # See Backend Trainers below","title":"train_step"},{"location":"api-reference/trainers/#sfttrainerbase","text":"Abstract base trainer for SFT training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. Source code in src/aligntune/core/sft/trainer_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class SFTTrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. \"\"\" def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics def _evaluate_with_custom ( self , eval_dataset : Optional [ Any ], metrics : Optional [ List ], ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Evaluate using BaseEvaluator (SFT only - no RL metrics).\"\"\" # Auto-setup if not configured if self . custom_evaluator is None : self . setup_custom_evaluator ( evaluator_type = \"base\" , metrics = metrics ) # Use provided dataset or fall back to configured dataset dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Base Evaluation (SFT) task_name = kwargs . pop ( 'task_name' , 'text_generation' ) return self . custom_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = task_name , ** kwargs ) def _evaluate_native ( self , eval_dataset : Optional [ Any ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Use SFTEvaluator (native evaluation).\"\"\" dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} return self . evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , config = self . config , ** kwargs ) def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader","title":"SFTTrainerBase"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.__init__","text":"Initialize trainer with configuration. Source code in src/aligntune/core/sft/trainer_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" )","title":"__init__"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.cleanup","text":"Cleanup resources. Source code in src/aligntune/core/sft/trainer_base.py 592 593 594 595 596 597 598 599 600 601 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader","title":"cleanup"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.create_data_loader","text":"Create data loader - to be implemented by subclasses. Source code in src/aligntune/core/sft/trainer_base.py 395 396 397 def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" )","title":"create_data_loader"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.evaluate","text":"Run evaluation and return metrics. Parameters: Name Type Description Default eval_dataset Optional [ Any ] Dataset to evaluate on (defaults to self.eval_dataset) None metric_key_prefix str Prefix for metric keys 'eval' use_custom_evaluator bool If True, use BaseEvaluator. If False, use native SFTEvaluator. False metrics Optional [ List ] Metrics to compute (only for custom evaluator, overrides setup) None **kwargs Any Additional evaluation arguments {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics Source code in src/aligntune/core/sft/trainer_base.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def evaluate ( self , eval_dataset : Optional [ Any ] = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs : Any , ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics","title":"evaluate"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.get_next_batch","text":"Get next batch from data loader. Source code in src/aligntune/core/sft/trainer_base.py 383 384 385 386 387 388 389 390 391 392 393 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader )","title":"get_next_batch"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.load_checkpoint","text":"Load checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ())","title":"load_checkpoint"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.on_eval","text":"Hook called after each evaluation. Source code in src/aligntune/core/sft/trainer_base.py 403 404 405 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass","title":"on_eval"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.on_step","text":"Hook called after each training step. Source code in src/aligntune/core/sft/trainer_base.py 399 400 401 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass","title":"on_step"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.predict","text":"Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Any Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/sft/trainer_base.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs : Any , ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions","title":"predict"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.push_to_hub","text":"Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Any Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/sft/trainer_base.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs : Any , ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url","title":"push_to_hub"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.save_checkpoint","text":"Save checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control )","title":"save_checkpoint"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.setup_data","text":"Setup datasets and data loaders. Source code in src/aligntune/core/sft/trainer_base.py 99 100 101 102 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass","title":"setup_data"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.setup_model","text":"Setup model, tokenizer, and optimization. Source code in src/aligntune/core/sft/trainer_base.py 94 95 96 97 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass","title":"setup_model"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.train","text":"Main training loop with hooks. Source code in src/aligntune/core/sft/trainer_base.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" )","title":"train"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.train_step","text":"Execute single training step. Source code in src/aligntune/core/sft/trainer_base.py 104 105 106 107 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.sft.config import SFTConfig # SFTTrainerBase is abstract - use concrete implementations # See Backend Trainers below","title":"train_step"},{"location":"api-reference/trainers/#trainingstate","text":"Training state tracking dataclass. Training state tracking. Source code in src/aligntune/core/rl/trainer_base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TrainingState : \"\"\"Training state tracking.\"\"\" step : int = 0 epoch : int = 0 best_metric : float = 0.0 checkpoint_path : Optional [ str ] = None start_time : float = field ( default_factory = time . time ) last_eval_time : float = field ( default_factory = time . time ) last_save_time : float = field ( default_factory = time . time ) def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time","title":"TrainingState"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.get_elapsed_time","text":"Get elapsed training time in seconds. Source code in src/aligntune/core/rl/trainer_base.py 60 61 62 def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time","title":"get_elapsed_time"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_best_metric","text":"Update best metric if improved. Source code in src/aligntune/core/rl/trainer_base.py 53 54 55 56 57 58 def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False","title":"update_best_metric"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_epoch","text":"Update current epoch. Source code in src/aligntune/core/rl/trainer_base.py 49 50 51 def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch","title":"update_epoch"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_step","text":"Update current step. Source code in src/aligntune/core/rl/trainer_base.py 45 46 47 def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step options: show_source: true heading_level: 3","title":"update_step"},{"location":"api-reference/trainers/#backend-trainers","text":"","title":"Backend Trainers"},{"location":"api-reference/trainers/#trl-backends","text":"","title":"TRL Backends"},{"location":"api-reference/trainers/#trlsfttrainer","text":"TRL backend for Supervised Fine-Tuning. Use create_sft_trainer() with backend=\"trl\" instead of instantiating directly. Example : from aligntune.backends.trl.sft.sft import TRLSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = TRLSFTTrainer ( config ) trainer . train ()","title":"TRLSFTTrainer"},{"location":"api-reference/trainers/#trldpotrainer","text":"TRL backend for Direct Preference Optimization. Use create_rl_trainer() with algorithm=\"dpo\" and backend=\"trl\" . Example : from aligntune.backends.trl.rl.dpo.dpo import TRLDPOTrainer from aligntune.core.rl.config import UnifiedConfig , AlgorithmType config = UnifiedConfig ( algo = AlgorithmType . DPO , ... ) trainer = TRLDPOTrainer ( config ) trainer . train ()","title":"TRLDPOTrainer"},{"location":"api-reference/trainers/#trlppotrainer","text":"TRL backend for Proximal Policy Optimization. TRL backend for Proximal Policy Optimization. Use create_rl_trainer() with algorithm=\"ppo\" and backend=\"trl\" . Example : from aligntune.backends.trl.rl.ppo.ppo import TRLPPOTrainer from aligntune.core.rl.config import UnifiedConfig config = UnifiedConfig ( algo = AlgorithmType . PPO , ... ) trainer = TRLPPOTrainer ( config ) trainer . train ()","title":"TRLPPOTrainer"},{"location":"api-reference/trainers/#trlgrpotrainer","text":"TRL backend for Group Relative Policy Optimization. TRL backend for Group Relative Policy Optimization. Use create_rl_trainer() with algorithm=\"grpo\" and backend=\"trl\" .","title":"TRLGRPOTrainer"},{"location":"api-reference/trainers/#unsloth-backends","text":"","title":"Unsloth Backends"},{"location":"api-reference/trainers/#unslothsfttrainer","text":"Unsloth backend for Supervised Fine-Tuning (faster). Unsloth backend for Supervised Fine-Tuning (faster). Use create_sft_trainer() with backend=\"unsloth\" . Example : from aligntune.backends.unsloth.sft.sft import UnslothSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = UnslothSFTTrainer ( config ) trainer . train ()","title":"UnslothSFTTrainer"},{"location":"api-reference/trainers/#unslothdpotrainer","text":"Unsloth backend for Direct Preference Optimization. Unsloth backend for Direct Preference Optimization. Use create_rl_trainer() with algorithm=\"dpo\" and backend=\"unsloth\" .","title":"UnslothDPOTrainer"},{"location":"api-reference/trainers/#unslothppotrainer","text":"Unsloth backend for Proximal Policy Optimization. Unsloth backend for Proximal Policy Optimization. Use create_rl_trainer() with algorithm=\"ppo\" and backend=\"unsloth\" .","title":"UnslothPPOTrainer"},{"location":"api-reference/trainers/#unslothgrpotrainer","text":"Unsloth backend for Group Relative Policy Optimization. Unsloth backend for Group Relative Policy Optimization. Use create_rl_trainer() with algorithm=\"grpo\" and backend=\"unsloth\" .","title":"UnslothGRPOTrainer"},{"location":"api-reference/trainers/#specialized-trainers","text":"","title":"Specialized Trainers"},{"location":"api-reference/trainers/#classificationtrainer","text":"Specialized trainer for text classification tasks. Specialized trainer for text classification. Use with TRL backend via create_sft_trainer() and task type configuration. Note: Only available for TRL backend.","title":"ClassificationTrainer"},{"location":"api-reference/trainers/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/trainers/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Predict result = trainer . predict ( \"What is AI?\" )","title":"SFT Training"},{"location":"api-reference/trainers/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Push to Hub url = trainer . push_to_hub ( \"username/my-model\" )","title":"RL Training"},{"location":"api-reference/trainers/#next-steps","text":"Backend Factory - Trainer creation functions Configuration Classes - Configuration options User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/unified-api/","text":"Unified RLHF API Documentation \u00b6 This document describes the unified RLHF training system that provides a consistent interface for training with PPO, DPO, GRPO, GSPO, DAPO, and Dr. GRPO algorithms. Overview \u00b6 The unified system provides: - Consistent API : Same interface for all RLHF algorithms - No Interactive Prompts : Everything is configuration-driven - No Placeholder Defaults : All critical parameters must be explicitly provided - Distributed Training : Support for DDP, FSDP, and DeepSpeed - Comprehensive Logging : TensorBoard and WandB integration - Dataset Caching : Efficient caching with hash-based keys - Extensible : Easy to add new algorithms, datasets, and rewards Quick Start \u00b6 Python Library Usage \u00b6 from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , ConfigLoader , TrainerFactory ) # Create configuration config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-small\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , percent = 1 )], train = TrainingConfig ( max_steps = 100 ) ) # Create and run trainer trainer = TrainerFactory . create_trainer ( config ) trainer . train () CLI Usage \u00b6 # Train with YAML config python -m aligntune.cli.train_unified --config examples/configs/ppo_minimal.yaml # Train with CLI arguments python -m aligntune.cli.train_unified \\ --algo ppo \\ --model microsoft/DialoGPT-small \\ --dataset Anthropic/hh-rlhf:train#percent = 1 \\ --max-steps 100 Configuration \u00b6 UnifiedConfig \u00b6 The main configuration class that contains all training parameters: @dataclass class UnifiedConfig : algo : AlgorithmType # Algorithm to use model : ModelConfig # Model configuration datasets : List [ DatasetConfig ] # Dataset configurations tasks : List [ Dict [ str , Any ]] # Task definitions rewards : List [ RewardConfig ] # Reward functions train : TrainingConfig # Training parameters distributed : DistributedConfig # Distributed training logging : LoggingConfig # Logging configuration chat_template : Optional [ str ] # Chat template caching : Dict [ str , Any ] # Caching configuration Algorithm Types \u00b6 class AlgorithmType ( Enum ): PPO = \"ppo\" # Proximal Policy Optimization DPO = \"dpo\" # Direct Preference Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective DAPO = \"dapo\" # Decouple Clip and Dynamic sAmpling Policy Optimization DRGRPO = \"drgrpo\" # Dr. GRPO (GRPO Done Right) Model Configuration \u00b6 @dataclass class ModelConfig : name_or_path : str # Model name or path (required) sft_path : Optional [ str ] # SFT checkpoint path reward_path : Optional [ str ] # Reward model path precision : PrecisionType # Model precision quantization : Dict [ str , Any ] # Quantization settings attn_implementation : str # Attention implementation gradient_checkpointing : bool # Enable gradient checkpointing max_memory : Optional [ Dict [ str , str ]] # Memory limits per device Dataset Configuration \u00b6 @dataclass class DatasetConfig : name : str # Dataset name (required) split : str # Dataset split percent : Optional [ float ] # Percentage of dataset to use max_samples : Optional [ int ] # Maximum number of samples column_mapping : Dict [ str , str ] # Column mapping task_type : str # Task type weight : float # Dataset weight chat_template : Optional [ str ] # Chat template Training Configuration \u00b6 @dataclass class TrainingConfig : per_device_batch_size : int # Batch size per device gradient_accumulation_steps : int # Gradient accumulation steps max_steps : Optional [ int ] # Maximum training steps epochs : Optional [ int ] # Number of epochs eval_interval : int # Evaluation interval save_interval : int # Save interval rollout_batch_size : int # Rollout batch size kl_coef : float # KL coefficient cliprange : float # PPO clip range num_ppo_epochs : Optional [ int ] # PPO epochs learning_rate : float # Learning rate Dataset Specifications \u00b6 Format \u00b6 name[:split][#percent=N|max=N][?map.key=value] Examples \u00b6 # Basic dataset Anthropic/hh-rlhf:train#percent = 25 # With column mapping HuggingFaceH4/ultrafeedback_binarized:train?map.prompt = prompt & map.chosen = chosen & map.rejected = rejected # With max samples gsm8k:train#max = 1000 Reward Specifications \u00b6 Format \u00b6 type:weight:param1=value1:param2=value2 Examples \u00b6 # Length reward length:weight = 1 .0:min_length = 10 :max_length = 200 # Math reward math:weight = 0 .8:tolerance = 1e-6 # Safety reward with shield safety:weight = 0 .5:strict = true Built-in Components \u00b6 Dataset Loaders \u00b6 huggingface : Load from HuggingFace Hub local : Load from local files Schema Adapters \u00b6 chat : Chat format (messages) hh-rlhf : Anthropic HH-RLHF format ultrafeedback : UltraFeedback format alpaca : Alpaca format Reward Functions \u00b6 length : Text length reward sentiment : Sentiment analysis reward coherence : Text coherence reward math : Mathematical correctness reward code : Code quality reward safety : Safety check reward Tasks \u00b6 conversation : Conversational AI classification : Text classification summarization : Text summarization math : Mathematical reasoning code : Code generation Distributed Training \u00b6 Backends \u00b6 single : Single GPU/CPU training ddp : DistributedDataParallel via accelerate fsdp : FullyShardedDataParallel deepspeed : DeepSpeed ZeRO-2/3 Example Configuration \u00b6 distributed : backend : ddp nodes : 1 gpus_per_node : 2 seed : 42 Logging \u00b6 Supported Loggers \u00b6 tensorboard : TensorBoard logging wandb : Weights & Biases logging Example Configuration \u00b6 logging : loggers : [ \"tensorboard\" , \"wandb\" ] output_dir : \"./output\" run_name : \"my_training_run\" Caching \u00b6 Configuration \u00b6 caching : root : \"./cache\" enabled : true Features \u00b6 Hash-based cache keys Automatic cache validation Cache cleanup utilities Support for multiple datasets Examples \u00b6 Minimal PPO Training \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-small\" precision : bf16 datasets : - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 1 train : per_device_batch_size : 1 max_steps : 100 eval_interval : 50 distributed : backend : single logging : output_dir : \"./output\" Multi-task Training \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" precision : bf16 datasets : - name : \"gsm8k\" split : \"train\" max_samples : 5000 task_type : \"math\" weight : 0.4 - name : \"sahil2801/CodeAlpaca-20k\" split : \"train\" percent : 20 task_type : \"code\" weight : 0.3 - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 10 task_type : \"conversation\" weight : 0.3 tasks : - name : \"math\" weight : 0.4 - name : \"code\" weight : 0.3 - name : \"conversation\" weight : 0.3 rewards : - type : \"math\" weight : 0.4 - type : \"code\" weight : 0.3 - type : \"length\" weight : 0.2 - type : \"safety\" weight : 0.1 shield : true Migration Guide \u00b6 From Old API \u00b6 Replace interactive prompts : Use YAML configuration files Update imports : Use unified system imports Update configuration : Use UnifiedConfig instead of algorithm-specific configs Update training : Use TrainerFactory.create_trainer() Example Migration \u00b6 Old (Interactive) : from aligntune import PPOTrainer trainer = PPOTrainer () trainer . setup_interactive () # Interactive prompts trainer . train () New (Unified) : from aligntune import UnifiedConfig , TrainerFactory config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"model_name\" ), datasets = [ DatasetConfig ( name = \"dataset_name\" )], train = TrainingConfig ( max_steps = 1000 ) ) trainer = TrainerFactory . create_trainer ( config ) trainer . train () Troubleshooting \u00b6 Common Issues \u00b6 Missing required fields : All critical parameters must be explicitly provided Invalid dataset specs : Check dataset specification format Invalid reward specs : Check reward specification format Import errors : Ensure all dependencies are installed Debug Mode \u00b6 Enable debug logging: import logging logging . basicConfig ( level = logging . DEBUG ) Validation \u00b6 Validate configuration before training: from aligntune import ConfigLoader ConfigLoader . validate_config ( config ) API Reference \u00b6 Core Classes \u00b6 UnifiedConfig : Main configuration class ConfigLoader : Configuration loading and validation TrainerFactory : Trainer creation factory TrainerBase : Abstract base trainer class Registries \u00b6 DatasetRegistry : Dataset loader registry RewardRegistry : Reward function registry TaskRegistry : Task definition registry Utilities \u00b6 UnifiedLogger : Logging system UnifiedEvaluator : Evaluation system DatasetCache : Dataset caching system RolloutEngine : Rollout generation engine Model Wrappers \u00b6 PolicyModel : Policy model wrapper ReferenceModel : Reference model wrapper ValueModel : Value model wrapper","title":"Unified RLHF API Documentation"},{"location":"api-reference/unified-api/#unified-rlhf-api-documentation","text":"This document describes the unified RLHF training system that provides a consistent interface for training with PPO, DPO, GRPO, GSPO, DAPO, and Dr. GRPO algorithms.","title":"Unified RLHF API Documentation"},{"location":"api-reference/unified-api/#overview","text":"The unified system provides: - Consistent API : Same interface for all RLHF algorithms - No Interactive Prompts : Everything is configuration-driven - No Placeholder Defaults : All critical parameters must be explicitly provided - Distributed Training : Support for DDP, FSDP, and DeepSpeed - Comprehensive Logging : TensorBoard and WandB integration - Dataset Caching : Efficient caching with hash-based keys - Extensible : Easy to add new algorithms, datasets, and rewards","title":"Overview"},{"location":"api-reference/unified-api/#quick-start","text":"","title":"Quick Start"},{"location":"api-reference/unified-api/#python-library-usage","text":"from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , ConfigLoader , TrainerFactory ) # Create configuration config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-small\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , percent = 1 )], train = TrainingConfig ( max_steps = 100 ) ) # Create and run trainer trainer = TrainerFactory . create_trainer ( config ) trainer . train ()","title":"Python Library Usage"},{"location":"api-reference/unified-api/#cli-usage","text":"# Train with YAML config python -m aligntune.cli.train_unified --config examples/configs/ppo_minimal.yaml # Train with CLI arguments python -m aligntune.cli.train_unified \\ --algo ppo \\ --model microsoft/DialoGPT-small \\ --dataset Anthropic/hh-rlhf:train#percent = 1 \\ --max-steps 100","title":"CLI Usage"},{"location":"api-reference/unified-api/#configuration","text":"","title":"Configuration"},{"location":"api-reference/unified-api/#unifiedconfig","text":"The main configuration class that contains all training parameters: @dataclass class UnifiedConfig : algo : AlgorithmType # Algorithm to use model : ModelConfig # Model configuration datasets : List [ DatasetConfig ] # Dataset configurations tasks : List [ Dict [ str , Any ]] # Task definitions rewards : List [ RewardConfig ] # Reward functions train : TrainingConfig # Training parameters distributed : DistributedConfig # Distributed training logging : LoggingConfig # Logging configuration chat_template : Optional [ str ] # Chat template caching : Dict [ str , Any ] # Caching configuration","title":"UnifiedConfig"},{"location":"api-reference/unified-api/#algorithm-types","text":"class AlgorithmType ( Enum ): PPO = \"ppo\" # Proximal Policy Optimization DPO = \"dpo\" # Direct Preference Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective DAPO = \"dapo\" # Decouple Clip and Dynamic sAmpling Policy Optimization DRGRPO = \"drgrpo\" # Dr. GRPO (GRPO Done Right)","title":"Algorithm Types"},{"location":"api-reference/unified-api/#model-configuration","text":"@dataclass class ModelConfig : name_or_path : str # Model name or path (required) sft_path : Optional [ str ] # SFT checkpoint path reward_path : Optional [ str ] # Reward model path precision : PrecisionType # Model precision quantization : Dict [ str , Any ] # Quantization settings attn_implementation : str # Attention implementation gradient_checkpointing : bool # Enable gradient checkpointing max_memory : Optional [ Dict [ str , str ]] # Memory limits per device","title":"Model Configuration"},{"location":"api-reference/unified-api/#dataset-configuration","text":"@dataclass class DatasetConfig : name : str # Dataset name (required) split : str # Dataset split percent : Optional [ float ] # Percentage of dataset to use max_samples : Optional [ int ] # Maximum number of samples column_mapping : Dict [ str , str ] # Column mapping task_type : str # Task type weight : float # Dataset weight chat_template : Optional [ str ] # Chat template","title":"Dataset Configuration"},{"location":"api-reference/unified-api/#training-configuration","text":"@dataclass class TrainingConfig : per_device_batch_size : int # Batch size per device gradient_accumulation_steps : int # Gradient accumulation steps max_steps : Optional [ int ] # Maximum training steps epochs : Optional [ int ] # Number of epochs eval_interval : int # Evaluation interval save_interval : int # Save interval rollout_batch_size : int # Rollout batch size kl_coef : float # KL coefficient cliprange : float # PPO clip range num_ppo_epochs : Optional [ int ] # PPO epochs learning_rate : float # Learning rate","title":"Training Configuration"},{"location":"api-reference/unified-api/#dataset-specifications","text":"","title":"Dataset Specifications"},{"location":"api-reference/unified-api/#format","text":"name[:split][#percent=N|max=N][?map.key=value]","title":"Format"},{"location":"api-reference/unified-api/#examples","text":"# Basic dataset Anthropic/hh-rlhf:train#percent = 25 # With column mapping HuggingFaceH4/ultrafeedback_binarized:train?map.prompt = prompt & map.chosen = chosen & map.rejected = rejected # With max samples gsm8k:train#max = 1000","title":"Examples"},{"location":"api-reference/unified-api/#reward-specifications","text":"","title":"Reward Specifications"},{"location":"api-reference/unified-api/#format_1","text":"type:weight:param1=value1:param2=value2","title":"Format"},{"location":"api-reference/unified-api/#examples_1","text":"# Length reward length:weight = 1 .0:min_length = 10 :max_length = 200 # Math reward math:weight = 0 .8:tolerance = 1e-6 # Safety reward with shield safety:weight = 0 .5:strict = true","title":"Examples"},{"location":"api-reference/unified-api/#built-in-components","text":"","title":"Built-in Components"},{"location":"api-reference/unified-api/#dataset-loaders","text":"huggingface : Load from HuggingFace Hub local : Load from local files","title":"Dataset Loaders"},{"location":"api-reference/unified-api/#schema-adapters","text":"chat : Chat format (messages) hh-rlhf : Anthropic HH-RLHF format ultrafeedback : UltraFeedback format alpaca : Alpaca format","title":"Schema Adapters"},{"location":"api-reference/unified-api/#reward-functions","text":"length : Text length reward sentiment : Sentiment analysis reward coherence : Text coherence reward math : Mathematical correctness reward code : Code quality reward safety : Safety check reward","title":"Reward Functions"},{"location":"api-reference/unified-api/#tasks","text":"conversation : Conversational AI classification : Text classification summarization : Text summarization math : Mathematical reasoning code : Code generation","title":"Tasks"},{"location":"api-reference/unified-api/#distributed-training","text":"","title":"Distributed Training"},{"location":"api-reference/unified-api/#backends","text":"single : Single GPU/CPU training ddp : DistributedDataParallel via accelerate fsdp : FullyShardedDataParallel deepspeed : DeepSpeed ZeRO-2/3","title":"Backends"},{"location":"api-reference/unified-api/#example-configuration","text":"distributed : backend : ddp nodes : 1 gpus_per_node : 2 seed : 42","title":"Example Configuration"},{"location":"api-reference/unified-api/#logging","text":"","title":"Logging"},{"location":"api-reference/unified-api/#supported-loggers","text":"tensorboard : TensorBoard logging wandb : Weights & Biases logging","title":"Supported Loggers"},{"location":"api-reference/unified-api/#example-configuration_1","text":"logging : loggers : [ \"tensorboard\" , \"wandb\" ] output_dir : \"./output\" run_name : \"my_training_run\"","title":"Example Configuration"},{"location":"api-reference/unified-api/#caching","text":"","title":"Caching"},{"location":"api-reference/unified-api/#configuration_1","text":"caching : root : \"./cache\" enabled : true","title":"Configuration"},{"location":"api-reference/unified-api/#features","text":"Hash-based cache keys Automatic cache validation Cache cleanup utilities Support for multiple datasets","title":"Features"},{"location":"api-reference/unified-api/#examples_2","text":"","title":"Examples"},{"location":"api-reference/unified-api/#minimal-ppo-training","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-small\" precision : bf16 datasets : - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 1 train : per_device_batch_size : 1 max_steps : 100 eval_interval : 50 distributed : backend : single logging : output_dir : \"./output\"","title":"Minimal PPO Training"},{"location":"api-reference/unified-api/#multi-task-training","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" precision : bf16 datasets : - name : \"gsm8k\" split : \"train\" max_samples : 5000 task_type : \"math\" weight : 0.4 - name : \"sahil2801/CodeAlpaca-20k\" split : \"train\" percent : 20 task_type : \"code\" weight : 0.3 - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 10 task_type : \"conversation\" weight : 0.3 tasks : - name : \"math\" weight : 0.4 - name : \"code\" weight : 0.3 - name : \"conversation\" weight : 0.3 rewards : - type : \"math\" weight : 0.4 - type : \"code\" weight : 0.3 - type : \"length\" weight : 0.2 - type : \"safety\" weight : 0.1 shield : true","title":"Multi-task Training"},{"location":"api-reference/unified-api/#migration-guide","text":"","title":"Migration Guide"},{"location":"api-reference/unified-api/#from-old-api","text":"Replace interactive prompts : Use YAML configuration files Update imports : Use unified system imports Update configuration : Use UnifiedConfig instead of algorithm-specific configs Update training : Use TrainerFactory.create_trainer()","title":"From Old API"},{"location":"api-reference/unified-api/#example-migration","text":"Old (Interactive) : from aligntune import PPOTrainer trainer = PPOTrainer () trainer . setup_interactive () # Interactive prompts trainer . train () New (Unified) : from aligntune import UnifiedConfig , TrainerFactory config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"model_name\" ), datasets = [ DatasetConfig ( name = \"dataset_name\" )], train = TrainingConfig ( max_steps = 1000 ) ) trainer = TrainerFactory . create_trainer ( config ) trainer . train ()","title":"Example Migration"},{"location":"api-reference/unified-api/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api-reference/unified-api/#common-issues","text":"Missing required fields : All critical parameters must be explicitly provided Invalid dataset specs : Check dataset specification format Invalid reward specs : Check reward specification format Import errors : Ensure all dependencies are installed","title":"Common Issues"},{"location":"api-reference/unified-api/#debug-mode","text":"Enable debug logging: import logging logging . basicConfig ( level = logging . DEBUG )","title":"Debug Mode"},{"location":"api-reference/unified-api/#validation","text":"Validate configuration before training: from aligntune import ConfigLoader ConfigLoader . validate_config ( config )","title":"Validation"},{"location":"api-reference/unified-api/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/unified-api/#core-classes","text":"UnifiedConfig : Main configuration class ConfigLoader : Configuration loading and validation TrainerFactory : Trainer creation factory TrainerBase : Abstract base trainer class","title":"Core Classes"},{"location":"api-reference/unified-api/#registries","text":"DatasetRegistry : Dataset loader registry RewardRegistry : Reward function registry TaskRegistry : Task definition registry","title":"Registries"},{"location":"api-reference/unified-api/#utilities","text":"UnifiedLogger : Logging system UnifiedEvaluator : Evaluation system DatasetCache : Dataset caching system RolloutEngine : Rollout generation engine","title":"Utilities"},{"location":"api-reference/unified-api/#model-wrappers","text":"PolicyModel : Policy model wrapper ReferenceModel : Reference model wrapper ValueModel : Value model wrapper","title":"Model Wrappers"},{"location":"backends/comparison/","text":"Backend Comparison \u00b6 Detailed side-by-side comparison of TRL and Unsloth backends. Feature Comparison \u00b6 Feature TRL Backend Unsloth Backend Winner Training Speed Baseline faster Unsloth Memory Usage Baseline Less memory usage Unsloth Reliability Battle-tested Production-ready Tie Algorithm Support All algorithms Most algorithms TRL CPU Support Yes No TRL Setup Complexity Low Medium TRL Model Compatibility All models Optimized models TRL Error Handling Comprehensive Comprehensive Tie Algorithm Support \u00b6 Complete Support Matrix \u00b6 Algorithm TRL Unsloth Notes SFT Yes Yes Both fully supported DPO Yes Yes Both fully supported PPO Yes Yes Both fully supported GRPO Yes Yes Both fully supported GSPO Yes No TRL only DAPO Yes Yes Both fully supported Dr. GRPO Yes Yes Both fully supported Use Case Recommendations \u00b6 Use TRL Backend When: \u00b6 Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow Use Unsloth Backend When: \u00b6 Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup Code Examples \u00b6 TRL Backend Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () Unsloth Backend Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () Migration Scenarios \u00b6 Scenario 1: Speed Optimization \u00b6 Current : TRL backend, slow training Goal : Faster training Solution : Switch to Unsloth backend backend = \"unsloth\" # faster Scenario 2: Memory Issues \u00b6 Current : TRL backend, OOM errors Goal : Reduce memory usage Solution : Switch to Unsloth with QLoRA backend = \"unsloth\" model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" # 4-bit quantized Scenario 3: Algorithm Support \u00b6 Current : Using GSPO with Unsloth (not supported) Goal : Use GSPO Solution : Switch to TRL backend backend = \"trl\" # GSPO only supported on TRL Decision Tree \u00b6 flowchart TD Start[Start] --> GSPO{Need GSPO?} GSPO -->|Yes| TRL1[Use TRL Backend] GSPO -->|No| GPU{Have CUDA GPU?} GPU -->|No| TRL2[Use TRL Backend] GPU -->|Yes| Speed{Need Maximum Speed?} Speed -->|Yes| Unsloth1[Use Unsloth Backend] Speed -->|No| Memory{Memory Constrained?} Memory -->|Yes| Unsloth2[Use Unsloth Backend] Memory -->|No| TRL3[Use TRL Backend] Summary \u00b6 Choose TRL Backend if: - You need maximum reliability - You're using GSPO - You're on CPU or have CUDA issues - You want the simplest setup Choose Unsloth Backend if: - You have a CUDA GPU - You need maximum speed - You're memory constrained - You're using supported algorithms Use Auto Selection if: - You want AlignTune to choose - You're unsure which to use - You want automatic fallback Next Steps \u00b6 TRL Backend - Detailed TRL guide Unsloth Backend - Detailed Unsloth guide Backend Selection - Selection guide","title":"Comparison"},{"location":"backends/comparison/#backend-comparison","text":"Detailed side-by-side comparison of TRL and Unsloth backends.","title":"Backend Comparison"},{"location":"backends/comparison/#feature-comparison","text":"Feature TRL Backend Unsloth Backend Winner Training Speed Baseline faster Unsloth Memory Usage Baseline Less memory usage Unsloth Reliability Battle-tested Production-ready Tie Algorithm Support All algorithms Most algorithms TRL CPU Support Yes No TRL Setup Complexity Low Medium TRL Model Compatibility All models Optimized models TRL Error Handling Comprehensive Comprehensive Tie","title":"Feature Comparison"},{"location":"backends/comparison/#algorithm-support","text":"","title":"Algorithm Support"},{"location":"backends/comparison/#complete-support-matrix","text":"Algorithm TRL Unsloth Notes SFT Yes Yes Both fully supported DPO Yes Yes Both fully supported PPO Yes Yes Both fully supported GRPO Yes Yes Both fully supported GSPO Yes No TRL only DAPO Yes Yes Both fully supported Dr. GRPO Yes Yes Both fully supported","title":"Complete Support Matrix"},{"location":"backends/comparison/#use-case-recommendations","text":"","title":"Use Case Recommendations"},{"location":"backends/comparison/#use-trl-backend-when","text":"Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow","title":"Use TRL Backend When:"},{"location":"backends/comparison/#use-unsloth-backend-when","text":"Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup","title":"Use Unsloth Backend When:"},{"location":"backends/comparison/#code-examples","text":"","title":"Code Examples"},{"location":"backends/comparison/#trl-backend-example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"TRL Backend Example"},{"location":"backends/comparison/#unsloth-backend-example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"Unsloth Backend Example"},{"location":"backends/comparison/#migration-scenarios","text":"","title":"Migration Scenarios"},{"location":"backends/comparison/#scenario-1-speed-optimization","text":"Current : TRL backend, slow training Goal : Faster training Solution : Switch to Unsloth backend backend = \"unsloth\" # faster","title":"Scenario 1: Speed Optimization"},{"location":"backends/comparison/#scenario-2-memory-issues","text":"Current : TRL backend, OOM errors Goal : Reduce memory usage Solution : Switch to Unsloth with QLoRA backend = \"unsloth\" model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" # 4-bit quantized","title":"Scenario 2: Memory Issues"},{"location":"backends/comparison/#scenario-3-algorithm-support","text":"Current : Using GSPO with Unsloth (not supported) Goal : Use GSPO Solution : Switch to TRL backend backend = \"trl\" # GSPO only supported on TRL","title":"Scenario 3: Algorithm Support"},{"location":"backends/comparison/#decision-tree","text":"flowchart TD Start[Start] --> GSPO{Need GSPO?} GSPO -->|Yes| TRL1[Use TRL Backend] GSPO -->|No| GPU{Have CUDA GPU?} GPU -->|No| TRL2[Use TRL Backend] GPU -->|Yes| Speed{Need Maximum Speed?} Speed -->|Yes| Unsloth1[Use Unsloth Backend] Speed -->|No| Memory{Memory Constrained?} Memory -->|Yes| Unsloth2[Use Unsloth Backend] Memory -->|No| TRL3[Use TRL Backend]","title":"Decision Tree"},{"location":"backends/comparison/#summary","text":"Choose TRL Backend if: - You need maximum reliability - You're using GSPO - You're on CPU or have CUDA issues - You want the simplest setup Choose Unsloth Backend if: - You have a CUDA GPU - You need maximum speed - You're memory constrained - You're using supported algorithms Use Auto Selection if: - You want AlignTune to choose - You're unsure which to use - You want automatic fallback","title":"Summary"},{"location":"backends/comparison/#next-steps","text":"TRL Backend - Detailed TRL guide Unsloth Backend - Detailed Unsloth guide Backend Selection - Selection guide","title":"Next Steps"},{"location":"backends/overview/","text":"Backends Overview \u00b6 AlignTune provides two backend implementations for training: TRL (reliable, battle-tested) and Unsloth (faster, memory efficient). Backend Comparison \u00b6 Feature TRL Backend Unsloth Backend Speed Standard Faster Faster Memory Standard Optimized Reliability Battle-tested Production-ready GPU Required Works on CPU CUDA required Algorithm Support All algorithms Most algorithms Model Compatibility All HuggingFace models Optimized models Setup Complexity Low Medium TRL Backend \u00b6 Overview \u00b6 The TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation based on HuggingFace's TRL library. Key Features \u00b6 Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues When to Use TRL \u00b6 You need maximum reliability You're using algorithms not supported by Unsloth (GSPO) You're on CPU or have CUDA compatibility issues You want the most stable training experience Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Explicit TRL backend num_epochs = 3 ) Unsloth Backend \u00b6 Overview \u00b6 The Unsloth backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. Key Features \u00b6 Fast : faster training Memory Efficient : Memory reduction GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported When to Use Unsloth \u00b6 You have a CUDA-capable GPU You want maximum training speed You're using supported algorithms (DPO, PPO, GRPO, etc.) You need memory-efficient training Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth backend num_epochs = 3 ) Algorithm Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Backend Selection \u00b6 Automatic Selection \u00b6 AlignTune can automatically select the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatic selection ) Selection Logic : 1. Checks if Unsloth is available and compatible 2. Falls back to TRL if Unsloth unavailable 3. Considers algorithm support Explicit Selection \u00b6 You can explicitly choose a backend: # TRL backend trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Unsloth backend trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Performance Comparison \u00b6 Training Speed \u00b6 Model Size TRL Backend Unsloth Backend Speedup Small (100M-1B) Baseline faster 2-3x Medium (1B-7B) Baseline faster 3-4x Large (7B+) Baseline faster Faster Memory Usage \u00b6 Configuration TRL Backend Unsloth Backend Reduction Full Fine-tuning Baseline 60-70% less 60-70% LoRA Baseline 70-80% less 70-80% QLoRA (4-bit) Baseline 80-90% less 80-90% Best Practices \u00b6 1. Use String-Based Selection \u00b6 Always use string-based backend selection to prevent interference: # CORRECT backend = \"trl\" # or \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference 2. Check Backend Availability \u00b6 # Run diagnostics aligntune diagnose # List available backends aligntune list-backends-cmd 3. Handle Backend Errors \u00b6 try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ImportError as e : # Fallback to TRL print ( f \"Unsloth not available: { e } \" ) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) Migration Guide \u00b6 From TRL to Unsloth \u00b6 Check Compatibility : aligntune diagnose Update Model Name : # TRL model_name = \"microsoft/DialoGPT-small\" # Unsloth (if available) model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" Update Backend : backend = \"unsloth\" Test Training : # Start with small dataset max_samples = 100 Troubleshooting \u00b6 Unsloth Not Available \u00b6 See Troubleshooting Guide for solutions. Backend Interference \u00b6 See Troubleshooting Guide for solutions. Next Steps \u00b6 TRL Backend - Detailed TRL backend guide Unsloth Backend - Detailed Unsloth backend guide Backend Comparison - Side-by-side comparison Backend Selection - Choose the right backend","title":"Overview"},{"location":"backends/overview/#backends-overview","text":"AlignTune provides two backend implementations for training: TRL (reliable, battle-tested) and Unsloth (faster, memory efficient).","title":"Backends Overview"},{"location":"backends/overview/#backend-comparison","text":"Feature TRL Backend Unsloth Backend Speed Standard Faster Faster Memory Standard Optimized Reliability Battle-tested Production-ready GPU Required Works on CPU CUDA required Algorithm Support All algorithms Most algorithms Model Compatibility All HuggingFace models Optimized models Setup Complexity Low Medium","title":"Backend Comparison"},{"location":"backends/overview/#trl-backend","text":"","title":"TRL Backend"},{"location":"backends/overview/#overview","text":"The TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation based on HuggingFace's TRL library.","title":"Overview"},{"location":"backends/overview/#key-features","text":"Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues","title":"Key Features"},{"location":"backends/overview/#when-to-use-trl","text":"You need maximum reliability You're using algorithms not supported by Unsloth (GSPO) You're on CPU or have CUDA compatibility issues You want the most stable training experience","title":"When to Use TRL"},{"location":"backends/overview/#example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Explicit TRL backend num_epochs = 3 )","title":"Example"},{"location":"backends/overview/#unsloth-backend","text":"","title":"Unsloth Backend"},{"location":"backends/overview/#overview_1","text":"The Unsloth backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations.","title":"Overview"},{"location":"backends/overview/#key-features_1","text":"Fast : faster training Memory Efficient : Memory reduction GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported","title":"Key Features"},{"location":"backends/overview/#when-to-use-unsloth","text":"You have a CUDA-capable GPU You want maximum training speed You're using supported algorithms (DPO, PPO, GRPO, etc.) You need memory-efficient training","title":"When to Use Unsloth"},{"location":"backends/overview/#example_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth backend num_epochs = 3 )","title":"Example"},{"location":"backends/overview/#algorithm-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Algorithm Support Matrix"},{"location":"backends/overview/#backend-selection","text":"","title":"Backend Selection"},{"location":"backends/overview/#automatic-selection","text":"AlignTune can automatically select the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatic selection ) Selection Logic : 1. Checks if Unsloth is available and compatible 2. Falls back to TRL if Unsloth unavailable 3. Considers algorithm support","title":"Automatic Selection"},{"location":"backends/overview/#explicit-selection","text":"You can explicitly choose a backend: # TRL backend trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Unsloth backend trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" )","title":"Explicit Selection"},{"location":"backends/overview/#performance-comparison","text":"","title":"Performance Comparison"},{"location":"backends/overview/#training-speed","text":"Model Size TRL Backend Unsloth Backend Speedup Small (100M-1B) Baseline faster 2-3x Medium (1B-7B) Baseline faster 3-4x Large (7B+) Baseline faster Faster","title":"Training Speed"},{"location":"backends/overview/#memory-usage","text":"Configuration TRL Backend Unsloth Backend Reduction Full Fine-tuning Baseline 60-70% less 60-70% LoRA Baseline 70-80% less 70-80% QLoRA (4-bit) Baseline 80-90% less 80-90%","title":"Memory Usage"},{"location":"backends/overview/#best-practices","text":"","title":"Best Practices"},{"location":"backends/overview/#1-use-string-based-selection","text":"Always use string-based backend selection to prevent interference: # CORRECT backend = \"trl\" # or \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference","title":"1. Use String-Based Selection"},{"location":"backends/overview/#2-check-backend-availability","text":"# Run diagnostics aligntune diagnose # List available backends aligntune list-backends-cmd","title":"2. Check Backend Availability"},{"location":"backends/overview/#3-handle-backend-errors","text":"try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ImportError as e : # Fallback to TRL print ( f \"Unsloth not available: { e } \" ) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" )","title":"3. Handle Backend Errors"},{"location":"backends/overview/#migration-guide","text":"","title":"Migration Guide"},{"location":"backends/overview/#from-trl-to-unsloth","text":"Check Compatibility : aligntune diagnose Update Model Name : # TRL model_name = \"microsoft/DialoGPT-small\" # Unsloth (if available) model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" Update Backend : backend = \"unsloth\" Test Training : # Start with small dataset max_samples = 100","title":"From TRL to Unsloth"},{"location":"backends/overview/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/overview/#unsloth-not-available","text":"See Troubleshooting Guide for solutions.","title":"Unsloth Not Available"},{"location":"backends/overview/#backend-interference","text":"See Troubleshooting Guide for solutions.","title":"Backend Interference"},{"location":"backends/overview/#next-steps","text":"TRL Backend - Detailed TRL backend guide Unsloth Backend - Detailed Unsloth backend guide Backend Comparison - Side-by-side comparison Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"backends/trl/","text":"TRL Backend \u00b6 Complete guide to using the TRL (Transformers Reinforcement Learning) backend in AlignTune. Overview \u00b6 The TRL Backend is the standard, battle-tested implementation based on HuggingFace's TRL library. It provides reliable, production-ready training for all supported algorithms. Key Features \u00b6 Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues When to Use TRL Backend \u00b6 Use TRL backend when: Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO (Unsloth doesn't support) Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow Usage \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () Algorithm Support \u00b6 TRL backend supports all algorithms: Algorithm Support Notes SFT Full support DPO Full support PPO Full support GRPO Full support GSPO Only TRL supports GSPO DAPO Full support Dr. GRPO Full support Configuration \u00b6 Basic Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 ) Advanced Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , gradient_checkpointing = True , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 , warmup_steps = 100 , weight_decay = 0.01 ) Performance \u00b6 Training Speed \u00b6 TRL backend provides standard training speed: - Baseline performance - No special optimizations - Reliable and consistent Memory Usage \u00b6 TRL backend uses standard memory: - Full fine-tuning: Baseline memory usage - LoRA: Reduced memory with PEFT - Gradient checkpointing: Further memory reduction Best Practices \u00b6 1. Use String-Based Selection \u00b6 # CORRECT backend = \"trl\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference 2. Enable Gradient Checkpointing \u00b6 gradient_checkpointing = True # Reduces memory usage 3. Use LoRA for Large Models \u00b6 use_peft = True , lora_r = 8 , lora_alpha = 16 4. Monitor Training \u00b6 logging_steps = 10 , # Log every 10 steps save_steps = 500 , # Save checkpoint every 500 steps Troubleshooting \u00b6 Common Issues \u00b6 Slow Training : Normal for TRL backend, consider Unsloth for speed Memory Issues : Enable gradient checkpointing or use LoRA CUDA Errors : TRL is more compatible, check PyTorch version See Troubleshooting Guide for more details. Comparison with Unsloth \u00b6 Feature TRL Unsloth Speed Baseline faster Memory Baseline 60-80% less Algorithm Support All Most CPU Support Yes No Reliability Battle-tested Production-ready See Backend Comparison for detailed comparison. Next Steps \u00b6 Backend Comparison - Compare with Unsloth Unsloth Backend - Learn about Unsloth backend Backend Selection - Choose the right backend","title":"TRL Backend"},{"location":"backends/trl/#trl-backend","text":"Complete guide to using the TRL (Transformers Reinforcement Learning) backend in AlignTune.","title":"TRL Backend"},{"location":"backends/trl/#overview","text":"The TRL Backend is the standard, battle-tested implementation based on HuggingFace's TRL library. It provides reliable, production-ready training for all supported algorithms.","title":"Overview"},{"location":"backends/trl/#key-features","text":"Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues","title":"Key Features"},{"location":"backends/trl/#when-to-use-trl-backend","text":"Use TRL backend when: Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO (Unsloth doesn't support) Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow","title":"When to Use TRL Backend"},{"location":"backends/trl/#usage","text":"","title":"Usage"},{"location":"backends/trl/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"SFT Training"},{"location":"backends/trl/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"DPO Training"},{"location":"backends/trl/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO Training"},{"location":"backends/trl/#algorithm-support","text":"TRL backend supports all algorithms: Algorithm Support Notes SFT Full support DPO Full support PPO Full support GRPO Full support GSPO Only TRL supports GSPO DAPO Full support Dr. GRPO Full support","title":"Algorithm Support"},{"location":"backends/trl/#configuration","text":"","title":"Configuration"},{"location":"backends/trl/#basic-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 )","title":"Basic Configuration"},{"location":"backends/trl/#advanced-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , gradient_checkpointing = True , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 , warmup_steps = 100 , weight_decay = 0.01 )","title":"Advanced Configuration"},{"location":"backends/trl/#performance","text":"","title":"Performance"},{"location":"backends/trl/#training-speed","text":"TRL backend provides standard training speed: - Baseline performance - No special optimizations - Reliable and consistent","title":"Training Speed"},{"location":"backends/trl/#memory-usage","text":"TRL backend uses standard memory: - Full fine-tuning: Baseline memory usage - LoRA: Reduced memory with PEFT - Gradient checkpointing: Further memory reduction","title":"Memory Usage"},{"location":"backends/trl/#best-practices","text":"","title":"Best Practices"},{"location":"backends/trl/#1-use-string-based-selection","text":"# CORRECT backend = \"trl\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference","title":"1. Use String-Based Selection"},{"location":"backends/trl/#2-enable-gradient-checkpointing","text":"gradient_checkpointing = True # Reduces memory usage","title":"2. Enable Gradient Checkpointing"},{"location":"backends/trl/#3-use-lora-for-large-models","text":"use_peft = True , lora_r = 8 , lora_alpha = 16","title":"3. Use LoRA for Large Models"},{"location":"backends/trl/#4-monitor-training","text":"logging_steps = 10 , # Log every 10 steps save_steps = 500 , # Save checkpoint every 500 steps","title":"4. Monitor Training"},{"location":"backends/trl/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/trl/#common-issues","text":"Slow Training : Normal for TRL backend, consider Unsloth for speed Memory Issues : Enable gradient checkpointing or use LoRA CUDA Errors : TRL is more compatible, check PyTorch version See Troubleshooting Guide for more details.","title":"Common Issues"},{"location":"backends/trl/#comparison-with-unsloth","text":"Feature TRL Unsloth Speed Baseline faster Memory Baseline 60-80% less Algorithm Support All Most CPU Support Yes No Reliability Battle-tested Production-ready See Backend Comparison for detailed comparison.","title":"Comparison with Unsloth"},{"location":"backends/trl/#next-steps","text":"Backend Comparison - Compare with Unsloth Unsloth Backend - Learn about Unsloth backend Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"backends/unsloth/","text":"Unsloth Backend \u00b6 Complete guide to using the Unsloth backend in AlignTune for faster training. Overview \u00b6 The Unsloth Backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. It's ideal for GPU-based training when speed is critical. Key Features \u00b6 Fast : faster training Memory Efficient : Reduction in memory GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported When to Use Unsloth Backend \u00b6 Use Unsloth backend when: Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup Usage \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () Algorithm Support \u00b6 Unsloth backend supports most algorithms: Algorithm Support Notes SFT Yes Full support DPO Yes Full support PPO Yes Full support GRPO Yes Full support GSPO No Not supported DAPO Yes Full support Dr. GRPO Yes Full support Configuration \u00b6 Basic Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) Advanced Configuration with QLoRA \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 ) Performance \u00b6 Training Speed \u00b6 Unsloth backend provides faster training: - Small models (100M-1B) : faster - Medium models (1B-7B) : faster - Large models (7B+) : faster Memory Usage \u00b6 Unsloth backend uses less memory overall Best Practices \u00b6 1. Use Quantized Models \u00b6 # Use 4-bit quantized models for maximum memory efficiency model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" 2. Enable LoRA/QLoRA \u00b6 use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 3. Optimize Batch Size \u00b6 # Start with small batch size, increase if memory allows batch_size = 1 # For large models batch_size = 4 # For smaller models 4. Use String-Based Selection \u00b6 # CORRECT backend = \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . UNSLOTH # May cause interference Troubleshooting \u00b6 Common Issues \u00b6 Unsloth Not Available : Check CUDA installation and compatibility CUDA Errors : Verify PyTorch and CUDA versions match Memory Issues : Use quantized models or reduce batch size See Troubleshooting Guide and Unsloth Compatibility for more details. Comparison with TRL \u00b6 Feature Unsloth TRL Speed faster Baseline Memory Less memory Baseline Algorithm Support Most All CPU Support No Yes GPU Required Yes No See Backend Comparison for detailed comparison. Next Steps \u00b6 Backend Comparison - Compare with TRL TRL Backend - Learn about TRL backend Unsloth Compatibility - Compatibility guide Backend Selection - Choose the right backend","title":"Unsloth Backend"},{"location":"backends/unsloth/#unsloth-backend","text":"Complete guide to using the Unsloth backend in AlignTune for faster training.","title":"Unsloth Backend"},{"location":"backends/unsloth/#overview","text":"The Unsloth Backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. It's ideal for GPU-based training when speed is critical.","title":"Overview"},{"location":"backends/unsloth/#key-features","text":"Fast : faster training Memory Efficient : Reduction in memory GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported","title":"Key Features"},{"location":"backends/unsloth/#when-to-use-unsloth-backend","text":"Use Unsloth backend when: Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup","title":"When to Use Unsloth Backend"},{"location":"backends/unsloth/#usage","text":"","title":"Usage"},{"location":"backends/unsloth/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"SFT Training"},{"location":"backends/unsloth/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"DPO Training"},{"location":"backends/unsloth/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO Training"},{"location":"backends/unsloth/#algorithm-support","text":"Unsloth backend supports most algorithms: Algorithm Support Notes SFT Yes Full support DPO Yes Full support PPO Yes Full support GRPO Yes Full support GSPO No Not supported DAPO Yes Full support Dr. GRPO Yes Full support","title":"Algorithm Support"},{"location":"backends/unsloth/#configuration","text":"","title":"Configuration"},{"location":"backends/unsloth/#basic-configuration","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 )","title":"Basic Configuration"},{"location":"backends/unsloth/#advanced-configuration-with-qlora","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 )","title":"Advanced Configuration with QLoRA"},{"location":"backends/unsloth/#performance","text":"","title":"Performance"},{"location":"backends/unsloth/#training-speed","text":"Unsloth backend provides faster training: - Small models (100M-1B) : faster - Medium models (1B-7B) : faster - Large models (7B+) : faster","title":"Training Speed"},{"location":"backends/unsloth/#memory-usage","text":"Unsloth backend uses less memory overall","title":"Memory Usage"},{"location":"backends/unsloth/#best-practices","text":"","title":"Best Practices"},{"location":"backends/unsloth/#1-use-quantized-models","text":"# Use 4-bit quantized models for maximum memory efficiency model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"","title":"1. Use Quantized Models"},{"location":"backends/unsloth/#2-enable-loraqlora","text":"use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05","title":"2. Enable LoRA/QLoRA"},{"location":"backends/unsloth/#3-optimize-batch-size","text":"# Start with small batch size, increase if memory allows batch_size = 1 # For large models batch_size = 4 # For smaller models","title":"3. Optimize Batch Size"},{"location":"backends/unsloth/#4-use-string-based-selection","text":"# CORRECT backend = \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . UNSLOTH # May cause interference","title":"4. Use String-Based Selection"},{"location":"backends/unsloth/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/unsloth/#common-issues","text":"Unsloth Not Available : Check CUDA installation and compatibility CUDA Errors : Verify PyTorch and CUDA versions match Memory Issues : Use quantized models or reduce batch size See Troubleshooting Guide and Unsloth Compatibility for more details.","title":"Common Issues"},{"location":"backends/unsloth/#comparison-with-trl","text":"Feature Unsloth TRL Speed faster Baseline Memory Less memory Baseline Algorithm Support Most All CPU Support No Yes GPU Required Yes No See Backend Comparison for detailed comparison.","title":"Comparison with TRL"},{"location":"backends/unsloth/#next-steps","text":"Backend Comparison - Compare with TRL TRL Backend - Learn about TRL backend Unsloth Compatibility - Compatibility guide Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"cli/commands/","text":"CLI Commands \u00b6 Detailed reference for all AlignTune CLI commands and options. Core Commands \u00b6 aligntune info \u00b6 Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status aligntune list-backends-cmd \u00b6 List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status Training Commands \u00b6 aligntune train \u00b6 Main training command with flexible configuration options. aligntune train [ OPTIONS ] aligntune train --help # To get all parameters Required Options \u00b6 Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type --type sft Optional Options \u00b6 Option Default Description --backend auto Backend to use (trl, unsloth, auto) --epochs 1 Number of training epochs --batch-size 4 Batch size per device --learning-rate 5e-5 Learning rate --max-length 512 Maximum sequence length --output-dir ./output Output directory --seed 42 Random seed --config - YAML configuration file Examples \u00b6 # Basic SFT training aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ # DPO training with specific backend aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --backend trl \\ --epochs 1 \\ --batch-size 2 \\ --learning-rate 1e-6 # Training with configuration file aligntune train --config examples/configs/dpo_minimal.yaml Configuration File Support \u00b6 The CLI supports YAML configuration files for complex training setups: # config.yaml algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Error Handling \u00b6 The CLI provides helpful error messages and suggestions: Model not found : Lists similar available models Backend unavailable : Shows installation instructions Configuration errors : Points to specific validation issues Memory issues : Suggests batch size adjustments See Also \u00b6 CLI Overview - Main CLI overview Configuration Files - YAML configuration format Getting Started - Quick start guide","title":"CLI Commands"},{"location":"cli/commands/#cli-commands","text":"Detailed reference for all AlignTune CLI commands and options.","title":"CLI Commands"},{"location":"cli/commands/#core-commands","text":"","title":"Core Commands"},{"location":"cli/commands/#aligntune-info","text":"Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status","title":"aligntune info"},{"location":"cli/commands/#aligntune-list-backends-cmd","text":"List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status","title":"aligntune list-backends-cmd"},{"location":"cli/commands/#training-commands","text":"","title":"Training Commands"},{"location":"cli/commands/#aligntune-train","text":"Main training command with flexible configuration options. aligntune train [ OPTIONS ] aligntune train --help # To get all parameters","title":"aligntune train"},{"location":"cli/commands/#required-options","text":"Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type --type sft","title":"Required Options"},{"location":"cli/commands/#optional-options","text":"Option Default Description --backend auto Backend to use (trl, unsloth, auto) --epochs 1 Number of training epochs --batch-size 4 Batch size per device --learning-rate 5e-5 Learning rate --max-length 512 Maximum sequence length --output-dir ./output Output directory --seed 42 Random seed --config - YAML configuration file","title":"Optional Options"},{"location":"cli/commands/#examples","text":"# Basic SFT training aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ # DPO training with specific backend aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --backend trl \\ --epochs 1 \\ --batch-size 2 \\ --learning-rate 1e-6 # Training with configuration file aligntune train --config examples/configs/dpo_minimal.yaml","title":"Examples"},{"location":"cli/commands/#configuration-file-support","text":"The CLI supports YAML configuration files for complex training setups: # config.yaml algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Configuration File Support"},{"location":"cli/commands/#error-handling","text":"The CLI provides helpful error messages and suggestions: Model not found : Lists similar available models Backend unavailable : Shows installation instructions Configuration errors : Points to specific validation issues Memory issues : Suggests batch size adjustments","title":"Error Handling"},{"location":"cli/commands/#see-also","text":"CLI Overview - Main CLI overview Configuration Files - YAML configuration format Getting Started - Quick start guide","title":"See Also"},{"location":"cli/configuration/","text":"Configuration Files \u00b6 Detailed guide for creating and using YAML configuration files with the AlignTune CLI. Overview \u00b6 Configuration files provide a declarative way to specify complex training setups, making it easy to: Share training configurations Version control experiments Reproduce results Manage complex setups Basic Structure \u00b6 # Basic configuration structure algo : dpo # Algorithm (dpo, ppo, grpo, etc.) model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Configuration Sections \u00b6 Algorithm Configuration \u00b6 algo : dpo # Options: dpo, ppo, grpo, gspo, dapo, dr-grpo Model Configuration \u00b6 model : name_or_path : \"microsoft/DialoGPT-medium\" # Required: Model identifier sft_path : null # Optional: Pre-trained SFT model path reward_path : null # Optional: Reward model path precision : \"bf16\" # Optional: bf16, fp16, fp32 max_seq_length : 2048 # Optional: Maximum sequence length use_unsloth : false # Optional: Enable Unsloth gradient_checkpointing : true # Optional: Enable gradient checkpointing Dataset Configuration \u00b6 datasets : - name : \"Anthropic/hh-rlhf\" # Required: Dataset name split : \"train\" # Optional: Dataset split percent : 100 # Optional: Percentage to use (1-100) max_samples : null # Optional: Maximum samples task_type : \"conversation\" # Optional: Task type weight : 1.0 # Optional: Dataset weight for multi-dataset training Training Configuration \u00b6 train : per_device_batch_size : 4 # Required: Batch size per device gradient_accumulation_steps : 1 # Optional: Gradient accumulation max_steps : 1000 # Optional: Maximum training steps learning_rate : 5e-5 # Optional: Learning rate weight_decay : 0.01 # Optional: Weight decay warmup_steps : 100 # Optional: Warmup steps max_length : 512 # Optional: Maximum sequence length temperature : 0.7 # Optional: Generation temperature # RL-specific parameters beta : 0.1 # Optional: DPO beta parameter kl_coef : 0.1 # Optional: PPO KL coefficient cliprange : 0.2 # Optional: PPO clip range Reward Configuration (RL only) \u00b6 rewards : - reward_type : \"safety\" # Required: Reward function type weight : 1.0 # Optional: Reward weight params : {} # Optional: Reward parameters model_name : null # Optional: Model for reward function device : \"auto\" # Optional: Device for computation Logging Configuration \u00b6 logging : output_dir : \"./output\" # Required: Output directory run_name : null # Optional: Run name loggers : [ \"tensorboard\" ] # Optional: Logging backends level : \"INFO\" # Optional: Log level save_steps : 500 # Optional: Checkpoint save interval eval_steps : 100 # Optional: Evaluation interval report_to : [ \"tensorboard\" ] # Optional: Reporting backends push_to_hub : false # Optional: Push to HuggingFace Hub hub_model_id : null # Optional: Hub model ID Sample Logging Configuration \u00b6 sample_logging : enabled : true # Enable qualitative sample logging num_samples : 2 # Number of samples to generate max_new_tokens : 64 # Maximum tokens per sample interval_steps : null # Log every N steps percent_of_max_steps : 0.5 # Log at percentage of training prompts : null # Custom prompts (uses defaults if null) temperature : 0.7 # Sampling temperature top_p : 0.9 # Nucleus sampling parameter Complete Examples \u00b6 SFT Configuration \u00b6 algo : sft model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : per_device_batch_size : 4 max_steps : 1000 learning_rate : 5e-5 weight_decay : 0.01 warmup_steps : 100 logging : output_dir : \"./output/sft_training\" run_name : \"sft_experiment\" save_steps : 500 eval_steps : 100 DPO Configuration \u00b6 algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : per_device_batch_size : 2 max_steps : 1000 learning_rate : 1e-6 beta : 0.1 weight_decay : 0.01 warmup_steps : 50 rewards : - reward_type : \"safety\" weight : 2.0 - reward_type : \"helpfulness\" weight : 1.5 logging : output_dir : \"./output/dpo_training\" run_name : \"dpo_experiment\" save_steps : 500 eval_steps : 100 PPO Configuration \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 500 train : per_device_batch_size : 1 max_steps : 1000 learning_rate : 1e-6 kl_coef : 0.1 cliprange : 0.2 vf_coef : 0.1 ent_coef : 0.0 rewards : - reward_type : \"length\" weight : 0.5 - reward_type : \"sentiment\" weight : 1.0 - reward_type : \"safety\" weight : 2.0 sample_logging : enabled : true num_samples : 2 max_new_tokens : 64 percent_of_max_steps : 0.5 logging : output_dir : \"./output/ppo_training\" run_name : \"ppo_experiment\" save_steps : 500 eval_steps : 100 Using Configuration Files \u00b6 Command Line Usage \u00b6 # Train with configuration file aligntune train --config config.yaml # Override specific values aligntune train --config config.yaml --max-steps 2000 --learning-rate 1e-5 Best Practices \u00b6 Organization \u00b6 Use descriptive filenames: dpo_experiment_001.yaml Group related configs in directories Version control your configurations Parameter Tuning \u00b6 Start with provided examples Gradually modify parameters Keep records of what worked Reproducibility \u00b6 Set explicit random seeds Record exact configurations used Include environment information See Also \u00b6 CLI Commands - Command line interface CLI Overview - Main CLI overview Examples - Configuration examples","title":"CLI Configuration Files"},{"location":"cli/configuration/#configuration-files","text":"Detailed guide for creating and using YAML configuration files with the AlignTune CLI.","title":"Configuration Files"},{"location":"cli/configuration/#overview","text":"Configuration files provide a declarative way to specify complex training setups, making it easy to: Share training configurations Version control experiments Reproduce results Manage complex setups","title":"Overview"},{"location":"cli/configuration/#basic-structure","text":"# Basic configuration structure algo : dpo # Algorithm (dpo, ppo, grpo, etc.) model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Basic Structure"},{"location":"cli/configuration/#configuration-sections","text":"","title":"Configuration Sections"},{"location":"cli/configuration/#algorithm-configuration","text":"algo : dpo # Options: dpo, ppo, grpo, gspo, dapo, dr-grpo","title":"Algorithm Configuration"},{"location":"cli/configuration/#model-configuration","text":"model : name_or_path : \"microsoft/DialoGPT-medium\" # Required: Model identifier sft_path : null # Optional: Pre-trained SFT model path reward_path : null # Optional: Reward model path precision : \"bf16\" # Optional: bf16, fp16, fp32 max_seq_length : 2048 # Optional: Maximum sequence length use_unsloth : false # Optional: Enable Unsloth gradient_checkpointing : true # Optional: Enable gradient checkpointing","title":"Model Configuration"},{"location":"cli/configuration/#dataset-configuration","text":"datasets : - name : \"Anthropic/hh-rlhf\" # Required: Dataset name split : \"train\" # Optional: Dataset split percent : 100 # Optional: Percentage to use (1-100) max_samples : null # Optional: Maximum samples task_type : \"conversation\" # Optional: Task type weight : 1.0 # Optional: Dataset weight for multi-dataset training","title":"Dataset Configuration"},{"location":"cli/configuration/#training-configuration","text":"train : per_device_batch_size : 4 # Required: Batch size per device gradient_accumulation_steps : 1 # Optional: Gradient accumulation max_steps : 1000 # Optional: Maximum training steps learning_rate : 5e-5 # Optional: Learning rate weight_decay : 0.01 # Optional: Weight decay warmup_steps : 100 # Optional: Warmup steps max_length : 512 # Optional: Maximum sequence length temperature : 0.7 # Optional: Generation temperature # RL-specific parameters beta : 0.1 # Optional: DPO beta parameter kl_coef : 0.1 # Optional: PPO KL coefficient cliprange : 0.2 # Optional: PPO clip range","title":"Training Configuration"},{"location":"cli/configuration/#reward-configuration-rl-only","text":"rewards : - reward_type : \"safety\" # Required: Reward function type weight : 1.0 # Optional: Reward weight params : {} # Optional: Reward parameters model_name : null # Optional: Model for reward function device : \"auto\" # Optional: Device for computation","title":"Reward Configuration (RL only)"},{"location":"cli/configuration/#logging-configuration","text":"logging : output_dir : \"./output\" # Required: Output directory run_name : null # Optional: Run name loggers : [ \"tensorboard\" ] # Optional: Logging backends level : \"INFO\" # Optional: Log level save_steps : 500 # Optional: Checkpoint save interval eval_steps : 100 # Optional: Evaluation interval report_to : [ \"tensorboard\" ] # Optional: Reporting backends push_to_hub : false # Optional: Push to HuggingFace Hub hub_model_id : null # Optional: Hub model ID","title":"Logging Configuration"},{"location":"cli/configuration/#sample-logging-configuration","text":"sample_logging : enabled : true # Enable qualitative sample logging num_samples : 2 # Number of samples to generate max_new_tokens : 64 # Maximum tokens per sample interval_steps : null # Log every N steps percent_of_max_steps : 0.5 # Log at percentage of training prompts : null # Custom prompts (uses defaults if null) temperature : 0.7 # Sampling temperature top_p : 0.9 # Nucleus sampling parameter","title":"Sample Logging Configuration"},{"location":"cli/configuration/#complete-examples","text":"","title":"Complete Examples"},{"location":"cli/configuration/#sft-configuration","text":"algo : sft model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : per_device_batch_size : 4 max_steps : 1000 learning_rate : 5e-5 weight_decay : 0.01 warmup_steps : 100 logging : output_dir : \"./output/sft_training\" run_name : \"sft_experiment\" save_steps : 500 eval_steps : 100","title":"SFT Configuration"},{"location":"cli/configuration/#dpo-configuration","text":"algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : per_device_batch_size : 2 max_steps : 1000 learning_rate : 1e-6 beta : 0.1 weight_decay : 0.01 warmup_steps : 50 rewards : - reward_type : \"safety\" weight : 2.0 - reward_type : \"helpfulness\" weight : 1.5 logging : output_dir : \"./output/dpo_training\" run_name : \"dpo_experiment\" save_steps : 500 eval_steps : 100","title":"DPO Configuration"},{"location":"cli/configuration/#ppo-configuration","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 500 train : per_device_batch_size : 1 max_steps : 1000 learning_rate : 1e-6 kl_coef : 0.1 cliprange : 0.2 vf_coef : 0.1 ent_coef : 0.0 rewards : - reward_type : \"length\" weight : 0.5 - reward_type : \"sentiment\" weight : 1.0 - reward_type : \"safety\" weight : 2.0 sample_logging : enabled : true num_samples : 2 max_new_tokens : 64 percent_of_max_steps : 0.5 logging : output_dir : \"./output/ppo_training\" run_name : \"ppo_experiment\" save_steps : 500 eval_steps : 100","title":"PPO Configuration"},{"location":"cli/configuration/#using-configuration-files","text":"","title":"Using Configuration Files"},{"location":"cli/configuration/#command-line-usage","text":"# Train with configuration file aligntune train --config config.yaml # Override specific values aligntune train --config config.yaml --max-steps 2000 --learning-rate 1e-5","title":"Command Line Usage"},{"location":"cli/configuration/#best-practices","text":"","title":"Best Practices"},{"location":"cli/configuration/#organization","text":"Use descriptive filenames: dpo_experiment_001.yaml Group related configs in directories Version control your configurations","title":"Organization"},{"location":"cli/configuration/#parameter-tuning","text":"Start with provided examples Gradually modify parameters Keep records of what worked","title":"Parameter Tuning"},{"location":"cli/configuration/#reproducibility","text":"Set explicit random seeds Record exact configurations used Include environment information","title":"Reproducibility"},{"location":"cli/configuration/#see-also","text":"CLI Commands - Command line interface CLI Overview - Main CLI overview Examples - Configuration examples","title":"See Also"},{"location":"cli/overview/","text":"CLI Overview \u00b6 AlignTune provides a comprehensive command-line interface for training and managing models without writing Python code. Quick Start \u00b6 # Show system information aligntune info # Train a model aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" Main Commands \u00b6 aligntune info \u00b6 Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status aligntune list-backends-cmd \u00b6 List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status aligntune train \u00b6 Train models with comprehensive options. aligntune train --help # To get all parameters # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" --type sft # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl --split train [ :100 ] # PPO Training aligntune train --model \"Qwen/Qwen3-0.6B\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth --split \"train[:100]\" Common Options: Option Description Example --model , -m Model name or path --model \"microsoft/DialoGPT-small\" --dataset , -d Dataset name --dataset \"tatsu-lab/alpaca\" --type , -t Training type (sft, dpo, ppo, etc.) --type dpo --backend , -b Backend (trl, unsloth, auto) --backend auto --epochs , -e Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate , --lr Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir , -o Output directory --output-dir ./output --4bit Load model in 4-bit quantization --4bit --8bit Load model in 8-bit quantization --8bit --wandb-project Weights & Biases project name --wandb-project my-project Configuration Files \u00b6 The CLI supports YAML configuration files for complex setups. See Configuration Files for detailed format documentation. Example configuration: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Using configuration files: # Train with config file aligntune train --config my_config.yaml Examples \u00b6 Quick SFT Training \u00b6 aligntune train examples/sft_customer_support_unsloth/config_sft_customer_support.yaml Quick RL Training \u00b6 aligntune train examples/grpo_gsm8k_trl/config_grpo_gsm8k.yaml Advanced Usage \u00b6 For detailed command reference, see: - CLI Commands - Complete command reference - CLI Configuration - Configuration file format See Also \u00b6 Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference Examples - Code examples","title":"CLI Overview"},{"location":"cli/overview/#cli-overview","text":"AlignTune provides a comprehensive command-line interface for training and managing models without writing Python code.","title":"CLI Overview"},{"location":"cli/overview/#quick-start","text":"# Show system information aligntune info # Train a model aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\"","title":"Quick Start"},{"location":"cli/overview/#main-commands","text":"","title":"Main Commands"},{"location":"cli/overview/#aligntune-info","text":"Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status","title":"aligntune info"},{"location":"cli/overview/#aligntune-list-backends-cmd","text":"List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status","title":"aligntune list-backends-cmd"},{"location":"cli/overview/#aligntune-train","text":"Train models with comprehensive options. aligntune train --help # To get all parameters # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" --type sft # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl --split train [ :100 ] # PPO Training aligntune train --model \"Qwen/Qwen3-0.6B\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth --split \"train[:100]\" Common Options: Option Description Example --model , -m Model name or path --model \"microsoft/DialoGPT-small\" --dataset , -d Dataset name --dataset \"tatsu-lab/alpaca\" --type , -t Training type (sft, dpo, ppo, etc.) --type dpo --backend , -b Backend (trl, unsloth, auto) --backend auto --epochs , -e Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate , --lr Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir , -o Output directory --output-dir ./output --4bit Load model in 4-bit quantization --4bit --8bit Load model in 8-bit quantization --8bit --wandb-project Weights & Biases project name --wandb-project my-project","title":"aligntune train"},{"location":"cli/overview/#configuration-files","text":"The CLI supports YAML configuration files for complex setups. See Configuration Files for detailed format documentation. Example configuration: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Using configuration files: # Train with config file aligntune train --config my_config.yaml","title":"Configuration Files"},{"location":"cli/overview/#examples","text":"","title":"Examples"},{"location":"cli/overview/#quick-sft-training","text":"aligntune train examples/sft_customer_support_unsloth/config_sft_customer_support.yaml","title":"Quick SFT Training"},{"location":"cli/overview/#quick-rl-training","text":"aligntune train examples/grpo_gsm8k_trl/config_grpo_gsm8k.yaml","title":"Quick RL Training"},{"location":"cli/overview/#advanced-usage","text":"For detailed command reference, see: - CLI Commands - Complete command reference - CLI Configuration - Configuration file format","title":"Advanced Usage"},{"location":"cli/overview/#see-also","text":"Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference Examples - Code examples","title":"See Also"},{"location":"compatibility/backend-matrix/","text":"Backend Support Matrix \u00b6 Complete compatibility matrix for AlignTune backends and algorithms. Algorithm Support \u00b6 Algorithm TRL Backend Unsloth Backend Notes SFT Yes Yes Both backends fully supported DPO Yes Yes Both backends fully supported PPO Yes Yes Both backends fully supported GRPO Yes Yes Both backends fully supported GSPO Yes No TRL only, Unsloth not supported DAPO Yes Yes Both backends fully supported Dr. GRPO Yes Yes Both backends fully supported Backend Comparison \u00b6 TRL Backend \u00b6 Advantages: - Maximum compatibility - Supports all algorithms (including GSPO) - Battle-tested reliability - Works on CPU and GPU - No special requirements Use When: - Need GSPO support - Maximum compatibility required - Working with standard models - CPU-only environments Unsloth Backend \u00b6 Advantages: - faster training - Memory efficient - Optimized kernels - Automatic optimizations Limitations: - GSPO not supported - Requires GPU - CUDA compatibility needed Use When: - Need faster training - Working with large models - Memory constraints - GPU available Model Compatibility \u00b6 TRL Backend \u00b6 Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Yes Full support Encoder-Decoder Yes Full support Classification Yes Full support Unsloth Backend \u00b6 Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Limited Limited support Encoder-Decoder Limited Limited support Classification No Not recommended Task Type Support \u00b6 SFT Tasks \u00b6 Task Type TRL Unsloth Notes Instruction Following Yes Yes Both supported Supervised Fine-Tuning Yes Yes Both supported Text Classification Yes Limited TRL recommended Token Classification Yes Limited TRL recommended Text Generation Yes Yes Both supported Chat Completion Yes Yes Both supported Requirements \u00b6 TRL Backend \u00b6 Python 3.8+ PyTorch 1.13+ Transformers 4.35+ TRL 0.7+ CPU or GPU Unsloth Backend \u00b6 Python 3.8+ PyTorch 2.0+ (CUDA) CUDA 11.8+ or 12.1+ GPU required Unsloth package Performance Comparison \u00b6 Training Speed \u00b6 Model Size TRL Unsloth Speedup Small (<1B) Baseline 2-3x 2-3x Medium (1-7B) Baseline 3-4x 3-4x Large (7B+) Baseline Faster Faster Memory Usage \u00b6 Configuration TRL Unsloth Reduction Full Precision Baseline -20% 20% LoRA Baseline -30% 30% QLoRA (4-bit) Baseline -50% 50% Migration Guide \u00b6 From TRL to Unsloth \u00b6 # TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"trl\" ) # Unsloth (if compatible) trainer = create_sft_trainer ( model_name = \"unsloth/model-bnb-4bit\" , # Use Unsloth model backend = \"unsloth\" ) From Unsloth to TRL \u00b6 # Unsloth trainer = create_sft_trainer ( model_name = \"unsloth/model\" , backend = \"unsloth\" ) # TRL (always works) trainer = create_sft_trainer ( model_name = \"model\" , # Standard model backend = \"trl\" ) Troubleshooting \u00b6 Unsloth Not Available \u00b6 # Automatic fallback to TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"auto\" # Falls back to TRL if Unsloth unavailable ) GSPO with Unsloth \u00b6 # This will fail trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"unsloth\" # Error: GSPO not supported ) # Use TRL instead trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"trl\" # Works ) Next Steps \u00b6 Backend Selection - Choosing backends Unsloth Compatibility - Unsloth setup Performance - Performance optimization","title":"Backend Support Matrix"},{"location":"compatibility/backend-matrix/#backend-support-matrix","text":"Complete compatibility matrix for AlignTune backends and algorithms.","title":"Backend Support Matrix"},{"location":"compatibility/backend-matrix/#algorithm-support","text":"Algorithm TRL Backend Unsloth Backend Notes SFT Yes Yes Both backends fully supported DPO Yes Yes Both backends fully supported PPO Yes Yes Both backends fully supported GRPO Yes Yes Both backends fully supported GSPO Yes No TRL only, Unsloth not supported DAPO Yes Yes Both backends fully supported Dr. GRPO Yes Yes Both backends fully supported","title":"Algorithm Support"},{"location":"compatibility/backend-matrix/#backend-comparison","text":"","title":"Backend Comparison"},{"location":"compatibility/backend-matrix/#trl-backend","text":"Advantages: - Maximum compatibility - Supports all algorithms (including GSPO) - Battle-tested reliability - Works on CPU and GPU - No special requirements Use When: - Need GSPO support - Maximum compatibility required - Working with standard models - CPU-only environments","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend","text":"Advantages: - faster training - Memory efficient - Optimized kernels - Automatic optimizations Limitations: - GSPO not supported - Requires GPU - CUDA compatibility needed Use When: - Need faster training - Working with large models - Memory constraints - GPU available","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#model-compatibility","text":"","title":"Model Compatibility"},{"location":"compatibility/backend-matrix/#trl-backend_1","text":"Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Yes Full support Encoder-Decoder Yes Full support Classification Yes Full support","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend_1","text":"Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Limited Limited support Encoder-Decoder Limited Limited support Classification No Not recommended","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#task-type-support","text":"","title":"Task Type Support"},{"location":"compatibility/backend-matrix/#sft-tasks","text":"Task Type TRL Unsloth Notes Instruction Following Yes Yes Both supported Supervised Fine-Tuning Yes Yes Both supported Text Classification Yes Limited TRL recommended Token Classification Yes Limited TRL recommended Text Generation Yes Yes Both supported Chat Completion Yes Yes Both supported","title":"SFT Tasks"},{"location":"compatibility/backend-matrix/#requirements","text":"","title":"Requirements"},{"location":"compatibility/backend-matrix/#trl-backend_2","text":"Python 3.8+ PyTorch 1.13+ Transformers 4.35+ TRL 0.7+ CPU or GPU","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend_2","text":"Python 3.8+ PyTorch 2.0+ (CUDA) CUDA 11.8+ or 12.1+ GPU required Unsloth package","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#performance-comparison","text":"","title":"Performance Comparison"},{"location":"compatibility/backend-matrix/#training-speed","text":"Model Size TRL Unsloth Speedup Small (<1B) Baseline 2-3x 2-3x Medium (1-7B) Baseline 3-4x 3-4x Large (7B+) Baseline Faster Faster","title":"Training Speed"},{"location":"compatibility/backend-matrix/#memory-usage","text":"Configuration TRL Unsloth Reduction Full Precision Baseline -20% 20% LoRA Baseline -30% 30% QLoRA (4-bit) Baseline -50% 50%","title":"Memory Usage"},{"location":"compatibility/backend-matrix/#migration-guide","text":"","title":"Migration Guide"},{"location":"compatibility/backend-matrix/#from-trl-to-unsloth","text":"# TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"trl\" ) # Unsloth (if compatible) trainer = create_sft_trainer ( model_name = \"unsloth/model-bnb-4bit\" , # Use Unsloth model backend = \"unsloth\" )","title":"From TRL to Unsloth"},{"location":"compatibility/backend-matrix/#from-unsloth-to-trl","text":"# Unsloth trainer = create_sft_trainer ( model_name = \"unsloth/model\" , backend = \"unsloth\" ) # TRL (always works) trainer = create_sft_trainer ( model_name = \"model\" , # Standard model backend = \"trl\" )","title":"From Unsloth to TRL"},{"location":"compatibility/backend-matrix/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"compatibility/backend-matrix/#unsloth-not-available","text":"# Automatic fallback to TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"auto\" # Falls back to TRL if Unsloth unavailable )","title":"Unsloth Not Available"},{"location":"compatibility/backend-matrix/#gspo-with-unsloth","text":"# This will fail trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"unsloth\" # Error: GSPO not supported ) # Use TRL instead trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"trl\" # Works )","title":"GSPO with Unsloth"},{"location":"compatibility/backend-matrix/#next-steps","text":"Backend Selection - Choosing backends Unsloth Compatibility - Unsloth setup Performance - Performance optimization","title":"Next Steps"},{"location":"contributing/code-style/","text":"Code Style Guide \u00b6 Code style guidelines for contributing to AlignTune. Overview \u00b6 AlignTune follows Python best practices and uses automated formatting tools to ensure consistent code style. Code Formatting \u00b6 Black \u00b6 We use Black for code formatting. Configuration: - Line length: 100 characters - Target Python versions: 3.8, 3.9, 3.10, 3.11 Usage: # Format code black src/ # Check formatting black --check src/ isort \u00b6 We use isort for import sorting. Configuration: - Profile: black - Line length: 100 - Multi-line output: 3 Usage: # Sort imports isort src/ # Check imports isort --check src/ Linting \u00b6 Flake8 \u00b6 We use Flake8 for linting. Configuration: - Line length: 100 - Ignore: E501 (handled by Black) Usage: flake8 src/ Ruff (Alternative) \u00b6 We also support Ruff as a modern alternative. Usage: ruff check src/ ruff format src/ Type Hints \u00b6 Type Annotations \u00b6 Always use type hints for function signatures: from typing import Dict , List , Optional , Union def create_trainer ( model_name : str , dataset_name : str , backend : str = \"auto\" , max_samples : Optional [ int ] = None ) -> TrainerBase : \"\"\"Create trainer with specified parameters.\"\"\" pass Type Checking \u00b6 We use mypy for type checking. Usage: mypy src/ Documentation \u00b6 Docstrings \u00b6 Use Google-style docstrings: def train_model ( model : Model , dataset : Dataset , epochs : int = 3 ) -> Dict [ str , Any ]: \"\"\"Train model on dataset. Args: model: Model to train dataset: Training dataset epochs: Number of training epochs Returns: Dictionary with training results Raises: TrainingError: If training fails \"\"\" pass Inline Comments \u00b6 Use comments to explain \"why\", not \"what\" Keep comments up-to-date with code Remove commented-out code Naming Conventions \u00b6 Variables and Functions \u00b6 Use snake_case for variables and functions Use descriptive names Avoid abbreviations # Good def create_sft_trainer (): ... model_name = \"microsoft/DialoGPT-medium\" # Bad def createSFT (): ... mn = \"microsoft/DialoGPT-medium\" Classes \u00b6 Use PascalCase for classes Use descriptive names # Good class SFTTrainerBase : ... class BackendFactory : ... # Bad class Trainer : ... class Factory : ... Constants \u00b6 Use UPPER_SNAKE_CASE for constants # Good MAX_SEQUENCE_LENGTH = 2048 DEFAULT_BATCH_SIZE = 4 # Bad maxSequenceLength = 2048 default_batch_size = 4 Code Organization \u00b6 Imports \u00b6 Order imports as follows: Standard library Third-party packages Local imports # Standard library import logging from typing import Dict , List # Third-party import torch from transformers import AutoModel # Local from aligntune.core.backend_factory import create_sft_trainer File Structure \u00b6 \"\"\"Module docstring.\"\"\" # Imports import ... # Constants DEFAULT_VALUE = ... # Classes class MyClass : ... # Functions def my_function (): ... # Main (if applicable) if __name__ == \"__main__\" : ... Error Handling \u00b6 Exceptions \u00b6 Use custom exceptions with clear messages: from aligntune.utils.errors import AlignTuneError class ConfigurationError ( AlignTuneError ): \"\"\"Configuration error.\"\"\" pass # Usage if not config . model . name : raise ConfigurationError ( \"Model name is required\" ) Error Messages \u00b6 Be specific and actionable Include context Suggest solutions # Good raise ValueError ( f \"Invalid backend ' { backend } '. \" f \"Available backends: { available_backends } . \" f \"Use 'auto' for automatic selection.\" ) # Bad raise ValueError ( \"Invalid backend\" ) Testing \u00b6 Test Structure \u00b6 One test file per module Test file: test_<module_name>.py Test class: Test<ClassName> Test function: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass Pre-commit Hooks \u00b6 Setup \u00b6 pip install pre-commit pre-commit install Hooks \u00b6 We use pre-commit hooks for: - Black formatting - isort import sorting - Flake8 linting - mypy type checking Best Practices \u00b6 Format Before Committing : Run Black and isort Type Hints : Add type hints to all functions Documentation : Document public APIs Error Handling : Use custom exceptions Testing : Write tests for new features Next Steps \u00b6 Testing Guide - Testing guidelines Contributing Guide - General contributing guide","title":"Code Style"},{"location":"contributing/code-style/#code-style-guide","text":"Code style guidelines for contributing to AlignTune.","title":"Code Style Guide"},{"location":"contributing/code-style/#overview","text":"AlignTune follows Python best practices and uses automated formatting tools to ensure consistent code style.","title":"Overview"},{"location":"contributing/code-style/#code-formatting","text":"","title":"Code Formatting"},{"location":"contributing/code-style/#black","text":"We use Black for code formatting. Configuration: - Line length: 100 characters - Target Python versions: 3.8, 3.9, 3.10, 3.11 Usage: # Format code black src/ # Check formatting black --check src/","title":"Black"},{"location":"contributing/code-style/#isort","text":"We use isort for import sorting. Configuration: - Profile: black - Line length: 100 - Multi-line output: 3 Usage: # Sort imports isort src/ # Check imports isort --check src/","title":"isort"},{"location":"contributing/code-style/#linting","text":"","title":"Linting"},{"location":"contributing/code-style/#flake8","text":"We use Flake8 for linting. Configuration: - Line length: 100 - Ignore: E501 (handled by Black) Usage: flake8 src/","title":"Flake8"},{"location":"contributing/code-style/#ruff-alternative","text":"We also support Ruff as a modern alternative. Usage: ruff check src/ ruff format src/","title":"Ruff (Alternative)"},{"location":"contributing/code-style/#type-hints","text":"","title":"Type Hints"},{"location":"contributing/code-style/#type-annotations","text":"Always use type hints for function signatures: from typing import Dict , List , Optional , Union def create_trainer ( model_name : str , dataset_name : str , backend : str = \"auto\" , max_samples : Optional [ int ] = None ) -> TrainerBase : \"\"\"Create trainer with specified parameters.\"\"\" pass","title":"Type Annotations"},{"location":"contributing/code-style/#type-checking","text":"We use mypy for type checking. Usage: mypy src/","title":"Type Checking"},{"location":"contributing/code-style/#documentation","text":"","title":"Documentation"},{"location":"contributing/code-style/#docstrings","text":"Use Google-style docstrings: def train_model ( model : Model , dataset : Dataset , epochs : int = 3 ) -> Dict [ str , Any ]: \"\"\"Train model on dataset. Args: model: Model to train dataset: Training dataset epochs: Number of training epochs Returns: Dictionary with training results Raises: TrainingError: If training fails \"\"\" pass","title":"Docstrings"},{"location":"contributing/code-style/#inline-comments","text":"Use comments to explain \"why\", not \"what\" Keep comments up-to-date with code Remove commented-out code","title":"Inline Comments"},{"location":"contributing/code-style/#naming-conventions","text":"","title":"Naming Conventions"},{"location":"contributing/code-style/#variables-and-functions","text":"Use snake_case for variables and functions Use descriptive names Avoid abbreviations # Good def create_sft_trainer (): ... model_name = \"microsoft/DialoGPT-medium\" # Bad def createSFT (): ... mn = \"microsoft/DialoGPT-medium\"","title":"Variables and Functions"},{"location":"contributing/code-style/#classes","text":"Use PascalCase for classes Use descriptive names # Good class SFTTrainerBase : ... class BackendFactory : ... # Bad class Trainer : ... class Factory : ...","title":"Classes"},{"location":"contributing/code-style/#constants","text":"Use UPPER_SNAKE_CASE for constants # Good MAX_SEQUENCE_LENGTH = 2048 DEFAULT_BATCH_SIZE = 4 # Bad maxSequenceLength = 2048 default_batch_size = 4","title":"Constants"},{"location":"contributing/code-style/#code-organization","text":"","title":"Code Organization"},{"location":"contributing/code-style/#imports","text":"Order imports as follows: Standard library Third-party packages Local imports # Standard library import logging from typing import Dict , List # Third-party import torch from transformers import AutoModel # Local from aligntune.core.backend_factory import create_sft_trainer","title":"Imports"},{"location":"contributing/code-style/#file-structure","text":"\"\"\"Module docstring.\"\"\" # Imports import ... # Constants DEFAULT_VALUE = ... # Classes class MyClass : ... # Functions def my_function (): ... # Main (if applicable) if __name__ == \"__main__\" : ...","title":"File Structure"},{"location":"contributing/code-style/#error-handling","text":"","title":"Error Handling"},{"location":"contributing/code-style/#exceptions","text":"Use custom exceptions with clear messages: from aligntune.utils.errors import AlignTuneError class ConfigurationError ( AlignTuneError ): \"\"\"Configuration error.\"\"\" pass # Usage if not config . model . name : raise ConfigurationError ( \"Model name is required\" )","title":"Exceptions"},{"location":"contributing/code-style/#error-messages","text":"Be specific and actionable Include context Suggest solutions # Good raise ValueError ( f \"Invalid backend ' { backend } '. \" f \"Available backends: { available_backends } . \" f \"Use 'auto' for automatic selection.\" ) # Bad raise ValueError ( \"Invalid backend\" )","title":"Error Messages"},{"location":"contributing/code-style/#testing","text":"","title":"Testing"},{"location":"contributing/code-style/#test-structure","text":"One test file per module Test file: test_<module_name>.py Test class: Test<ClassName> Test function: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass","title":"Test Structure"},{"location":"contributing/code-style/#pre-commit-hooks","text":"","title":"Pre-commit Hooks"},{"location":"contributing/code-style/#setup","text":"pip install pre-commit pre-commit install","title":"Setup"},{"location":"contributing/code-style/#hooks","text":"We use pre-commit hooks for: - Black formatting - isort import sorting - Flake8 linting - mypy type checking","title":"Hooks"},{"location":"contributing/code-style/#best-practices","text":"Format Before Committing : Run Black and isort Type Hints : Add type hints to all functions Documentation : Document public APIs Error Handling : Use custom exceptions Testing : Write tests for new features","title":"Best Practices"},{"location":"contributing/code-style/#next-steps","text":"Testing Guide - Testing guidelines Contributing Guide - General contributing guide","title":"Next Steps"},{"location":"contributing/guide/","text":"Contributing Guide \u00b6 Thank you for your interest in contributing to AlignTune! This guide will help you get started. Getting Started \u00b6 1. Fork and Clone \u00b6 # Fork the repository on GitHub # Then clone your fork git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune 2. Set Up Development Environment \u00b6 # Install in development mode pip install -e \".[dev]\" # Install pre-commit hooks pre-commit install 3. Create a Branch \u00b6 git checkout -b feature/your-feature-name Contribution Areas \u00b6 Code Contributions \u00b6 Bug fixes New features Performance improvements Documentation updates Documentation Contributions \u00b6 Fix typos Improve clarity Add examples Update guides Testing Contributions \u00b6 Add test cases Improve test coverage Fix failing tests Development Workflow \u00b6 1. Make Changes \u00b6 Make your changes following the Code Style Guide . 2. Write Tests \u00b6 Add tests for your changes: # tests/test_your_feature.py def test_your_feature (): # Your test code pass 3. Run Tests \u00b6 # Run all tests pytest # Run specific test pytest tests/test_your_feature.py # Run with coverage pytest --cov = aligntune 4. Check Code Quality \u00b6 # Format code black src/ tests/ # Sort imports isort src/ tests/ # Type check mypy src/ # Lint ruff check src/ tests/ 5. Commit Changes \u00b6 git add . git commit -m \"Add: your feature description\" 6. Push and Create PR \u00b6 git push origin feature/your-feature-name # Create PR on GitHub Code Style \u00b6 See Code Style Guide for detailed guidelines. Testing \u00b6 See Testing Guide for testing guidelines. Pull Request Process \u00b6 Update Documentation : Update relevant documentation Add Tests : Add tests for new features Update Changelog : Add entry to CHANGELOG.md Ensure Tests Pass : All tests must pass Code Review : Address review comments Questions? \u00b6 Open an issue for questions Join discussions on GitHub Check existing issues and PRs Thank you for contributing!","title":"Contributing Guide"},{"location":"contributing/guide/#contributing-guide","text":"Thank you for your interest in contributing to AlignTune! This guide will help you get started.","title":"Contributing Guide"},{"location":"contributing/guide/#getting-started","text":"","title":"Getting Started"},{"location":"contributing/guide/#1-fork-and-clone","text":"# Fork the repository on GitHub # Then clone your fork git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune","title":"1. Fork and Clone"},{"location":"contributing/guide/#2-set-up-development-environment","text":"# Install in development mode pip install -e \".[dev]\" # Install pre-commit hooks pre-commit install","title":"2. Set Up Development Environment"},{"location":"contributing/guide/#3-create-a-branch","text":"git checkout -b feature/your-feature-name","title":"3. Create a Branch"},{"location":"contributing/guide/#contribution-areas","text":"","title":"Contribution Areas"},{"location":"contributing/guide/#code-contributions","text":"Bug fixes New features Performance improvements Documentation updates","title":"Code Contributions"},{"location":"contributing/guide/#documentation-contributions","text":"Fix typos Improve clarity Add examples Update guides","title":"Documentation Contributions"},{"location":"contributing/guide/#testing-contributions","text":"Add test cases Improve test coverage Fix failing tests","title":"Testing Contributions"},{"location":"contributing/guide/#development-workflow","text":"","title":"Development Workflow"},{"location":"contributing/guide/#1-make-changes","text":"Make your changes following the Code Style Guide .","title":"1. Make Changes"},{"location":"contributing/guide/#2-write-tests","text":"Add tests for your changes: # tests/test_your_feature.py def test_your_feature (): # Your test code pass","title":"2. Write Tests"},{"location":"contributing/guide/#3-run-tests","text":"# Run all tests pytest # Run specific test pytest tests/test_your_feature.py # Run with coverage pytest --cov = aligntune","title":"3. Run Tests"},{"location":"contributing/guide/#4-check-code-quality","text":"# Format code black src/ tests/ # Sort imports isort src/ tests/ # Type check mypy src/ # Lint ruff check src/ tests/","title":"4. Check Code Quality"},{"location":"contributing/guide/#5-commit-changes","text":"git add . git commit -m \"Add: your feature description\"","title":"5. Commit Changes"},{"location":"contributing/guide/#6-push-and-create-pr","text":"git push origin feature/your-feature-name # Create PR on GitHub","title":"6. Push and Create PR"},{"location":"contributing/guide/#code-style","text":"See Code Style Guide for detailed guidelines.","title":"Code Style"},{"location":"contributing/guide/#testing","text":"See Testing Guide for testing guidelines.","title":"Testing"},{"location":"contributing/guide/#pull-request-process","text":"Update Documentation : Update relevant documentation Add Tests : Add tests for new features Update Changelog : Add entry to CHANGELOG.md Ensure Tests Pass : All tests must pass Code Review : Address review comments","title":"Pull Request Process"},{"location":"contributing/guide/#questions","text":"Open an issue for questions Join discussions on GitHub Check existing issues and PRs Thank you for contributing!","title":"Questions?"},{"location":"contributing/testing/","text":"Testing Guide \u00b6 Testing guidelines for contributing to AlignTune. Overview \u00b6 AlignTune uses pytest for testing with comprehensive coverage requirements. Test Structure \u00b6 Directory Layout \u00b6 tests/ __init__.py core/ test_backend_factory.py test_config.py backends/ trl/ unsloth/ integration/ test_training_pipeline.py fixtures/ sample_data.py Test File Naming \u00b6 Test files: test_<module_name>.py Test classes: Test<ClassName> Test functions: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass Writing Tests \u00b6 Basic Test \u00b6 import pytest from aligntune.core.backend_factory import create_sft_trainer def test_create_sft_trainer (): \"\"\"Test basic SFT trainer creation.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 ) assert trainer is not None assert trainer . config . model . name_or_path == \"microsoft/DialoGPT-small\" Fixtures \u00b6 Use pytest fixtures for reusable test data: import pytest from aligntune.core.sft.config import SFTConfig @pytest . fixture def sample_config (): \"\"\"Sample SFT configuration.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = \"test-model\" ), dataset = DatasetConfig ( name = \"test-dataset\" ) ) def test_trainer_with_config ( sample_config ): \"\"\"Test trainer with sample config.\"\"\" trainer = create_sft_trainer_from_config ( sample_config ) assert trainer . config == sample_config Parametrized Tests \u00b6 Use @pytest.mark.parametrize for multiple test cases: @pytest . mark . parametrize ( \"backend\" , [ \"trl\" , \"unsloth\" ]) def test_backend_selection ( backend ): \"\"\"Test different backend selections.\"\"\" trainer = create_sft_trainer ( model_name = \"model\" , dataset_name = \"dataset\" , backend = backend ) assert trainer . backend == backend Test Markers \u00b6 Available Markers \u00b6 @pytest . mark . slow # Slow tests @pytest . mark . integration # Integration tests @pytest . mark . gpu # Requires GPU @pytest . mark . unsloth # Requires Unsloth @pytest . mark . wandb # Requires WandB Usage \u00b6 @pytest . mark . slow def test_large_model_training (): \"\"\"Test training with large model.\"\"\" pass @pytest . mark . gpu def test_unsloth_backend (): \"\"\"Test Unsloth backend (requires GPU).\"\"\" pass Running Marked Tests \u00b6 # Run all tests except slow pytest -m \"not slow\" # Run only GPU tests pytest -m gpu # Run integration tests pytest -m integration Test Coverage \u00b6 Coverage Requirements \u00b6 Target: >80% coverage Critical paths: 100% coverage New features: Must include tests Running Coverage \u00b6 # Run with coverage pytest --cov = src/aligntune --cov-report = html # View HTML report open htmlcov/index.html Mocking \u00b6 External Dependencies \u00b6 Mock external dependencies: from unittest.mock import Mock , patch @patch ( 'aligntune.core.backend_factory.load_model' ) def test_model_loading ( mock_load ): \"\"\"Test model loading with mock.\"\"\" mock_load . return_value = Mock () trainer = create_sft_trainer ( ... ) mock_load . assert_called_once () Integration Tests \u00b6 Training Pipeline \u00b6 @pytest . mark . integration def test_sft_training_pipeline (): \"\"\"Test complete SFT training pipeline.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 , num_epochs = 1 ) # Train results = trainer . train () assert \"train_loss\" in results # Evaluate metrics = trainer . evaluate () assert \"eval_loss\" in metrics # Save path = trainer . save_model () assert path is not None Best Practices \u00b6 Test Isolation : Each test should be independent Fast Tests : Keep unit tests fast (<1s) Clear Names : Use descriptive test names One Assertion : One concept per test Fixtures : Use fixtures for setup/teardown Running Tests \u00b6 All Tests \u00b6 pytest Specific Test \u00b6 pytest tests/core/test_backend_factory.py::test_create_sft_trainer Verbose Output \u00b6 pytest -v With Coverage \u00b6 pytest --cov = src/aligntune --cov-report = term-missing Continuous Integration \u00b6 Tests run automatically on: - Pull requests - Pushes to main - Scheduled runs Next Steps \u00b6 Code Style Guide - Code style guidelines Contributing Guide - General contributing guide","title":"Testing"},{"location":"contributing/testing/#testing-guide","text":"Testing guidelines for contributing to AlignTune.","title":"Testing Guide"},{"location":"contributing/testing/#overview","text":"AlignTune uses pytest for testing with comprehensive coverage requirements.","title":"Overview"},{"location":"contributing/testing/#test-structure","text":"","title":"Test Structure"},{"location":"contributing/testing/#directory-layout","text":"tests/ __init__.py core/ test_backend_factory.py test_config.py backends/ trl/ unsloth/ integration/ test_training_pipeline.py fixtures/ sample_data.py","title":"Directory Layout"},{"location":"contributing/testing/#test-file-naming","text":"Test files: test_<module_name>.py Test classes: Test<ClassName> Test functions: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass","title":"Test File Naming"},{"location":"contributing/testing/#writing-tests","text":"","title":"Writing Tests"},{"location":"contributing/testing/#basic-test","text":"import pytest from aligntune.core.backend_factory import create_sft_trainer def test_create_sft_trainer (): \"\"\"Test basic SFT trainer creation.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 ) assert trainer is not None assert trainer . config . model . name_or_path == \"microsoft/DialoGPT-small\"","title":"Basic Test"},{"location":"contributing/testing/#fixtures","text":"Use pytest fixtures for reusable test data: import pytest from aligntune.core.sft.config import SFTConfig @pytest . fixture def sample_config (): \"\"\"Sample SFT configuration.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = \"test-model\" ), dataset = DatasetConfig ( name = \"test-dataset\" ) ) def test_trainer_with_config ( sample_config ): \"\"\"Test trainer with sample config.\"\"\" trainer = create_sft_trainer_from_config ( sample_config ) assert trainer . config == sample_config","title":"Fixtures"},{"location":"contributing/testing/#parametrized-tests","text":"Use @pytest.mark.parametrize for multiple test cases: @pytest . mark . parametrize ( \"backend\" , [ \"trl\" , \"unsloth\" ]) def test_backend_selection ( backend ): \"\"\"Test different backend selections.\"\"\" trainer = create_sft_trainer ( model_name = \"model\" , dataset_name = \"dataset\" , backend = backend ) assert trainer . backend == backend","title":"Parametrized Tests"},{"location":"contributing/testing/#test-markers","text":"","title":"Test Markers"},{"location":"contributing/testing/#available-markers","text":"@pytest . mark . slow # Slow tests @pytest . mark . integration # Integration tests @pytest . mark . gpu # Requires GPU @pytest . mark . unsloth # Requires Unsloth @pytest . mark . wandb # Requires WandB","title":"Available Markers"},{"location":"contributing/testing/#usage","text":"@pytest . mark . slow def test_large_model_training (): \"\"\"Test training with large model.\"\"\" pass @pytest . mark . gpu def test_unsloth_backend (): \"\"\"Test Unsloth backend (requires GPU).\"\"\" pass","title":"Usage"},{"location":"contributing/testing/#running-marked-tests","text":"# Run all tests except slow pytest -m \"not slow\" # Run only GPU tests pytest -m gpu # Run integration tests pytest -m integration","title":"Running Marked Tests"},{"location":"contributing/testing/#test-coverage","text":"","title":"Test Coverage"},{"location":"contributing/testing/#coverage-requirements","text":"Target: >80% coverage Critical paths: 100% coverage New features: Must include tests","title":"Coverage Requirements"},{"location":"contributing/testing/#running-coverage","text":"# Run with coverage pytest --cov = src/aligntune --cov-report = html # View HTML report open htmlcov/index.html","title":"Running Coverage"},{"location":"contributing/testing/#mocking","text":"","title":"Mocking"},{"location":"contributing/testing/#external-dependencies","text":"Mock external dependencies: from unittest.mock import Mock , patch @patch ( 'aligntune.core.backend_factory.load_model' ) def test_model_loading ( mock_load ): \"\"\"Test model loading with mock.\"\"\" mock_load . return_value = Mock () trainer = create_sft_trainer ( ... ) mock_load . assert_called_once ()","title":"External Dependencies"},{"location":"contributing/testing/#integration-tests","text":"","title":"Integration Tests"},{"location":"contributing/testing/#training-pipeline","text":"@pytest . mark . integration def test_sft_training_pipeline (): \"\"\"Test complete SFT training pipeline.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 , num_epochs = 1 ) # Train results = trainer . train () assert \"train_loss\" in results # Evaluate metrics = trainer . evaluate () assert \"eval_loss\" in metrics # Save path = trainer . save_model () assert path is not None","title":"Training Pipeline"},{"location":"contributing/testing/#best-practices","text":"Test Isolation : Each test should be independent Fast Tests : Keep unit tests fast (<1s) Clear Names : Use descriptive test names One Assertion : One concept per test Fixtures : Use fixtures for setup/teardown","title":"Best Practices"},{"location":"contributing/testing/#running-tests","text":"","title":"Running Tests"},{"location":"contributing/testing/#all-tests","text":"pytest","title":"All Tests"},{"location":"contributing/testing/#specific-test","text":"pytest tests/core/test_backend_factory.py::test_create_sft_trainer","title":"Specific Test"},{"location":"contributing/testing/#verbose-output","text":"pytest -v","title":"Verbose Output"},{"location":"contributing/testing/#with-coverage","text":"pytest --cov = src/aligntune --cov-report = term-missing","title":"With Coverage"},{"location":"contributing/testing/#continuous-integration","text":"Tests run automatically on: - Pull requests - Pushes to main - Scheduled runs","title":"Continuous Integration"},{"location":"contributing/testing/#next-steps","text":"Code Style Guide - Code style guidelines Contributing Guide - General contributing guide","title":"Next Steps"},{"location":"examples/advanced/","text":"Advanced Examples \u00b6 Advanced examples demonstrating sophisticated use cases with AlignTune. Distributed Training \u00b6 distributed training DDP Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer import torch # Configure for DDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 2 , # Per GPU gradient_accumulation_steps = 4 ) # Run with accelerate # accelerate launch --multi_gpu train.py trainer . train () FSDP Training \u00b6 # Configure for FSDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 1 , gradient_accumulation_steps = 8 ) # Run with accelerate FSDP # accelerate launch --fsdp train.py trainer . train () Custom Reward Functions \u00b6 Creating Custom Rewards \u00b6 from aligntune.rewards.types import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry class DomainSpecificReward ( RewardFunction ): def __init__ ( self ): def compute ( self , text : str , ** kwargs ) -> float : # Your custom logic score = 0.0 if \"domain_keyword\" in text . lower (): score += 0.5 # ... more logic return score # Register RewardRegistry . register_reward ( \"domain_specific\" , DomainSpecificReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , reward_functions = [ \"domain_specific\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] ) Production Pipeline \u00b6 Complete Production Setup \u00b6 from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer import logging from pathlib import Path logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def production_pipeline (): \"\"\"Complete production training pipeline.\"\"\" # Stage 1: SFT logger . info ( \"Stage 1: SFT Training\" ) sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , output_dir = \"./output/sft\" , eval_interval = 100 , save_interval = 500 ) sft_trainer . train () sft_path = sft_trainer . save_model () logger . info ( f \"SFT model saved to: { sft_path } \" ) # Stage 2: DPO logger . info ( \"Stage 2: DPO Training\" ) dpo_trainer = create_rl_trainer ( model_name = sft_path , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , output_dir = \"./output/dpo\" ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () logger . info ( f \"DPO model saved to: { dpo_path } \" ) # Stage 3: Evaluation logger . info ( \"Stage 3: Evaluation\" ) metrics = dpo_trainer . evaluate () logger . info ( f \"Final metrics: { metrics } \" ) # Stage 4: Push to Hub logger . info ( \"Stage 4: Pushing to Hub\" ) url = dpo_trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) logger . info ( f \"Model available at: { url } \" ) if __name__ == \"__main__\" : production_pipeline () Custom Domain Training \u00b6 Medical Domain Example \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"medical-dataset\" , backend = \"trl\" , task_type = \"instruction_following\" , num_epochs = 5 , batch_size = 4 , learning_rate = 1e-5 , max_seq_length = 1024 , # Custom column mappings column_mapping = { \"instruction\" : \"medical_question\" , \"output\" : \"medical_answer\" , \"input\" : \"patient_context\" } ) trainer . train () Integration Examples \u00b6 Weights & Biases Integration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"wandb\" ], # Enable W&B run_name = \"my-experiment\" ) trainer . train () TensorBoard Integration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"tensorboard\" ], output_dir = \"./output/tensorboard\" ) trainer . train () # View with: tensorboard --logdir ./output/tensorboard Memory Optimization \u00b6 Large Model Training \u00b6 trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Memory optimizations quantization = { \"load_in_4bit\" : True }, peft_enabled = True , lora_r = 16 , lora_alpha = 32 , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 , activation_offloading = True ) trainer . train () Next Steps \u00b6 SFT Examples - Basic SFT examples RL Examples - RL training examples Advanced Topics - Architecture details","title":"Advanced Examples"},{"location":"examples/advanced/#advanced-examples","text":"Advanced examples demonstrating sophisticated use cases with AlignTune.","title":"Advanced Examples"},{"location":"examples/advanced/#distributed-training","text":"","title":"Distributed Training"},{"location":"examples/advanced/#distributed-training-ddp-training","text":"from aligntune.core.backend_factory import create_sft_trainer import torch # Configure for DDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 2 , # Per GPU gradient_accumulation_steps = 4 ) # Run with accelerate # accelerate launch --multi_gpu train.py trainer . train ()","title":"distributed training DDP Training"},{"location":"examples/advanced/#fsdp-training","text":"# Configure for FSDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 1 , gradient_accumulation_steps = 8 ) # Run with accelerate FSDP # accelerate launch --fsdp train.py trainer . train ()","title":"FSDP Training"},{"location":"examples/advanced/#custom-reward-functions","text":"","title":"Custom Reward Functions"},{"location":"examples/advanced/#creating-custom-rewards","text":"from aligntune.rewards.types import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry class DomainSpecificReward ( RewardFunction ): def __init__ ( self ): def compute ( self , text : str , ** kwargs ) -> float : # Your custom logic score = 0.0 if \"domain_keyword\" in text . lower (): score += 0.5 # ... more logic return score # Register RewardRegistry . register_reward ( \"domain_specific\" , DomainSpecificReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , reward_functions = [ \"domain_specific\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] )","title":"Creating Custom Rewards"},{"location":"examples/advanced/#production-pipeline","text":"","title":"Production Pipeline"},{"location":"examples/advanced/#complete-production-setup","text":"from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer import logging from pathlib import Path logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def production_pipeline (): \"\"\"Complete production training pipeline.\"\"\" # Stage 1: SFT logger . info ( \"Stage 1: SFT Training\" ) sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , output_dir = \"./output/sft\" , eval_interval = 100 , save_interval = 500 ) sft_trainer . train () sft_path = sft_trainer . save_model () logger . info ( f \"SFT model saved to: { sft_path } \" ) # Stage 2: DPO logger . info ( \"Stage 2: DPO Training\" ) dpo_trainer = create_rl_trainer ( model_name = sft_path , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , output_dir = \"./output/dpo\" ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () logger . info ( f \"DPO model saved to: { dpo_path } \" ) # Stage 3: Evaluation logger . info ( \"Stage 3: Evaluation\" ) metrics = dpo_trainer . evaluate () logger . info ( f \"Final metrics: { metrics } \" ) # Stage 4: Push to Hub logger . info ( \"Stage 4: Pushing to Hub\" ) url = dpo_trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) logger . info ( f \"Model available at: { url } \" ) if __name__ == \"__main__\" : production_pipeline ()","title":"Complete Production Setup"},{"location":"examples/advanced/#custom-domain-training","text":"","title":"Custom Domain Training"},{"location":"examples/advanced/#medical-domain-example","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"medical-dataset\" , backend = \"trl\" , task_type = \"instruction_following\" , num_epochs = 5 , batch_size = 4 , learning_rate = 1e-5 , max_seq_length = 1024 , # Custom column mappings column_mapping = { \"instruction\" : \"medical_question\" , \"output\" : \"medical_answer\" , \"input\" : \"patient_context\" } ) trainer . train ()","title":"Medical Domain Example"},{"location":"examples/advanced/#integration-examples","text":"","title":"Integration Examples"},{"location":"examples/advanced/#weights-biases-integration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"wandb\" ], # Enable W&B run_name = \"my-experiment\" ) trainer . train ()","title":"Weights &amp; Biases Integration"},{"location":"examples/advanced/#tensorboard-integration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"tensorboard\" ], output_dir = \"./output/tensorboard\" ) trainer . train () # View with: tensorboard --logdir ./output/tensorboard","title":"TensorBoard Integration"},{"location":"examples/advanced/#memory-optimization","text":"","title":"Memory Optimization"},{"location":"examples/advanced/#large-model-training","text":"trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Memory optimizations quantization = { \"load_in_4bit\" : True }, peft_enabled = True , lora_r = 16 , lora_alpha = 32 , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 , activation_offloading = True ) trainer . train ()","title":"Large Model Training"},{"location":"examples/advanced/#next-steps","text":"SFT Examples - Basic SFT examples RL Examples - RL training examples Advanced Topics - Architecture details","title":"Next Steps"},{"location":"examples/overview/","text":"Examples Overview \u00b6 This section contains comprehensive examples for using AlignTune. Quick Links \u00b6 SFT Examples - Supervised Fine-Tuning examples RL Examples - Reinforcement Learning examples Advanced Examples - Advanced use cases Example Categories \u00b6 Basic Examples \u00b6 Simple SFT training Basic DPO training Model evaluation Model saving and loading Intermediate Examples \u00b6 Custom reward functions Multi-dataset training Custom evaluation metrics Checkpoint management Advanced Examples \u00b6 Distributed training Custom backends Reward model training Performance optimization Running Examples \u00b6 All examples can be run directly: # Run SFT example python examples/sft_customer_support_trl/train_sft_direct_api.py # Run DPO example python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py Example Structure \u00b6 Examples are organized by: Task Type : SFT, DPO, PPO, GRPO, GSPO Backend : TRL, Unsloth Complexity : Basic, Intermediate, Advanced Next Steps \u00b6 SFT Examples - Start with SFT RL Examples - Learn RL training Advanced Examples - Explore advanced features","title":"Overview"},{"location":"examples/overview/#examples-overview","text":"This section contains comprehensive examples for using AlignTune.","title":"Examples Overview"},{"location":"examples/overview/#quick-links","text":"SFT Examples - Supervised Fine-Tuning examples RL Examples - Reinforcement Learning examples Advanced Examples - Advanced use cases","title":"Quick Links"},{"location":"examples/overview/#example-categories","text":"","title":"Example Categories"},{"location":"examples/overview/#basic-examples","text":"Simple SFT training Basic DPO training Model evaluation Model saving and loading","title":"Basic Examples"},{"location":"examples/overview/#intermediate-examples","text":"Custom reward functions Multi-dataset training Custom evaluation metrics Checkpoint management","title":"Intermediate Examples"},{"location":"examples/overview/#advanced-examples","text":"Distributed training Custom backends Reward model training Performance optimization","title":"Advanced Examples"},{"location":"examples/overview/#running-examples","text":"All examples can be run directly: # Run SFT example python examples/sft_customer_support_trl/train_sft_direct_api.py # Run DPO example python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py","title":"Running Examples"},{"location":"examples/overview/#example-structure","text":"Examples are organized by: Task Type : SFT, DPO, PPO, GRPO, GSPO Backend : TRL, Unsloth Complexity : Basic, Intermediate, Advanced","title":"Example Structure"},{"location":"examples/overview/#next-steps","text":"SFT Examples - Start with SFT RL Examples - Learn RL training Advanced Examples - Explore advanced features","title":"Next Steps"},{"location":"examples/rl/","text":"RL Examples \u00b6 Comprehensive examples for Reinforcement Learning training with AlignTune. Basic Examples \u00b6 1. DPO Training \u00b6 Basic DPO training example: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/1: 100%|| 250/250 [08:45<00:00, loss=0.234] Model saved to: ./output/model 2. PPO with Pre-trained Reward Model \u00b6 PPO training with HuggingFace reward model: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , cliprange = 0.2 ) trainer . train () 3. PPO with Custom Reward Model \u00b6 Train custom reward model during PPO: def load_training_texts (): return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () Advanced Examples \u00b6 4. GRPO Training \u00b6 Group Relative Policy Optimization: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' , ) trainer . train () 5. GSPO Training \u00b6 Group Sequential Policy Optimization (TRL only): trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , ) trainer . train () 6. Multi-Stage Pipeline \u00b6 SFT \u2192 DPO \u2192 PPO pipeline: # Stage 1: SFT from aligntune.core.backend_factory import create_sft_trainer sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) sft_trainer . train () sft_path = sft_trainer . save_model () # Stage 2: DPO dpo_trainer = create_rl_trainer ( model_name = sft_path , # Start from SFT model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () # Stage 3: PPO ppo_trainer = create_rl_trainer ( model_name = dpo_path , # Start from DPO model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 ) ppo_trainer . train () Complete Workflow Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer from datasets import load_dataset # Create trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ] # or set use_peft = False eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Running Examples \u00b6 From Command Line \u00b6 # DPO training python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py # PPO training python examples/ppo_ultrachat_unsloth/train_ppo_ultrachat_unsloth.py # GRPO training python examples/grpo_gsm8k_trl/train_grpo_direct_api.py Tips \u00b6 Start with DPO : Simpler setup, no reward model needed Model Family Consistency : For PPO, ensure all models same family Reward Models : Use pre-trained when available, train custom for domains Backend Selection : Unsloth for speed, TRL for compatibility/GSPO Memory Management : Use smaller batch sizes for PPO Next Steps \u00b6 SFT Examples - SFT training examples Advanced Examples - Advanced use cases RL Guide - Complete RL guide","title":"RL Examples"},{"location":"examples/rl/#rl-examples","text":"Comprehensive examples for Reinforcement Learning training with AlignTune.","title":"RL Examples"},{"location":"examples/rl/#basic-examples","text":"","title":"Basic Examples"},{"location":"examples/rl/#1-dpo-training","text":"Basic DPO training example: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/1: 100%|| 250/250 [08:45<00:00, loss=0.234] Model saved to: ./output/model","title":"1. DPO Training"},{"location":"examples/rl/#2-ppo-with-pre-trained-reward-model","text":"PPO training with HuggingFace reward model: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , cliprange = 0.2 ) trainer . train ()","title":"2. PPO with Pre-trained Reward Model"},{"location":"examples/rl/#3-ppo-with-custom-reward-model","text":"Train custom reward model during PPO: def load_training_texts (): return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"3. PPO with Custom Reward Model"},{"location":"examples/rl/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"examples/rl/#4-grpo-training","text":"Group Relative Policy Optimization: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' , ) trainer . train ()","title":"4. GRPO Training"},{"location":"examples/rl/#5-gspo-training","text":"Group Sequential Policy Optimization (TRL only): trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , ) trainer . train ()","title":"5. GSPO Training"},{"location":"examples/rl/#6-multi-stage-pipeline","text":"SFT \u2192 DPO \u2192 PPO pipeline: # Stage 1: SFT from aligntune.core.backend_factory import create_sft_trainer sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) sft_trainer . train () sft_path = sft_trainer . save_model () # Stage 2: DPO dpo_trainer = create_rl_trainer ( model_name = sft_path , # Start from SFT model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () # Stage 3: PPO ppo_trainer = create_rl_trainer ( model_name = dpo_path , # Start from DPO model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 ) ppo_trainer . train ()","title":"6. Multi-Stage Pipeline"},{"location":"examples/rl/#complete-workflow-example","text":"from aligntune.core.backend_factory import create_rl_trainer from datasets import load_dataset # Create trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ] # or set use_peft = False eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Complete Workflow Example"},{"location":"examples/rl/#running-examples","text":"","title":"Running Examples"},{"location":"examples/rl/#from-command-line","text":"# DPO training python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py # PPO training python examples/ppo_ultrachat_unsloth/train_ppo_ultrachat_unsloth.py # GRPO training python examples/grpo_gsm8k_trl/train_grpo_direct_api.py","title":"From Command Line"},{"location":"examples/rl/#tips","text":"Start with DPO : Simpler setup, no reward model needed Model Family Consistency : For PPO, ensure all models same family Reward Models : Use pre-trained when available, train custom for domains Backend Selection : Unsloth for speed, TRL for compatibility/GSPO Memory Management : Use smaller batch sizes for PPO","title":"Tips"},{"location":"examples/rl/#next-steps","text":"SFT Examples - SFT training examples Advanced Examples - Advanced use cases RL Guide - Complete RL guide","title":"Next Steps"},{"location":"examples/sft/","text":"SFT Examples \u00b6 Comprehensive examples for Supervised Fine-Tuning with AlignTune. Basic Examples \u00b6 1. Basic SFT Training \u00b6 Simple SFT training example: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 based models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/3: 100%|| 250/250 [05:23<00:00, loss=2.34] Epoch 2/3: 100%|| 250/250 [05:18<00:00, loss=1.89] Epoch 3/3: 100%|| 250/250 [05:21<00:00, loss=1.67] Model saved to: ./output/model 2. Instruction Following \u00b6 Train model to follow instructions: trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , # Use Qwen/Llama instead of GPT2 dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , peft_enabled = True , instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () 3. Text Classification \u00b6 Note: For GPT2 models, use backend=\"trl\" and lora_target_modules=[\"c_attn\", \"c_proj\"] . GPT2 is not supported by Unsloth. Train classification model: trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) trainer . train () metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) Advanced Examples \u00b6 4. LoRA Fine-Tuning \u00b6 Efficient fine-tuning with LoRA: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , quantization = { \"load_in_4bit\" : True }, num_epochs = 3 , batch_size = 2 , learning_rate = 2e-4 ) trainer . train () 5. Sequence Packing \u00b6 Efficient training with sequence packing: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models packing = True , packing_strategy = \"bfd\" , padding_free = True , max_seq_length = 2048 , num_epochs = 3 , batch_size = 4 ) trainer . train () 6. Chat Completion \u00b6 Train conversational model: trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"train_sft\" , # Dataset split backend = \"unsloth\" , task_type = \"chat_completion\" , num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" ) trainer . train () Complete Workflow Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) # Push to Hub (optional) # url = trainer.push_to_hub(\"username/my-model\") # print(f\"Model available at: {url}\") Running Examples \u00b6 From Command Line \u00b6 # Basic SFT python examples/sft_customer_support_trl/train_sft_direct_api.py # Instruction following python examples/sft_financial_summarization_trl/train_sft_direct_api.py Tips \u00b6 Start Small : Use max_samples=1000 for testing Monitor Training : Set eval_interval to track progress Save Regularly : Use save_interval for checkpoints Choose Backend : Use Unsloth for speed, TRL for compatibility Memory Management : Use LoRA and quantization for large models Next Steps \u00b6 RL Examples - RL training examples Advanced Examples - Advanced use cases SFT Guide - Complete SFT guide","title":"SFT Examples"},{"location":"examples/sft/#sft-examples","text":"Comprehensive examples for Supervised Fine-Tuning with AlignTune.","title":"SFT Examples"},{"location":"examples/sft/#basic-examples","text":"","title":"Basic Examples"},{"location":"examples/sft/#1-basic-sft-training","text":"Simple SFT training example: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 based models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/3: 100%|| 250/250 [05:23<00:00, loss=2.34] Epoch 2/3: 100%|| 250/250 [05:18<00:00, loss=1.89] Epoch 3/3: 100%|| 250/250 [05:21<00:00, loss=1.67] Model saved to: ./output/model","title":"1. Basic SFT Training"},{"location":"examples/sft/#2-instruction-following","text":"Train model to follow instructions: trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , # Use Qwen/Llama instead of GPT2 dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , peft_enabled = True , instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train ()","title":"2. Instruction Following"},{"location":"examples/sft/#3-text-classification","text":"Note: For GPT2 models, use backend=\"trl\" and lora_target_modules=[\"c_attn\", \"c_proj\"] . GPT2 is not supported by Unsloth. Train classification model: trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) trainer . train () metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" )","title":"3. Text Classification"},{"location":"examples/sft/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"examples/sft/#4-lora-fine-tuning","text":"Efficient fine-tuning with LoRA: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , quantization = { \"load_in_4bit\" : True }, num_epochs = 3 , batch_size = 2 , learning_rate = 2e-4 ) trainer . train ()","title":"4. LoRA Fine-Tuning"},{"location":"examples/sft/#5-sequence-packing","text":"Efficient training with sequence packing: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models packing = True , packing_strategy = \"bfd\" , padding_free = True , max_seq_length = 2048 , num_epochs = 3 , batch_size = 4 ) trainer . train ()","title":"5. Sequence Packing"},{"location":"examples/sft/#6-chat-completion","text":"Train conversational model: trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"train_sft\" , # Dataset split backend = \"unsloth\" , task_type = \"chat_completion\" , num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" ) trainer . train ()","title":"6. Chat Completion"},{"location":"examples/sft/#complete-workflow-example","text":"from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) # Push to Hub (optional) # url = trainer.push_to_hub(\"username/my-model\") # print(f\"Model available at: {url}\")","title":"Complete Workflow Example"},{"location":"examples/sft/#running-examples","text":"","title":"Running Examples"},{"location":"examples/sft/#from-command-line","text":"# Basic SFT python examples/sft_customer_support_trl/train_sft_direct_api.py # Instruction following python examples/sft_financial_summarization_trl/train_sft_direct_api.py","title":"From Command Line"},{"location":"examples/sft/#tips","text":"Start Small : Use max_samples=1000 for testing Monitor Training : Set eval_interval to track progress Save Regularly : Use save_interval for checkpoints Choose Backend : Use Unsloth for speed, TRL for compatibility Memory Management : Use LoRA and quantization for large models","title":"Tips"},{"location":"examples/sft/#next-steps","text":"RL Examples - RL training examples Advanced Examples - Advanced use cases SFT Guide - Complete SFT guide","title":"Next Steps"},{"location":"getting-started/backend-selection/","text":"Backend Selection \u00b6 AlignTune supports multiple backends for training. Choose the one that best fits your needs. Available Backends \u00b6 TRL Backend \u00b6 Reliability : Battle-tested, stable Compatibility : Works everywhere Performance : Standard speed Use Case : Production, reliability-focused Unsloth Backend \u00b6 Speed : faster training Memory : Optimized memory usage Compatibility : Requires GPU, specific setup Use Case : Fast iteration, research Backend Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Automatic Backend Selection \u00b6 AlignTune can automatically select the best backend: from aligntune.core.backend_factory import create_sft_trainer # Auto-select backend (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically chooses best backend ) Manual Backend Selection \u00b6 TRL Backend \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Explicitly use TRL ) Unsloth Backend \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Explicitly use Unsloth ) Backend Selection Logic \u00b6 When using backend=\"auto\" , AlignTune: Checks if Unsloth is available Checks if model is compatible with Unsloth Falls back to TRL if Unsloth unavailable or incompatible Logs the selected backend Checking Backend Status \u00b6 from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl' ][ 'available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" ) When to Use Each Backend \u00b6 Use TRL Backend When: \u00b6 You need maximum reliability You're in production You don't have GPU or Unsloth setup You need GSPO algorithm You want battle-tested code Use Unsloth Backend When: \u00b6 You need faster training (faster training) You have GPU available You're doing research/experimentation You want memory optimization You're fine-tuning large models Backend-Specific Configuration \u00b6 TRL Backend \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Standard configuration works num_epochs = 3 , batch_size = 4 ) Unsloth Backend \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth-specific optimizations enabled num_epochs = 3 , batch_size = 4 ) Troubleshooting \u00b6 Unsloth Not Available \u00b6 If Unsloth is not available, AlignTune automatically falls back to TRL: # This will use TRL if Unsloth unavailable trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Falls back to TRL if Unsloth unavailable ) Force TRL Backend \u00b6 # Explicitly force TRL trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Always uses TRL ) Next Steps \u00b6 SFT Guide - SFT training details RL Guide - RL training details Unsloth Compatibility - Unsloth setup and troubleshooting","title":"Backend Selection"},{"location":"getting-started/backend-selection/#backend-selection","text":"AlignTune supports multiple backends for training. Choose the one that best fits your needs.","title":"Backend Selection"},{"location":"getting-started/backend-selection/#available-backends","text":"","title":"Available Backends"},{"location":"getting-started/backend-selection/#trl-backend","text":"Reliability : Battle-tested, stable Compatibility : Works everywhere Performance : Standard speed Use Case : Production, reliability-focused","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend","text":"Speed : faster training Memory : Optimized memory usage Compatibility : Requires GPU, specific setup Use Case : Fast iteration, research","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#backend-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Backend Support Matrix"},{"location":"getting-started/backend-selection/#automatic-backend-selection","text":"AlignTune can automatically select the best backend: from aligntune.core.backend_factory import create_sft_trainer # Auto-select backend (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically chooses best backend )","title":"Automatic Backend Selection"},{"location":"getting-started/backend-selection/#manual-backend-selection","text":"","title":"Manual Backend Selection"},{"location":"getting-started/backend-selection/#trl-backend_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Explicitly use TRL )","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Explicitly use Unsloth )","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#backend-selection-logic","text":"When using backend=\"auto\" , AlignTune: Checks if Unsloth is available Checks if model is compatible with Unsloth Falls back to TRL if Unsloth unavailable or incompatible Logs the selected backend","title":"Backend Selection Logic"},{"location":"getting-started/backend-selection/#checking-backend-status","text":"from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl' ][ 'available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" )","title":"Checking Backend Status"},{"location":"getting-started/backend-selection/#when-to-use-each-backend","text":"","title":"When to Use Each Backend"},{"location":"getting-started/backend-selection/#use-trl-backend-when","text":"You need maximum reliability You're in production You don't have GPU or Unsloth setup You need GSPO algorithm You want battle-tested code","title":"Use TRL Backend When:"},{"location":"getting-started/backend-selection/#use-unsloth-backend-when","text":"You need faster training (faster training) You have GPU available You're doing research/experimentation You want memory optimization You're fine-tuning large models","title":"Use Unsloth Backend When:"},{"location":"getting-started/backend-selection/#backend-specific-configuration","text":"","title":"Backend-Specific Configuration"},{"location":"getting-started/backend-selection/#trl-backend_2","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Standard configuration works num_epochs = 3 , batch_size = 4 )","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend_2","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth-specific optimizations enabled num_epochs = 3 , batch_size = 4 )","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/backend-selection/#unsloth-not-available","text":"If Unsloth is not available, AlignTune automatically falls back to TRL: # This will use TRL if Unsloth unavailable trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Falls back to TRL if Unsloth unavailable )","title":"Unsloth Not Available"},{"location":"getting-started/backend-selection/#force-trl-backend","text":"# Explicitly force TRL trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Always uses TRL )","title":"Force TRL Backend"},{"location":"getting-started/backend-selection/#next-steps","text":"SFT Guide - SFT training details RL Guide - RL training details Unsloth Compatibility - Unsloth setup and troubleshooting","title":"Next Steps"},{"location":"getting-started/basic-concepts/","text":"Basic Concepts \u00b6 This guide introduces the core concepts you need to understand to effectively use AlignTune. Training Types \u00b6 AlignTune supports two main training paradigms: Supervised Fine-Tuning (SFT) \u00b6 SFT is the process of fine-tuning a pre-trained language model on a task-specific dataset. It's used for: Instruction following Text classification Chat completion Domain adaptation from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) trainer . train () Reinforcement Learning (RL) \u00b6 RL training uses reinforcement learning algorithms to align models with human preferences or optimize for specific objectives. Supported algorithms include: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Requires reward model GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) trainer . train () Backends \u00b6 AlignTune provides two backend implementations: TRL Backend \u00b6 TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation: Reliable : Production-ready, extensively tested Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models Slower : Standard training speed Use TRL when: - You need maximum reliability - You're using algorithms not supported by Unsloth (GSPO) - You're on CPU or have compatibility issues with Unsloth Unsloth Backend \u00b6 Unsloth backend provides optimized training with significant speed improvements: Fast : faster training Memory Efficient : Optimized memory usage with LoRA/QLoRA GPU Optimized : CUDA-specific optimizations Limited : Some algorithms not supported (GSPO) GPU Required : Needs CUDA-capable GPU Use Unsloth when: - You have a CUDA-capable GPU - You want maximum training speed - You're using supported algorithms (DPO, PPO, GRPO, etc.) Backend Selection \u00b6 You can specify the backend explicitly or use auto-selection: # Explicit backend selection trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # or \"unsloth\" ) # Auto-selection (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically selects best available backend ) Configuration \u00b6 AlignTune supports three configuration methods: 1. Python API \u00b6 Direct function calls with keyword arguments: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) 2. YAML Configuration \u00b6 Declarative configuration files: # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : num_epochs : 3 per_device_batch_size : 4 learning_rate : 5e-5 from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config ) 3. CLI \u00b6 Command-line interface: aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 Reward Functions \u00b6 Reward functions are used in RL training to evaluate and guide model behavior. AlignTune provides 27+ built-in reward functions: Quality Metrics \u00b6 helpfulness - Measures how helpful the response is coherence - Evaluates text coherence and flow relevance - Measures relevance to the prompt fluency - Evaluates language fluency Safety Metrics \u00b6 toxicity - Detects toxic content bias - Measures bias in responses privacy - Detects privacy violations harmful_content - Identifies harmful content Style Metrics \u00b6 formality - Measures formality level tone - Evaluates tone appropriateness length - Measures response length structure - Evaluates text structure Task-Specific Metrics \u00b6 code_quality - Evaluates code quality math_accuracy - Measures mathematical accuracy factual_correctness - Evaluates factual accuracy Using Reward Functions \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.4 , 0.3 , 0.3 ] ) Evaluation \u00b6 AlignTune provides comprehensive evaluation capabilities: Basic Metrics \u00b6 Loss : Training loss Perplexity : Language modeling quality Runtime : Training time Quality Metrics \u00b6 BLEU : Bilingual Evaluation Understudy score ROUGE : Recall-Oriented Understudy for Gisting Evaluation Task-Specific : Code syntax, math accuracy, etc. Safety Metrics \u00b6 Toxicity : Toxicity detection Bias : Bias measurement Factual Accuracy : Factual correctness # Evaluate trained model metrics = trainer . evaluate () print ( metrics ) Model Management \u00b6 Saving Models \u00b6 # Save model to local directory path = trainer . save_model ( \"./output/my_model\" ) # Push to HuggingFace Hub trainer . push_to_hub ( \"username/my-model\" , token = \"hf_...\" ) Loading Models \u00b6 from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) Next Steps \u00b6 Configuration Guide - Learn about all configuration options Backend Selection - Detailed backend comparison SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#basic-concepts","text":"This guide introduces the core concepts you need to understand to effectively use AlignTune.","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#training-types","text":"AlignTune supports two main training paradigms:","title":"Training Types"},{"location":"getting-started/basic-concepts/#supervised-fine-tuning-sft","text":"SFT is the process of fine-tuning a pre-trained language model on a task-specific dataset. It's used for: Instruction following Text classification Chat completion Domain adaptation from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) trainer . train ()","title":"Supervised Fine-Tuning (SFT)"},{"location":"getting-started/basic-concepts/#reinforcement-learning-rl","text":"RL training uses reinforcement learning algorithms to align models with human preferences or optimize for specific objectives. Supported algorithms include: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Requires reward model GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) trainer . train ()","title":"Reinforcement Learning (RL)"},{"location":"getting-started/basic-concepts/#backends","text":"AlignTune provides two backend implementations:","title":"Backends"},{"location":"getting-started/basic-concepts/#trl-backend","text":"TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation: Reliable : Production-ready, extensively tested Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models Slower : Standard training speed Use TRL when: - You need maximum reliability - You're using algorithms not supported by Unsloth (GSPO) - You're on CPU or have compatibility issues with Unsloth","title":"TRL Backend"},{"location":"getting-started/basic-concepts/#unsloth-backend","text":"Unsloth backend provides optimized training with significant speed improvements: Fast : faster training Memory Efficient : Optimized memory usage with LoRA/QLoRA GPU Optimized : CUDA-specific optimizations Limited : Some algorithms not supported (GSPO) GPU Required : Needs CUDA-capable GPU Use Unsloth when: - You have a CUDA-capable GPU - You want maximum training speed - You're using supported algorithms (DPO, PPO, GRPO, etc.)","title":"Unsloth Backend"},{"location":"getting-started/basic-concepts/#backend-selection","text":"You can specify the backend explicitly or use auto-selection: # Explicit backend selection trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # or \"unsloth\" ) # Auto-selection (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically selects best available backend )","title":"Backend Selection"},{"location":"getting-started/basic-concepts/#configuration","text":"AlignTune supports three configuration methods:","title":"Configuration"},{"location":"getting-started/basic-concepts/#1-python-api","text":"Direct function calls with keyword arguments: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 )","title":"1. Python API"},{"location":"getting-started/basic-concepts/#2-yaml-configuration","text":"Declarative configuration files: # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : num_epochs : 3 per_device_batch_size : 4 learning_rate : 5e-5 from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config )","title":"2. YAML Configuration"},{"location":"getting-started/basic-concepts/#3-cli","text":"Command-line interface: aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4","title":"3. CLI"},{"location":"getting-started/basic-concepts/#reward-functions","text":"Reward functions are used in RL training to evaluate and guide model behavior. AlignTune provides 27+ built-in reward functions:","title":"Reward Functions"},{"location":"getting-started/basic-concepts/#quality-metrics","text":"helpfulness - Measures how helpful the response is coherence - Evaluates text coherence and flow relevance - Measures relevance to the prompt fluency - Evaluates language fluency","title":"Quality Metrics"},{"location":"getting-started/basic-concepts/#safety-metrics","text":"toxicity - Detects toxic content bias - Measures bias in responses privacy - Detects privacy violations harmful_content - Identifies harmful content","title":"Safety Metrics"},{"location":"getting-started/basic-concepts/#style-metrics","text":"formality - Measures formality level tone - Evaluates tone appropriateness length - Measures response length structure - Evaluates text structure","title":"Style Metrics"},{"location":"getting-started/basic-concepts/#task-specific-metrics","text":"code_quality - Evaluates code quality math_accuracy - Measures mathematical accuracy factual_correctness - Evaluates factual accuracy","title":"Task-Specific Metrics"},{"location":"getting-started/basic-concepts/#using-reward-functions","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.4 , 0.3 , 0.3 ] )","title":"Using Reward Functions"},{"location":"getting-started/basic-concepts/#evaluation","text":"AlignTune provides comprehensive evaluation capabilities:","title":"Evaluation"},{"location":"getting-started/basic-concepts/#basic-metrics","text":"Loss : Training loss Perplexity : Language modeling quality Runtime : Training time","title":"Basic Metrics"},{"location":"getting-started/basic-concepts/#quality-metrics_1","text":"BLEU : Bilingual Evaluation Understudy score ROUGE : Recall-Oriented Understudy for Gisting Evaluation Task-Specific : Code syntax, math accuracy, etc.","title":"Quality Metrics"},{"location":"getting-started/basic-concepts/#safety-metrics_1","text":"Toxicity : Toxicity detection Bias : Bias measurement Factual Accuracy : Factual correctness # Evaluate trained model metrics = trainer . evaluate () print ( metrics )","title":"Safety Metrics"},{"location":"getting-started/basic-concepts/#model-management","text":"","title":"Model Management"},{"location":"getting-started/basic-concepts/#saving-models","text":"# Save model to local directory path = trainer . save_model ( \"./output/my_model\" ) # Push to HuggingFace Hub trainer . push_to_hub ( \"username/my-model\" , token = \"hf_...\" )","title":"Saving Models"},{"location":"getting-started/basic-concepts/#loading-models","text":"from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" )","title":"Loading Models"},{"location":"getting-started/basic-concepts/#next-steps","text":"Configuration Guide - Learn about all configuration options Backend Selection - Detailed backend comparison SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Next Steps"},{"location":"getting-started/configuration/","text":"Configuration \u00b6 AlignTune supports flexible configuration through Python code, YAML files, or CLI arguments. Configuration Methods \u00b6 1. Python Code (Recommended for Development) \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , output_dir = \"./output\" ) 2. YAML Files (Recommended for Production) \u00b6 # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 logging : output_dir : \"./output\" run_name : \"my_experiment\" 3. CLI Arguments \u00b6 aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --epochs 3 \\ --batch-size 4 \\ --lr 5e-5 Configuration Structure \u00b6 Model Configuration \u00b6 model_config = { \"name_or_path\" : \"microsoft/DialoGPT-small\" , # Required \"precision\" : \"fp32\" , # fp32, fp16, bf16 \"max_seq_length\" : 2048 , \"gradient_checkpointing\" : True , \"use_unsloth\" : False } Dataset Configuration \u00b6 dataset_config = { \"name\" : \"tatsu-lab/alpaca\" , # Required \"split\" : \"train\" , \"max_samples\" : 1000 , \"percent\" : 100 } Training Configuration \u00b6 training_config = { \"per_device_batch_size\" : 4 , # Required \"num_epochs\" : 3 , \"learning_rate\" : 5e-5 , \"gradient_accumulation_steps\" : 1 , \"max_steps\" : None , \"warmup_steps\" : 100 , \"weight_decay\" : 0.01 } Logging Configuration \u00b6 logging_config = { \"output_dir\" : \"./output\" , # Required \"run_name\" : \"my_experiment\" , \"loggers\" : [ \"tensorboard\" ], \"save_steps\" : 500 , \"eval_steps\" : 100 } Complete Example \u00b6 YAML Configuration \u00b6 # Complete SFT configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 max_seq_length : 2048 gradient_checkpointing : true datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 gradient_accumulation_steps : 1 num_epochs : 3 learning_rate : 5e-5 warmup_steps : 100 weight_decay : 0.01 max_steps : null eval_interval : 100 save_interval : 500 logging : output_dir : \"./output\" run_name : \"sft_experiment\" loggers : [ \"tensorboard\" ] save_steps : 500 eval_steps : 100 Python Code \u00b6 from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config ) trainer . train () Configuration Validation \u00b6 from aligntune.core.rl.config_loader import ConfigLoader from aligntune.core.sft.config_loader import SFTConfigLoader # Load and validate config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader . validate_config ( config ) Environment Variables \u00b6 You can override configuration with environment variables: export ALIGNTUNE_OUTPUT_DIR = \"./custom_output\" export ALIGNTUNE_LEARNING_RATE = 1e-4 export ALIGNTUNE_BATCH_SIZE = 8 Next Steps \u00b6 Backend Selection - Choose the right backend SFT Guide - SFT-specific configuration RL Guide - RL-specific configuration","title":"Configuration"},{"location":"getting-started/configuration/#configuration","text":"AlignTune supports flexible configuration through Python code, YAML files, or CLI arguments.","title":"Configuration"},{"location":"getting-started/configuration/#configuration-methods","text":"","title":"Configuration Methods"},{"location":"getting-started/configuration/#1-python-code-recommended-for-development","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , output_dir = \"./output\" )","title":"1. Python Code (Recommended for Development)"},{"location":"getting-started/configuration/#2-yaml-files-recommended-for-production","text":"# config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"2. YAML Files (Recommended for Production)"},{"location":"getting-started/configuration/#3-cli-arguments","text":"aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --epochs 3 \\ --batch-size 4 \\ --lr 5e-5","title":"3. CLI Arguments"},{"location":"getting-started/configuration/#configuration-structure","text":"","title":"Configuration Structure"},{"location":"getting-started/configuration/#model-configuration","text":"model_config = { \"name_or_path\" : \"microsoft/DialoGPT-small\" , # Required \"precision\" : \"fp32\" , # fp32, fp16, bf16 \"max_seq_length\" : 2048 , \"gradient_checkpointing\" : True , \"use_unsloth\" : False }","title":"Model Configuration"},{"location":"getting-started/configuration/#dataset-configuration","text":"dataset_config = { \"name\" : \"tatsu-lab/alpaca\" , # Required \"split\" : \"train\" , \"max_samples\" : 1000 , \"percent\" : 100 }","title":"Dataset Configuration"},{"location":"getting-started/configuration/#training-configuration","text":"training_config = { \"per_device_batch_size\" : 4 , # Required \"num_epochs\" : 3 , \"learning_rate\" : 5e-5 , \"gradient_accumulation_steps\" : 1 , \"max_steps\" : None , \"warmup_steps\" : 100 , \"weight_decay\" : 0.01 }","title":"Training Configuration"},{"location":"getting-started/configuration/#logging-configuration","text":"logging_config = { \"output_dir\" : \"./output\" , # Required \"run_name\" : \"my_experiment\" , \"loggers\" : [ \"tensorboard\" ], \"save_steps\" : 500 , \"eval_steps\" : 100 }","title":"Logging Configuration"},{"location":"getting-started/configuration/#complete-example","text":"","title":"Complete Example"},{"location":"getting-started/configuration/#yaml-configuration","text":"# Complete SFT configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 max_seq_length : 2048 gradient_checkpointing : true datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 gradient_accumulation_steps : 1 num_epochs : 3 learning_rate : 5e-5 warmup_steps : 100 weight_decay : 0.01 max_steps : null eval_interval : 100 save_interval : 500 logging : output_dir : \"./output\" run_name : \"sft_experiment\" loggers : [ \"tensorboard\" ] save_steps : 500 eval_steps : 100","title":"YAML Configuration"},{"location":"getting-started/configuration/#python-code","text":"from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config ) trainer . train ()","title":"Python Code"},{"location":"getting-started/configuration/#configuration-validation","text":"from aligntune.core.rl.config_loader import ConfigLoader from aligntune.core.sft.config_loader import SFTConfigLoader # Load and validate config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader . validate_config ( config )","title":"Configuration Validation"},{"location":"getting-started/configuration/#environment-variables","text":"You can override configuration with environment variables: export ALIGNTUNE_OUTPUT_DIR = \"./custom_output\" export ALIGNTUNE_LEARNING_RATE = 1e-4 export ALIGNTUNE_BATCH_SIZE = 8","title":"Environment Variables"},{"location":"getting-started/configuration/#next-steps","text":"Backend Selection - Choose the right backend SFT Guide - SFT-specific configuration RL Guide - RL-specific configuration","title":"Next Steps"},{"location":"getting-started/installation/","text":"Installation \u00b6 This guide will help you install AlignTune and set up your environment. Requirements \u00b6 Python 3.8 or higher PyTorch 2.0 or higher CUDA-capable GPU (recommended for training, optional for inference) Installation Methods \u00b6 Install from PyPI (Recommended) \u00b6 pip install aligntune Install from Source \u00b6 # Clone the repository git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune # Install in development mode pip install -e . # Or install with all optional dependencies pip install -e \".[all]\" Install with Optional Dependencies \u00b6 # Install with Unsloth support (for faster training) pip install aligntune [ unsloth ] # Install with evaluation tools pip install aligntune [ eval ] # Install with all optional dependencies pip install aligntune [ all ] Verify Installation \u00b6 # Check installation python -c \"import aligntune; print(aligntune.__version__)\" # Check CLI aligntune --help # Check system information aligntune info Dependencies \u00b6 Core Dependencies \u00b6 transformers - HuggingFace Transformers library trl - Transformer Reinforcement Learning datasets - HuggingFace Datasets torch - PyTorch numpy , pandas - Data processing pyyaml - Configuration management tqdm - Progress bars Optional Dependencies \u00b6 unsloth - Fast training acceleration (requires GPU) wandb - Weights & Biases logging tensorboard - TensorBoard logging lm-eval - Language model evaluation scikit-learn - Evaluation metrics seqeval - Sequence evaluation GPU Setup \u00b6 CUDA Installation \u00b6 AlignTune works with CUDA-enabled GPUs. Install PyTorch with CUDA support: # For CUDA 11.8 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 Verify GPU Access \u00b6 import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"CUDA device: { torch . cuda . get_device_name ( 0 ) if torch . cuda . is_available () else 'N/A' } \" ) Unsloth Setup (Optional) \u00b6 Unsloth provides faster training but requires specific setup: # Install Unsloth pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" # Or for local installation pip install unsloth Verify Unsloth \u00b6 from aligntune.core.backend_factory import BackendType from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" ) Environment Variables \u00b6 Set these environment variables for optimal performance: # HuggingFace cache directory export HF_HOME = /path/to/cache # CUDA device selection export CUDA_VISIBLE_DEVICES = 0 ,1 # Disable tokenizers parallelism (if you see warnings) export TOKENIZERS_PARALLELISM = false # HuggingFace offline mode (if needed) export HF_DATASETS_OFFLINE = 1 Troubleshooting \u00b6 Import Errors \u00b6 # Ensure proper Python path export PYTHONPATH = \" ${ PYTHONPATH } : $( pwd ) /src\" # Reinstall in development mode pip install -e . CUDA Issues \u00b6 # Check CUDA version nvidia-smi # Verify PyTorch CUDA python -c \"import torch; print(torch.cuda.is_available())\" Unsloth Compatibility \u00b6 If Unsloth has compatibility issues, AlignTune automatically falls back to TRL backends. See Unsloth Compatibility for details. Next Steps \u00b6 Quick Start Guide - Get started with your first training Configuration Guide - Learn about configuration options Backend Selection - Choose the right backend","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This guide will help you install AlignTune and set up your environment.","title":"Installation"},{"location":"getting-started/installation/#requirements","text":"Python 3.8 or higher PyTorch 2.0 or higher CUDA-capable GPU (recommended for training, optional for inference)","title":"Requirements"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#install-from-pypi-recommended","text":"pip install aligntune","title":"Install from PyPI (Recommended)"},{"location":"getting-started/installation/#install-from-source","text":"# Clone the repository git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune # Install in development mode pip install -e . # Or install with all optional dependencies pip install -e \".[all]\"","title":"Install from Source"},{"location":"getting-started/installation/#install-with-optional-dependencies","text":"# Install with Unsloth support (for faster training) pip install aligntune [ unsloth ] # Install with evaluation tools pip install aligntune [ eval ] # Install with all optional dependencies pip install aligntune [ all ]","title":"Install with Optional Dependencies"},{"location":"getting-started/installation/#verify-installation","text":"# Check installation python -c \"import aligntune; print(aligntune.__version__)\" # Check CLI aligntune --help # Check system information aligntune info","title":"Verify Installation"},{"location":"getting-started/installation/#dependencies","text":"","title":"Dependencies"},{"location":"getting-started/installation/#core-dependencies","text":"transformers - HuggingFace Transformers library trl - Transformer Reinforcement Learning datasets - HuggingFace Datasets torch - PyTorch numpy , pandas - Data processing pyyaml - Configuration management tqdm - Progress bars","title":"Core Dependencies"},{"location":"getting-started/installation/#optional-dependencies","text":"unsloth - Fast training acceleration (requires GPU) wandb - Weights & Biases logging tensorboard - TensorBoard logging lm-eval - Language model evaluation scikit-learn - Evaluation metrics seqeval - Sequence evaluation","title":"Optional Dependencies"},{"location":"getting-started/installation/#gpu-setup","text":"","title":"GPU Setup"},{"location":"getting-started/installation/#cuda-installation","text":"AlignTune works with CUDA-enabled GPUs. Install PyTorch with CUDA support: # For CUDA 11.8 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","title":"CUDA Installation"},{"location":"getting-started/installation/#verify-gpu-access","text":"import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"CUDA device: { torch . cuda . get_device_name ( 0 ) if torch . cuda . is_available () else 'N/A' } \" )","title":"Verify GPU Access"},{"location":"getting-started/installation/#unsloth-setup-optional","text":"Unsloth provides faster training but requires specific setup: # Install Unsloth pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" # Or for local installation pip install unsloth","title":"Unsloth Setup (Optional)"},{"location":"getting-started/installation/#verify-unsloth","text":"from aligntune.core.backend_factory import BackendType from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" )","title":"Verify Unsloth"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # HuggingFace cache directory export HF_HOME = /path/to/cache # CUDA device selection export CUDA_VISIBLE_DEVICES = 0 ,1 # Disable tokenizers parallelism (if you see warnings) export TOKENIZERS_PARALLELISM = false # HuggingFace offline mode (if needed) export HF_DATASETS_OFFLINE = 1","title":"Environment Variables"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#import-errors","text":"# Ensure proper Python path export PYTHONPATH = \" ${ PYTHONPATH } : $( pwd ) /src\" # Reinstall in development mode pip install -e .","title":"Import Errors"},{"location":"getting-started/installation/#cuda-issues","text":"# Check CUDA version nvidia-smi # Verify PyTorch CUDA python -c \"import torch; print(torch.cuda.is_available())\"","title":"CUDA Issues"},{"location":"getting-started/installation/#unsloth-compatibility","text":"If Unsloth has compatibility issues, AlignTune automatically falls back to TRL backends. See Unsloth Compatibility for details.","title":"Unsloth Compatibility"},{"location":"getting-started/installation/#next-steps","text":"Quick Start Guide - Get started with your first training Configuration Guide - Learn about configuration options Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"getting-started/quickstart/","text":"Quick Start \u00b6 This quick start guide demonstrates how to run a complete end-to-end workflow with AlignTune in just a few steps. 1. Prepare Your Environment \u00b6 Ensure you have installed AlignTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source aligntune-env/bin/activate # If using conda conda activate aligntune Check installation: python -c \"import aligntune; print('AlignTune installed successfully')\" 2. Load a Dataset \u00b6 We'll use the Alpaca dataset for this example. from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Preview the data print ( dataset [ 0 ]) # Output: {'instruction': '...', 'input': '...', 'output': '...'} 3. Initialize and Configure the Trainer \u00b6 Use the create_sft_trainer function to define your model, dataset, and training configuration. Supervised Fine-Tuning (SFT) \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_samples = 1000 # Limit for quick start ) 4. Train the Model \u00b6 Train your pipeline on the training data. # Train the model trainer . train () During training, AlignTune will automatically handle data preprocessing and apply the chosen training strategy. You'll see progress bars and training metrics. 5. Evaluate and Predict \u00b6 After training, evaluate performance and generate predictions: # Evaluate on test set (if available) metrics = trainer . evaluate () print ( \"Evaluation metrics:\" , metrics ) # Output: {'eval_loss': 2.34, 'eval_perplexity': 10.4, ...} # Make predictions result = trainer . predict ( \"What is machine learning?\" ) print ( \"Prediction:\" , result ) Supported metrics include: Loss , Perplexity , BLEU , and ROUGE scores. 6. Save and Load the Model \u00b6 Persist your trained model for later use: # Save to disk model_path = trainer . save_model ( \"./output/my_model\" ) print ( f \"Model saved to: { model_path } \" ) # Load from disk (in a new session) from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) # Push to HuggingFace Hub (optional) # trainer.push_to_hub(\"username/my-model\", token=\"hf_...\") 7. Try Reinforcement Learning (DPO) \u00b6 Switch to reinforcement learning with minimal code changes: Reinforcement Learning (DPO) \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Create DPO trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Direct Preference Optimization backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , max_samples = 500 # Limit for quick start ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( \"DPO metrics:\" , metrics ) # Save model trainer . save_model () Next Steps \u00b6 Explore advanced configurations in the User Guide Learn about different algorithms in Algorithms Overview Compare backends with Backend Comparison Dive into RL training in RL Guide Using YAML Configuration \u00b6 Create a configuration file config.yaml : # SFT Configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 output_dir : \"./output\" logging : output_dir : \"./output\" run_name : \"my_first_training\" Then train: from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config ) trainer . train () CLI Quick Start \u00b6 # Train SFT model aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 # Train DPO model aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --epochs 1 \\ --batch-size 1 What's Next? \u00b6 Configuration Guide - Learn about all configuration options Backend Selection - Choose between TRL and Unsloth SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Quick Start"},{"location":"getting-started/quickstart/#quick-start","text":"This quick start guide demonstrates how to run a complete end-to-end workflow with AlignTune in just a few steps.","title":"Quick Start"},{"location":"getting-started/quickstart/#1-prepare-your-environment","text":"Ensure you have installed AlignTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source aligntune-env/bin/activate # If using conda conda activate aligntune Check installation: python -c \"import aligntune; print('AlignTune installed successfully')\"","title":"1. Prepare Your Environment"},{"location":"getting-started/quickstart/#2-load-a-dataset","text":"We'll use the Alpaca dataset for this example. from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Preview the data print ( dataset [ 0 ]) # Output: {'instruction': '...', 'input': '...', 'output': '...'}","title":"2. Load a Dataset"},{"location":"getting-started/quickstart/#3-initialize-and-configure-the-trainer","text":"Use the create_sft_trainer function to define your model, dataset, and training configuration.","title":"3. Initialize and Configure the Trainer"},{"location":"getting-started/quickstart/#supervised-fine-tuning-sft","text":"from aligntune.core.backend_factory import create_sft_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_samples = 1000 # Limit for quick start )","title":"Supervised Fine-Tuning (SFT)"},{"location":"getting-started/quickstart/#4-train-the-model","text":"Train your pipeline on the training data. # Train the model trainer . train () During training, AlignTune will automatically handle data preprocessing and apply the chosen training strategy. You'll see progress bars and training metrics.","title":"4. Train the Model"},{"location":"getting-started/quickstart/#5-evaluate-and-predict","text":"After training, evaluate performance and generate predictions: # Evaluate on test set (if available) metrics = trainer . evaluate () print ( \"Evaluation metrics:\" , metrics ) # Output: {'eval_loss': 2.34, 'eval_perplexity': 10.4, ...} # Make predictions result = trainer . predict ( \"What is machine learning?\" ) print ( \"Prediction:\" , result ) Supported metrics include: Loss , Perplexity , BLEU , and ROUGE scores.","title":"5. Evaluate and Predict"},{"location":"getting-started/quickstart/#6-save-and-load-the-model","text":"Persist your trained model for later use: # Save to disk model_path = trainer . save_model ( \"./output/my_model\" ) print ( f \"Model saved to: { model_path } \" ) # Load from disk (in a new session) from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) # Push to HuggingFace Hub (optional) # trainer.push_to_hub(\"username/my-model\", token=\"hf_...\")","title":"6. Save and Load the Model"},{"location":"getting-started/quickstart/#7-try-reinforcement-learning-dpo","text":"Switch to reinforcement learning with minimal code changes:","title":"7. Try Reinforcement Learning (DPO)"},{"location":"getting-started/quickstart/#reinforcement-learning-dpo","text":"from aligntune.core.backend_factory import create_rl_trainer # Create DPO trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Direct Preference Optimization backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , max_samples = 500 # Limit for quick start ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( \"DPO metrics:\" , metrics ) # Save model trainer . save_model ()","title":"Reinforcement Learning (DPO)"},{"location":"getting-started/quickstart/#next-steps","text":"Explore advanced configurations in the User Guide Learn about different algorithms in Algorithms Overview Compare backends with Backend Comparison Dive into RL training in RL Guide","title":"Next Steps"},{"location":"getting-started/quickstart/#using-yaml-configuration","text":"Create a configuration file config.yaml : # SFT Configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 output_dir : \"./output\" logging : output_dir : \"./output\" run_name : \"my_first_training\" Then train: from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( config = config ) trainer . train ()","title":"Using YAML Configuration"},{"location":"getting-started/quickstart/#cli-quick-start","text":"# Train SFT model aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 # Train DPO model aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --epochs 1 \\ --batch-size 1","title":"CLI Quick Start"},{"location":"getting-started/quickstart/#whats-next","text":"Configuration Guide - Learn about all configuration options Backend Selection - Choose between TRL and Unsloth SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"What's Next?"},{"location":"notebooks/","text":"AlignTune Notebooks \u00b6 Two notebooks demonstrate the core AlignTune workflows. - __01 - Basic SFT Training__ --- End-to-end Supervised Fine-Tuning on a small dataset for a fast demo. :octicons-arrow-right-24: [Open notebook](./01_basic_sft_training.ipynb) - __02 - DPO Preference Training__ --- Direct Preference Optimization training using a preference dataset. :octicons-arrow-right-24: [Open notebook](./02_dpo_preference_training.ipynb) Running the Notebooks \u00b6 Local Setup \u00b6 Clone the repository: git clone https://github.com/Lexsi-Labs/aligntune.git cd AlignTune Install dependencies: pip install -e . pip install jupyter notebook Launch Jupyter: jupyter notebook docs/notebooks/ Requirements \u00b6 Python 3.8+ PyTorch 2.0+ CUDA-compatible GPU (recommended for faster training) Demo Notebooks \u00b6 Interactive Colab notebooks demonstrating various AlignTune workflows: Supervised Fine-Tuning (SFT) \u00b6 Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab Reinforcement Learning (RL) \u00b6 Backend Algorithm Dataset Link TRL DPO - Open in Colab TRL GRPO GSM8K Open in Colab Contributing \u00b6 Want to improve a notebook? See our Contributing Guide .","title":"Notebooks"},{"location":"notebooks/#aligntune-notebooks","text":"Two notebooks demonstrate the core AlignTune workflows. - __01 - Basic SFT Training__ --- End-to-end Supervised Fine-Tuning on a small dataset for a fast demo. :octicons-arrow-right-24: [Open notebook](./01_basic_sft_training.ipynb) - __02 - DPO Preference Training__ --- Direct Preference Optimization training using a preference dataset. :octicons-arrow-right-24: [Open notebook](./02_dpo_preference_training.ipynb)","title":"AlignTune Notebooks"},{"location":"notebooks/#running-the-notebooks","text":"","title":"Running the Notebooks"},{"location":"notebooks/#local-setup","text":"Clone the repository: git clone https://github.com/Lexsi-Labs/aligntune.git cd AlignTune Install dependencies: pip install -e . pip install jupyter notebook Launch Jupyter: jupyter notebook docs/notebooks/","title":"Local Setup"},{"location":"notebooks/#requirements","text":"Python 3.8+ PyTorch 2.0+ CUDA-compatible GPU (recommended for faster training)","title":"Requirements"},{"location":"notebooks/#demo-notebooks","text":"Interactive Colab notebooks demonstrating various AlignTune workflows:","title":"Demo Notebooks"},{"location":"notebooks/#supervised-fine-tuning-sft","text":"Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab","title":"Supervised Fine-Tuning (SFT)"},{"location":"notebooks/#reinforcement-learning-rl","text":"Backend Algorithm Dataset Link TRL DPO - Open in Colab TRL GRPO GSM8K Open in Colab","title":"Reinforcement Learning (RL)"},{"location":"notebooks/#contributing","text":"Want to improve a notebook? See our Contributing Guide .","title":"Contributing"},{"location":"user-guide/evaluation/","text":"Evaluation Guide \u00b6 Complete guide to evaluating fine-tuned models with AlignTune, covering training evaluation, standalone evaluation, benchmarks, and custom evaluation tasks. Overview \u00b6 AlignTune provides comprehensive evaluation capabilities: Training Evaluation - Evaluate during training Standalone Evaluation - Evaluate saved models Benchmark Evaluation - Standard benchmarks (lm-eval) Custom Evaluation - Task-specific evaluation RL Evaluation - DPO, PPO, GRPO evaluation Quick Start \u00b6 Basic Training Evaluation \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) # Train with automatic evaluation trainer . train () # Evaluate on validation set metrics = trainer . evaluate () print ( metrics ) Standalone Model Evaluation \u00b6 from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" ) RL Model Evaluation \u00b6 from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , use_lora = True , base_model = \"microsoft/phi-2\" ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" ) Evaluation Types \u00b6 1. Training Evaluation \u00b6 Automatic evaluation during training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 , # Evaluate every 100 steps save_interval = 500 # Save every 500 steps ) trainer . train () # Evaluation runs automatically 2. Standalone Evaluation \u00b6 Evaluate after training: # After training metrics = trainer . evaluate () # With custom dataset from datasets import load_dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # With custom metric prefix metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) 3. Zero-Shot Evaluation \u00b6 Generate predictions for test prompts: # Single prediction result = trainer . predict ( \"What is machine learning?\" ) print ( result ) # Batch predictions prompts = [ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) 4. Benchmark Evaluation \u00b6 Run standard benchmarks with lm-eval: from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run standard benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , \"arc\" , \"mmlu\" ], batch_size = 1 ) print ( results ) 5. Custom Evaluation \u00b6 Create custom evaluation tasks: from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom task task = EvalTask ( name = \"custom_sentiment\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"Custom sentiment analysis task\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ) # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , [ task ]) 6. RL Evaluation \u00b6 Evaluate RL-trained models (DPO, PPO, GRPO): DPO Evaluation \u00b6 from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results/dpo\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , data_task_type = \"dpo\" , # DPO metrics metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , # For KL divergence # Dataset dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , # Generation max_length = 1536 , temperature = 0.0 , batch_size = 2 , # Model loading use_lora = True , base_model = \"microsoft/phi-2\" , use_unsloth = True ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" ) PPO Evaluation \u00b6 config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) print ( f \"Perplexity: { results . get ( 'perplexity' , 0.0 ) : .4f } \" ) Before/After Comparison \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Train DPO model trainer = create_rl_trainer ( model_name = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = \"microsoft/phi-2\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = False , output_dir = \"./eval_results/base\" ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = True , base_model = \"microsoft/phi-2\" , output_dir = \"./eval_results/trained\" ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Compare print ( f \"Base: { base_results [ 'win_rate' ] : .2% } \" ) print ( f \"Trained: { trained_results [ 'win_rate' ] : .2% } \" ) print ( f \"Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" ) Evaluation Metrics \u00b6 Classification Metrics \u00b6 # For text classification metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: accuracy, f1, precision, recall Generation Metrics \u00b6 # For text generation metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: perplexity, loss, BLEU, ROUGE (if applicable) RL Metrics \u00b6 DPO Metrics \u00b6 metrics = [ \"reward_margin\" , # Difference between chosen/rejected rewards \"preference_accuracy\" , # How often model prefers chosen \"win_rate\" , # % matching human preferences \"kl_divergence\" , # KL from reference (needs reference_model_path) \"log_ratio\" , # Log probability ratio \"implicit_reward\" , # Implicit reward signal \"calibration\" # Calibration score ] PPO Metrics \u00b6 metrics = [ \"perplexity\" , # Language modeling quality \"reward_accuracy\" , # Alignment with reward model \"policy_entropy\" # Policy output diversity ] RL Metrics \u00b6 metrics = [ \"kl_divergence\" , # KL divergence from reference \"reward_accuracy\" , # Reward model alignment \"policy_entropy\" # Output diversity ] Custom Metrics \u00b6 from aligntune.eval.core import EvalTask , TaskCategory task = EvalTask ( name = \"custom_task\" , category = TaskCategory . TEXT_GENERATION , metrics = [ \"accuracy\" , \"f1\" , \"custom_metric\" ], custom_metrics = [ your_custom_metric_function ] ) Evaluation Configuration \u00b6 Training Evaluation Config \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Evaluation settings eval_interval = 100 , # Evaluate every N steps save_interval = 500 , # Save every N steps load_best_model_at_end = True , # Load best model metric_for_best_model = \"eval_loss\" , # Metric to track greater_is_better = False # Lower is better for loss ) Standalone Evaluation Config \u00b6 from aligntune.eval.core import EvalConfig , EvalType , TaskCategory config = EvalConfig ( eval_type = EvalType . STANDALONE , task_categories = [ TaskCategory . TEXT_GENERATION ], metrics = [ \"accuracy\" , \"f1\" , \"bleu\" , \"rouge\" ], batch_size = 32 , max_samples = 1000 , device = \"auto\" , precision = \"bf16\" , output_dir = \"./eval_results\" , save_predictions = True , save_metrics = True ) RL Evaluation Config \u00b6 from aligntune.eval.core import EvalConfig , run_eval config = EvalConfig ( # Model configuration model_path = \"./model\" , output_dir = \"./eval_results\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , # Evaluation task: dpo, ppo, grpo, text, math, code data_task_type = \"dpo\" , # Data schema: dpo, sft, grpo # Metrics metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = None , # Reference model (for KL) reward_model = None , # Reward model (optional) # Dataset dataset_name = \"dataset_name\" , dataset_config = None , split = \"test\" , max_samples = 100 , # Generation max_length = 2048 , # Context window max_new_tokens = 512 , # Generation length temperature = 0.0 , # Sampling temperature batch_size = 8 , # Model loading use_lora = False , base_model = None , use_unsloth = False , use_vllm = False , precision = \"bf16\" , # Advanced column_mapping = None , trust_remote_code = True ) results = run_eval ( config ) Task Type Reference \u00b6 Supported task_type values for evaluation: Task Type Description Default Metrics Data Schema \"text\" Text generation perplexity, accuracy sft \"math\" Math problems math_accuracy, pass_at_k sft \"code\" Code generation pass_at_k code \"dpo\" DPO models reward_margin, preference_accuracy, win_rate dpo \"ppo\" PPO models perplexity, reward_accuracy sft \"generic\" Generic evaluation perplexity, accuracy sft Note : For RL evaluation, task_type and data_task_type must match (e.g., both \"dpo\" for DPO models). Complete Examples \u00b6 SFT Evaluation \u00b6 from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 ) # Train trainer . train () # Evaluate on test set test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Test metrics: { metrics } \" ) # Zero-shot evaluation prompts = [ \"What is AI?\" , \"Explain machine learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \\n A: { result } \\n \" ) RL Evaluation \u00b6 from aligntune.core.backend_factory import create_rl_trainer from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" # Train DPO model trainer = create_rl_trainer ( model_name = MODEL_NAME , dataset_name = DATASET , algorithm = \"dpo\" , backend = \"trl\" , output_dir = OUTPUT_DIR , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = MODEL_NAME , output_dir = f \" { OUTPUT_DIR } /eval_base\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = False , use_unsloth = True ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Print comparison print ( f \" \\n Base Model:\" ) print ( f \" Win Rate: { base_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Trained Model:\" ) print ( f \" Win Rate: { trained_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" ) Benchmark Evaluation \u00b6 from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run multiple benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , # Commonsense reasoning \"arc\" , # Science questions \"mmlu\" , # Multitask language understanding \"truthfulqa\" # Truthful question answering ], batch_size = 1 , limit = 100 # Limit samples per task ) # Print results for task , metrics in results . items (): print ( f \" { task } : { metrics } \" ) Custom Task Evaluation \u00b6 from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom tasks tasks = [ EvalTask ( name = \"sentiment_analysis\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"IMDB sentiment analysis\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ), EvalTask ( name = \"text_generation\" , category = TaskCategory . TEXT_GENERATION , description = \"Alpaca instruction following\" , dataset_name = \"tatsu-lab/alpaca\" , input_column = \"instruction\" , target_column = \"output\" , metrics = [ \"bleu\" , \"rouge\" ] ) ] # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , tasks ) # Print results for task_name , metrics in results . items (): print ( f \" { task_name } : { metrics } \" ) Best Practices \u00b6 1. Evaluation During Training \u00b6 Set appropriate eval_interval (100-500 steps) Use validation split for evaluation Track best model with load_best_model_at_end 2. Comprehensive Evaluation \u00b6 Evaluate on multiple metrics Use both automatic and manual evaluation Test on diverse datasets 3. Zero-Shot Evaluation \u00b6 Test on unseen prompts Evaluate qualitative samples Check for common failure modes 4. Benchmark Evaluation \u00b6 Use standard benchmarks for comparison Run multiple benchmarks Document results for reproducibility 5. Custom Evaluation \u00b6 Create task-specific evaluation Use domain-specific metrics Validate on real-world data 6. RL Evaluation \u00b6 Always compare base and trained models Use consistent evaluation settings Use greedy decoding (temperature=0.0) for deterministic results Reuse dataset_dict when possible Match task_type and data_task_type for RL models Troubleshooting \u00b6 Evaluation Too Slow \u00b6 # Reduce batch size or max samples metrics = trainer . evaluate ( eval_dataset = eval_dataset , batch_size = 16 , # Reduce from default max_samples = 500 # Limit samples ) Out of Memory \u00b6 # Use smaller batch size config = EvalConfig ( batch_size = 8 , # Smaller batch max_samples = 100 , # Limit samples precision = \"fp16\" # Lower precision ) Missing Metrics \u00b6 # Specify metrics explicitly task = EvalTask ( name = \"my_task\" , category = TaskCategory . TEXT_CLASSIFICATION , metrics = [ \"accuracy\" , \"f1\" , \"precision\" , \"recall\" ] ) RL Evaluation Issues \u00b6 # Missing required columns for DPO # Fix: Ensure task types match config = EvalConfig ( task_type = \"dpo\" , data_task_type = \"dpo\" # Must match ) # KL divergence requires reference model config = EvalConfig ( metrics = [ \"kl_divergence\" ], reference_model_path = \"microsoft/phi-2\" ) # LoRA adapter not loading config = EvalConfig ( model_path = \"./lora_adapter\" , use_lora = True , base_model = \"microsoft/phi-2\" ) Next Steps \u00b6 SFT Guide - SFT training and evaluation RL Guide - RL training and evaluation Model Management - Model saving and loading Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Evaluation System - Detailed system docs","title":"Evaluation"},{"location":"user-guide/evaluation/#evaluation-guide","text":"Complete guide to evaluating fine-tuned models with AlignTune, covering training evaluation, standalone evaluation, benchmarks, and custom evaluation tasks.","title":"Evaluation Guide"},{"location":"user-guide/evaluation/#overview","text":"AlignTune provides comprehensive evaluation capabilities: Training Evaluation - Evaluate during training Standalone Evaluation - Evaluate saved models Benchmark Evaluation - Standard benchmarks (lm-eval) Custom Evaluation - Task-specific evaluation RL Evaluation - DPO, PPO, GRPO evaluation","title":"Overview"},{"location":"user-guide/evaluation/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/evaluation/#basic-training-evaluation","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) # Train with automatic evaluation trainer . train () # Evaluate on validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Training Evaluation"},{"location":"user-guide/evaluation/#standalone-model-evaluation","text":"from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" )","title":"Standalone Model Evaluation"},{"location":"user-guide/evaluation/#rl-model-evaluation","text":"from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , use_lora = True , base_model = \"microsoft/phi-2\" ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" )","title":"RL Model Evaluation"},{"location":"user-guide/evaluation/#evaluation-types","text":"","title":"Evaluation Types"},{"location":"user-guide/evaluation/#1-training-evaluation","text":"Automatic evaluation during training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 , # Evaluate every 100 steps save_interval = 500 # Save every 500 steps ) trainer . train () # Evaluation runs automatically","title":"1. Training Evaluation"},{"location":"user-guide/evaluation/#2-standalone-evaluation","text":"Evaluate after training: # After training metrics = trainer . evaluate () # With custom dataset from datasets import load_dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # With custom metric prefix metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" )","title":"2. Standalone Evaluation"},{"location":"user-guide/evaluation/#3-zero-shot-evaluation","text":"Generate predictions for test prompts: # Single prediction result = trainer . predict ( \"What is machine learning?\" ) print ( result ) # Batch predictions prompts = [ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"3. Zero-Shot Evaluation"},{"location":"user-guide/evaluation/#4-benchmark-evaluation","text":"Run standard benchmarks with lm-eval: from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run standard benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , \"arc\" , \"mmlu\" ], batch_size = 1 ) print ( results )","title":"4. Benchmark Evaluation"},{"location":"user-guide/evaluation/#5-custom-evaluation","text":"Create custom evaluation tasks: from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom task task = EvalTask ( name = \"custom_sentiment\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"Custom sentiment analysis task\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ) # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , [ task ])","title":"5. Custom Evaluation"},{"location":"user-guide/evaluation/#6-rl-evaluation","text":"Evaluate RL-trained models (DPO, PPO, GRPO):","title":"6. RL Evaluation"},{"location":"user-guide/evaluation/#dpo-evaluation","text":"from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results/dpo\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , data_task_type = \"dpo\" , # DPO metrics metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , # For KL divergence # Dataset dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , # Generation max_length = 1536 , temperature = 0.0 , batch_size = 2 , # Model loading use_lora = True , base_model = \"microsoft/phi-2\" , use_unsloth = True ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" )","title":"DPO Evaluation"},{"location":"user-guide/evaluation/#ppo-evaluation","text":"config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) print ( f \"Perplexity: { results . get ( 'perplexity' , 0.0 ) : .4f } \" )","title":"PPO Evaluation"},{"location":"user-guide/evaluation/#beforeafter-comparison","text":"from aligntune.core.backend_factory import create_rl_trainer # Train DPO model trainer = create_rl_trainer ( model_name = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = \"microsoft/phi-2\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = False , output_dir = \"./eval_results/base\" ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = True , base_model = \"microsoft/phi-2\" , output_dir = \"./eval_results/trained\" ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Compare print ( f \"Base: { base_results [ 'win_rate' ] : .2% } \" ) print ( f \"Trained: { trained_results [ 'win_rate' ] : .2% } \" ) print ( f \"Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" )","title":"Before/After Comparison"},{"location":"user-guide/evaluation/#evaluation-metrics","text":"","title":"Evaluation Metrics"},{"location":"user-guide/evaluation/#classification-metrics","text":"# For text classification metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: accuracy, f1, precision, recall","title":"Classification Metrics"},{"location":"user-guide/evaluation/#generation-metrics","text":"# For text generation metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: perplexity, loss, BLEU, ROUGE (if applicable)","title":"Generation Metrics"},{"location":"user-guide/evaluation/#rl-metrics","text":"","title":"RL Metrics"},{"location":"user-guide/evaluation/#dpo-metrics","text":"metrics = [ \"reward_margin\" , # Difference between chosen/rejected rewards \"preference_accuracy\" , # How often model prefers chosen \"win_rate\" , # % matching human preferences \"kl_divergence\" , # KL from reference (needs reference_model_path) \"log_ratio\" , # Log probability ratio \"implicit_reward\" , # Implicit reward signal \"calibration\" # Calibration score ]","title":"DPO Metrics"},{"location":"user-guide/evaluation/#ppo-metrics","text":"metrics = [ \"perplexity\" , # Language modeling quality \"reward_accuracy\" , # Alignment with reward model \"policy_entropy\" # Policy output diversity ]","title":"PPO Metrics"},{"location":"user-guide/evaluation/#rl-metrics_1","text":"metrics = [ \"kl_divergence\" , # KL divergence from reference \"reward_accuracy\" , # Reward model alignment \"policy_entropy\" # Output diversity ]","title":"RL Metrics"},{"location":"user-guide/evaluation/#custom-metrics","text":"from aligntune.eval.core import EvalTask , TaskCategory task = EvalTask ( name = \"custom_task\" , category = TaskCategory . TEXT_GENERATION , metrics = [ \"accuracy\" , \"f1\" , \"custom_metric\" ], custom_metrics = [ your_custom_metric_function ] )","title":"Custom Metrics"},{"location":"user-guide/evaluation/#evaluation-configuration","text":"","title":"Evaluation Configuration"},{"location":"user-guide/evaluation/#training-evaluation-config","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Evaluation settings eval_interval = 100 , # Evaluate every N steps save_interval = 500 , # Save every N steps load_best_model_at_end = True , # Load best model metric_for_best_model = \"eval_loss\" , # Metric to track greater_is_better = False # Lower is better for loss )","title":"Training Evaluation Config"},{"location":"user-guide/evaluation/#standalone-evaluation-config","text":"from aligntune.eval.core import EvalConfig , EvalType , TaskCategory config = EvalConfig ( eval_type = EvalType . STANDALONE , task_categories = [ TaskCategory . TEXT_GENERATION ], metrics = [ \"accuracy\" , \"f1\" , \"bleu\" , \"rouge\" ], batch_size = 32 , max_samples = 1000 , device = \"auto\" , precision = \"bf16\" , output_dir = \"./eval_results\" , save_predictions = True , save_metrics = True )","title":"Standalone Evaluation Config"},{"location":"user-guide/evaluation/#rl-evaluation-config","text":"from aligntune.eval.core import EvalConfig , run_eval config = EvalConfig ( # Model configuration model_path = \"./model\" , output_dir = \"./eval_results\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , # Evaluation task: dpo, ppo, grpo, text, math, code data_task_type = \"dpo\" , # Data schema: dpo, sft, grpo # Metrics metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = None , # Reference model (for KL) reward_model = None , # Reward model (optional) # Dataset dataset_name = \"dataset_name\" , dataset_config = None , split = \"test\" , max_samples = 100 , # Generation max_length = 2048 , # Context window max_new_tokens = 512 , # Generation length temperature = 0.0 , # Sampling temperature batch_size = 8 , # Model loading use_lora = False , base_model = None , use_unsloth = False , use_vllm = False , precision = \"bf16\" , # Advanced column_mapping = None , trust_remote_code = True ) results = run_eval ( config )","title":"RL Evaluation Config"},{"location":"user-guide/evaluation/#task-type-reference","text":"Supported task_type values for evaluation: Task Type Description Default Metrics Data Schema \"text\" Text generation perplexity, accuracy sft \"math\" Math problems math_accuracy, pass_at_k sft \"code\" Code generation pass_at_k code \"dpo\" DPO models reward_margin, preference_accuracy, win_rate dpo \"ppo\" PPO models perplexity, reward_accuracy sft \"generic\" Generic evaluation perplexity, accuracy sft Note : For RL evaluation, task_type and data_task_type must match (e.g., both \"dpo\" for DPO models).","title":"Task Type Reference"},{"location":"user-guide/evaluation/#complete-examples","text":"","title":"Complete Examples"},{"location":"user-guide/evaluation/#sft-evaluation","text":"from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 ) # Train trainer . train () # Evaluate on test set test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Test metrics: { metrics } \" ) # Zero-shot evaluation prompts = [ \"What is AI?\" , \"Explain machine learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \\n A: { result } \\n \" )","title":"SFT Evaluation"},{"location":"user-guide/evaluation/#rl-evaluation","text":"from aligntune.core.backend_factory import create_rl_trainer from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" # Train DPO model trainer = create_rl_trainer ( model_name = MODEL_NAME , dataset_name = DATASET , algorithm = \"dpo\" , backend = \"trl\" , output_dir = OUTPUT_DIR , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = MODEL_NAME , output_dir = f \" { OUTPUT_DIR } /eval_base\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = False , use_unsloth = True ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Print comparison print ( f \" \\n Base Model:\" ) print ( f \" Win Rate: { base_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Trained Model:\" ) print ( f \" Win Rate: { trained_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" )","title":"RL Evaluation"},{"location":"user-guide/evaluation/#benchmark-evaluation","text":"from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run multiple benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , # Commonsense reasoning \"arc\" , # Science questions \"mmlu\" , # Multitask language understanding \"truthfulqa\" # Truthful question answering ], batch_size = 1 , limit = 100 # Limit samples per task ) # Print results for task , metrics in results . items (): print ( f \" { task } : { metrics } \" )","title":"Benchmark Evaluation"},{"location":"user-guide/evaluation/#custom-task-evaluation","text":"from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom tasks tasks = [ EvalTask ( name = \"sentiment_analysis\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"IMDB sentiment analysis\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ), EvalTask ( name = \"text_generation\" , category = TaskCategory . TEXT_GENERATION , description = \"Alpaca instruction following\" , dataset_name = \"tatsu-lab/alpaca\" , input_column = \"instruction\" , target_column = \"output\" , metrics = [ \"bleu\" , \"rouge\" ] ) ] # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , tasks ) # Print results for task_name , metrics in results . items (): print ( f \" { task_name } : { metrics } \" )","title":"Custom Task Evaluation"},{"location":"user-guide/evaluation/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/evaluation/#1-evaluation-during-training","text":"Set appropriate eval_interval (100-500 steps) Use validation split for evaluation Track best model with load_best_model_at_end","title":"1. Evaluation During Training"},{"location":"user-guide/evaluation/#2-comprehensive-evaluation","text":"Evaluate on multiple metrics Use both automatic and manual evaluation Test on diverse datasets","title":"2. Comprehensive Evaluation"},{"location":"user-guide/evaluation/#3-zero-shot-evaluation_1","text":"Test on unseen prompts Evaluate qualitative samples Check for common failure modes","title":"3. Zero-Shot Evaluation"},{"location":"user-guide/evaluation/#4-benchmark-evaluation_1","text":"Use standard benchmarks for comparison Run multiple benchmarks Document results for reproducibility","title":"4. Benchmark Evaluation"},{"location":"user-guide/evaluation/#5-custom-evaluation_1","text":"Create task-specific evaluation Use domain-specific metrics Validate on real-world data","title":"5. Custom Evaluation"},{"location":"user-guide/evaluation/#6-rl-evaluation_1","text":"Always compare base and trained models Use consistent evaluation settings Use greedy decoding (temperature=0.0) for deterministic results Reuse dataset_dict when possible Match task_type and data_task_type for RL models","title":"6. RL Evaluation"},{"location":"user-guide/evaluation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/evaluation/#evaluation-too-slow","text":"# Reduce batch size or max samples metrics = trainer . evaluate ( eval_dataset = eval_dataset , batch_size = 16 , # Reduce from default max_samples = 500 # Limit samples )","title":"Evaluation Too Slow"},{"location":"user-guide/evaluation/#out-of-memory","text":"# Use smaller batch size config = EvalConfig ( batch_size = 8 , # Smaller batch max_samples = 100 , # Limit samples precision = \"fp16\" # Lower precision )","title":"Out of Memory"},{"location":"user-guide/evaluation/#missing-metrics","text":"# Specify metrics explicitly task = EvalTask ( name = \"my_task\" , category = TaskCategory . TEXT_CLASSIFICATION , metrics = [ \"accuracy\" , \"f1\" , \"precision\" , \"recall\" ] )","title":"Missing Metrics"},{"location":"user-guide/evaluation/#rl-evaluation-issues","text":"# Missing required columns for DPO # Fix: Ensure task types match config = EvalConfig ( task_type = \"dpo\" , data_task_type = \"dpo\" # Must match ) # KL divergence requires reference model config = EvalConfig ( metrics = [ \"kl_divergence\" ], reference_model_path = \"microsoft/phi-2\" ) # LoRA adapter not loading config = EvalConfig ( model_path = \"./lora_adapter\" , use_lora = True , base_model = \"microsoft/phi-2\" )","title":"RL Evaluation Issues"},{"location":"user-guide/evaluation/#next-steps","text":"SFT Guide - SFT training and evaluation RL Guide - RL training and evaluation Model Management - Model saving and loading","title":"Next Steps"},{"location":"user-guide/evaluation/#additional-resources","text":"API Reference - API documentation Examples - Code examples Evaluation System - Detailed system docs","title":"Additional Resources"},{"location":"user-guide/model-management/","text":"Model Management \u00b6 Learn how to save, load, and share your trained models with AlignTune. Saving Models \u00b6 Basic Save \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( ... ) trainer . train () # Save model path = trainer . save_model () print ( f \"Model saved to: { path } \" ) Custom Save Path \u00b6 # Save to custom directory path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" ) What Gets Saved \u00b6 When you call save_model() , AlignTune saves: Model weights ( pytorch_model.bin or model.safetensors ) Tokenizer files ( tokenizer.json , tokenizer_config.json , etc.) Model configuration ( config.json ) Training configuration ( training_config.yaml ) Task metadata (if applicable) Loading Models \u00b6 Load Saved Model \u00b6 from transformers import AutoModelForCausalLM , AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) Load with AlignTune \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer and load model trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Model is automatically loaded trainer . predict ( \"Hello!\" ) Pushing to HuggingFace Hub \u00b6 Basic Push \u00b6 trainer = create_sft_trainer ( ... ) trainer . train () trainer . save_model () # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-model\" , private = False ) print ( f \"Model available at: { url } \" ) Private Repository \u00b6 url = trainer . push_to_hub ( repo_id = \"username/my-private-model\" , private = True , token = \"hf_...\" # Your HuggingFace token ) Custom Commit Message \u00b6 url = trainer . push_to_hub ( repo_id = \"username/my-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" ) Generating Predictions \u00b6 Single Prediction \u00b6 # Generate from single input result = trainer . predict ( \"What is machine learning?\" ) print ( result ) Batch Predictions \u00b6 # Generate from multiple inputs results = trainer . predict ([ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ]) for result in results : print ( result ) Custom Generation Parameters \u00b6 result = trainer . predict ( \"What is machine learning?\" , max_new_tokens = 200 , temperature = 0.7 , top_p = 0.9 , do_sample = True ) Model Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on default validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation Dataset \u00b6 from datasets import load_dataset # Load custom evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( metrics ) Custom Metric Prefix \u00b6 metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) # Metrics will have \"test/\" prefix Checkpointing \u00b6 Save Checkpoints \u00b6 Checkpoints are automatically saved during training: trainer = create_sft_trainer ( ... , save_interval = 500 # Save every 500 steps ) trainer . train () Load Checkpoint \u00b6 # Resume from checkpoint trainer . load_checkpoint ( \"./output/checkpoint-1000\" ) Model Information \u00b6 Get Model Path \u00b6 # After saving path = trainer . save_model () print ( f \"Model path: { path } \" ) Check Model Status \u00b6 # Check if model is loaded if trainer . model is not None : print ( \"Model is loaded\" ) print ( f \"Device: { next ( trainer . model . parameters ()) . device } \" ) Best Practices \u00b6 1. Save Regularly \u00b6 # Save after training trainer . train () trainer . save_model () # Also save checkpoints during training # (automatic with save_interval) 2. Version Your Models \u00b6 import datetime timestamp = datetime . datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) path = trainer . save_model ( output_dir = f \"./models/experiment_ { timestamp } \" ) 3. Push Important Models \u00b6 # Push production models to Hub if is_production_model : trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) 4. Document Your Models \u00b6 # Save with metadata trainer . save_model () # Edit training_config.yaml to add notes Troubleshooting \u00b6 Model Not Found \u00b6 # Check if model exists from pathlib import Path model_path = Path ( \"./output/my_model\" ) if not model_path . exists (): print ( \"Model not found!\" ) Push to Hub Fails \u00b6 # Ensure you're logged in from huggingface_hub import login login ( token = \"hf_...\" ) # Then push trainer . push_to_hub ( \"username/my-model\" ) Memory Issues \u00b6 # Use smaller batch size for prediction result = trainer . predict ( \"Your input\" , max_new_tokens = 50 # Reduce if needed ) Next Steps \u00b6 Evaluation Guide - Learn about evaluation metrics SFT Guide - SFT-specific model management RL Guide - RL-specific model management","title":"Model Management"},{"location":"user-guide/model-management/#model-management","text":"Learn how to save, load, and share your trained models with AlignTune.","title":"Model Management"},{"location":"user-guide/model-management/#saving-models","text":"","title":"Saving Models"},{"location":"user-guide/model-management/#basic-save","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( ... ) trainer . train () # Save model path = trainer . save_model () print ( f \"Model saved to: { path } \" )","title":"Basic Save"},{"location":"user-guide/model-management/#custom-save-path","text":"# Save to custom directory path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" )","title":"Custom Save Path"},{"location":"user-guide/model-management/#what-gets-saved","text":"When you call save_model() , AlignTune saves: Model weights ( pytorch_model.bin or model.safetensors ) Tokenizer files ( tokenizer.json , tokenizer_config.json , etc.) Model configuration ( config.json ) Training configuration ( training_config.yaml ) Task metadata (if applicable)","title":"What Gets Saved"},{"location":"user-guide/model-management/#loading-models","text":"","title":"Loading Models"},{"location":"user-guide/model-management/#load-saved-model","text":"from transformers import AutoModelForCausalLM , AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" )","title":"Load Saved Model"},{"location":"user-guide/model-management/#load-with-aligntune","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer and load model trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Model is automatically loaded trainer . predict ( \"Hello!\" )","title":"Load with AlignTune"},{"location":"user-guide/model-management/#pushing-to-huggingface-hub","text":"","title":"Pushing to HuggingFace Hub"},{"location":"user-guide/model-management/#basic-push","text":"trainer = create_sft_trainer ( ... ) trainer . train () trainer . save_model () # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-model\" , private = False ) print ( f \"Model available at: { url } \" )","title":"Basic Push"},{"location":"user-guide/model-management/#private-repository","text":"url = trainer . push_to_hub ( repo_id = \"username/my-private-model\" , private = True , token = \"hf_...\" # Your HuggingFace token )","title":"Private Repository"},{"location":"user-guide/model-management/#custom-commit-message","text":"url = trainer . push_to_hub ( repo_id = \"username/my-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" )","title":"Custom Commit Message"},{"location":"user-guide/model-management/#generating-predictions","text":"","title":"Generating Predictions"},{"location":"user-guide/model-management/#single-prediction","text":"# Generate from single input result = trainer . predict ( \"What is machine learning?\" ) print ( result )","title":"Single Prediction"},{"location":"user-guide/model-management/#batch-predictions","text":"# Generate from multiple inputs results = trainer . predict ([ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ]) for result in results : print ( result )","title":"Batch Predictions"},{"location":"user-guide/model-management/#custom-generation-parameters","text":"result = trainer . predict ( \"What is machine learning?\" , max_new_tokens = 200 , temperature = 0.7 , top_p = 0.9 , do_sample = True )","title":"Custom Generation Parameters"},{"location":"user-guide/model-management/#model-evaluation","text":"","title":"Model Evaluation"},{"location":"user-guide/model-management/#basic-evaluation","text":"# Evaluate on default validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/model-management/#custom-evaluation-dataset","text":"from datasets import load_dataset # Load custom evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( metrics )","title":"Custom Evaluation Dataset"},{"location":"user-guide/model-management/#custom-metric-prefix","text":"metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) # Metrics will have \"test/\" prefix","title":"Custom Metric Prefix"},{"location":"user-guide/model-management/#checkpointing","text":"","title":"Checkpointing"},{"location":"user-guide/model-management/#save-checkpoints","text":"Checkpoints are automatically saved during training: trainer = create_sft_trainer ( ... , save_interval = 500 # Save every 500 steps ) trainer . train ()","title":"Save Checkpoints"},{"location":"user-guide/model-management/#load-checkpoint","text":"# Resume from checkpoint trainer . load_checkpoint ( \"./output/checkpoint-1000\" )","title":"Load Checkpoint"},{"location":"user-guide/model-management/#model-information","text":"","title":"Model Information"},{"location":"user-guide/model-management/#get-model-path","text":"# After saving path = trainer . save_model () print ( f \"Model path: { path } \" )","title":"Get Model Path"},{"location":"user-guide/model-management/#check-model-status","text":"# Check if model is loaded if trainer . model is not None : print ( \"Model is loaded\" ) print ( f \"Device: { next ( trainer . model . parameters ()) . device } \" )","title":"Check Model Status"},{"location":"user-guide/model-management/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/model-management/#1-save-regularly","text":"# Save after training trainer . train () trainer . save_model () # Also save checkpoints during training # (automatic with save_interval)","title":"1. Save Regularly"},{"location":"user-guide/model-management/#2-version-your-models","text":"import datetime timestamp = datetime . datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) path = trainer . save_model ( output_dir = f \"./models/experiment_ { timestamp } \" )","title":"2. Version Your Models"},{"location":"user-guide/model-management/#3-push-important-models","text":"# Push production models to Hub if is_production_model : trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False )","title":"3. Push Important Models"},{"location":"user-guide/model-management/#4-document-your-models","text":"# Save with metadata trainer . save_model () # Edit training_config.yaml to add notes","title":"4. Document Your Models"},{"location":"user-guide/model-management/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/model-management/#model-not-found","text":"# Check if model exists from pathlib import Path model_path = Path ( \"./output/my_model\" ) if not model_path . exists (): print ( \"Model not found!\" )","title":"Model Not Found"},{"location":"user-guide/model-management/#push-to-hub-fails","text":"# Ensure you're logged in from huggingface_hub import login login ( token = \"hf_...\" ) # Then push trainer . push_to_hub ( \"username/my-model\" )","title":"Push to Hub Fails"},{"location":"user-guide/model-management/#memory-issues","text":"# Use smaller batch size for prediction result = trainer . predict ( \"Your input\" , max_new_tokens = 50 # Reduce if needed )","title":"Memory Issues"},{"location":"user-guide/model-management/#next-steps","text":"Evaluation Guide - Learn about evaluation metrics SFT Guide - SFT-specific model management RL Guide - RL-specific model management","title":"Next Steps"},{"location":"user-guide/overview/","text":"User Guide Overview \u00b6 Welcome to the AlignTune User Guide! This section provides comprehensive tutorials and guides for using AlignTune effectively. What is AlignTune? \u00b6 AlignTune is a production-ready fine-tuning library for Large Language Models (LLMs) that supports: Supervised Fine-Tuning (SFT) : Adapt pre-trained models to specific tasks Reinforcement Learning (RL) : Align models with human preferences using RLHF algorithms Multi-Backend Support : Choose between TRL (reliable) and Unsloth (faster) backends RL Algorithms : DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Core Components \u00b6 1. Backend Factory \u00b6 The BackendFactory is the main entry point for creating trainers: from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Create RL trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) 2. Configuration System \u00b6 AlignTune supports three configuration methods: Python API : Direct function calls with keyword arguments YAML Files : Declarative configuration files CLI : Command-line interface 3. Reward System \u00b6 27+ built-in reward functions for quality, safety, style, and task-specific metrics. 4. Evaluation System \u00b6 Comprehensive evaluation with basic metrics, quality metrics, and safety metrics. Getting Started \u00b6 For SFT Training \u00b6 Start with SFT Guide : Complete guide to Supervised Fine-Tuning Learn Configuration : Understand configuration options Explore Examples : See real-world examples For RL Training \u00b6 Start with RL Guide : Complete guide to Reinforcement Learning Learn Reward Functions : Understand reward functions Explore RL Examples : See RL training examples Guide Structure \u00b6 Supervised Fine-Tuning (SFT) \u00b6 SFT Guide : Complete SFT training guide Task types (instruction following, classification, chat) Configuration options Best practices Examples Reinforcement Learning (RL) \u00b6 RL Guide : Complete RL training guide Algorithm overview (DPO, PPO, GRPO, etc.) Configuration options Best practices Examples Reward Functions \u00b6 Reward Functions : Using reward functions Built-in reward functions Custom reward functions Composite rewards Reward Model Training \u00b6 Reward Model Training : Training custom reward models Training from rule-based functions Integration with PPO Best practices Evaluation \u00b6 Evaluation : Model evaluation Basic metrics Quality metrics Safety metrics Task-specific metrics Model Management \u00b6 Model Management : Saving, loading, and sharing models Local model saving HuggingFace Hub integration Checkpoint management Sample Logging \u00b6 Sample Logging : Qualitative sample generation Configuration Best practices Examples Troubleshooting \u00b6 Troubleshooting : Common issues and solutions Backend issues CUDA/GPU issues Configuration issues Training issues Quick Reference \u00b6 Common Patterns \u00b6 SFT Training Pattern \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () trainer . evaluate () trainer . save_model () DPO Training Pattern \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model () PPO Training Pattern \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model () Next Steps \u00b6 Getting Started : Installation and setup Basic Concepts : Core concepts SFT Guide : Start with SFT training RL Guide : Explore RL training Examples : See real-world examples Additional Resources \u00b6 API Reference : Complete API documentation CLI Overview : Command-line interface API Reference : Complete API documentation Examples : Code examples and tutorials Advanced Topics : Architecture and advanced usage Contributing : Contribute to AlignTune Ready to start? Begin with SFT Guide or RL Guide !","title":"Overview"},{"location":"user-guide/overview/#user-guide-overview","text":"Welcome to the AlignTune User Guide! This section provides comprehensive tutorials and guides for using AlignTune effectively.","title":"User Guide Overview"},{"location":"user-guide/overview/#what-is-aligntune","text":"AlignTune is a production-ready fine-tuning library for Large Language Models (LLMs) that supports: Supervised Fine-Tuning (SFT) : Adapt pre-trained models to specific tasks Reinforcement Learning (RL) : Align models with human preferences using RLHF algorithms Multi-Backend Support : Choose between TRL (reliable) and Unsloth (faster) backends RL Algorithms : DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO","title":"What is AlignTune?"},{"location":"user-guide/overview/#core-components","text":"","title":"Core Components"},{"location":"user-guide/overview/#1-backend-factory","text":"The BackendFactory is the main entry point for creating trainers: from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Create RL trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"1. Backend Factory"},{"location":"user-guide/overview/#2-configuration-system","text":"AlignTune supports three configuration methods: Python API : Direct function calls with keyword arguments YAML Files : Declarative configuration files CLI : Command-line interface","title":"2. Configuration System"},{"location":"user-guide/overview/#3-reward-system","text":"27+ built-in reward functions for quality, safety, style, and task-specific metrics.","title":"3. Reward System"},{"location":"user-guide/overview/#4-evaluation-system","text":"Comprehensive evaluation with basic metrics, quality metrics, and safety metrics.","title":"4. Evaluation System"},{"location":"user-guide/overview/#getting-started","text":"","title":"Getting Started"},{"location":"user-guide/overview/#for-sft-training","text":"Start with SFT Guide : Complete guide to Supervised Fine-Tuning Learn Configuration : Understand configuration options Explore Examples : See real-world examples","title":"For SFT Training"},{"location":"user-guide/overview/#for-rl-training","text":"Start with RL Guide : Complete guide to Reinforcement Learning Learn Reward Functions : Understand reward functions Explore RL Examples : See RL training examples","title":"For RL Training"},{"location":"user-guide/overview/#guide-structure","text":"","title":"Guide Structure"},{"location":"user-guide/overview/#supervised-fine-tuning-sft","text":"SFT Guide : Complete SFT training guide Task types (instruction following, classification, chat) Configuration options Best practices Examples","title":"Supervised Fine-Tuning (SFT)"},{"location":"user-guide/overview/#reinforcement-learning-rl","text":"RL Guide : Complete RL training guide Algorithm overview (DPO, PPO, GRPO, etc.) Configuration options Best practices Examples","title":"Reinforcement Learning (RL)"},{"location":"user-guide/overview/#reward-functions","text":"Reward Functions : Using reward functions Built-in reward functions Custom reward functions Composite rewards","title":"Reward Functions"},{"location":"user-guide/overview/#reward-model-training","text":"Reward Model Training : Training custom reward models Training from rule-based functions Integration with PPO Best practices","title":"Reward Model Training"},{"location":"user-guide/overview/#evaluation","text":"Evaluation : Model evaluation Basic metrics Quality metrics Safety metrics Task-specific metrics","title":"Evaluation"},{"location":"user-guide/overview/#model-management","text":"Model Management : Saving, loading, and sharing models Local model saving HuggingFace Hub integration Checkpoint management","title":"Model Management"},{"location":"user-guide/overview/#sample-logging","text":"Sample Logging : Qualitative sample generation Configuration Best practices Examples","title":"Sample Logging"},{"location":"user-guide/overview/#troubleshooting","text":"Troubleshooting : Common issues and solutions Backend issues CUDA/GPU issues Configuration issues Training issues","title":"Troubleshooting"},{"location":"user-guide/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"user-guide/overview/#common-patterns","text":"","title":"Common Patterns"},{"location":"user-guide/overview/#sft-training-pattern","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"SFT Training Pattern"},{"location":"user-guide/overview/#dpo-training-pattern","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"DPO Training Pattern"},{"location":"user-guide/overview/#ppo-training-pattern","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"PPO Training Pattern"},{"location":"user-guide/overview/#next-steps","text":"Getting Started : Installation and setup Basic Concepts : Core concepts SFT Guide : Start with SFT training RL Guide : Explore RL training Examples : See real-world examples","title":"Next Steps"},{"location":"user-guide/overview/#additional-resources","text":"API Reference : Complete API documentation CLI Overview : Command-line interface API Reference : Complete API documentation Examples : Code examples and tutorials Advanced Topics : Architecture and advanced usage Contributing : Contribute to AlignTune Ready to start? Begin with SFT Guide or RL Guide !","title":"Additional Resources"},{"location":"user-guide/reward-functions/","text":"Reward Functions Guide \u00b6 Complete guide to using reward functions in AlignTune for RLHF training. Overview \u00b6 Reward functions measure the quality of model outputs and guide reinforcement learning training. AlignTune provides 27+ prebuilt reward functions covering quality, safety, task-specific metrics, and more. Quick Start \u00b6 Using Reward Functions in PPO \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # Reward functions for custom reward model training reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], num_epochs = 1 , batch_size = 1 ) trainer . train () Using Reward Functions in Reward Model Training \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions from registry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer with reward functions trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 ) Reward Function Categories \u00b6 Basic Quality Rewards \u00b6 Length Reward \u00b6 Encourages appropriate response length. reward_functions = [ \"length\" ] # With parameters reward_config = { \"type\" : \"length\" , \"params\" : { \"min_length\" : 10 , \"max_length\" : 500 }, \"weight\" : 1.0 } Coherence Reward \u00b6 Measures logical flow and coherence. reward_functions = [ \"coherence\" ] Fluency Reward \u00b6 Measures grammatical correctness. reward_functions = [ \"fluency\" ] Task-Specific Rewards \u00b6 Sentiment Reward \u00b6 Encourages specific sentiment. reward_functions = [ \"sentiment\" ] # With target sentiment reward_config = { \"type\" : \"sentiment\" , \"params\" : { \"target_sentiment\" : \"positive\" }, \"weight\" : 1.0 } Safety Reward \u00b6 Detects and penalizes harmful content. reward_functions = [ \"safety\" ] # With strict mode reward_config = { \"type\" : \"safety\" , \"params\" : { \"strict\" : True }, \"weight\" : 2.0 } Factuality Reward \u00b6 Measures factual accuracy. reward_functions = [ \"factuality\" ] Generation Quality Metrics \u00b6 BLEU Reward \u00b6 Measures n-gram overlap with reference. reward_functions = [ \"bleu\" ] reward_config = { \"type\" : \"bleu\" , \"params\" : { \"n_gram\" : 4 }, \"weight\" : 0.8 } ROUGE Reward \u00b6 Measures recall-oriented evaluation. reward_functions = [ \"rouge\" ] reward_config = { \"type\" : \"rouge\" , \"params\" : { \"rouge_type\" : \"rouge-l\" }, \"weight\" : 1.0 } Code Quality Rewards \u00b6 Code Syntax Reward \u00b6 Validates code syntax correctness. reward_functions = [ \"code_syntax\" ] reward_config = { \"type\" : \"code_syntax\" , \"params\" : { \"language\" : \"python\" }, \"weight\" : 1.0 } Code Execution Reward \u00b6 Tests if code runs without errors. reward_functions = [ \"code_execution\" ] reward_config = { \"type\" : \"code_execution\" , \"params\" : { \"timeout\" : 5 , \"safe_mode\" : True }, \"weight\" : 1.5 } Math and Reasoning Rewards \u00b6 Math Correctness Reward \u00b6 Validates mathematical correctness. reward_functions = [ \"math_correctness\" ] reward_config = { \"type\" : \"math_correctness\" , \"params\" : { \"tolerance\" : 1e-6 }, \"weight\" : 1.0 } Logical Consistency Reward \u00b6 Measures logical consistency. reward_functions = [ \"logical_consistency\" ] Composite Rewards \u00b6 Combine multiple reward functions with weights: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Multiple reward functions reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" , \"helpfulness\" ], reward_function_weights = [ 0.2 , # length 0.2 , # sentiment 0.3 , # safety (higher weight) 0.15 , # coherence 0.15 # helpfulness ], train_custom_reward_model = True , reward_training_texts = training_texts , reward_training_base_model = \"microsoft/DialoGPT-medium\" ) Using Reward Registry \u00b6 Get Available Functions \u00b6 from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () # List all available functions available = registry . list_rewards () print ( available ) # Get specific function length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) Create Custom Reward Function \u00b6 from aligntune.rewards import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry from aligntune.core.backend_factory import create_rl_trainer class CustomReward ( RewardFunction ): def __init__ ( self ): super () . __init__ ( RewardType . CUSTOM ) def compute ( self , text : str , ** kwargs ) -> float : # Your custom reward logic score = 0.0 # ... compute score based on text return score # Register custom function RewardRegistry . register_custom_reward ( \"custom\" , CustomReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_functions = [ \"custom\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] ) Best Practices \u00b6 1. Choose Relevant Rewards \u00b6 Select rewards that match your task: Customer Service : sentiment, politeness, helpfulness Code Generation : code_syntax, code_execution, code_completeness Math Problems : math_correctness, logical_consistency Safety-Critical : safety, toxicity, bias 2. Weight Balancing \u00b6 Balance reward weights based on importance: # Safety is critical - higher weight reward_function_weights = [ 0.1 , 0.1 , 0.5 , 0.2 , 0.1 ] # length, sentiment, safety, coherence, helpfulness 3. Start Simple \u00b6 Begin with a few key rewards, then expand: # Start with basics reward_functions = [ \"length\" , \"safety\" ] # Add more as needed reward_functions = [ \"length\" , \"safety\" , \"sentiment\" , \"coherence\" ] 4. Test Reward Functions \u00b6 Test rewards independently before combining: from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) # Test on sample text test_text = \"This is a test response.\" score = length_func . compute ( test_text ) print ( f \"Length reward: { score } \" ) Complete Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain.\" , # ... more texts ] # Create trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model with multiple functions train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest weight) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train trainer . train () Available Reward Functions \u00b6 For a complete list of all 27+ reward functions, see the Reward Functions Reference . Categories: - Basic Quality (length, coherence, fluency) - Task-Specific (sentiment, safety, factuality, bias) - Generation Metrics (BLEU, ROUGE, METEOR, BERTScore) - Code Quality (syntax, execution, completeness) - Math & Reasoning (correctness, logical consistency, commonsense) - Specialized (hallucination, toxicity, politeness, helpfulness, honesty) Next Steps \u00b6 Reward Model Training - Train custom reward models RL Training Guide - Complete RL training guide Reward Functions Reference - Complete function list Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Reward Model Training System - Detailed system docs","title":"Reward Functions"},{"location":"user-guide/reward-functions/#reward-functions-guide","text":"Complete guide to using reward functions in AlignTune for RLHF training.","title":"Reward Functions Guide"},{"location":"user-guide/reward-functions/#overview","text":"Reward functions measure the quality of model outputs and guide reinforcement learning training. AlignTune provides 27+ prebuilt reward functions covering quality, safety, task-specific metrics, and more.","title":"Overview"},{"location":"user-guide/reward-functions/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/reward-functions/#using-reward-functions-in-ppo","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # Reward functions for custom reward model training reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], num_epochs = 1 , batch_size = 1 ) trainer . train ()","title":"Using Reward Functions in PPO"},{"location":"user-guide/reward-functions/#using-reward-functions-in-reward-model-training","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions from registry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer with reward functions trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 )","title":"Using Reward Functions in Reward Model Training"},{"location":"user-guide/reward-functions/#reward-function-categories","text":"","title":"Reward Function Categories"},{"location":"user-guide/reward-functions/#basic-quality-rewards","text":"","title":"Basic Quality Rewards"},{"location":"user-guide/reward-functions/#length-reward","text":"Encourages appropriate response length. reward_functions = [ \"length\" ] # With parameters reward_config = { \"type\" : \"length\" , \"params\" : { \"min_length\" : 10 , \"max_length\" : 500 }, \"weight\" : 1.0 }","title":"Length Reward"},{"location":"user-guide/reward-functions/#coherence-reward","text":"Measures logical flow and coherence. reward_functions = [ \"coherence\" ]","title":"Coherence Reward"},{"location":"user-guide/reward-functions/#fluency-reward","text":"Measures grammatical correctness. reward_functions = [ \"fluency\" ]","title":"Fluency Reward"},{"location":"user-guide/reward-functions/#task-specific-rewards","text":"","title":"Task-Specific Rewards"},{"location":"user-guide/reward-functions/#sentiment-reward","text":"Encourages specific sentiment. reward_functions = [ \"sentiment\" ] # With target sentiment reward_config = { \"type\" : \"sentiment\" , \"params\" : { \"target_sentiment\" : \"positive\" }, \"weight\" : 1.0 }","title":"Sentiment Reward"},{"location":"user-guide/reward-functions/#safety-reward","text":"Detects and penalizes harmful content. reward_functions = [ \"safety\" ] # With strict mode reward_config = { \"type\" : \"safety\" , \"params\" : { \"strict\" : True }, \"weight\" : 2.0 }","title":"Safety Reward"},{"location":"user-guide/reward-functions/#factuality-reward","text":"Measures factual accuracy. reward_functions = [ \"factuality\" ]","title":"Factuality Reward"},{"location":"user-guide/reward-functions/#generation-quality-metrics","text":"","title":"Generation Quality Metrics"},{"location":"user-guide/reward-functions/#bleu-reward","text":"Measures n-gram overlap with reference. reward_functions = [ \"bleu\" ] reward_config = { \"type\" : \"bleu\" , \"params\" : { \"n_gram\" : 4 }, \"weight\" : 0.8 }","title":"BLEU Reward"},{"location":"user-guide/reward-functions/#rouge-reward","text":"Measures recall-oriented evaluation. reward_functions = [ \"rouge\" ] reward_config = { \"type\" : \"rouge\" , \"params\" : { \"rouge_type\" : \"rouge-l\" }, \"weight\" : 1.0 }","title":"ROUGE Reward"},{"location":"user-guide/reward-functions/#code-quality-rewards","text":"","title":"Code Quality Rewards"},{"location":"user-guide/reward-functions/#code-syntax-reward","text":"Validates code syntax correctness. reward_functions = [ \"code_syntax\" ] reward_config = { \"type\" : \"code_syntax\" , \"params\" : { \"language\" : \"python\" }, \"weight\" : 1.0 }","title":"Code Syntax Reward"},{"location":"user-guide/reward-functions/#code-execution-reward","text":"Tests if code runs without errors. reward_functions = [ \"code_execution\" ] reward_config = { \"type\" : \"code_execution\" , \"params\" : { \"timeout\" : 5 , \"safe_mode\" : True }, \"weight\" : 1.5 }","title":"Code Execution Reward"},{"location":"user-guide/reward-functions/#math-and-reasoning-rewards","text":"","title":"Math and Reasoning Rewards"},{"location":"user-guide/reward-functions/#math-correctness-reward","text":"Validates mathematical correctness. reward_functions = [ \"math_correctness\" ] reward_config = { \"type\" : \"math_correctness\" , \"params\" : { \"tolerance\" : 1e-6 }, \"weight\" : 1.0 }","title":"Math Correctness Reward"},{"location":"user-guide/reward-functions/#logical-consistency-reward","text":"Measures logical consistency. reward_functions = [ \"logical_consistency\" ]","title":"Logical Consistency Reward"},{"location":"user-guide/reward-functions/#composite-rewards","text":"Combine multiple reward functions with weights: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Multiple reward functions reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" , \"helpfulness\" ], reward_function_weights = [ 0.2 , # length 0.2 , # sentiment 0.3 , # safety (higher weight) 0.15 , # coherence 0.15 # helpfulness ], train_custom_reward_model = True , reward_training_texts = training_texts , reward_training_base_model = \"microsoft/DialoGPT-medium\" )","title":"Composite Rewards"},{"location":"user-guide/reward-functions/#using-reward-registry","text":"","title":"Using Reward Registry"},{"location":"user-guide/reward-functions/#get-available-functions","text":"from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () # List all available functions available = registry . list_rewards () print ( available ) # Get specific function length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" )","title":"Get Available Functions"},{"location":"user-guide/reward-functions/#create-custom-reward-function","text":"from aligntune.rewards import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry from aligntune.core.backend_factory import create_rl_trainer class CustomReward ( RewardFunction ): def __init__ ( self ): super () . __init__ ( RewardType . CUSTOM ) def compute ( self , text : str , ** kwargs ) -> float : # Your custom reward logic score = 0.0 # ... compute score based on text return score # Register custom function RewardRegistry . register_custom_reward ( \"custom\" , CustomReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_functions = [ \"custom\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] )","title":"Create Custom Reward Function"},{"location":"user-guide/reward-functions/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/reward-functions/#1-choose-relevant-rewards","text":"Select rewards that match your task: Customer Service : sentiment, politeness, helpfulness Code Generation : code_syntax, code_execution, code_completeness Math Problems : math_correctness, logical_consistency Safety-Critical : safety, toxicity, bias","title":"1. Choose Relevant Rewards"},{"location":"user-guide/reward-functions/#2-weight-balancing","text":"Balance reward weights based on importance: # Safety is critical - higher weight reward_function_weights = [ 0.1 , 0.1 , 0.5 , 0.2 , 0.1 ] # length, sentiment, safety, coherence, helpfulness","title":"2. Weight Balancing"},{"location":"user-guide/reward-functions/#3-start-simple","text":"Begin with a few key rewards, then expand: # Start with basics reward_functions = [ \"length\" , \"safety\" ] # Add more as needed reward_functions = [ \"length\" , \"safety\" , \"sentiment\" , \"coherence\" ]","title":"3. Start Simple"},{"location":"user-guide/reward-functions/#4-test-reward-functions","text":"Test rewards independently before combining: from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) # Test on sample text test_text = \"This is a test response.\" score = length_func . compute ( test_text ) print ( f \"Length reward: { score } \" )","title":"4. Test Reward Functions"},{"location":"user-guide/reward-functions/#complete-example","text":"from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain.\" , # ... more texts ] # Create trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model with multiple functions train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest weight) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train trainer . train ()","title":"Complete Example"},{"location":"user-guide/reward-functions/#available-reward-functions","text":"For a complete list of all 27+ reward functions, see the Reward Functions Reference . Categories: - Basic Quality (length, coherence, fluency) - Task-Specific (sentiment, safety, factuality, bias) - Generation Metrics (BLEU, ROUGE, METEOR, BERTScore) - Code Quality (syntax, execution, completeness) - Math & Reasoning (correctness, logical consistency, commonsense) - Specialized (hallucination, toxicity, politeness, helpfulness, honesty)","title":"Available Reward Functions"},{"location":"user-guide/reward-functions/#next-steps","text":"Reward Model Training - Train custom reward models RL Training Guide - Complete RL training guide Reward Functions Reference - Complete function list","title":"Next Steps"},{"location":"user-guide/reward-functions/#additional-resources","text":"API Reference - API documentation Examples - Code examples Reward Model Training System - Detailed system docs","title":"Additional Resources"},{"location":"user-guide/reward-model-training/","text":"Reward Model Training Guide \u00b6 Complete guide to training custom reward models from rule-based reward functions and integrating them into PPO training. Overview \u00b6 Reward models learn to score text quality based on training data generated from rule-based reward functions. AlignTune provides a complete system for training reward models and seamlessly integrating them into PPO training pipelines. Quick Start \u00b6 Standalone Reward Model Training \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_texts = [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" ] training_data = trainer . generate_training_data ( texts = training_texts , batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Reward model saved to: { model_path } \" ) PPO with Custom Reward Model Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load your training texts.\"\"\" return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train (reward model is trained first, then PPO) trainer . train () Training Modes \u00b6 1. Standalone Training \u00b6 Train reward models independently for later use: from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_training_texts , batch_size = 32 ) # Train model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/my_model\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) 2. Integrated PPO Training \u00b6 Train reward model as part of PPO training: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" ) Configuration \u00b6 Reward Model Training Config \u00b6 from aligntune.core.rl.config import RewardModelTrainingConfig config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , # Required training_texts = your_texts , # Required reward_functions = [ \"length\" , \"sentiment\" ], # Required output_dir = \"./reward_models/custom\" , # Required # Optional parameters reward_weights = [ 0.5 , 0.5 ], num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 , gradient_accumulation_steps = 4 , max_length = 512 ) Reward Model Source Config \u00b6 from aligntune.core.rl.config import RewardModelSourceConfig , RewardModelTrainingConfig # For custom training training_config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , training_texts = texts , reward_functions = [ \"length\" , \"safety\" ], output_dir = \"./reward_models/custom\" ) source_config = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) # For HuggingFace Hub source_config = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # For local model source_config = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = \"./reward_models/my_model\" ) Using Pre-trained Reward Models \u00b6 From HuggingFace Hub \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) From Local Path \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = \"./reward_models/my_custom_model\" ) Training Data Generation \u00b6 Basic Generation \u00b6 from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_texts , batch_size = 32 ) With Reference Texts \u00b6 training_data = trainer . generate_training_data ( texts = your_texts , reference_texts = reference_texts , # Optional reference texts batch_size = 32 ) Best Practices \u00b6 1. Training Data Quality \u00b6 Use diverse, representative texts Include both good and bad examples Ensure sufficient quantity (100+ texts recommended) 2. Reward Function Selection \u00b6 Choose functions relevant to your task Balance weights appropriately Start with a few key functions, expand as needed 3. Model Selection \u00b6 Use base models compatible with your policy model Consider model family consistency (for PPO) Smaller models work well for reward models 4. Training Parameters \u00b6 # Recommended settings num_epochs = 3 # 3-5 epochs usually sufficient learning_rate = 1e-5 # Lower learning rate for reward models batch_size = 8 # Adjust based on model size gradient_accumulation_steps = 4 # Effective batch size = 32 5. Validation \u00b6 Validate reward model on held-out data Test reward scores on sample texts Ensure rewards align with expectations Complete Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer from aligntune.rewards.registry import RewardRegistry def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response that addresses the question.\" , \"I'm not entirely sure, but I think the answer might be related to this.\" , \"That's a great question! Let me break it down step by step for you.\" , \"I don't have enough information to provide a complete answer.\" , \"Here's a comprehensive explanation of the topic you asked about.\" , # ... more diverse texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # Training parameters reward_training_epochs = 3 , reward_training_lr = 1e-5 , reward_training_batch_size = 8 , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 , max_samples = 100 ) # Train (reward model trained first, then PPO) print ( \"Starting training...\" ) trainer . train () # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) if __name__ == \"__main__\" : main () Validation \u00b6 Strict Validation Rules \u00b6 AlignTune enforces strict validation: Exactly One Source : Must specify exactly one reward source No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Positive Weights : All reward weights must be positive Error Handling \u00b6 No Fallbacks : System fails fast with clear errors Clear Messages : Errors include actionable information Comprehensive Validation : All inputs validated before processing Troubleshooting \u00b6 Insufficient Training Data \u00b6 # Ensure at least 10 texts if len ( training_texts ) < 10 : raise ValueError ( \"Need at least 10 training texts\" ) Invalid Reward Functions \u00b6 from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () available = registry . list_reward_functions () # Check if function exists if \"my_function\" not in available : raise ValueError ( f \"Reward function not found. Available: { available } \" ) Model Family Mismatch (PPO) \u00b6 # Ensure reward model matches policy model family # Correct: Both Qwen model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"Qwen/Qwen3-0.6B\" # Wrong: Different families model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"meta-llama/Llama-2-7b-hf\" Next Steps \u00b6 Reward Functions Guide - Explore reward functions RL Training Guide - Complete RL training guide Reward Model Training System - Detailed system docs Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Reward Functions Reference - Complete function list","title":"Reward Model Training"},{"location":"user-guide/reward-model-training/#reward-model-training-guide","text":"Complete guide to training custom reward models from rule-based reward functions and integrating them into PPO training.","title":"Reward Model Training Guide"},{"location":"user-guide/reward-model-training/#overview","text":"Reward models learn to score text quality based on training data generated from rule-based reward functions. AlignTune provides a complete system for training reward models and seamlessly integrating them into PPO training pipelines.","title":"Overview"},{"location":"user-guide/reward-model-training/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/reward-model-training/#standalone-reward-model-training","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_texts = [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" ] training_data = trainer . generate_training_data ( texts = training_texts , batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Reward model saved to: { model_path } \" )","title":"Standalone Reward Model Training"},{"location":"user-guide/reward-model-training/#ppo-with-custom-reward-model-training","text":"from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load your training texts.\"\"\" return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train (reward model is trained first, then PPO) trainer . train ()","title":"PPO with Custom Reward Model Training"},{"location":"user-guide/reward-model-training/#training-modes","text":"","title":"Training Modes"},{"location":"user-guide/reward-model-training/#1-standalone-training","text":"Train reward models independently for later use: from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_training_texts , batch_size = 32 ) # Train model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/my_model\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 )","title":"1. Standalone Training"},{"location":"user-guide/reward-model-training/#2-integrated-ppo-training","text":"Train reward model as part of PPO training: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" )","title":"2. Integrated PPO Training"},{"location":"user-guide/reward-model-training/#configuration","text":"","title":"Configuration"},{"location":"user-guide/reward-model-training/#reward-model-training-config","text":"from aligntune.core.rl.config import RewardModelTrainingConfig config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , # Required training_texts = your_texts , # Required reward_functions = [ \"length\" , \"sentiment\" ], # Required output_dir = \"./reward_models/custom\" , # Required # Optional parameters reward_weights = [ 0.5 , 0.5 ], num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 , gradient_accumulation_steps = 4 , max_length = 512 )","title":"Reward Model Training Config"},{"location":"user-guide/reward-model-training/#reward-model-source-config","text":"from aligntune.core.rl.config import RewardModelSourceConfig , RewardModelTrainingConfig # For custom training training_config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , training_texts = texts , reward_functions = [ \"length\" , \"safety\" ], output_dir = \"./reward_models/custom\" ) source_config = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) # For HuggingFace Hub source_config = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # For local model source_config = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = \"./reward_models/my_model\" )","title":"Reward Model Source Config"},{"location":"user-guide/reward-model-training/#using-pre-trained-reward-models","text":"","title":"Using Pre-trained Reward Models"},{"location":"user-guide/reward-model-training/#from-huggingface-hub","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" )","title":"From HuggingFace Hub"},{"location":"user-guide/reward-model-training/#from-local-path","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = \"./reward_models/my_custom_model\" )","title":"From Local Path"},{"location":"user-guide/reward-model-training/#training-data-generation","text":"","title":"Training Data Generation"},{"location":"user-guide/reward-model-training/#basic-generation","text":"from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_texts , batch_size = 32 )","title":"Basic Generation"},{"location":"user-guide/reward-model-training/#with-reference-texts","text":"training_data = trainer . generate_training_data ( texts = your_texts , reference_texts = reference_texts , # Optional reference texts batch_size = 32 )","title":"With Reference Texts"},{"location":"user-guide/reward-model-training/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/reward-model-training/#1-training-data-quality","text":"Use diverse, representative texts Include both good and bad examples Ensure sufficient quantity (100+ texts recommended)","title":"1. Training Data Quality"},{"location":"user-guide/reward-model-training/#2-reward-function-selection","text":"Choose functions relevant to your task Balance weights appropriately Start with a few key functions, expand as needed","title":"2. Reward Function Selection"},{"location":"user-guide/reward-model-training/#3-model-selection","text":"Use base models compatible with your policy model Consider model family consistency (for PPO) Smaller models work well for reward models","title":"3. Model Selection"},{"location":"user-guide/reward-model-training/#4-training-parameters","text":"# Recommended settings num_epochs = 3 # 3-5 epochs usually sufficient learning_rate = 1e-5 # Lower learning rate for reward models batch_size = 8 # Adjust based on model size gradient_accumulation_steps = 4 # Effective batch size = 32","title":"4. Training Parameters"},{"location":"user-guide/reward-model-training/#5-validation","text":"Validate reward model on held-out data Test reward scores on sample texts Ensure rewards align with expectations","title":"5. Validation"},{"location":"user-guide/reward-model-training/#complete-example","text":"from aligntune.core.backend_factory import create_rl_trainer from aligntune.rewards.registry import RewardRegistry def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response that addresses the question.\" , \"I'm not entirely sure, but I think the answer might be related to this.\" , \"That's a great question! Let me break it down step by step for you.\" , \"I don't have enough information to provide a complete answer.\" , \"Here's a comprehensive explanation of the topic you asked about.\" , # ... more diverse texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # Training parameters reward_training_epochs = 3 , reward_training_lr = 1e-5 , reward_training_batch_size = 8 , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 , max_samples = 100 ) # Train (reward model trained first, then PPO) print ( \"Starting training...\" ) trainer . train () # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) if __name__ == \"__main__\" : main ()","title":"Complete Example"},{"location":"user-guide/reward-model-training/#validation","text":"","title":"Validation"},{"location":"user-guide/reward-model-training/#strict-validation-rules","text":"AlignTune enforces strict validation: Exactly One Source : Must specify exactly one reward source No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Positive Weights : All reward weights must be positive","title":"Strict Validation Rules"},{"location":"user-guide/reward-model-training/#error-handling","text":"No Fallbacks : System fails fast with clear errors Clear Messages : Errors include actionable information Comprehensive Validation : All inputs validated before processing","title":"Error Handling"},{"location":"user-guide/reward-model-training/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/reward-model-training/#insufficient-training-data","text":"# Ensure at least 10 texts if len ( training_texts ) < 10 : raise ValueError ( \"Need at least 10 training texts\" )","title":"Insufficient Training Data"},{"location":"user-guide/reward-model-training/#invalid-reward-functions","text":"from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () available = registry . list_reward_functions () # Check if function exists if \"my_function\" not in available : raise ValueError ( f \"Reward function not found. Available: { available } \" )","title":"Invalid Reward Functions"},{"location":"user-guide/reward-model-training/#model-family-mismatch-ppo","text":"# Ensure reward model matches policy model family # Correct: Both Qwen model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"Qwen/Qwen3-0.6B\" # Wrong: Different families model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"meta-llama/Llama-2-7b-hf\"","title":"Model Family Mismatch (PPO)"},{"location":"user-guide/reward-model-training/#next-steps","text":"Reward Functions Guide - Explore reward functions RL Training Guide - Complete RL training guide Reward Model Training System - Detailed system docs","title":"Next Steps"},{"location":"user-guide/reward-model-training/#additional-resources","text":"API Reference - API documentation Examples - Code examples Reward Functions Reference - Complete function list","title":"Additional Resources"},{"location":"user-guide/rl/","text":"Reinforcement Learning (RL) Training Guide \u00b6 Complete guide to Reinforcement Learning from Human Feedback (RLHF) training with AlignTune, covering all supported RLHF algorithms. Overview \u00b6 Reinforcement Learning training aligns language models with human preferences using various algorithms. AlignTune supports multiple RLHF algorithms: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Flexible policy optimization with reward models GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) - Scaled RL for LLMs while addressing key limitations in GRPO Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant that corrects optimization biases Quick Start \u00b6 Basic DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Create and train DPO model trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-3.2-3B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) # Train the model trainer . train () Basic PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , ) trainer . train () Algorithms \u00b6 1. DPO (Direct Preference Optimization) \u00b6 DPO trains models directly on preference pairs without requiring a separate reward model. Advantages: - No reward model needed - Simpler training pipeline - Direct preference learning Use Cases: - Preference alignment - Human feedback integration - General RLHF training Example \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B # DPO-specific parameters beta = 0.1 , # KL penalty coefficient reference_free = False , # Use reference model loss_type = \"sigmoid\" # Loss function type ) trainer . train () Dataset Format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know.\" } 2. PPO (Proximal Policy Optimization) \u00b6 PPO optimizes policies using reward models and value functions. Advantages: - Flexible reward shaping - Can use custom reward models - Supports complex reward landscapes Use Cases: - Custom reward model training - Complex reward functions - Production RLHF systems Example \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Reward model configuration reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # PPO-specific parameters num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , # KL penalty coefficient cliprange = 0.2 , # PPO clip range vf_coef = 0.1 , # Value function coefficient gamma = 1.0 , # Discount factor lam = 0.95 # GAE lambda ) trainer . train () Custom Reward Model Training \u00b6 def load_training_texts (): \"\"\"Load training texts for reward model training.\"\"\" return [ \"This is a helpful and informative response that addresses the question clearly.\" , \"I'm not sure about this answer.\" , \"This response is clear, concise, and well-structured.\" , ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () 3. GRPO (Group Relative Policy Optimization) \u00b6 GRPO optimizes policies using group-based relative comparisons. Advantages: - Multi-criteria optimization - Group-based learning - Flexible reward combinations Use Cases: - Multi-objective optimization - Complex reward landscapes - Group-based preference learning Example \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' ) trainer . train () 4. GSPO (Group Sequential Policy Optimization) \u00b6 GSPO uses sequential group learning for policy optimization. Note: GSPO is only supported by TRL backend, not Unsloth. Advantages: - Sequential learning - Group-based optimization - Structured policy updates Use Cases: - Sequential preference learning - Structured policy optimization - Group-based RLHF Example \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , # Note: group_size and sequential_steps are not in standard GRPO config # These parameters are specific to GSPO implementation ) trainer . train () Backend Selection \u00b6 TRL Backend \u00b6 Use TRL when: - Need maximum compatibility - Using GSPO (TRL only) - Working with standard models - Need reliable, battle-tested training trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) Unsloth Backend \u00b6 Use Unsloth when: - Need faster training - Working with large models - Need memory efficiency - Training PPO, DPO, or GRPO trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" ) Configuration \u00b6 Model Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Model settings max_seq_length = 2048 , quantization = { \"load_in_4bit\" : True }, use_peft = True , # Enable LoRA lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , use_gradient_checkpointing = True , # Reward model settings (for PPO) reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , reward_model_quantization = { \"load_in_4bit\" : True } ) Training Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Training settings num_epochs = 1 , max_steps = None , # Use epochs batch_size = 4 , gradient_accumulation_steps = 4 , learning_rate = 5e-5 , weight_decay = 0.01 , warmup_steps = 100 , max_grad_norm = 1.0 , # Algorithm-specific beta = 0.1 , # DPO: KL coefficient kl_coef = 0.1 , # PPO: KL coefficient cliprange = 0.2 , # PPO: clip range temperature = 0.7 # Generation temperature ) Dataset Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Dataset settings max_samples = 1000 , percent = 10.0 , split = \"train\" , # Field mappings column_mapping = { \"prompt\" : \"prompt\" , \"chosen\" : \"chosen\" , \"rejected\" : \"rejected\" }, truncation_mode = \"keep_end\" , padding_free = False ) Reward Models \u00b6 Using Pre-trained Reward Models \u00b6 # From HuggingFace Hub trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # From local path trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_path = \"./reward_models/my_reward_model\" ) Training Custom Reward Models \u00b6 See Reward Model Training Guide for detailed instructions. trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" ) Advanced Features \u00b6 Model Family Consistency (PPO) \u00b6 AlignTune automatically checks that all models in PPO training belong to the same family: # Correct: All Qwen models trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # Error: Mixed families trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! ) LoRA/QLoRA Fine-Tuning \u00b6 trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # LoRA configuration use_peft = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True } ) Distributed Training \u00b6 # Distributed training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Distributed settings (configure via accelerate) # See distributed training guide ) Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation \u00b6 from datasets import load_dataset eval_dataset = load_dataset ( \"Anthropic/hh-rlhf\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) Zero-Shot Evaluation \u00b6 # Generate predictions prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Best Practices \u00b6 1. Algorithm Selection \u00b6 DPO : Start here for preference alignment, simpler setup PPO : Use for custom rewards, complex scenarios GRPO : Multi-criteria optimization GSPO : Sequential learning (TRL only) 2. Backend Selection \u00b6 TRL : Maximum compatibility, GSPO support Unsloth : Faster training (Faster), memory efficient 3. Reward Models \u00b6 Use pre-trained models when available Train custom models for domain-specific tasks Ensure model family consistency for PPO 4. Hyperparameters \u00b6 # DPO recommended settings beta = 0.1 # KL coefficient learning_rate = 5e-5 , batch_size = 4 # PPO recommended settings kl_coef = 0.1 cliprange = 0.2 learning_rate = 1e-6 , batch_size = 1 # Smaller for PPO 5. Memory Optimization \u00b6 # For large models trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , quantization = { \"load_in_4bit\" : True }, use_peft = True , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 ) Troubleshooting \u00b6 Out of Memory \u00b6 # Reduce batch size, use quantization trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , batch_size = 1 , # Reduce gradient_accumulation_steps = 8 , # Compensate quantization = { \"load_in_4bit\" : True }, use_gradient_checkpointing = True ) Model Family Mismatch (PPO) \u00b6 # Ensure all models are same family # Correct model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" # Wrong model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! Slow Training \u00b6 # Use Unsloth backend trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" # faster ) Poor Convergence \u00b6 # Adjust learning rate and KL coefficient trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , algorithm = \"dpo\" , learning_rate = 1e-5 , # Lower learning rate beta = 0.05 , # Lower KL penalty num_epochs = 2 # More epochs ) Complete Examples \u00b6 DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 ) trainer . train () metrics = trainer . evaluate () model_path = trainer . save_model () PPO with Custom Reward Model \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () Next Steps \u00b6 Reward Functions Guide - Explore reward functions Reward Model Training - Train custom reward models Evaluation Guide - Comprehensive evaluation SFT Guide - Supervised fine-tuning Additional Resources \u00b6 API Reference - Complete API documentation Examples - More RL examples Backend Selection - Backend guide Unsloth Compatibility - Unsloth setup","title":"Reinforcement Learning (RL)"},{"location":"user-guide/rl/#reinforcement-learning-rl-training-guide","text":"Complete guide to Reinforcement Learning from Human Feedback (RLHF) training with AlignTune, covering all supported RLHF algorithms.","title":"Reinforcement Learning (RL) Training Guide"},{"location":"user-guide/rl/#overview","text":"Reinforcement Learning training aligns language models with human preferences using various algorithms. AlignTune supports multiple RLHF algorithms: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Flexible policy optimization with reward models GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) - Scaled RL for LLMs while addressing key limitations in GRPO Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant that corrects optimization biases","title":"Overview"},{"location":"user-guide/rl/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/rl/#basic-dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer # Create and train DPO model trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-3.2-3B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) # Train the model trainer . train ()","title":"Basic DPO Training"},{"location":"user-guide/rl/#basic-ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , ) trainer . train ()","title":"Basic PPO Training"},{"location":"user-guide/rl/#algorithms","text":"","title":"Algorithms"},{"location":"user-guide/rl/#1-dpo-direct-preference-optimization","text":"DPO trains models directly on preference pairs without requiring a separate reward model. Advantages: - No reward model needed - Simpler training pipeline - Direct preference learning Use Cases: - Preference alignment - Human feedback integration - General RLHF training","title":"1. DPO (Direct Preference Optimization)"},{"location":"user-guide/rl/#example","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B # DPO-specific parameters beta = 0.1 , # KL penalty coefficient reference_free = False , # Use reference model loss_type = \"sigmoid\" # Loss function type ) trainer . train () Dataset Format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know.\" }","title":"Example"},{"location":"user-guide/rl/#2-ppo-proximal-policy-optimization","text":"PPO optimizes policies using reward models and value functions. Advantages: - Flexible reward shaping - Can use custom reward models - Supports complex reward landscapes Use Cases: - Custom reward model training - Complex reward functions - Production RLHF systems","title":"2. PPO (Proximal Policy Optimization)"},{"location":"user-guide/rl/#example_1","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Reward model configuration reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # PPO-specific parameters num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , # KL penalty coefficient cliprange = 0.2 , # PPO clip range vf_coef = 0.1 , # Value function coefficient gamma = 1.0 , # Discount factor lam = 0.95 # GAE lambda ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#custom-reward-model-training","text":"def load_training_texts (): \"\"\"Load training texts for reward model training.\"\"\" return [ \"This is a helpful and informative response that addresses the question clearly.\" , \"I'm not sure about this answer.\" , \"This response is clear, concise, and well-structured.\" , ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"Custom Reward Model Training"},{"location":"user-guide/rl/#3-grpo-group-relative-policy-optimization","text":"GRPO optimizes policies using group-based relative comparisons. Advantages: - Multi-criteria optimization - Group-based learning - Flexible reward combinations Use Cases: - Multi-objective optimization - Complex reward landscapes - Group-based preference learning","title":"3. GRPO (Group Relative Policy Optimization)"},{"location":"user-guide/rl/#example_2","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#4-gspo-group-sequential-policy-optimization","text":"GSPO uses sequential group learning for policy optimization. Note: GSPO is only supported by TRL backend, not Unsloth. Advantages: - Sequential learning - Group-based optimization - Structured policy updates Use Cases: - Sequential preference learning - Structured policy optimization - Group-based RLHF","title":"4. GSPO (Group Sequential Policy Optimization)"},{"location":"user-guide/rl/#example_3","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , # Note: group_size and sequential_steps are not in standard GRPO config # These parameters are specific to GSPO implementation ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#backend-selection","text":"","title":"Backend Selection"},{"location":"user-guide/rl/#trl-backend","text":"Use TRL when: - Need maximum compatibility - Using GSPO (TRL only) - Working with standard models - Need reliable, battle-tested training trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"TRL Backend"},{"location":"user-guide/rl/#unsloth-backend","text":"Use Unsloth when: - Need faster training - Working with large models - Need memory efficiency - Training PPO, DPO, or GRPO trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" )","title":"Unsloth Backend"},{"location":"user-guide/rl/#configuration","text":"","title":"Configuration"},{"location":"user-guide/rl/#model-configuration","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Model settings max_seq_length = 2048 , quantization = { \"load_in_4bit\" : True }, use_peft = True , # Enable LoRA lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , use_gradient_checkpointing = True , # Reward model settings (for PPO) reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , reward_model_quantization = { \"load_in_4bit\" : True } )","title":"Model Configuration"},{"location":"user-guide/rl/#training-configuration","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Training settings num_epochs = 1 , max_steps = None , # Use epochs batch_size = 4 , gradient_accumulation_steps = 4 , learning_rate = 5e-5 , weight_decay = 0.01 , warmup_steps = 100 , max_grad_norm = 1.0 , # Algorithm-specific beta = 0.1 , # DPO: KL coefficient kl_coef = 0.1 , # PPO: KL coefficient cliprange = 0.2 , # PPO: clip range temperature = 0.7 # Generation temperature )","title":"Training Configuration"},{"location":"user-guide/rl/#dataset-configuration","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Dataset settings max_samples = 1000 , percent = 10.0 , split = \"train\" , # Field mappings column_mapping = { \"prompt\" : \"prompt\" , \"chosen\" : \"chosen\" , \"rejected\" : \"rejected\" }, truncation_mode = \"keep_end\" , padding_free = False )","title":"Dataset Configuration"},{"location":"user-guide/rl/#reward-models","text":"","title":"Reward Models"},{"location":"user-guide/rl/#using-pre-trained-reward-models","text":"# From HuggingFace Hub trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # From local path trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_path = \"./reward_models/my_reward_model\" )","title":"Using Pre-trained Reward Models"},{"location":"user-guide/rl/#training-custom-reward-models","text":"See Reward Model Training Guide for detailed instructions. trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" )","title":"Training Custom Reward Models"},{"location":"user-guide/rl/#advanced-features","text":"","title":"Advanced Features"},{"location":"user-guide/rl/#model-family-consistency-ppo","text":"AlignTune automatically checks that all models in PPO training belong to the same family: # Correct: All Qwen models trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # Error: Mixed families trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! )","title":"Model Family Consistency (PPO)"},{"location":"user-guide/rl/#loraqlora-fine-tuning","text":"trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # LoRA configuration use_peft = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True } )","title":"LoRA/QLoRA Fine-Tuning"},{"location":"user-guide/rl/#distributed-training","text":"# Distributed training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Distributed settings (configure via accelerate) # See distributed training guide )","title":"Distributed Training"},{"location":"user-guide/rl/#evaluation","text":"","title":"Evaluation"},{"location":"user-guide/rl/#basic-evaluation","text":"# Evaluate on validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/rl/#custom-evaluation","text":"from datasets import load_dataset eval_dataset = load_dataset ( \"Anthropic/hh-rlhf\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset )","title":"Custom Evaluation"},{"location":"user-guide/rl/#zero-shot-evaluation","text":"# Generate predictions prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Zero-Shot Evaluation"},{"location":"user-guide/rl/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/rl/#1-algorithm-selection","text":"DPO : Start here for preference alignment, simpler setup PPO : Use for custom rewards, complex scenarios GRPO : Multi-criteria optimization GSPO : Sequential learning (TRL only)","title":"1. Algorithm Selection"},{"location":"user-guide/rl/#2-backend-selection","text":"TRL : Maximum compatibility, GSPO support Unsloth : Faster training (Faster), memory efficient","title":"2. Backend Selection"},{"location":"user-guide/rl/#3-reward-models","text":"Use pre-trained models when available Train custom models for domain-specific tasks Ensure model family consistency for PPO","title":"3. Reward Models"},{"location":"user-guide/rl/#4-hyperparameters","text":"# DPO recommended settings beta = 0.1 # KL coefficient learning_rate = 5e-5 , batch_size = 4 # PPO recommended settings kl_coef = 0.1 cliprange = 0.2 learning_rate = 1e-6 , batch_size = 1 # Smaller for PPO","title":"4. Hyperparameters"},{"location":"user-guide/rl/#5-memory-optimization","text":"# For large models trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , quantization = { \"load_in_4bit\" : True }, use_peft = True , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 )","title":"5. Memory Optimization"},{"location":"user-guide/rl/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/rl/#out-of-memory","text":"# Reduce batch size, use quantization trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , batch_size = 1 , # Reduce gradient_accumulation_steps = 8 , # Compensate quantization = { \"load_in_4bit\" : True }, use_gradient_checkpointing = True )","title":"Out of Memory"},{"location":"user-guide/rl/#model-family-mismatch-ppo","text":"# Ensure all models are same family # Correct model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" # Wrong model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family!","title":"Model Family Mismatch (PPO)"},{"location":"user-guide/rl/#slow-training","text":"# Use Unsloth backend trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" # faster )","title":"Slow Training"},{"location":"user-guide/rl/#poor-convergence","text":"# Adjust learning rate and KL coefficient trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , algorithm = \"dpo\" , learning_rate = 1e-5 , # Lower learning rate beta = 0.05 , # Lower KL penalty num_epochs = 2 # More epochs )","title":"Poor Convergence"},{"location":"user-guide/rl/#complete-examples","text":"","title":"Complete Examples"},{"location":"user-guide/rl/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 ) trainer . train () metrics = trainer . evaluate () model_path = trainer . save_model ()","title":"DPO Training"},{"location":"user-guide/rl/#ppo-with-custom-reward-model","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"PPO with Custom Reward Model"},{"location":"user-guide/rl/#next-steps","text":"Reward Functions Guide - Explore reward functions Reward Model Training - Train custom reward models Evaluation Guide - Comprehensive evaluation SFT Guide - Supervised fine-tuning","title":"Next Steps"},{"location":"user-guide/rl/#additional-resources","text":"API Reference - Complete API documentation Examples - More RL examples Backend Selection - Backend guide Unsloth Compatibility - Unsloth setup","title":"Additional Resources"},{"location":"user-guide/sample-logging/","text":"Qualitative Sample Logging \u00b6 AlignTune can periodically generate qualitative samples during RL training to spot regressions long before metrics change. The feature is powered by aligntune.core.rl.sample_logger and governed by SampleLoggingConfig . When Samples Are Logged \u00b6 Available on TRL RL trainers ( ppo , grpo , gspo ) and Unsloth PPO via their calls to generate_and_log_samples . Samples run after key checkpoints (for PPO this happens at the end of training, and additional hooks log mid-training when interval_steps or percent_of_max_steps triggers fire). Responses and optional reward scores are streamed to both the configured logger and stdout so they appear in notebooks and terminal logs. Configuration Fields \u00b6 SampleLoggingConfig lives under the logging section of UnifiedConfig : Field Type Default Description enabled bool False Master switch for sample generation prompts list[str] built-in prompt trio Prompts to feed into model.generate ; falls back to defaults when empty interval_steps int? None Log every N optimizer steps percent_of_max_steps float? None Log at percentages of train.max_steps (e.g., 0.25 ) num_samples int 3 How many prompts to evaluate (capped at the number of prompts) max_new_tokens int 80 Max tokens per response temperature float 0.7 Sampling temperature top_p float 0.9 Nucleus sampling parameter Validation in __post_init__ ensures all numeric values are positive and that percentages fall inside (0, 1] . YAML Example \u00b6 logging : output_dir : ./output/ppo_run loggers : [ tensorboard ] sample_logging : enabled : true prompts : - \"Summarize the core idea of reinforcement learning in two sentences.\" - \"Write a concise plan for debugging PPO training instabilities.\" interval_steps : 200 num_samples : 2 max_new_tokens : 96 temperature : 0.6 top_p : 0.95 Python API Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , sample_logging = { \"enabled\" : True , \"percent_of_max_steps\" : 0.5 , \"num_samples\" : 2 , \"max_new_tokens\" : 72 , \"temperature\" : 0.8 , }, ) trainer . train () Behind the scenes, the backend factory merges the provided dictionary into a SampleLoggingConfig instance before passing it to the trainer. Reward-Aware Samples \u00b6 When custom reward functions are registered on the trainer (e.g., PPO with multiple heuristic rewards), each generated response is scored and the per-reward outputs are printed alongside the prompt/response pair: ================================================================================ Qualitative samples (post-train) - prompts: 2 [Sample 1/2] Prompt: Summarize the core idea... [Sample 1/2] Response: ... [Sample 1/2] Rewards: {'reward_0': 0.81, 'reward_1': 0.42} ================================================================================ Best Practices \u00b6 Keep prompts short and deterministic so diffs are easier to read in CI logs. Use percent_of_max_steps for long-running jobs where max_steps is known, otherwise rely on step intervals. On distributed training setups the samples run on the trainer's primary device; make sure enough GPU memory remains to run generation with the configured max_new_tokens . Disable qualitative samples on highly resource-constrained runs to avoid extra generation overhead. For more context on logging knobs, revisit the logging.sample_logging section of the README or the API docs for SampleLoggingConfig .","title":"Sample Logging"},{"location":"user-guide/sample-logging/#qualitative-sample-logging","text":"AlignTune can periodically generate qualitative samples during RL training to spot regressions long before metrics change. The feature is powered by aligntune.core.rl.sample_logger and governed by SampleLoggingConfig .","title":"Qualitative Sample Logging"},{"location":"user-guide/sample-logging/#when-samples-are-logged","text":"Available on TRL RL trainers ( ppo , grpo , gspo ) and Unsloth PPO via their calls to generate_and_log_samples . Samples run after key checkpoints (for PPO this happens at the end of training, and additional hooks log mid-training when interval_steps or percent_of_max_steps triggers fire). Responses and optional reward scores are streamed to both the configured logger and stdout so they appear in notebooks and terminal logs.","title":"When Samples Are Logged"},{"location":"user-guide/sample-logging/#configuration-fields","text":"SampleLoggingConfig lives under the logging section of UnifiedConfig : Field Type Default Description enabled bool False Master switch for sample generation prompts list[str] built-in prompt trio Prompts to feed into model.generate ; falls back to defaults when empty interval_steps int? None Log every N optimizer steps percent_of_max_steps float? None Log at percentages of train.max_steps (e.g., 0.25 ) num_samples int 3 How many prompts to evaluate (capped at the number of prompts) max_new_tokens int 80 Max tokens per response temperature float 0.7 Sampling temperature top_p float 0.9 Nucleus sampling parameter Validation in __post_init__ ensures all numeric values are positive and that percentages fall inside (0, 1] .","title":"Configuration Fields"},{"location":"user-guide/sample-logging/#yaml-example","text":"logging : output_dir : ./output/ppo_run loggers : [ tensorboard ] sample_logging : enabled : true prompts : - \"Summarize the core idea of reinforcement learning in two sentences.\" - \"Write a concise plan for debugging PPO training instabilities.\" interval_steps : 200 num_samples : 2 max_new_tokens : 96 temperature : 0.6 top_p : 0.95","title":"YAML Example"},{"location":"user-guide/sample-logging/#python-api-example","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , sample_logging = { \"enabled\" : True , \"percent_of_max_steps\" : 0.5 , \"num_samples\" : 2 , \"max_new_tokens\" : 72 , \"temperature\" : 0.8 , }, ) trainer . train () Behind the scenes, the backend factory merges the provided dictionary into a SampleLoggingConfig instance before passing it to the trainer.","title":"Python API Example"},{"location":"user-guide/sample-logging/#reward-aware-samples","text":"When custom reward functions are registered on the trainer (e.g., PPO with multiple heuristic rewards), each generated response is scored and the per-reward outputs are printed alongside the prompt/response pair: ================================================================================ Qualitative samples (post-train) - prompts: 2 [Sample 1/2] Prompt: Summarize the core idea... [Sample 1/2] Response: ... [Sample 1/2] Rewards: {'reward_0': 0.81, 'reward_1': 0.42} ================================================================================","title":"Reward-Aware Samples"},{"location":"user-guide/sample-logging/#best-practices","text":"Keep prompts short and deterministic so diffs are easier to read in CI logs. Use percent_of_max_steps for long-running jobs where max_steps is known, otherwise rely on step intervals. On distributed training setups the samples run on the trainer's primary device; make sure enough GPU memory remains to run generation with the configured max_new_tokens . Disable qualitative samples on highly resource-constrained runs to avoid extra generation overhead. For more context on logging knobs, revisit the logging.sample_logging section of the README or the API docs for SampleLoggingConfig .","title":"Best Practices"},{"location":"user-guide/sft/","text":"Supervised Fine-Tuning (SFT) Guide \u00b6 Complete guide to Supervised Fine-Tuning with AlignTune, covering all task types, configurations, and best practices. Overview \u00b6 Supervised Fine-Tuning (SFT) is the process of training a pre-trained language model on a labeled dataset to adapt it for specific tasks. AlignTune provides a unified interface for SFT across multiple task types with support for both TRL and Unsloth backends. Supported Task Types \u00b6 AlignTune supports six main SFT task types: Instruction Following - Teach models to follow instructions Supervised Fine-Tuning - General-purpose fine-tuning Text Classification - Classify text into categories Token Classification - Named Entity Recognition (NER), POS tagging Text Generation - Generate coherent text Chat Completion - Conversational AI training Quick Start \u00b6 Basic SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) # Train the model trainer . train () # Save the model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) Using YAML Configuration \u00b6 # config.yaml model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 dataset : name : \"tatsu-lab/alpaca\" max_samples : 1000 task_type : \"supervised_fine_tuning\" train : epochs : 3 per_device_batch_size : 4 learning_rate : 2e-4 logging : output_dir : \"./output\" from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = \"trl\" , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , max_samples = config . dataset . max_samples , task_type = config . dataset . task_type . value ) trainer . train () Task Types \u00b6 1. Instruction Following \u00b6 Train models to follow instructions and generate appropriate responses. from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth recommended for generation tasks task_type = \"instruction_following\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , # Longer sequences for instructions max_samples = 1000 , # Column mappings for instruction datasets instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () Dataset Format: { \"instruction\" : \"Explain machine learning\" , \"input\" : \"\" , \"output\" : \"Machine learning is a subset of AI...\" } 2. Supervised Fine-Tuning \u00b6 General-purpose fine-tuning for any text-to-text task. trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"databricks/databricks-dolly-15k\" , backend = \"trl\" , task_type = \"supervised_fine_tuning\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , auto_detect_fields = True # Automatically detect dataset fields ) trainer . train () 3. Text Classification \u00b6 Classify text into predefined categories (sentiment, topic, etc.). trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL recommended for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , # Classification-specific text_column = \"text\" , label_column = \"label\" , num_labels = 2 # Binary classification ) trainer . train () Dataset Format: { \"text\" : \"This movie is fantastic!\" , \"label\" : 1 } 4. Token Classification \u00b6 Named Entity Recognition (NER), Part-of-Speech tagging, etc. trainer = create_sft_trainer ( model_name = \"bert-base-uncased\" , dataset_name = \"lhoestq/conll2003\" , backend = \"trl\" , # TRL recommended for token classification task_type = \"token_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , split = \"train\" , # Dataset split tokens_column = \"tokens\" , tags_column = \"ner_tags\" , num_labels = 9 # Number of NER tag classes ) trainer . train () Dataset Format: { \"tokens\" : [ \"Apple\" , \"is\" , \"a\" , \"company\" ], \"ner_tags\" : [ 1 , 0 , 0 , 0 ] # B - ORG , O , O , O } 5. Text Generation \u00b6 Generate coherent, context-aware text. trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Salesforce/wikitext\" , backend = \"trl\" , # Unsloth recommended for generation task_type = \"text_generation\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , text_column = \"text\" ) trainer . train () 6. Chat Completion \u00b6 Train conversational AI models with chat templates. trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , backend = \"unsloth\" , task_type = \"chat_completion\" , split = \"train_sft\" , # Dataset split num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , # Longer for conversations gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" # Auto-detect chat template ) trainer . train () Dataset Format: { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Hello!\" }, { \"role\" : \"assistant\" , \"content\" : \"Hi! How can I help?\" } ] } Backend Selection \u00b6 TRL Backend (Recommended for Classification) \u00b6 Use TRL when: - Training classification models - Training token classification (NER) - Need maximum compatibility - Working with smaller models trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # Explicit TRL backend task_type = \"text_classification\" ) Unsloth Backend (Recommended for Generation) \u00b6 Use Unsloth when: - Training generation models (faster) - Working with large models - Need memory efficiency - Training instruction following or chat models trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth for speed task_type = \"instruction_following\" ) Auto Backend Selection \u00b6 Let AlignTune choose the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , # Automatic selection task_type = \"instruction_following\" ) Configuration Options \u00b6 Model Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Model settings max_seq_length = 512 , quantization = { \"load_in_4bit\" : True }, # 4-bit quantization peft_enabled = True , # Enable LoRA lora_r = 16 , # LoRA rank lora_alpha = 32 , # LoRA alpha lora_dropout = 0.1 , # For GPT2 models, use: lora_target_modules=[\"c_attn\", \"c_proj\"] use_gradient_checkpointing = True , # Memory optimization use_flash_attention_2 = True # Flash Attention (if available) ) Training Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Training settings num_epochs = 3 , max_steps = None , # Use epochs instead batch_size = 4 , gradient_accumulation_steps = 4 , # Effective batch size = 16 learning_rate = 2e-4 , weight_decay = 0.01 , warmup_steps = 100 , warmup_ratio = 0.1 , max_grad_norm = 1.0 , # Advanced settings packing = True , # Sequence packing for efficiency # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth packing_strategy = \"bfd\" , # \"bfd\" or \"wrapped\" loss_type = \"nll\" , # \"nll\" or \"dft\" completion_only_loss = True , # Only compute loss on completion activation_offloading = False # Offload activations to CPU ) Dataset Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Dataset settings max_samples = 1000 , # Limit dataset size percent = 10.0 , # Use 10% of dataset split = \"train\" , # Dataset split # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth # Column mappings column_mapping = { \"instruction\" : \"instruction\" , \"output\" : \"response\" }, # Field detection auto_detect_fields = True , # Auto-detect dataset fields dataset_num_proc = 4 # Parallel processing ) Advanced Features \u00b6 LoRA/QLoRA Fine-Tuning \u00b6 Efficient fine-tuning with parameter-efficient methods: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # LoRA configuration peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True , \"bnb_4bit_compute_dtype\" : \"float16\" , \"bnb_4bit_quant_type\" : \"nf4\" } ) Sequence Packing \u00b6 Pack multiple sequences into a single batch for efficiency: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models or models not supported by Unsloth packing = True , # Enable packing packing_strategy = \"bfd\" , # \"bfd\" (best-fit-decreasing) or \"wrapped\" padding_free = True , # No padding needed with packing max_seq_length = 2048 # Longer sequences with packing ) Mixed Precision Training \u00b6 Use FP16 or BF16 for faster training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Precision settings (handled automatically by backend) # Unsloth uses optimal precision automatically # TRL: set fp16=True or bf16=True in training config ) Gradient Checkpointing \u00b6 Reduce memory usage during training: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , use_gradient_checkpointing = True , # Enable checkpointing activation_offloading = True # Offload activations to CPU ) Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on default validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation Dataset \u00b6 from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" ) Zero-Shot Evaluation \u00b6 # Generate predictions for test prompts # Note: Make sure to call trainer.train() or trainer.setup_model() before using predict() test_prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( test_prompts ) for prompt , result in zip ( test_prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Model Management \u00b6 Saving Models \u00b6 # Save after training model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Save to custom directory model_path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" ) Pushing to HuggingFace Hub \u00b6 # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , private = False ) print ( f \"Model available at: { url } \" ) # Push with custom commit message url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" ) Loading Saved Models \u00b6 # Load saved model from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Use for inference result = trainer . predict ( \"What is AI?\" ) Best Practices \u00b6 1. Choose the Right Task Type \u00b6 Instruction Following : For teaching models to follow instructions Text Classification : For categorization tasks Token Classification : For NER, POS tagging Chat Completion : For conversational AI Text Generation : For general text generation 2. Backend Selection \u00b6 TRL : Use for classification, token classification, maximum compatibility Unsloth : Use for generation tasks (faster), large models 3. Memory Optimization \u00b6 # For large models, use: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , # LoRA quantization = { \"load_in_4bit\" : True }, # 4-bit quantization use_gradient_checkpointing = True , # Gradient checkpointing batch_size = 1 , # Smaller batch size gradient_accumulation_steps = 8 # Compensate with accumulation ) 4. Learning Rate Scheduling \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , learning_rate = 2e-4 , warmup_steps = 100 , # Warmup for stable training warmup_ratio = 0.1 # Or use ratio instead ) 5. Dataset Size \u00b6 Start with a subset ( max_samples=1000 ) for testing Gradually increase dataset size Use percent parameter for quick experiments 6. Sequence Length \u00b6 Classification : 512 tokens is usually sufficient Instruction Following : 1024-2048 tokens Chat Completion : 2048+ tokens for conversations Text Generation : 1024-2048 tokens 7. Regular Checkpointing \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , save_interval = 500 , # Save every 500 steps eval_interval = 100 # Evaluate every 100 steps ) Troubleshooting \u00b6 Out of Memory Errors \u00b6 # Reduce batch size and use gradient accumulation trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 , # Reduce batch size gradient_accumulation_steps = 8 , # Increase accumulation use_gradient_checkpointing = True , # Enable checkpointing quantization = { \"load_in_4bit\" : True } # Use quantization ) Slow Training \u00b6 # Use Unsloth backend for generation tasks trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # faster packing = True , # Enable sequence packing dataset_num_proc = 4 # Parallel data processing ) Poor Model Performance \u00b6 # Increase training duration and adjust learning rate trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 5 , # More epochs learning_rate = 1e-4 , # Lower learning rate warmup_steps = 200 , # More warmup max_samples = None # Use full dataset ) Dataset Format Issues \u00b6 # Use column mapping for custom datasets trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"my-custom-dataset\" , column_mapping = { \"instruction\" : \"prompt\" , # Map custom column names \"output\" : \"completion\" }, auto_detect_fields = False # Disable auto-detection ) Examples \u00b6 Complete Instruction Following Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"gpt2\" , # Use Qwen/Llama instead of GPT2 for Unsloth dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , max_samples = 1000 , peft_enabled = True , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 models quantization = { \"load_in_4bit\" : True } ) # Train print ( \"Starting training...\" ) trainer . train () # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test result = trainer . predict ( \"What is machine learning?\" ) print ( f \"Prediction: { result } \" ) Complete Classification Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) # Save model_path = trainer . save_model () Next Steps \u00b6 RL Training Guide - Learn about reinforcement learning training Reward Functions Guide - Explore reward functions for RL Evaluation Guide - Comprehensive evaluation guide Model Management - Advanced model management Additional Resources \u00b6 API Reference - Complete API documentation Examples - More SFT examples Backend Selection - Detailed backend guide Configuration Guide - Configuration reference","title":"Supervised Fine-Tuning (SFT)"},{"location":"user-guide/sft/#supervised-fine-tuning-sft-guide","text":"Complete guide to Supervised Fine-Tuning with AlignTune, covering all task types, configurations, and best practices.","title":"Supervised Fine-Tuning (SFT) Guide"},{"location":"user-guide/sft/#overview","text":"Supervised Fine-Tuning (SFT) is the process of training a pre-trained language model on a labeled dataset to adapt it for specific tasks. AlignTune provides a unified interface for SFT across multiple task types with support for both TRL and Unsloth backends.","title":"Overview"},{"location":"user-guide/sft/#supported-task-types","text":"AlignTune supports six main SFT task types: Instruction Following - Teach models to follow instructions Supervised Fine-Tuning - General-purpose fine-tuning Text Classification - Classify text into categories Token Classification - Named Entity Recognition (NER), POS tagging Text Generation - Generate coherent text Chat Completion - Conversational AI training","title":"Supported Task Types"},{"location":"user-guide/sft/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/sft/#basic-sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) # Train the model trainer . train () # Save the model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" )","title":"Basic SFT Training"},{"location":"user-guide/sft/#using-yaml-configuration","text":"# config.yaml model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 dataset : name : \"tatsu-lab/alpaca\" max_samples : 1000 task_type : \"supervised_fine_tuning\" train : epochs : 3 per_device_batch_size : 4 learning_rate : 2e-4 logging : output_dir : \"./output\" from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = \"trl\" , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , max_samples = config . dataset . max_samples , task_type = config . dataset . task_type . value ) trainer . train ()","title":"Using YAML Configuration"},{"location":"user-guide/sft/#task-types","text":"","title":"Task Types"},{"location":"user-guide/sft/#1-instruction-following","text":"Train models to follow instructions and generate appropriate responses. from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth recommended for generation tasks task_type = \"instruction_following\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , # Longer sequences for instructions max_samples = 1000 , # Column mappings for instruction datasets instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () Dataset Format: { \"instruction\" : \"Explain machine learning\" , \"input\" : \"\" , \"output\" : \"Machine learning is a subset of AI...\" }","title":"1. Instruction Following"},{"location":"user-guide/sft/#2-supervised-fine-tuning","text":"General-purpose fine-tuning for any text-to-text task. trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"databricks/databricks-dolly-15k\" , backend = \"trl\" , task_type = \"supervised_fine_tuning\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , auto_detect_fields = True # Automatically detect dataset fields ) trainer . train ()","title":"2. Supervised Fine-Tuning"},{"location":"user-guide/sft/#3-text-classification","text":"Classify text into predefined categories (sentiment, topic, etc.). trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL recommended for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , # Classification-specific text_column = \"text\" , label_column = \"label\" , num_labels = 2 # Binary classification ) trainer . train () Dataset Format: { \"text\" : \"This movie is fantastic!\" , \"label\" : 1 }","title":"3. Text Classification"},{"location":"user-guide/sft/#4-token-classification","text":"Named Entity Recognition (NER), Part-of-Speech tagging, etc. trainer = create_sft_trainer ( model_name = \"bert-base-uncased\" , dataset_name = \"lhoestq/conll2003\" , backend = \"trl\" , # TRL recommended for token classification task_type = \"token_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , split = \"train\" , # Dataset split tokens_column = \"tokens\" , tags_column = \"ner_tags\" , num_labels = 9 # Number of NER tag classes ) trainer . train () Dataset Format: { \"tokens\" : [ \"Apple\" , \"is\" , \"a\" , \"company\" ], \"ner_tags\" : [ 1 , 0 , 0 , 0 ] # B - ORG , O , O , O }","title":"4. Token Classification"},{"location":"user-guide/sft/#5-text-generation","text":"Generate coherent, context-aware text. trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Salesforce/wikitext\" , backend = \"trl\" , # Unsloth recommended for generation task_type = \"text_generation\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , text_column = \"text\" ) trainer . train ()","title":"5. Text Generation"},{"location":"user-guide/sft/#6-chat-completion","text":"Train conversational AI models with chat templates. trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , backend = \"unsloth\" , task_type = \"chat_completion\" , split = \"train_sft\" , # Dataset split num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , # Longer for conversations gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" # Auto-detect chat template ) trainer . train () Dataset Format: { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Hello!\" }, { \"role\" : \"assistant\" , \"content\" : \"Hi! How can I help?\" } ] }","title":"6. Chat Completion"},{"location":"user-guide/sft/#backend-selection","text":"","title":"Backend Selection"},{"location":"user-guide/sft/#trl-backend-recommended-for-classification","text":"Use TRL when: - Training classification models - Training token classification (NER) - Need maximum compatibility - Working with smaller models trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # Explicit TRL backend task_type = \"text_classification\" )","title":"TRL Backend (Recommended for Classification)"},{"location":"user-guide/sft/#unsloth-backend-recommended-for-generation","text":"Use Unsloth when: - Training generation models (faster) - Working with large models - Need memory efficiency - Training instruction following or chat models trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth for speed task_type = \"instruction_following\" )","title":"Unsloth Backend (Recommended for Generation)"},{"location":"user-guide/sft/#auto-backend-selection","text":"Let AlignTune choose the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , # Automatic selection task_type = \"instruction_following\" )","title":"Auto Backend Selection"},{"location":"user-guide/sft/#configuration-options","text":"","title":"Configuration Options"},{"location":"user-guide/sft/#model-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Model settings max_seq_length = 512 , quantization = { \"load_in_4bit\" : True }, # 4-bit quantization peft_enabled = True , # Enable LoRA lora_r = 16 , # LoRA rank lora_alpha = 32 , # LoRA alpha lora_dropout = 0.1 , # For GPT2 models, use: lora_target_modules=[\"c_attn\", \"c_proj\"] use_gradient_checkpointing = True , # Memory optimization use_flash_attention_2 = True # Flash Attention (if available) )","title":"Model Configuration"},{"location":"user-guide/sft/#training-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Training settings num_epochs = 3 , max_steps = None , # Use epochs instead batch_size = 4 , gradient_accumulation_steps = 4 , # Effective batch size = 16 learning_rate = 2e-4 , weight_decay = 0.01 , warmup_steps = 100 , warmup_ratio = 0.1 , max_grad_norm = 1.0 , # Advanced settings packing = True , # Sequence packing for efficiency # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth packing_strategy = \"bfd\" , # \"bfd\" or \"wrapped\" loss_type = \"nll\" , # \"nll\" or \"dft\" completion_only_loss = True , # Only compute loss on completion activation_offloading = False # Offload activations to CPU )","title":"Training Configuration"},{"location":"user-guide/sft/#dataset-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Dataset settings max_samples = 1000 , # Limit dataset size percent = 10.0 , # Use 10% of dataset split = \"train\" , # Dataset split # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth # Column mappings column_mapping = { \"instruction\" : \"instruction\" , \"output\" : \"response\" }, # Field detection auto_detect_fields = True , # Auto-detect dataset fields dataset_num_proc = 4 # Parallel processing )","title":"Dataset Configuration"},{"location":"user-guide/sft/#advanced-features","text":"","title":"Advanced Features"},{"location":"user-guide/sft/#loraqlora-fine-tuning","text":"Efficient fine-tuning with parameter-efficient methods: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # LoRA configuration peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True , \"bnb_4bit_compute_dtype\" : \"float16\" , \"bnb_4bit_quant_type\" : \"nf4\" } )","title":"LoRA/QLoRA Fine-Tuning"},{"location":"user-guide/sft/#sequence-packing","text":"Pack multiple sequences into a single batch for efficiency: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models or models not supported by Unsloth packing = True , # Enable packing packing_strategy = \"bfd\" , # \"bfd\" (best-fit-decreasing) or \"wrapped\" padding_free = True , # No padding needed with packing max_seq_length = 2048 # Longer sequences with packing )","title":"Sequence Packing"},{"location":"user-guide/sft/#mixed-precision-training","text":"Use FP16 or BF16 for faster training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Precision settings (handled automatically by backend) # Unsloth uses optimal precision automatically # TRL: set fp16=True or bf16=True in training config )","title":"Mixed Precision Training"},{"location":"user-guide/sft/#gradient-checkpointing","text":"Reduce memory usage during training: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , use_gradient_checkpointing = True , # Enable checkpointing activation_offloading = True # Offload activations to CPU )","title":"Gradient Checkpointing"},{"location":"user-guide/sft/#evaluation","text":"","title":"Evaluation"},{"location":"user-guide/sft/#basic-evaluation","text":"# Evaluate on default validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/sft/#custom-evaluation-dataset","text":"from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" )","title":"Custom Evaluation Dataset"},{"location":"user-guide/sft/#zero-shot-evaluation","text":"# Generate predictions for test prompts # Note: Make sure to call trainer.train() or trainer.setup_model() before using predict() test_prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( test_prompts ) for prompt , result in zip ( test_prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Zero-Shot Evaluation"},{"location":"user-guide/sft/#model-management","text":"","title":"Model Management"},{"location":"user-guide/sft/#saving-models","text":"# Save after training model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Save to custom directory model_path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" )","title":"Saving Models"},{"location":"user-guide/sft/#pushing-to-huggingface-hub","text":"# Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , private = False ) print ( f \"Model available at: { url } \" ) # Push with custom commit message url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" )","title":"Pushing to HuggingFace Hub"},{"location":"user-guide/sft/#loading-saved-models","text":"# Load saved model from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Use for inference result = trainer . predict ( \"What is AI?\" )","title":"Loading Saved Models"},{"location":"user-guide/sft/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/sft/#1-choose-the-right-task-type","text":"Instruction Following : For teaching models to follow instructions Text Classification : For categorization tasks Token Classification : For NER, POS tagging Chat Completion : For conversational AI Text Generation : For general text generation","title":"1. Choose the Right Task Type"},{"location":"user-guide/sft/#2-backend-selection","text":"TRL : Use for classification, token classification, maximum compatibility Unsloth : Use for generation tasks (faster), large models","title":"2. Backend Selection"},{"location":"user-guide/sft/#3-memory-optimization","text":"# For large models, use: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , # LoRA quantization = { \"load_in_4bit\" : True }, # 4-bit quantization use_gradient_checkpointing = True , # Gradient checkpointing batch_size = 1 , # Smaller batch size gradient_accumulation_steps = 8 # Compensate with accumulation )","title":"3. Memory Optimization"},{"location":"user-guide/sft/#4-learning-rate-scheduling","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , learning_rate = 2e-4 , warmup_steps = 100 , # Warmup for stable training warmup_ratio = 0.1 # Or use ratio instead )","title":"4. Learning Rate Scheduling"},{"location":"user-guide/sft/#5-dataset-size","text":"Start with a subset ( max_samples=1000 ) for testing Gradually increase dataset size Use percent parameter for quick experiments","title":"5. Dataset Size"},{"location":"user-guide/sft/#6-sequence-length","text":"Classification : 512 tokens is usually sufficient Instruction Following : 1024-2048 tokens Chat Completion : 2048+ tokens for conversations Text Generation : 1024-2048 tokens","title":"6. Sequence Length"},{"location":"user-guide/sft/#7-regular-checkpointing","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , save_interval = 500 , # Save every 500 steps eval_interval = 100 # Evaluate every 100 steps )","title":"7. Regular Checkpointing"},{"location":"user-guide/sft/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/sft/#out-of-memory-errors","text":"# Reduce batch size and use gradient accumulation trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 , # Reduce batch size gradient_accumulation_steps = 8 , # Increase accumulation use_gradient_checkpointing = True , # Enable checkpointing quantization = { \"load_in_4bit\" : True } # Use quantization )","title":"Out of Memory Errors"},{"location":"user-guide/sft/#slow-training","text":"# Use Unsloth backend for generation tasks trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # faster packing = True , # Enable sequence packing dataset_num_proc = 4 # Parallel data processing )","title":"Slow Training"},{"location":"user-guide/sft/#poor-model-performance","text":"# Increase training duration and adjust learning rate trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 5 , # More epochs learning_rate = 1e-4 , # Lower learning rate warmup_steps = 200 , # More warmup max_samples = None # Use full dataset )","title":"Poor Model Performance"},{"location":"user-guide/sft/#dataset-format-issues","text":"# Use column mapping for custom datasets trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"my-custom-dataset\" , column_mapping = { \"instruction\" : \"prompt\" , # Map custom column names \"output\" : \"completion\" }, auto_detect_fields = False # Disable auto-detection )","title":"Dataset Format Issues"},{"location":"user-guide/sft/#examples","text":"","title":"Examples"},{"location":"user-guide/sft/#complete-instruction-following-example","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"gpt2\" , # Use Qwen/Llama instead of GPT2 for Unsloth dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , max_samples = 1000 , peft_enabled = True , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 models quantization = { \"load_in_4bit\" : True } ) # Train print ( \"Starting training...\" ) trainer . train () # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test result = trainer . predict ( \"What is machine learning?\" ) print ( f \"Prediction: { result } \" )","title":"Complete Instruction Following Example"},{"location":"user-guide/sft/#complete-classification-example","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) # Save model_path = trainer . save_model ()","title":"Complete Classification Example"},{"location":"user-guide/sft/#next-steps","text":"RL Training Guide - Learn about reinforcement learning training Reward Functions Guide - Explore reward functions for RL Evaluation Guide - Comprehensive evaluation guide Model Management - Advanced model management","title":"Next Steps"},{"location":"user-guide/sft/#additional-resources","text":"API Reference - Complete API documentation Examples - More SFT examples Backend Selection - Detailed backend guide Configuration Guide - Configuration reference","title":"Additional Resources"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting \u00b6 This guide helps you resolve common issues when using AlignTune. Backend Issues \u00b6 Unsloth Not Available \u00b6 Error : Unsloth not available or ImportError: Unsloth backend not available Causes : - Unsloth not installed - CUDA not available - PyTorch version mismatch - CUDA version incompatibility Solutions : Install Unsloth : pip install unsloth Check CUDA Availability : import torch print ( torch . cuda . is_available ()) print ( torch . version . cuda ) Use TRL Backend Instead : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Use TRL instead ) Run Diagnostics : aligntune diagnose Backend Interference \u00b6 Error : Unsloth patches interfering with TRL backend Cause : Importing BackendType enum triggers Unsloth loading Solution : Use string-based backend selection # CORRECT - String-based selection from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" # Use string, not BackendType enum ) # AVOID - Importing BackendType causes Unsloth interference from aligntune.core.backend_factory import create_rl_trainer , BackendType # This import triggers Unsloth loading even when using TRL backend CUDA and GPU Issues \u00b6 CUDA Out of Memory (OOM) \u00b6 Error : RuntimeError: CUDA out of memory Solutions : Reduce Batch Size : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 # Reduce from default ) Enable Gradient Checkpointing : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , gradient_checkpointing = True ) Use LoRA/QLoRA : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , use_peft = True , lora_r = 8 , # Lower rank = less memory lora_alpha = 16 ) Use 4-bit Quantization (Unsloth): trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Use CPU : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , device_map = \"cpu\" ) CUDA Symbol Errors \u00b6 Error : CUDA symbol errors or undefined symbol: cublasLtMatmul Cause : PyTorch and CUDA version mismatch Solutions : Use TRL Backend : backend = \"trl\" # TRL is more compatible Reinstall PyTorch with Matching CUDA : pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Check Compatibility : aligntune diagnose Configuration Issues \u00b6 Invalid Configuration \u00b6 Error : ConfigurationError or validation errors Solutions : Validate Configuration : aligntune validate config.yaml Check Required Parameters : model_name : Must be a valid HuggingFace model ID dataset_name : Must be a valid HuggingFace dataset ID algorithm : Must be one of: dpo , ppo , grpo , gspo , dapo , drgrpo Check Parameter Types : # CORRECT num_epochs = 3 # int learning_rate = 5e-5 # float batch_size = 4 # int # INCORRECT num_epochs = \"3\" # string learning_rate = \"5e-5\" # string Missing Required Parameters \u00b6 Error : ValueError: Missing required parameter Common Missing Parameters : RL Training without Algorithm : # MISSING algorithm trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" ) # CORRECT trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" ) PPO without Reward Model : # PPO requires reward model trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"your-reward-model\" # Required for PPO ) Dataset Issues \u00b6 Dataset Not Found \u00b6 Error : DatasetNotFoundError or FileNotFoundError Solutions : Check Dataset Name : # Use correct HuggingFace dataset ID dataset_name = \"tatsu-lab/alpaca\" # dataset_name = \"alpaca\" # May not work Check Dataset Access : Some datasets require authentication Use huggingface-cli login to authenticate Use Local Dataset : from datasets import load_dataset dataset = load_dataset ( \"path/to/local/dataset\" ) Dataset Format Issues \u00b6 Error : KeyError or missing columns Solutions : Specify Column Mapping : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , column_mapping = { \"instruction\" : \"prompt\" , \"output\" : \"response\" } ) Check Dataset Structure : from datasets import load_dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" ) print ( dataset [ \"train\" ][ 0 ]) # Check structure Training Issues \u00b6 Loss Not Decreasing \u00b6 Symptoms : Loss stays constant or increases Solutions : Reduce Learning Rate : learning_rate = 1e-6 # Try lower learning rate Increase Warmup Steps : warmup_steps = 100 warmup_ratio = 0.1 Check Data Quality : Verify dataset is correct Check for data preprocessing issues Use Different Optimizer : optimizer = \"adamw_torch\" # or \"lion\", \"adafactor\" Training Too Slow \u00b6 Solutions : Use Unsloth Backend : backend = \"unsloth\" # faster Increase Batch Size : batch_size = 8 # If memory allows gradient_accumulation_steps = 4 Use Mixed Precision : precision = \"bf16\" # or \"fp16\" Enable Flash Attention : use_flash_attention_2 = True Training Crashes \u00b6 Error : Training process crashes or hangs Solutions : Check Logs : # Check training logs tail -f output/training.log Reduce Batch Size : batch_size = 1 # Minimal batch size Disable Gradient Checkpointing (if causing issues): gradient_checkpointing = False Use TRL Backend (more stable): backend = \"trl\" Model Issues \u00b6 Model Not Found \u00b6 Error : ModelNotFoundError or OSError: Can't load model Solutions : Check Model Name : # Use correct HuggingFace model ID model_name = \"microsoft/DialoGPT-small\" # model_name = \"DialoGPT-small\" # May not work Check Model Access : Some models require authentication Use huggingface-cli login to authenticate Use Local Model : model_name = \"./path/to/local/model\" Model Loading Errors \u00b6 Error : RuntimeError during model loading Solutions : Check Model Compatibility : Verify model architecture is supported Check model size vs. available memory Use Device Map : device_map = \"auto\" # Automatic device placement Use Quantization : quantization = { \"load_in_4bit\" : True } # 4-bit quantization Evaluation Issues \u00b6 Evaluation Fails \u00b6 Error : EvaluationError or evaluation crashes Solutions : Check Evaluation Dataset : # Ensure eval dataset exists if trainer . eval_dataset is None : print ( \"No evaluation dataset available\" ) Reduce Evaluation Samples : max_samples_for_quality_metrics = 50 # Limit samples Skip Quality Metrics : compute_rouge = False compute_bleu = False Memory Issues \u00b6 Out of Memory (OOM) \u00b6 Solutions : Reduce Model Size : Use smaller models Use quantized models (4-bit, 8-bit) Optimize Training : gradient_checkpointing = True activation_offloading = True use_peft = True # LoRA reduces memory Reduce Sequence Length : max_seq_length = 256 # Reduce from 512 Use Gradient Accumulation : batch_size = 1 gradient_accumulation_steps = 8 # Simulates batch_size=8 Getting Help \u00b6 Diagnostic Commands \u00b6 # Run comprehensive diagnostics aligntune diagnose # Validate configuration aligntune validate config.yaml # Check backend availability aligntune list-backends-cmd Useful Resources \u00b6 Unsloth Compatibility Guide - Unsloth-specific issues Backend Selection Guide - Backend comparison Configuration Guide - Configuration options GitHub Issues - Report bugs Reporting Issues \u00b6 When reporting issues, please include: Error Message : Full error traceback Configuration : Your configuration file or code Environment : Python version, PyTorch version, CUDA version Diagnostics : Output of aligntune diagnose Common Error Messages \u00b6 Backend not available \u00b6 Solution : Use a different backend or install missing dependencies Algorithm not supported \u00b6 Solution : Check algorithm name and backend compatibility Dataset format error \u00b6 Solution : Check dataset structure and column mappings Model loading failed \u00b6 Solution : Verify model name and access permissions CUDA out of memory \u00b6 Solution : Reduce batch size, enable gradient checkpointing, or use quantization Prevention Tips \u00b6 Always Validate Configuration : Use aligntune validate before training Start Small : Test with small datasets and models first Monitor Resources : Watch GPU memory and CPU usage Use Auto Backend : Let AlignTune select the best backend Check Compatibility : Run aligntune diagnose regularly Next Steps \u00b6 Backend Selection - Choose the right backend Configuration Guide - Configure training properly Performance Optimization - Optimize training speed and memory","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#troubleshooting","text":"This guide helps you resolve common issues when using AlignTune.","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#backend-issues","text":"","title":"Backend Issues"},{"location":"user-guide/troubleshooting/#unsloth-not-available","text":"Error : Unsloth not available or ImportError: Unsloth backend not available Causes : - Unsloth not installed - CUDA not available - PyTorch version mismatch - CUDA version incompatibility Solutions : Install Unsloth : pip install unsloth Check CUDA Availability : import torch print ( torch . cuda . is_available ()) print ( torch . version . cuda ) Use TRL Backend Instead : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Use TRL instead ) Run Diagnostics : aligntune diagnose","title":"Unsloth Not Available"},{"location":"user-guide/troubleshooting/#backend-interference","text":"Error : Unsloth patches interfering with TRL backend Cause : Importing BackendType enum triggers Unsloth loading Solution : Use string-based backend selection # CORRECT - String-based selection from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" # Use string, not BackendType enum ) # AVOID - Importing BackendType causes Unsloth interference from aligntune.core.backend_factory import create_rl_trainer , BackendType # This import triggers Unsloth loading even when using TRL backend","title":"Backend Interference"},{"location":"user-guide/troubleshooting/#cuda-and-gpu-issues","text":"","title":"CUDA and GPU Issues"},{"location":"user-guide/troubleshooting/#cuda-out-of-memory-oom","text":"Error : RuntimeError: CUDA out of memory Solutions : Reduce Batch Size : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 # Reduce from default ) Enable Gradient Checkpointing : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , gradient_checkpointing = True ) Use LoRA/QLoRA : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , use_peft = True , lora_r = 8 , # Lower rank = less memory lora_alpha = 16 ) Use 4-bit Quantization (Unsloth): trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Use CPU : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , device_map = \"cpu\" )","title":"CUDA Out of Memory (OOM)"},{"location":"user-guide/troubleshooting/#cuda-symbol-errors","text":"Error : CUDA symbol errors or undefined symbol: cublasLtMatmul Cause : PyTorch and CUDA version mismatch Solutions : Use TRL Backend : backend = \"trl\" # TRL is more compatible Reinstall PyTorch with Matching CUDA : pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Check Compatibility : aligntune diagnose","title":"CUDA Symbol Errors"},{"location":"user-guide/troubleshooting/#configuration-issues","text":"","title":"Configuration Issues"},{"location":"user-guide/troubleshooting/#invalid-configuration","text":"Error : ConfigurationError or validation errors Solutions : Validate Configuration : aligntune validate config.yaml Check Required Parameters : model_name : Must be a valid HuggingFace model ID dataset_name : Must be a valid HuggingFace dataset ID algorithm : Must be one of: dpo , ppo , grpo , gspo , dapo , drgrpo Check Parameter Types : # CORRECT num_epochs = 3 # int learning_rate = 5e-5 # float batch_size = 4 # int # INCORRECT num_epochs = \"3\" # string learning_rate = \"5e-5\" # string","title":"Invalid Configuration"},{"location":"user-guide/troubleshooting/#missing-required-parameters","text":"Error : ValueError: Missing required parameter Common Missing Parameters : RL Training without Algorithm : # MISSING algorithm trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" ) # CORRECT trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" ) PPO without Reward Model : # PPO requires reward model trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"your-reward-model\" # Required for PPO )","title":"Missing Required Parameters"},{"location":"user-guide/troubleshooting/#dataset-issues","text":"","title":"Dataset Issues"},{"location":"user-guide/troubleshooting/#dataset-not-found","text":"Error : DatasetNotFoundError or FileNotFoundError Solutions : Check Dataset Name : # Use correct HuggingFace dataset ID dataset_name = \"tatsu-lab/alpaca\" # dataset_name = \"alpaca\" # May not work Check Dataset Access : Some datasets require authentication Use huggingface-cli login to authenticate Use Local Dataset : from datasets import load_dataset dataset = load_dataset ( \"path/to/local/dataset\" )","title":"Dataset Not Found"},{"location":"user-guide/troubleshooting/#dataset-format-issues","text":"Error : KeyError or missing columns Solutions : Specify Column Mapping : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , column_mapping = { \"instruction\" : \"prompt\" , \"output\" : \"response\" } ) Check Dataset Structure : from datasets import load_dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" ) print ( dataset [ \"train\" ][ 0 ]) # Check structure","title":"Dataset Format Issues"},{"location":"user-guide/troubleshooting/#training-issues","text":"","title":"Training Issues"},{"location":"user-guide/troubleshooting/#loss-not-decreasing","text":"Symptoms : Loss stays constant or increases Solutions : Reduce Learning Rate : learning_rate = 1e-6 # Try lower learning rate Increase Warmup Steps : warmup_steps = 100 warmup_ratio = 0.1 Check Data Quality : Verify dataset is correct Check for data preprocessing issues Use Different Optimizer : optimizer = \"adamw_torch\" # or \"lion\", \"adafactor\"","title":"Loss Not Decreasing"},{"location":"user-guide/troubleshooting/#training-too-slow","text":"Solutions : Use Unsloth Backend : backend = \"unsloth\" # faster Increase Batch Size : batch_size = 8 # If memory allows gradient_accumulation_steps = 4 Use Mixed Precision : precision = \"bf16\" # or \"fp16\" Enable Flash Attention : use_flash_attention_2 = True","title":"Training Too Slow"},{"location":"user-guide/troubleshooting/#training-crashes","text":"Error : Training process crashes or hangs Solutions : Check Logs : # Check training logs tail -f output/training.log Reduce Batch Size : batch_size = 1 # Minimal batch size Disable Gradient Checkpointing (if causing issues): gradient_checkpointing = False Use TRL Backend (more stable): backend = \"trl\"","title":"Training Crashes"},{"location":"user-guide/troubleshooting/#model-issues","text":"","title":"Model Issues"},{"location":"user-guide/troubleshooting/#model-not-found","text":"Error : ModelNotFoundError or OSError: Can't load model Solutions : Check Model Name : # Use correct HuggingFace model ID model_name = \"microsoft/DialoGPT-small\" # model_name = \"DialoGPT-small\" # May not work Check Model Access : Some models require authentication Use huggingface-cli login to authenticate Use Local Model : model_name = \"./path/to/local/model\"","title":"Model Not Found"},{"location":"user-guide/troubleshooting/#model-loading-errors","text":"Error : RuntimeError during model loading Solutions : Check Model Compatibility : Verify model architecture is supported Check model size vs. available memory Use Device Map : device_map = \"auto\" # Automatic device placement Use Quantization : quantization = { \"load_in_4bit\" : True } # 4-bit quantization","title":"Model Loading Errors"},{"location":"user-guide/troubleshooting/#evaluation-issues","text":"","title":"Evaluation Issues"},{"location":"user-guide/troubleshooting/#evaluation-fails","text":"Error : EvaluationError or evaluation crashes Solutions : Check Evaluation Dataset : # Ensure eval dataset exists if trainer . eval_dataset is None : print ( \"No evaluation dataset available\" ) Reduce Evaluation Samples : max_samples_for_quality_metrics = 50 # Limit samples Skip Quality Metrics : compute_rouge = False compute_bleu = False","title":"Evaluation Fails"},{"location":"user-guide/troubleshooting/#memory-issues","text":"","title":"Memory Issues"},{"location":"user-guide/troubleshooting/#out-of-memory-oom","text":"Solutions : Reduce Model Size : Use smaller models Use quantized models (4-bit, 8-bit) Optimize Training : gradient_checkpointing = True activation_offloading = True use_peft = True # LoRA reduces memory Reduce Sequence Length : max_seq_length = 256 # Reduce from 512 Use Gradient Accumulation : batch_size = 1 gradient_accumulation_steps = 8 # Simulates batch_size=8","title":"Out of Memory (OOM)"},{"location":"user-guide/troubleshooting/#getting-help","text":"","title":"Getting Help"},{"location":"user-guide/troubleshooting/#diagnostic-commands","text":"# Run comprehensive diagnostics aligntune diagnose # Validate configuration aligntune validate config.yaml # Check backend availability aligntune list-backends-cmd","title":"Diagnostic Commands"},{"location":"user-guide/troubleshooting/#useful-resources","text":"Unsloth Compatibility Guide - Unsloth-specific issues Backend Selection Guide - Backend comparison Configuration Guide - Configuration options GitHub Issues - Report bugs","title":"Useful Resources"},{"location":"user-guide/troubleshooting/#reporting-issues","text":"When reporting issues, please include: Error Message : Full error traceback Configuration : Your configuration file or code Environment : Python version, PyTorch version, CUDA version Diagnostics : Output of aligntune diagnose","title":"Reporting Issues"},{"location":"user-guide/troubleshooting/#common-error-messages","text":"","title":"Common Error Messages"},{"location":"user-guide/troubleshooting/#backend-not-available","text":"Solution : Use a different backend or install missing dependencies","title":"Backend not available"},{"location":"user-guide/troubleshooting/#algorithm-not-supported","text":"Solution : Check algorithm name and backend compatibility","title":"Algorithm not supported"},{"location":"user-guide/troubleshooting/#dataset-format-error","text":"Solution : Check dataset structure and column mappings","title":"Dataset format error"},{"location":"user-guide/troubleshooting/#model-loading-failed","text":"Solution : Verify model name and access permissions","title":"Model loading failed"},{"location":"user-guide/troubleshooting/#cuda-out-of-memory","text":"Solution : Reduce batch size, enable gradient checkpointing, or use quantization","title":"CUDA out of memory"},{"location":"user-guide/troubleshooting/#prevention-tips","text":"Always Validate Configuration : Use aligntune validate before training Start Small : Test with small datasets and models first Monitor Resources : Watch GPU memory and CPU usage Use Auto Backend : Let AlignTune select the best backend Check Compatibility : Run aligntune diagnose regularly","title":"Prevention Tips"},{"location":"user-guide/troubleshooting/#next-steps","text":"Backend Selection - Choose the right backend Configuration Guide - Configure training properly Performance Optimization - Optimize training speed and memory","title":"Next Steps"}]}