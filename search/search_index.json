{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AlignTune is a production-ready fine-tuning library designed to simplify training and fine-tuning of Large Language Models (LLMs) with both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) methods. It provides a high-level, unified API that abstracts away the complexities of backend selection, algorithm configuration, and training loops, letting you focus on delivering results. Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced RLHF algorithms, AlignTune streamlines your workflow for LLM fine-tuning. Core Features \u00b6 Multi-Backend Architecture : Choose between TRL (reliable, battle-tested) and Unsloth (faster) backends with intelligent auto-selection. Complete RLHF Coverage : RL algorithms including DPO, PPO, GRPO, GSPO, DAPO, and Dr. GRPO. 27+ Reward Functions : Built-in reward functions for quality, safety, style, and task-specific metrics. Production-Ready : No mock code, comprehensive error handling, extensive testing, and robust validation. Quick Start \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) # Train the model trainer . train () # Evaluate metrics = trainer . evaluate () print ( metrics ) Get Started \u2192 Supported Algorithms \u00b6 Algorithm TRL Backend Unsloth Backend Description SFT Yes Yes Supervised Fine-Tuning DPO Yes Yes Direct Preference Optimization PPO Yes Yes Proximal Policy Optimization GRPO Yes Yes Group Relative Policy Optimization GSPO Yes No Group Sequential Policy Optimization (TRL only) DAPO Yes Yes Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes GRPO Done Right (unbiased variant) Key Capabilities \u00b6 Multiple Training Paradigms : Supports SFT, DPO, PPO, GRPO, and advanced RL algorithms Backend Flexibility : TRL and Unsloth backends with automatic fallback Reward Model Training : Train custom reward models from rule-based functions Comprehensive Evaluation : Multi-level evaluation with lm-eval integration Production Ready : Model serialization, reproducible training, and deployment-ready pipelines Extensible Architecture : Modular design for easy integration of custom algorithms and backends Demo Notebooks \u00b6 Interactive Colab notebooks demonstrating various AlignTune workflows: Supervised Fine-Tuning (SFT) \u00b6 Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab Reinforcement Learning (RL) \u00b6 Backend Algorithm Dataset Link TRL DPO hh-rlhf Open in Colab TRL GRPO GSM8K Open in Colab TRL PPO openai_summarize_tldr Open in Colab Unsloth DPO distilabel-intel-orca-dpo-pairs Open in Colab Unsloth GRPO GSM8K Open in Colab Explore the Documentation \u00b6 Getting Started : Installation, setup, and basic usage User Guide : In-depth tutorials for SFT and RL training API Reference : Complete Python API and class/method details Examples : End-to-end code examples Advanced Topics : Architecture, custom backends, and performance optimization Notebooks : Interactive Colab notebooks and local Jupyter notebooks Architecture \u00b6 AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend: SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend: SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO See Architecture for details. Why AlignTune? \u00b6 No Boilerplate : Avoids repetitive code for model-specific data loading, training, and inference Consistent Results : Automates best practices for RLHF research and model selection Fast Iteration : Easily compare different algorithms and backends with the same consistent API Production Ready : Model and config serialization for robust deployment and reproducibility Community-Driven : Extensible design and open contribution policy Contributing \u00b6 We welcome contributions! See Contributing Guide for details. License \u00b6 This project is licensed under the AlignTune Source Available License (ASAL) v1.0 - see the LICENSE file for details. Citation \u00b6 If you use AlignTune in your research, please cite: BibTeX: @software { alignTune2025 , title = {AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training} , author = {Chawla, Chirag and Lyngkhoi, Zera and Seth, Pratinav and Avaiya, Utsav and Bhattacharjee, Soham and Khandoga, Mykola and Yuan, Rui and Sankarapu, Vinay Kumar} , year = {2025} , note = {Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth} , organization = {Lexsi Labs} , url = {https://github.com/Lexsi-Labs/aligntune} , version = {0.0.0} } Plain Text: Chawla, C., Lyngkhoi, Z., Seth, P., Avaiya, U., Bhattacharjee, S., Khandoga, M., Yuan, R., & Sankarapu, V. K. (2025). AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training. Lexsi Labs. https://github.com/Lexsi-Labs/aligntune *Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth Acknowledgments \u00b6 AlignTune is built upon the excellent work of the following projects: HuggingFace Transformers - Model architectures and tokenizers TRL - Transformer Reinforcement Learning library Unsloth - Fast and memory-efficient training HuggingFace Datasets - Dataset loading and processing Support \u00b6 Documentation : docs.aligntune.io GitHub Issues : github.com/Lexsi-Labs/aligntune/issues Discussions : github.com/Lexsi-Labs/aligntune/discussions Get started with AlignTune and accelerate your LLM fine-tuning workflows today!","title":"Home"},{"location":"#core-features","text":"Multi-Backend Architecture : Choose between TRL (reliable, battle-tested) and Unsloth (faster) backends with intelligent auto-selection. Complete RLHF Coverage : RL algorithms including DPO, PPO, GRPO, GSPO, DAPO, and Dr. GRPO. 27+ Reward Functions : Built-in reward functions for quality, safety, style, and task-specific metrics. Production-Ready : No mock code, comprehensive error handling, extensive testing, and robust validation.","title":"Core Features"},{"location":"#quick-start","text":"from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) # Train the model trainer . train () # Evaluate metrics = trainer . evaluate () print ( metrics ) Get Started \u2192","title":"Quick Start"},{"location":"#supported-algorithms","text":"Algorithm TRL Backend Unsloth Backend Description SFT Yes Yes Supervised Fine-Tuning DPO Yes Yes Direct Preference Optimization PPO Yes Yes Proximal Policy Optimization GRPO Yes Yes Group Relative Policy Optimization GSPO Yes No Group Sequential Policy Optimization (TRL only) DAPO Yes Yes Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes GRPO Done Right (unbiased variant)","title":"Supported Algorithms"},{"location":"#key-capabilities","text":"Multiple Training Paradigms : Supports SFT, DPO, PPO, GRPO, and advanced RL algorithms Backend Flexibility : TRL and Unsloth backends with automatic fallback Reward Model Training : Train custom reward models from rule-based functions Comprehensive Evaluation : Multi-level evaluation with lm-eval integration Production Ready : Model serialization, reproducible training, and deployment-ready pipelines Extensible Architecture : Modular design for easy integration of custom algorithms and backends","title":"Key Capabilities"},{"location":"#demo-notebooks","text":"Interactive Colab notebooks demonstrating various AlignTune workflows:","title":"Demo Notebooks"},{"location":"#supervised-fine-tuning-sft","text":"Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab","title":"Supervised Fine-Tuning (SFT)"},{"location":"#reinforcement-learning-rl","text":"Backend Algorithm Dataset Link TRL DPO hh-rlhf Open in Colab TRL GRPO GSM8K Open in Colab TRL PPO openai_summarize_tldr Open in Colab Unsloth DPO distilabel-intel-orca-dpo-pairs Open in Colab Unsloth GRPO GSM8K Open in Colab","title":"Reinforcement Learning (RL)"},{"location":"#explore-the-documentation","text":"Getting Started : Installation, setup, and basic usage User Guide : In-depth tutorials for SFT and RL training API Reference : Complete Python API and class/method details Examples : End-to-end code examples Advanced Topics : Architecture, custom backends, and performance optimization Notebooks : Interactive Colab notebooks and local Jupyter notebooks","title":"Explore the Documentation"},{"location":"#architecture","text":"AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend: SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend: SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO See Architecture for details.","title":"Architecture"},{"location":"#why-aligntune","text":"No Boilerplate : Avoids repetitive code for model-specific data loading, training, and inference Consistent Results : Automates best practices for RLHF research and model selection Fast Iteration : Easily compare different algorithms and backends with the same consistent API Production Ready : Model and config serialization for robust deployment and reproducibility Community-Driven : Extensible design and open contribution policy","title":"Why AlignTune?"},{"location":"#contributing","text":"We welcome contributions! See Contributing Guide for details.","title":"Contributing"},{"location":"#license","text":"This project is licensed under the AlignTune Source Available License (ASAL) v1.0 - see the LICENSE file for details.","title":"License"},{"location":"#citation","text":"If you use AlignTune in your research, please cite: BibTeX: @software { alignTune2025 , title = {AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training} , author = {Chawla, Chirag and Lyngkhoi, Zera and Seth, Pratinav and Avaiya, Utsav and Bhattacharjee, Soham and Khandoga, Mykola and Yuan, Rui and Sankarapu, Vinay Kumar} , year = {2025} , note = {Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth} , organization = {Lexsi Labs} , url = {https://github.com/Lexsi-Labs/aligntune} , version = {0.0.0} } Plain Text: Chawla, C., Lyngkhoi, Z., Seth, P., Avaiya, U., Bhattacharjee, S., Khandoga, M., Yuan, R., & Sankarapu, V. K. (2025). AlignTune: A Comprehensive Fine-Tuning Library for SFT and RL Training. Lexsi Labs. https://github.com/Lexsi-Labs/aligntune *Equal contribution: Chirag Chawla, Zera Lyngkhoi, Pratinav Seth","title":"Citation"},{"location":"#acknowledgments","text":"AlignTune is built upon the excellent work of the following projects: HuggingFace Transformers - Model architectures and tokenizers TRL - Transformer Reinforcement Learning library Unsloth - Fast and memory-efficient training HuggingFace Datasets - Dataset loading and processing","title":"Acknowledgments"},{"location":"#support","text":"Documentation : docs.aligntune.io GitHub Issues : github.com/Lexsi-Labs/aligntune/issues Discussions : github.com/Lexsi-Labs/aligntune/discussions Get started with AlignTune and accelerate your LLM fine-tuning workflows today!","title":"Support"},{"location":"ISSUES/","text":"Known Issues and Troubleshooting \u00b6 This document tracks known issues, common problems, and their solutions. Reporting Issues \u00b6 If you encounter an issue not listed here, please report it: GitHub Issues : Open an issue Discussions : GitHub Discussions Common Issues \u00b6 Import Errors \u00b6 Problem : ImportError: cannot import name 'create_sft_trainer' Solution : Ensure you're using the correct import path: from aligntune.core.backend_factory import create_sft_trainer Backend Availability \u00b6 Problem : Unsloth backend not available Solution : - Check GPU compatibility - Install Unsloth: pip install unsloth - Use TRL backend as fallback: backend=\"trl\" Memory Issues \u00b6 Problem : Out of memory errors during training Solution : - Reduce batch_size - Enable PEFT/LoRA: peft_enabled=True - Use gradient checkpointing - Reduce max_seq_length - Use quantization: quantization={\"load_in_4bit\": True} Backend-Specific Issues \u00b6 TRL Backend \u00b6 See TRL Backend Documentation for TRL-specific issues Unsloth Backend \u00b6 See Unsloth Compatibility Guide for detailed troubleshooting Getting Help \u00b6 For more detailed troubleshooting, see: - Troubleshooting Guide - Unsloth Compatibility - Backend Support Matrix","title":"Known Issues and Troubleshooting"},{"location":"ISSUES/#known-issues-and-troubleshooting","text":"This document tracks known issues, common problems, and their solutions.","title":"Known Issues and Troubleshooting"},{"location":"ISSUES/#reporting-issues","text":"If you encounter an issue not listed here, please report it: GitHub Issues : Open an issue Discussions : GitHub Discussions","title":"Reporting Issues"},{"location":"ISSUES/#common-issues","text":"","title":"Common Issues"},{"location":"ISSUES/#import-errors","text":"Problem : ImportError: cannot import name 'create_sft_trainer' Solution : Ensure you're using the correct import path: from aligntune.core.backend_factory import create_sft_trainer","title":"Import Errors"},{"location":"ISSUES/#backend-availability","text":"Problem : Unsloth backend not available Solution : - Check GPU compatibility - Install Unsloth: pip install unsloth - Use TRL backend as fallback: backend=\"trl\"","title":"Backend Availability"},{"location":"ISSUES/#memory-issues","text":"Problem : Out of memory errors during training Solution : - Reduce batch_size - Enable PEFT/LoRA: peft_enabled=True - Use gradient checkpointing - Reduce max_seq_length - Use quantization: quantization={\"load_in_4bit\": True}","title":"Memory Issues"},{"location":"ISSUES/#backend-specific-issues","text":"","title":"Backend-Specific Issues"},{"location":"ISSUES/#trl-backend","text":"See TRL Backend Documentation for TRL-specific issues","title":"TRL Backend"},{"location":"ISSUES/#unsloth-backend","text":"See Unsloth Compatibility Guide for detailed troubleshooting","title":"Unsloth Backend"},{"location":"ISSUES/#getting-help","text":"For more detailed troubleshooting, see: - Troubleshooting Guide - Unsloth Compatibility - Backend Support Matrix","title":"Getting Help"},{"location":"OVERALL_SUMMARY/","text":"AlignTune Summary \u00b6 Backend Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Quick Start \u00b6 Supervised Fine-Tuning (SFT) \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"gsm8k\" , backend = \"trl\" , output_dir = \"./output/sft_model\" , max_seq_length = 512 , use_unsloth = False , peft_enabled = False , batch_size = 2 , learning_rate = 1e-5 , # Data Configuration subset = \"main\" , task_type = \"instruction_following\" , column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }, # Training Configuration max_steps = 5 , num_epochs = 1 , # Logging run_name = \"sft_training_run\" ) # Start training trainer . train () Reinforcement Learning Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , ) # Start training trainer . train () Detailed Usage \u00b6 1. Basic RL Training \u00b6 The simplest way to start training with any RL algorithm: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , # Options: dpo, ppo, grpo, gspo, dapo, etc. backend = \"trl\" , # Options: trl, unsloth output_dir = \"./output/my_model\" , batch_size = 8 , ) 2. Using Configuration Files \u00b6 You can organize your training settings in YAML or JSON configuration files for better reproducibility and version control: YAML Configuration Example \u00b6 config/grpo_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" config_name : \"main\" algorithm : \"grpo\" backend : \"trl\" output_dir : \"./output/grpo_model\" batch_size : 8 max_steps : 100 learning_rate : 2e-4 max_seq_length : 512 # Column mapping for dataset column_mapping : question : \"prompt\" answer : \"response\" # Reward configuration rewards : - type : \"accuracy\" weight : 1.0 Using the config file: from aligntune.core.backend_factory import create_rl_trainer # Method 1: Pass config file path trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" ) # Method 2: Override specific parameters trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" , batch_size = 16 , # Override the config value max_steps = 200 # Override the config value ) trainer . train () Python Dictionary Configuration \u00b6 from aligntune.core.backend_factory import create_rl_trainer config = { \"model_name\" : \"Qwen/Qwen3-0.6B\" , \"dataset_name\" : \"openai/gsm8k\" , \"config_name\" : \"main\" , \"algorithm\" : \"dpo\" , \"backend\" : \"trl\" , \"output_dir\" : \"./output/dpo_model\" , \"batch_size\" : 8 , \"max_steps\" : 100 , \"learning_rate\" : 1e-5 , } # Method 1: Pass as config parameter trainer = create_rl_trainer ( config = config ) # Method 2: Unpack as kwargs trainer = create_rl_trainer ( ** config ) # Method 3: Mix config with overrides trainer = create_rl_trainer ( config = config , batch_size = 16 # Override ) trainer . train () SFT Configuration Example \u00b6 config/sft_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" backend : \"trl\" output_dir : \"./output/sft_model\" batch_size : 8 num_epochs : 3 learning_rate : 2e-4 max_seq_length : 512 # Dataset configuration subset : \"main\" task_type : \"instruction_following\" column_mapping : question : \"instruction\" answer : \"response\" # Training settings gradient_accumulation_steps : 4 warmup_ratio : 0.1 Using the SFT config: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( config = \"config/sft_training.yaml\" ) trainer . train () 3. Custom Data Processing \u00b6 Define a custom preprocessing function to transform your data: def preprocess_function ( example ): \"\"\"Transform raw data into the required format\"\"\" return { \"prompt\" : example [ \"question\" ], \"chosen\" : example [ \"answer\" ], \"rejected\" : \"None, 0\" } trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/dpo_model\" , batch_size = 8 , processing_fn = preprocess_function , ) 4. Column Mapping \u00b6 Map dataset columns to expected format without writing a preprocessing function: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"google-research-datasets/mbpp\" , algorithm = \"bolt\" , column_mapping = { \"text\" : \"prompt\" , \"code\" : \"response\" }, backend = \"trl\" , output_dir = \"./output/bolt_model\" , batch_size = 8 , num_generations = 4 , ) 5. Custom Reward Functions \u00b6 For algorithms like GRPO, GSPO, and Neural Mirror GRPO: def my_math_reward ( text , reference = None , ** kwargs ): \"\"\" Custom reward function for math problems. Returns 1.0 if the answer contains numbers, 0.0 otherwise. \"\"\" has_numbers = any ( char . isdigit () for char in text ) return 1.0 if has_numbers else 0.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"unsloth\" , output_dir = \"./output/grpo_custom_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 1.0 , \"params\" : { \"reward_function\" : my_math_reward } } ] ) Multiple Reward Functions \u00b6 You can combine multiple reward functions with different weights: def accuracy_reward ( text , reference = None , ** kwargs ): \"\"\"Check if output matches reference\"\"\" if reference and text . strip () == reference . strip (): return 1.0 return 0.0 def length_penalty ( text , reference = None , ** kwargs ): \"\"\"Penalize overly long responses\"\"\" max_length = 200 return max ( 0.0 , 1.0 - ( len ( text ) - max_length ) / max_length ) if len ( text ) > max_length else 1.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_multi_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 0.7 , \"params\" : { \"reward_function\" : accuracy_reward } }, { \"type\" : \"custom\" , \"weight\" : 0.3 , \"params\" : { \"reward_function\" : length_penalty } } ] ) Configuration \u00b6 Common Parameters \u00b6 Parameter Type Description Default model_name str HuggingFace model identifier Required dataset_name str HuggingFace dataset identifier Required algorithm str RL algorithm to use Required backend str Training backend: \"trl\" or \"unsloth\" \"trl\" output_dir str Directory for saving outputs Required batch_size int Training batch size 8 learning_rate float Learning rate 1e-5 max_seq_length int Maximum sequence length 512 num_epochs int Number of training epochs 1 max_steps int Maximum training steps (overrides epochs) None config str/dict Path to config file or config dictionary None Configuration File Formats \u00b6 AlignTune supports two configuration file formats: YAML : .yaml or .yml files JSON : .json files Both formats support the same parameters and can be used interchangeably. Advanced Parameters \u00b6 For a complete list of available parameters, see PARAMETERS.md . Best Practices \u00b6 1. Dataset Configuration \u00b6 When working with datasets that have multiple subsets: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , # Important: specify the subset algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/model\" , ) 2. Dataset Loading Changes \u00b6 Note: Newer versions of the datasets library no longer support .py scripts in the data loader. Make sure to: - Use datasets with proper configuration files - Update your dataset loading code accordingly - Use config_name parameter for dataset subsets 3. Column Mapping Strategy \u00b6 Always map your dataset columns to match the expected prompt format: # Example: Mapping code dataset columns column_mapping = { \"text\" : \"prompt\" , # Input text/question \"code\" : \"response\" # Expected output/code } # Example: Mapping Q&A dataset columns column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" } 4. Configuration File Best Practices \u00b6 Version Control : Keep config files in version control for reproducibility Environment-Specific Configs : Create separate configs for dev/prod environments Parameter Overrides : Use command-line/code overrides for quick experiments Documentation : Add comments to YAML configs to document parameter choices Example directory structure: project/ configs/ base_grpo.yaml # Base configuration grpo_small_model.yaml # Small model variant grpo_production.yaml # Production settings train.py evaluate.py Evaluation \u00b6 For unified evaluation and custom metrics, see the Evaluation Guide . Troubleshooting \u00b6 Common Issues \u00b6 Import Errors # Ensure all dependencies are installed pip install -r requirements.txt Dataset Loading Issues Check that config_name matches your dataset's subset name Verify dataset exists on HuggingFace Hub Ensure proper column mapping Memory Issues Reduce batch_size Enable peft_enabled=True for parameter-efficient training Use gradient checkpointing Reduce max_seq_length Backend Compatibility Check the Backend Support Matrix for algorithm-backend compatibility GSPO is not currently supported with Unsloth backend Configuration File Issues Ensure YAML syntax is correct (proper indentation) Verify file path is correct when using config= parameter Check that all required parameters are present in the config Getting Help \u00b6 Check Examples for working examples Review PARAMETERS.md for parameter documentation Open an issue on GitHub with: Error message Code snippet Environment details (Python version, GPU info, etc.) Additional Resources \u00b6 Evaluation Guide : # Parameters Reference : PARAMETERS.md Examples : examples/overview.md - Working examples","title":"AlignTune Summary"},{"location":"OVERALL_SUMMARY/#aligntune-summary","text":"","title":"AlignTune Summary"},{"location":"OVERALL_SUMMARY/#backend-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Backend Support Matrix"},{"location":"OVERALL_SUMMARY/#quick-start","text":"","title":"Quick Start"},{"location":"OVERALL_SUMMARY/#supervised-fine-tuning-sft","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"gsm8k\" , backend = \"trl\" , output_dir = \"./output/sft_model\" , max_seq_length = 512 , use_unsloth = False , peft_enabled = False , batch_size = 2 , learning_rate = 1e-5 , # Data Configuration subset = \"main\" , task_type = \"instruction_following\" , column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }, # Training Configuration max_steps = 5 , num_epochs = 1 , # Logging run_name = \"sft_training_run\" ) # Start training trainer . train ()","title":"Supervised Fine-Tuning (SFT)"},{"location":"OVERALL_SUMMARY/#reinforcement-learning-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_model\" , batch_size = 8 , ) # Start training trainer . train ()","title":"Reinforcement Learning Training"},{"location":"OVERALL_SUMMARY/#detailed-usage","text":"","title":"Detailed Usage"},{"location":"OVERALL_SUMMARY/#1-basic-rl-training","text":"The simplest way to start training with any RL algorithm: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , # Options: dpo, ppo, grpo, gspo, dapo, etc. backend = \"trl\" , # Options: trl, unsloth output_dir = \"./output/my_model\" , batch_size = 8 , )","title":"1. Basic RL Training"},{"location":"OVERALL_SUMMARY/#2-using-configuration-files","text":"You can organize your training settings in YAML or JSON configuration files for better reproducibility and version control:","title":"2. Using Configuration Files"},{"location":"OVERALL_SUMMARY/#yaml-configuration-example","text":"config/grpo_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" config_name : \"main\" algorithm : \"grpo\" backend : \"trl\" output_dir : \"./output/grpo_model\" batch_size : 8 max_steps : 100 learning_rate : 2e-4 max_seq_length : 512 # Column mapping for dataset column_mapping : question : \"prompt\" answer : \"response\" # Reward configuration rewards : - type : \"accuracy\" weight : 1.0 Using the config file: from aligntune.core.backend_factory import create_rl_trainer # Method 1: Pass config file path trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" ) # Method 2: Override specific parameters trainer = create_rl_trainer ( config = \"config/grpo_training.yaml\" , batch_size = 16 , # Override the config value max_steps = 200 # Override the config value ) trainer . train ()","title":"YAML Configuration Example"},{"location":"OVERALL_SUMMARY/#python-dictionary-configuration","text":"from aligntune.core.backend_factory import create_rl_trainer config = { \"model_name\" : \"Qwen/Qwen3-0.6B\" , \"dataset_name\" : \"openai/gsm8k\" , \"config_name\" : \"main\" , \"algorithm\" : \"dpo\" , \"backend\" : \"trl\" , \"output_dir\" : \"./output/dpo_model\" , \"batch_size\" : 8 , \"max_steps\" : 100 , \"learning_rate\" : 1e-5 , } # Method 1: Pass as config parameter trainer = create_rl_trainer ( config = config ) # Method 2: Unpack as kwargs trainer = create_rl_trainer ( ** config ) # Method 3: Mix config with overrides trainer = create_rl_trainer ( config = config , batch_size = 16 # Override ) trainer . train ()","title":"Python Dictionary Configuration"},{"location":"OVERALL_SUMMARY/#sft-configuration-example","text":"config/sft_training.yaml: model_name : \"Qwen/Qwen3-0.6B\" dataset_name : \"openai/gsm8k\" backend : \"trl\" output_dir : \"./output/sft_model\" batch_size : 8 num_epochs : 3 learning_rate : 2e-4 max_seq_length : 512 # Dataset configuration subset : \"main\" task_type : \"instruction_following\" column_mapping : question : \"instruction\" answer : \"response\" # Training settings gradient_accumulation_steps : 4 warmup_ratio : 0.1 Using the SFT config: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( config = \"config/sft_training.yaml\" ) trainer . train ()","title":"SFT Configuration Example"},{"location":"OVERALL_SUMMARY/#3-custom-data-processing","text":"Define a custom preprocessing function to transform your data: def preprocess_function ( example ): \"\"\"Transform raw data into the required format\"\"\" return { \"prompt\" : example [ \"question\" ], \"chosen\" : example [ \"answer\" ], \"rejected\" : \"None, 0\" } trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/dpo_model\" , batch_size = 8 , processing_fn = preprocess_function , )","title":"3. Custom Data Processing"},{"location":"OVERALL_SUMMARY/#4-column-mapping","text":"Map dataset columns to expected format without writing a preprocessing function: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"google-research-datasets/mbpp\" , algorithm = \"bolt\" , column_mapping = { \"text\" : \"prompt\" , \"code\" : \"response\" }, backend = \"trl\" , output_dir = \"./output/bolt_model\" , batch_size = 8 , num_generations = 4 , )","title":"4. Column Mapping"},{"location":"OVERALL_SUMMARY/#5-custom-reward-functions","text":"For algorithms like GRPO, GSPO, and Neural Mirror GRPO: def my_math_reward ( text , reference = None , ** kwargs ): \"\"\" Custom reward function for math problems. Returns 1.0 if the answer contains numbers, 0.0 otherwise. \"\"\" has_numbers = any ( char . isdigit () for char in text ) return 1.0 if has_numbers else 0.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"unsloth\" , output_dir = \"./output/grpo_custom_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 1.0 , \"params\" : { \"reward_function\" : my_math_reward } } ] )","title":"5. Custom Reward Functions"},{"location":"OVERALL_SUMMARY/#multiple-reward-functions","text":"You can combine multiple reward functions with different weights: def accuracy_reward ( text , reference = None , ** kwargs ): \"\"\"Check if output matches reference\"\"\" if reference and text . strip () == reference . strip (): return 1.0 return 0.0 def length_penalty ( text , reference = None , ** kwargs ): \"\"\"Penalize overly long responses\"\"\" max_length = 200 return max ( 0.0 , 1.0 - ( len ( text ) - max_length ) / max_length ) if len ( text ) > max_length else 1.0 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , algorithm = \"grpo\" , backend = \"trl\" , output_dir = \"./output/grpo_multi_reward\" , batch_size = 4 , rewards = [ { \"type\" : \"custom\" , \"weight\" : 0.7 , \"params\" : { \"reward_function\" : accuracy_reward } }, { \"type\" : \"custom\" , \"weight\" : 0.3 , \"params\" : { \"reward_function\" : length_penalty } } ] )","title":"Multiple Reward Functions"},{"location":"OVERALL_SUMMARY/#configuration","text":"","title":"Configuration"},{"location":"OVERALL_SUMMARY/#common-parameters","text":"Parameter Type Description Default model_name str HuggingFace model identifier Required dataset_name str HuggingFace dataset identifier Required algorithm str RL algorithm to use Required backend str Training backend: \"trl\" or \"unsloth\" \"trl\" output_dir str Directory for saving outputs Required batch_size int Training batch size 8 learning_rate float Learning rate 1e-5 max_seq_length int Maximum sequence length 512 num_epochs int Number of training epochs 1 max_steps int Maximum training steps (overrides epochs) None config str/dict Path to config file or config dictionary None","title":"Common Parameters"},{"location":"OVERALL_SUMMARY/#configuration-file-formats","text":"AlignTune supports two configuration file formats: YAML : .yaml or .yml files JSON : .json files Both formats support the same parameters and can be used interchangeably.","title":"Configuration File Formats"},{"location":"OVERALL_SUMMARY/#advanced-parameters","text":"For a complete list of available parameters, see PARAMETERS.md .","title":"Advanced Parameters"},{"location":"OVERALL_SUMMARY/#best-practices","text":"","title":"Best Practices"},{"location":"OVERALL_SUMMARY/#1-dataset-configuration","text":"When working with datasets that have multiple subsets: trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"openai/gsm8k\" , config_name = \"main\" , # Important: specify the subset algorithm = \"dpo\" , backend = \"trl\" , output_dir = \"./output/model\" , )","title":"1. Dataset Configuration"},{"location":"OVERALL_SUMMARY/#2-dataset-loading-changes","text":"Note: Newer versions of the datasets library no longer support .py scripts in the data loader. Make sure to: - Use datasets with proper configuration files - Update your dataset loading code accordingly - Use config_name parameter for dataset subsets","title":"2. Dataset Loading Changes"},{"location":"OVERALL_SUMMARY/#3-column-mapping-strategy","text":"Always map your dataset columns to match the expected prompt format: # Example: Mapping code dataset columns column_mapping = { \"text\" : \"prompt\" , # Input text/question \"code\" : \"response\" # Expected output/code } # Example: Mapping Q&A dataset columns column_mapping = { \"question\" : \"instruction\" , \"answer\" : \"response\" }","title":"3. Column Mapping Strategy"},{"location":"OVERALL_SUMMARY/#4-configuration-file-best-practices","text":"Version Control : Keep config files in version control for reproducibility Environment-Specific Configs : Create separate configs for dev/prod environments Parameter Overrides : Use command-line/code overrides for quick experiments Documentation : Add comments to YAML configs to document parameter choices Example directory structure: project/ configs/ base_grpo.yaml # Base configuration grpo_small_model.yaml # Small model variant grpo_production.yaml # Production settings train.py evaluate.py","title":"4. Configuration File Best Practices"},{"location":"OVERALL_SUMMARY/#evaluation","text":"For unified evaluation and custom metrics, see the Evaluation Guide .","title":"Evaluation"},{"location":"OVERALL_SUMMARY/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"OVERALL_SUMMARY/#common-issues","text":"Import Errors # Ensure all dependencies are installed pip install -r requirements.txt Dataset Loading Issues Check that config_name matches your dataset's subset name Verify dataset exists on HuggingFace Hub Ensure proper column mapping Memory Issues Reduce batch_size Enable peft_enabled=True for parameter-efficient training Use gradient checkpointing Reduce max_seq_length Backend Compatibility Check the Backend Support Matrix for algorithm-backend compatibility GSPO is not currently supported with Unsloth backend Configuration File Issues Ensure YAML syntax is correct (proper indentation) Verify file path is correct when using config= parameter Check that all required parameters are present in the config","title":"Common Issues"},{"location":"OVERALL_SUMMARY/#getting-help","text":"Check Examples for working examples Review PARAMETERS.md for parameter documentation Open an issue on GitHub with: Error message Code snippet Environment details (Python version, GPU info, etc.)","title":"Getting Help"},{"location":"OVERALL_SUMMARY/#additional-resources","text":"Evaluation Guide : # Parameters Reference : PARAMETERS.md Examples : examples/overview.md - Working examples","title":"Additional Resources"},{"location":"PARAMETERS/","text":"AlignTune Configuration Parameters \u00b6 This document provides a comprehensive reference for all configuration parameters available in AlignTune, organized by training type (SFT vs RL) and algorithm-specific settings. Table of Contents \u00b6 SFT (Supervised Fine-Tuning) Parameters RL (Reinforcement Learning) Common Parameters Algorithm-Specific RL Parameters PPO (Proximal Policy Optimization) DPO (Direct Preference Optimization) GRPO (Group Relative Policy Optimization) DAPO / DRGRPO GSPO (Generalized Scoring Proximal Objective) Neural Mirror GRPO Meta-ES SFT (Supervised Fine-Tuning) Parameters \u00b6 Model Configuration ( ModelConfig ) \u00b6 Parameter Type Default Description name_or_path str Required HuggingFace model name or local path precision PrecisionType bf16 Model precision ( bf16 , fp16 , fp32 , auto ) quantization Dict {} Quantization config (e.g., {\"load_in_4bit\": True} ) attn_implementation str \"auto\" Attention implementation ( auto , flash_attention_2 , sdpa ) gradient_checkpointing bool True Enable gradient checkpointing for memory efficiency max_memory Dict None Max memory per device (e.g., {\"0\": \"20GB\"} ) device_map str/Dict \"auto\" Device mapping strategy use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length peft_enabled bool False Enable PEFT/LoRA lora_rank int 16 LoRA rank lora_alpha int 32 LoRA alpha scaling lora_dropout float 0.1 LoRA dropout rate target_modules List[str] None LoRA target modules (e.g., [\"q_proj\", \"v_proj\"] ) bias str \"none\" Bias training ( none , all , lora_only ) use_gradient_checkpointing bool True Enable gradient checkpointing (legacy) num_labels int None Number of labels (classification tasks) model_init_kwargs Dict {} Additional model initialization arguments Dataset Configuration ( DatasetConfig ) \u00b6 Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use subset / config str None Dataset subset/config name percent float None Percentage of data to use (0-100) max_samples int None Maximum number of samples column_mapping Dict {} Column name mappings task_type TaskType SUPERVISED_FINE_TUNING Task type enum auto_detect_fields bool False Auto-detect dataset fields format_type str None Dataset format ( alpaca , dolly , etc.) text_column str \"text\" Text column name instruction_column str \"instruction\" Instruction column name response_column str \"response\" Response column name output_column str \"output\" Output column name input_column str \"input\" Input column name context_column str \"context\" Context column name label_column str \"label\" Label column (classification) tokens_column str \"tokens\" Tokens column (token classification) tags_column str \"ner_tags\" Tags column (NER) messages_column str \"messages\" Messages column (chat) dataset_text_field str \"text\" Dataset text field for trainers chat_template str None Chat template string dataset_num_proc int None Number of parallel processes pad_token str None Padding token preserve_columns List[str] None Columns to preserve during processing processing_fn Callable None Custom processing function processing_batched bool False Whether processing is batched processing_fn_kwargs Dict {} Arguments for processing function Training Configuration ( TrainingConfig ) \u00b6 Parameter Type Default Description per_device_batch_size int 1 Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int 3 Number of training epochs (if max_steps not set) learning_rate float 1e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 0 Warmup steps (or calculated from warmup_ratio) warmup_ratio float 0.1 Warmup ratio of total steps eval_interval int 100 Evaluation interval (steps) save_interval int 500 Save checkpoint interval (steps) max_grad_norm float 1.0 Maximum gradient norm for clipping fp16 bool False Use FP16 training bf16 bool False Use BF16 training dataloader_num_workers int 0 Number of dataloader workers remove_unused_columns bool False Remove unused dataset columns optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" Learning rate scheduler group_by_length bool False Group sequences by length dataloader_drop_last bool False Drop last incomplete batch eval_accumulation_steps int None Evaluation accumulation steps label_smoothing_factor float 0.0 Label smoothing factor early_stopping_patience int None Early stopping patience early_stopping_threshold float 0.0 Early stopping threshold load_best_model_at_end bool True Load best model at end metric_for_best_model str \"eval_loss\" Metric for best model selection greater_is_better bool False Whether higher metric is better use_trl bool False Use TRL backend dataset_num_proc int None Dataset processing processes dataset_kwargs Dict {} Additional dataset arguments packing bool False Enable sequence packing packing_strategy str \"bfd\" Packing strategy ( bfd , wrapped ) eval_packing bool None Enable packing for evaluation padding_free bool False Padding-free training pad_to_multiple_of int None Pad sequences to multiple of N completion_only_loss bool None Compute loss only on completions assistant_only_loss bool False Compute loss only on assistant turns loss_type str \"nll\" Loss type ( nll , dft ) activation_offloading bool False Enable activation offloading use_flash_attention_2 bool None Use Flash Attention 2 gradient_checkpointing bool False Enable gradient checkpointing gradient_checkpointing_kwargs Dict {} Gradient checkpointing arguments Evaluation Configuration ( EvaluationConfig ) \u00b6 Parameter Type Default Description compute_perplexity bool True Compute perplexity metric compute_rouge bool True Compute ROUGE scores compute_bleu bool True Compute BLEU scores compute_meteor bool False Compute METEOR scores (requires nltk) compute_bertscore bool False Compute BERTScore (requires bert-score) compute_semantic_similarity bool False Compute semantic similarity compute_codebleu bool False Compute CodeBLEU (for code tasks) custom_metrics List[Callable] None Custom metric functions max_samples_for_quality_metrics int 50 Max samples for quality metrics bertscore_model str \"microsoft/deberta-xlarge-mnli\" Model for BERTScore semantic_similarity_model str \"sentence-transformers/all-MiniLM-L6-v2\" Model for semantic similarity Logging Configuration ( LoggingConfig ) \u00b6 Parameter Type Default Description output_dir str \"./output\" Output directory run_name str None Run name for logging loggers List[str] [\"tensorboard\"] Logger types ( tensorboard , wandb ) log_level str \"INFO\" Logging level log_interval int 10 Logging interval (steps) save_strategy str \"steps\" Save strategy eval_strategy str \"steps\" Evaluation strategy report_to str \"none\" Reporting destination RL (Reinforcement Learning) Common Parameters \u00b6 Model Configuration ( ModelConfig ) \u00b6 Parameter Type Default Description name_or_path str Required Policy model name or path sft_path str None SFT checkpoint path reward_path str None Reward model path reward_model_name str None Separate reward model name reward_model_source RewardModelSourceConfig None Reward model source configuration precision PrecisionType AUTO Model precision quantization Dict {} Quantization config attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool False Enable gradient checkpointing max_memory Dict None Max memory per device device_map str/Dict None Device mapping use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length reward_value_model str \"meta-llama/Llama-3.2-1B-Instruct\" Reward/value model name reward_value_loading_type str None Loading type ( unsloth , standard ) reward_model_quantization Dict {} Reward model quantization value_model_quantization Dict {} Value model quantization use_peft bool True Enable PEFT/LoRA lora_r int 16 LoRA rank lora_alpha int 32 LoRA alpha lora_dropout float 0.05 LoRA dropout lora_target_modules List[str] [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] LoRA target modules trust_remote_code bool True Trust remote code model_init_kwargs Dict {} Model initialization arguments ref_model_init_kwargs Dict {} Reference model init arguments model_adapter_name str None Model adapter name ref_adapter_name str None Reference adapter name force_use_ref_model bool False Force use of reference model disable_dropout bool True Disable dropout use_logits_to_keep bool False Use logits_to_keep optimization reward_device str \"auto\" Reward model device ( auto , cpu , cuda ) Dataset Configuration ( DatasetConfig ) \u00b6 Parameter Type Default Description name str Required Dataset name or path split str \"train\" Dataset split percent float None Percentage of data (0-100) max_samples int None Maximum samples field_mappings Dict {} Field name mappings format_type str None Dataset format type auto_detect_fields bool True Auto-detect fields column_mapping Dict {} Column mappings (legacy) task_type str \"conversation\" Task type weight float 1.0 Dataset weight chat_template str None Chat template system_prompt str None System prompt dataset_num_proc int None Processing processes pad_token str None Padding token label_pad_token_id int -100 Label padding token ID truncation_mode str \"keep_end\" Truncation mode padding_free bool False Padding-free training precompute_ref_log_probs bool False Precompute reference log probs precompute_ref_batch_size int None Batch size for precomputation tools Any None Tools configuration preserve_columns List[str] None Columns to preserve processing_fn Callable None Custom processing function processing_batched bool False Batched processing processing_fn_kwargs Dict {} Processing function arguments config_name str None Dataset config name Training Configuration ( TrainingConfig ) \u00b6 Parameter Type Default Description per_device_batch_size int 1 Batch size per device per_device_eval_batch_size int 1 Eval batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int None Number of epochs eval_interval int 100 Evaluation interval save_interval int 500 Save interval learning_rate float 1e-5 Learning rate max_grad_norm float 1.0 Max gradient norm weight_decay float 0.01 Weight decay optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" LR scheduler type warmup_steps int 0 Warmup steps warmup_ratio float 0.0 Warmup ratio rollout_batch_size int 1 Rollout batch size kl_coef float 0.1 KL divergence coefficient cliprange float 0.2 PPO clip range cliprange_value float 0.2 Value function clip range num_ppo_epochs int None PPO epochs per batch temperature float 0.6 Sampling temperature whiten_rewards bool False Whiten rewards kl_estimator str \"k1\" KL estimator type vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda response_length int 128 Response length stop_token str \"eos\" Stop token missing_eos_penalty float 1.0 Missing EOS penalty ds3_gather_for_generation bool True DeepSpeed Stage 3 gather generation_kwargs Dict None Generation arguments max_length int 1024 Max sequence length max_prompt_length int 512 Max prompt length max_target_length int None Max target length max_completion_length int 256 Max completion length top_p float 0.95 Nucleus sampling parameter padding_free bool False Padding-free training truncation_mode str \"keep_end\" Truncation mode beta float 0.1 DPO beta parameter loss_type str None Loss type loss_weights Dict None Loss weights f_divergence_type str \"reverse_kl\" F-divergence type f_alpha_divergence_coef float 1.0 Alpha divergence coefficient reference_free bool False Reference-free training label_smoothing float 0.0 Label smoothing use_weighting bool False Use importance weighting rpo_alpha float None RPO alpha parameter ld_alpha float None LD alpha parameter discopop_tau float 0.05 DiscoPOP tau parameter sync_ref_model bool False Sync reference model ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Reference sync steps grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter eval_steps int 100 Evaluation steps eval_strategy str \"no\" Evaluation strategy save_steps int 500 Save steps save_strategy str \"steps\" Save strategy save_total_limit int None Max checkpoints to keep load_best_model_at_end bool False Load best model at end metric_for_best_model str None Best model metric greater_is_better bool False Whether higher is better logging_steps int 10 Logging steps logging_strategy str \"steps\" Logging strategy num_generations int None Number of generations per prompt mask_truncated_completions bool True Mask truncated completions scale_rewards str \"group\" Reward scaling ( group , batch ) reward_weights List[float] None Reward function weights enable_thinking bool False Enable Qwen3 thinking mode fast_inference bool False Enable Unsloth vLLM (faster) vllm_gpu_memory_utilization float 0.7 vLLM GPU memory (0.95 for max) seed int 42 Random seed data_seed int 47 Data seed use_liger_kernel bool False Use Liger kernel use_liger_loss bool None Use Liger loss gradient_checkpointing_kwargs Dict {\"use_reentrant\": False} Gradient checkpointing args group_by_length bool True Group sequences by length Sample Logging Configuration ( SampleLoggingConfig ) \u00b6 Parameter Type Default Description enabled bool False Enable sample logging prompts List[str] None Prompts for sample generation interval_steps int None Steps between samples percent_of_max_steps float None Percent of max steps (0-1) max_new_tokens int 80 Max tokens to generate temperature float 0.6 Generation temperature top_p float 0.9 Nucleus sampling parameter num_samples int 3 Number of samples per prompt Logging Configuration ( LoggingConfig ) \u00b6 Parameter Type Default Description loggers List[str] [\"tensorboard\"] Logger types run_name str None Run name output_dir str \"./output\" Output directory log_level str \"INFO\" Logging level sample_logging SampleLoggingConfig See above Sample logging config report_to str \"none\" Reporting destination Reward Configuration ( RewardConfig ) \u00b6 Parameter Type Default Description type str Required Reward function type weight float 1.0 Reward weight params Dict {} Reward function parameters shield bool False Enable safety shield clip float None Clip reward values normalize bool False Normalize rewards Reward Model Training Configuration ( RewardModelTrainingConfig ) \u00b6 Parameter Type Default Description base_model_name str Required Base model for reward training training_texts List[str] Required Training texts (min 10) reward_functions List[str] Required Reward functions to use output_dir str Required Output directory reference_texts List[str] None Reference texts (optional) reward_weights List[float] None Reward function weights num_epochs int 3 Training epochs learning_rate float 1e-5 Learning rate batch_size int 8 Batch size gradient_accumulation_steps int 4 Gradient accumulation max_length int 512 Max sequence length Reward Model Source Configuration ( RewardModelSourceConfig ) \u00b6 Parameter Type Default Description source_type str Required Source type ( pretrained_hf , pretrained_local , custom_trained ) model_name str None HuggingFace model name (for pretrained_hf ) model_path str None Local model path (for pretrained_local ) training_config RewardModelTrainingConfig None Training config (for custom_trained ) fine_tune_with_rewards bool False Fine-tune pretrained with reward functions Algorithm-Specific RL Parameters \u00b6 PPO (Proximal Policy Optimization) \u00b6 PPO uses the common RL parameters plus: Parameter Type Default Description num_ppo_epochs int 4 Number of PPO epochs per batch cliprange float 0.2 PPO policy clip range cliprange_value float 0.2 PPO value function clip range vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor (GAE) lam float 0.95 GAE lambda parameter kl_coef float 0.1 KL penalty coefficient kl_estimator str \"k1\" KL estimator ( k1 , k2 , k3 ) whiten_rewards bool False Whiten advantages/rewards Note : For optimal performance, ensure policy_model, reward_model, and value_model are from the same model family. DPO (Direct Preference Optimization) \u00b6 DPO uses the common RL parameters plus: Parameter Type Default Description beta float 0.1 DPO beta (inverse temperature) loss_type str \"sigmoid\" Loss type ( sigmoid , hinge , ipo , kto ) label_smoothing float 0.0 Label smoothing factor reference_free bool False Reference-free DPO precompute_ref_log_probs bool False Precompute reference log probs sync_ref_model bool False Sync reference model periodically ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Steps between reference syncs DPO Evaluation Parameters: Parameter Type Default Description dpo_eval_enabled bool False Enable DPO evaluation dpo_eval_max_samples int None Max samples for eval dpo_zero_shot_max_samples int 50 Zero-shot eval samples dpo_few_shot_max_samples int 30 Few-shot eval samples dpo_few_shot_examples_text str None Few-shot examples GRPO (Group Relative Policy Optimization) \u00b6 GRPO uses the common RL parameters plus: Parameter Type Default Description grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling ( group , batch ) kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range DAPO / DRGRPO \u00b6 DAPO (Direct Advantage Policy Optimization) and DRGRPO (Distributional Robust GRPO) share identical parameters with GRPO: Parameter Type Default Description grpo_alpha float 0.1 Alpha parameter grpo_beta float 0.1 Beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range GSPO (Generalized Scoring Proximal Objective) \u00b6 GSPO uses scoring-based policy optimization: Note : GSPO is only supported by TRL backend, not Unsloth. Parameter Type Default Description gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling strategy kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range Neural Mirror GRPO \u00b6 Neural Mirror GRPO combines GRPO with neural mirror descent for improved optimization: Base GRPO Parameters plus neural mirror-specific optimizations. Uses standard GRPO parameters with enhanced optimization through mirror descent. No additional unique parameters. Meta-ES \u00b6 Meta-ES uses evolution strategies for meta-learning the reward function: Parameter Type Default Description meta_iterations int 15 Number of meta-iterations patience int 5 Early stopping patience min_delta float 0.001 Minimum improvement for early stopping init_scale float 0.01 Initial parameter scale N int 10 Population size (must be even) T int 100 Training steps per evaluation sigma float 0.01 ES noise standard deviation sigma_decay float 0.99 ES sigma decay per iteration alpha float 0.01 ES learning rate mirror_coefficient float 0.0001 Mirror descent coefficient debug_mode bool False Enable debug logging eval_timeout int 5 Evaluation timeout (seconds) eval_max_tokens int 512 Max tokens for evaluation eval_k int 1 Top-k sampling for evaluation eval_temperature float 0.8 Temperature for evaluation num_workers int 1 Number of parallel workers no_wandb bool False Disable Weights & Biases logging wandb_project str \"neural-mirror-es\" W&B project name resume str None Path to checkpoint for resuming Usage Notes: - Meta-ES optimizes the reward function itself through evolution - Population size N must be even for symmetric perturbations - T controls how long each candidate is trained before evaluation - sigma controls exploration; decay helps convergence - Suitable for scenarios where reward design is critical Backend Selection \u00b6 AlignTune supports two backends with automatic fallback: Backend Description Availability TRL HuggingFace Transformers RL library Standard, widely supported Unsloth Optimized training with memory efficiency Requires Unsloth installation Backend Priority: - SFT: Unsloth \u2192 TRL - RL: Unsloth \u2192 TRL Setting Backend: # Auto-select best available trainer = create_sft_trainer ( ... , backend = \"auto\" ) # Explicit selection trainer = create_sft_trainer ( ... , backend = \"trl\" ) trainer = create_rl_trainer ( ... , backend = \"unsloth\" , algorithm = \"grpo\" ) Note : When TRL is selected, Unsloth is disabled to prevent interference. GSPO algorithm is TRL-only. Task Types (SFT) \u00b6 Task Type Description INSTRUCTION_FOLLOWING Instruction-response pairs SUPERVISED_FINE_TUNING General supervised training TEXT_CLASSIFICATION Text classification tasks TOKEN_CLASSIFICATION Token-level classification (NER) TEXT_GENERATION General text generation CHAT_COMPLETION Multi-turn chat completion Convenience Functions \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" ) Notes \u00b6 All parameters with Required must be explicitly provided Parameters with defaults can be omitted to use default values Enum parameters accept both enum values and strings (e.g., \"bf16\" or PrecisionType.BF16 ) For classification tasks, num_labels must be set in ModelConfig Reward model configuration supports three modes: pretrained HF, pretrained local, or custom trained DPO evaluation is optional and controlled by dpo_eval_enabled Sample logging is optional for monitoring generation quality during training Additional Resources \u00b6 FinetuneHub Documentation Backend Comparison Guide Algorithm Selection Guide","title":"AlignTune Configuration Parameters"},{"location":"PARAMETERS/#aligntune-configuration-parameters","text":"This document provides a comprehensive reference for all configuration parameters available in AlignTune, organized by training type (SFT vs RL) and algorithm-specific settings.","title":"AlignTune Configuration Parameters"},{"location":"PARAMETERS/#table-of-contents","text":"SFT (Supervised Fine-Tuning) Parameters RL (Reinforcement Learning) Common Parameters Algorithm-Specific RL Parameters PPO (Proximal Policy Optimization) DPO (Direct Preference Optimization) GRPO (Group Relative Policy Optimization) DAPO / DRGRPO GSPO (Generalized Scoring Proximal Objective) Neural Mirror GRPO Meta-ES","title":"Table of Contents"},{"location":"PARAMETERS/#sft-supervised-fine-tuning-parameters","text":"","title":"SFT (Supervised Fine-Tuning) Parameters"},{"location":"PARAMETERS/#model-configuration-modelconfig","text":"Parameter Type Default Description name_or_path str Required HuggingFace model name or local path precision PrecisionType bf16 Model precision ( bf16 , fp16 , fp32 , auto ) quantization Dict {} Quantization config (e.g., {\"load_in_4bit\": True} ) attn_implementation str \"auto\" Attention implementation ( auto , flash_attention_2 , sdpa ) gradient_checkpointing bool True Enable gradient checkpointing for memory efficiency max_memory Dict None Max memory per device (e.g., {\"0\": \"20GB\"} ) device_map str/Dict \"auto\" Device mapping strategy use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length peft_enabled bool False Enable PEFT/LoRA lora_rank int 16 LoRA rank lora_alpha int 32 LoRA alpha scaling lora_dropout float 0.1 LoRA dropout rate target_modules List[str] None LoRA target modules (e.g., [\"q_proj\", \"v_proj\"] ) bias str \"none\" Bias training ( none , all , lora_only ) use_gradient_checkpointing bool True Enable gradient checkpointing (legacy) num_labels int None Number of labels (classification tasks) model_init_kwargs Dict {} Additional model initialization arguments","title":"Model Configuration (ModelConfig)"},{"location":"PARAMETERS/#dataset-configuration-datasetconfig","text":"Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use subset / config str None Dataset subset/config name percent float None Percentage of data to use (0-100) max_samples int None Maximum number of samples column_mapping Dict {} Column name mappings task_type TaskType SUPERVISED_FINE_TUNING Task type enum auto_detect_fields bool False Auto-detect dataset fields format_type str None Dataset format ( alpaca , dolly , etc.) text_column str \"text\" Text column name instruction_column str \"instruction\" Instruction column name response_column str \"response\" Response column name output_column str \"output\" Output column name input_column str \"input\" Input column name context_column str \"context\" Context column name label_column str \"label\" Label column (classification) tokens_column str \"tokens\" Tokens column (token classification) tags_column str \"ner_tags\" Tags column (NER) messages_column str \"messages\" Messages column (chat) dataset_text_field str \"text\" Dataset text field for trainers chat_template str None Chat template string dataset_num_proc int None Number of parallel processes pad_token str None Padding token preserve_columns List[str] None Columns to preserve during processing processing_fn Callable None Custom processing function processing_batched bool False Whether processing is batched processing_fn_kwargs Dict {} Arguments for processing function","title":"Dataset Configuration (DatasetConfig)"},{"location":"PARAMETERS/#training-configuration-trainingconfig","text":"Parameter Type Default Description per_device_batch_size int 1 Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int 3 Number of training epochs (if max_steps not set) learning_rate float 1e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 0 Warmup steps (or calculated from warmup_ratio) warmup_ratio float 0.1 Warmup ratio of total steps eval_interval int 100 Evaluation interval (steps) save_interval int 500 Save checkpoint interval (steps) max_grad_norm float 1.0 Maximum gradient norm for clipping fp16 bool False Use FP16 training bf16 bool False Use BF16 training dataloader_num_workers int 0 Number of dataloader workers remove_unused_columns bool False Remove unused dataset columns optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" Learning rate scheduler group_by_length bool False Group sequences by length dataloader_drop_last bool False Drop last incomplete batch eval_accumulation_steps int None Evaluation accumulation steps label_smoothing_factor float 0.0 Label smoothing factor early_stopping_patience int None Early stopping patience early_stopping_threshold float 0.0 Early stopping threshold load_best_model_at_end bool True Load best model at end metric_for_best_model str \"eval_loss\" Metric for best model selection greater_is_better bool False Whether higher metric is better use_trl bool False Use TRL backend dataset_num_proc int None Dataset processing processes dataset_kwargs Dict {} Additional dataset arguments packing bool False Enable sequence packing packing_strategy str \"bfd\" Packing strategy ( bfd , wrapped ) eval_packing bool None Enable packing for evaluation padding_free bool False Padding-free training pad_to_multiple_of int None Pad sequences to multiple of N completion_only_loss bool None Compute loss only on completions assistant_only_loss bool False Compute loss only on assistant turns loss_type str \"nll\" Loss type ( nll , dft ) activation_offloading bool False Enable activation offloading use_flash_attention_2 bool None Use Flash Attention 2 gradient_checkpointing bool False Enable gradient checkpointing gradient_checkpointing_kwargs Dict {} Gradient checkpointing arguments","title":"Training Configuration (TrainingConfig)"},{"location":"PARAMETERS/#evaluation-configuration-evaluationconfig","text":"Parameter Type Default Description compute_perplexity bool True Compute perplexity metric compute_rouge bool True Compute ROUGE scores compute_bleu bool True Compute BLEU scores compute_meteor bool False Compute METEOR scores (requires nltk) compute_bertscore bool False Compute BERTScore (requires bert-score) compute_semantic_similarity bool False Compute semantic similarity compute_codebleu bool False Compute CodeBLEU (for code tasks) custom_metrics List[Callable] None Custom metric functions max_samples_for_quality_metrics int 50 Max samples for quality metrics bertscore_model str \"microsoft/deberta-xlarge-mnli\" Model for BERTScore semantic_similarity_model str \"sentence-transformers/all-MiniLM-L6-v2\" Model for semantic similarity","title":"Evaluation Configuration (EvaluationConfig)"},{"location":"PARAMETERS/#logging-configuration-loggingconfig","text":"Parameter Type Default Description output_dir str \"./output\" Output directory run_name str None Run name for logging loggers List[str] [\"tensorboard\"] Logger types ( tensorboard , wandb ) log_level str \"INFO\" Logging level log_interval int 10 Logging interval (steps) save_strategy str \"steps\" Save strategy eval_strategy str \"steps\" Evaluation strategy report_to str \"none\" Reporting destination","title":"Logging Configuration (LoggingConfig)"},{"location":"PARAMETERS/#rl-reinforcement-learning-common-parameters","text":"","title":"RL (Reinforcement Learning) Common Parameters"},{"location":"PARAMETERS/#model-configuration-modelconfig_1","text":"Parameter Type Default Description name_or_path str Required Policy model name or path sft_path str None SFT checkpoint path reward_path str None Reward model path reward_model_name str None Separate reward model name reward_model_source RewardModelSourceConfig None Reward model source configuration precision PrecisionType AUTO Model precision quantization Dict {} Quantization config attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool False Enable gradient checkpointing max_memory Dict None Max memory per device device_map str/Dict None Device mapping use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length reward_value_model str \"meta-llama/Llama-3.2-1B-Instruct\" Reward/value model name reward_value_loading_type str None Loading type ( unsloth , standard ) reward_model_quantization Dict {} Reward model quantization value_model_quantization Dict {} Value model quantization use_peft bool True Enable PEFT/LoRA lora_r int 16 LoRA rank lora_alpha int 32 LoRA alpha lora_dropout float 0.05 LoRA dropout lora_target_modules List[str] [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] LoRA target modules trust_remote_code bool True Trust remote code model_init_kwargs Dict {} Model initialization arguments ref_model_init_kwargs Dict {} Reference model init arguments model_adapter_name str None Model adapter name ref_adapter_name str None Reference adapter name force_use_ref_model bool False Force use of reference model disable_dropout bool True Disable dropout use_logits_to_keep bool False Use logits_to_keep optimization reward_device str \"auto\" Reward model device ( auto , cpu , cuda )","title":"Model Configuration (ModelConfig)"},{"location":"PARAMETERS/#dataset-configuration-datasetconfig_1","text":"Parameter Type Default Description name str Required Dataset name or path split str \"train\" Dataset split percent float None Percentage of data (0-100) max_samples int None Maximum samples field_mappings Dict {} Field name mappings format_type str None Dataset format type auto_detect_fields bool True Auto-detect fields column_mapping Dict {} Column mappings (legacy) task_type str \"conversation\" Task type weight float 1.0 Dataset weight chat_template str None Chat template system_prompt str None System prompt dataset_num_proc int None Processing processes pad_token str None Padding token label_pad_token_id int -100 Label padding token ID truncation_mode str \"keep_end\" Truncation mode padding_free bool False Padding-free training precompute_ref_log_probs bool False Precompute reference log probs precompute_ref_batch_size int None Batch size for precomputation tools Any None Tools configuration preserve_columns List[str] None Columns to preserve processing_fn Callable None Custom processing function processing_batched bool False Batched processing processing_fn_kwargs Dict {} Processing function arguments config_name str None Dataset config name","title":"Dataset Configuration (DatasetConfig)"},{"location":"PARAMETERS/#training-configuration-trainingconfig_1","text":"Parameter Type Default Description per_device_batch_size int 1 Batch size per device per_device_eval_batch_size int 1 Eval batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int None Maximum training steps epochs int None Number of epochs eval_interval int 100 Evaluation interval save_interval int 500 Save interval learning_rate float 1e-5 Learning rate max_grad_norm float 1.0 Max gradient norm weight_decay float 0.01 Weight decay optimizer str \"adamw_torch\" Optimizer type lr_scheduler str \"cosine\" LR scheduler type warmup_steps int 0 Warmup steps warmup_ratio float 0.0 Warmup ratio rollout_batch_size int 1 Rollout batch size kl_coef float 0.1 KL divergence coefficient cliprange float 0.2 PPO clip range cliprange_value float 0.2 Value function clip range num_ppo_epochs int None PPO epochs per batch temperature float 0.6 Sampling temperature whiten_rewards bool False Whiten rewards kl_estimator str \"k1\" KL estimator type vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda response_length int 128 Response length stop_token str \"eos\" Stop token missing_eos_penalty float 1.0 Missing EOS penalty ds3_gather_for_generation bool True DeepSpeed Stage 3 gather generation_kwargs Dict None Generation arguments max_length int 1024 Max sequence length max_prompt_length int 512 Max prompt length max_target_length int None Max target length max_completion_length int 256 Max completion length top_p float 0.95 Nucleus sampling parameter padding_free bool False Padding-free training truncation_mode str \"keep_end\" Truncation mode beta float 0.1 DPO beta parameter loss_type str None Loss type loss_weights Dict None Loss weights f_divergence_type str \"reverse_kl\" F-divergence type f_alpha_divergence_coef float 1.0 Alpha divergence coefficient reference_free bool False Reference-free training label_smoothing float 0.0 Label smoothing use_weighting bool False Use importance weighting rpo_alpha float None RPO alpha parameter ld_alpha float None LD alpha parameter discopop_tau float 0.05 DiscoPOP tau parameter sync_ref_model bool False Sync reference model ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Reference sync steps grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter eval_steps int 100 Evaluation steps eval_strategy str \"no\" Evaluation strategy save_steps int 500 Save steps save_strategy str \"steps\" Save strategy save_total_limit int None Max checkpoints to keep load_best_model_at_end bool False Load best model at end metric_for_best_model str None Best model metric greater_is_better bool False Whether higher is better logging_steps int 10 Logging steps logging_strategy str \"steps\" Logging strategy num_generations int None Number of generations per prompt mask_truncated_completions bool True Mask truncated completions scale_rewards str \"group\" Reward scaling ( group , batch ) reward_weights List[float] None Reward function weights enable_thinking bool False Enable Qwen3 thinking mode fast_inference bool False Enable Unsloth vLLM (faster) vllm_gpu_memory_utilization float 0.7 vLLM GPU memory (0.95 for max) seed int 42 Random seed data_seed int 47 Data seed use_liger_kernel bool False Use Liger kernel use_liger_loss bool None Use Liger loss gradient_checkpointing_kwargs Dict {\"use_reentrant\": False} Gradient checkpointing args group_by_length bool True Group sequences by length","title":"Training Configuration (TrainingConfig)"},{"location":"PARAMETERS/#sample-logging-configuration-sampleloggingconfig","text":"Parameter Type Default Description enabled bool False Enable sample logging prompts List[str] None Prompts for sample generation interval_steps int None Steps between samples percent_of_max_steps float None Percent of max steps (0-1) max_new_tokens int 80 Max tokens to generate temperature float 0.6 Generation temperature top_p float 0.9 Nucleus sampling parameter num_samples int 3 Number of samples per prompt","title":"Sample Logging Configuration (SampleLoggingConfig)"},{"location":"PARAMETERS/#logging-configuration-loggingconfig_1","text":"Parameter Type Default Description loggers List[str] [\"tensorboard\"] Logger types run_name str None Run name output_dir str \"./output\" Output directory log_level str \"INFO\" Logging level sample_logging SampleLoggingConfig See above Sample logging config report_to str \"none\" Reporting destination","title":"Logging Configuration (LoggingConfig)"},{"location":"PARAMETERS/#reward-configuration-rewardconfig","text":"Parameter Type Default Description type str Required Reward function type weight float 1.0 Reward weight params Dict {} Reward function parameters shield bool False Enable safety shield clip float None Clip reward values normalize bool False Normalize rewards","title":"Reward Configuration (RewardConfig)"},{"location":"PARAMETERS/#reward-model-training-configuration-rewardmodeltrainingconfig","text":"Parameter Type Default Description base_model_name str Required Base model for reward training training_texts List[str] Required Training texts (min 10) reward_functions List[str] Required Reward functions to use output_dir str Required Output directory reference_texts List[str] None Reference texts (optional) reward_weights List[float] None Reward function weights num_epochs int 3 Training epochs learning_rate float 1e-5 Learning rate batch_size int 8 Batch size gradient_accumulation_steps int 4 Gradient accumulation max_length int 512 Max sequence length","title":"Reward Model Training Configuration (RewardModelTrainingConfig)"},{"location":"PARAMETERS/#reward-model-source-configuration-rewardmodelsourceconfig","text":"Parameter Type Default Description source_type str Required Source type ( pretrained_hf , pretrained_local , custom_trained ) model_name str None HuggingFace model name (for pretrained_hf ) model_path str None Local model path (for pretrained_local ) training_config RewardModelTrainingConfig None Training config (for custom_trained ) fine_tune_with_rewards bool False Fine-tune pretrained with reward functions","title":"Reward Model Source Configuration (RewardModelSourceConfig)"},{"location":"PARAMETERS/#algorithm-specific-rl-parameters","text":"","title":"Algorithm-Specific RL Parameters"},{"location":"PARAMETERS/#ppo-proximal-policy-optimization","text":"PPO uses the common RL parameters plus: Parameter Type Default Description num_ppo_epochs int 4 Number of PPO epochs per batch cliprange float 0.2 PPO policy clip range cliprange_value float 0.2 PPO value function clip range vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor (GAE) lam float 0.95 GAE lambda parameter kl_coef float 0.1 KL penalty coefficient kl_estimator str \"k1\" KL estimator ( k1 , k2 , k3 ) whiten_rewards bool False Whiten advantages/rewards Note : For optimal performance, ensure policy_model, reward_model, and value_model are from the same model family.","title":"PPO (Proximal Policy Optimization)"},{"location":"PARAMETERS/#dpo-direct-preference-optimization","text":"DPO uses the common RL parameters plus: Parameter Type Default Description beta float 0.1 DPO beta (inverse temperature) loss_type str \"sigmoid\" Loss type ( sigmoid , hinge , ipo , kto ) label_smoothing float 0.0 Label smoothing factor reference_free bool False Reference-free DPO precompute_ref_log_probs bool False Precompute reference log probs sync_ref_model bool False Sync reference model periodically ref_model_mixup_alpha float 0.6 Reference model mixup alpha ref_model_sync_steps int 512 Steps between reference syncs DPO Evaluation Parameters: Parameter Type Default Description dpo_eval_enabled bool False Enable DPO evaluation dpo_eval_max_samples int None Max samples for eval dpo_zero_shot_max_samples int 50 Zero-shot eval samples dpo_few_shot_max_samples int 30 Few-shot eval samples dpo_few_shot_examples_text str None Few-shot examples","title":"DPO (Direct Preference Optimization)"},{"location":"PARAMETERS/#grpo-group-relative-policy-optimization","text":"GRPO uses the common RL parameters plus: Parameter Type Default Description grpo_alpha float 0.1 GRPO alpha parameter grpo_beta float 0.1 GRPO beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling ( group , batch ) kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"GRPO (Group Relative Policy Optimization)"},{"location":"PARAMETERS/#dapo-drgrpo","text":"DAPO (Direct Advantage Policy Optimization) and DRGRPO (Distributional Robust GRPO) share identical parameters with GRPO: Parameter Type Default Description grpo_alpha float 0.1 Alpha parameter grpo_beta float 0.1 Beta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"DAPO / DRGRPO"},{"location":"PARAMETERS/#gspo-generalized-scoring-proximal-objective","text":"GSPO uses scoring-based policy optimization: Note : GSPO is only supported by TRL backend, not Unsloth. Parameter Type Default Description gspo_gamma float 0.1 GSPO gamma parameter gspo_delta float 0.1 GSPO delta parameter num_generations int batch_size Generations per prompt scale_rewards str \"group\" Reward scaling strategy kl_coef float 0.1 KL penalty coefficient cliprange float 0.2 Policy clip range","title":"GSPO (Generalized Scoring Proximal Objective)"},{"location":"PARAMETERS/#neural-mirror-grpo","text":"Neural Mirror GRPO combines GRPO with neural mirror descent for improved optimization: Base GRPO Parameters plus neural mirror-specific optimizations. Uses standard GRPO parameters with enhanced optimization through mirror descent. No additional unique parameters.","title":"Neural Mirror GRPO"},{"location":"PARAMETERS/#meta-es","text":"Meta-ES uses evolution strategies for meta-learning the reward function: Parameter Type Default Description meta_iterations int 15 Number of meta-iterations patience int 5 Early stopping patience min_delta float 0.001 Minimum improvement for early stopping init_scale float 0.01 Initial parameter scale N int 10 Population size (must be even) T int 100 Training steps per evaluation sigma float 0.01 ES noise standard deviation sigma_decay float 0.99 ES sigma decay per iteration alpha float 0.01 ES learning rate mirror_coefficient float 0.0001 Mirror descent coefficient debug_mode bool False Enable debug logging eval_timeout int 5 Evaluation timeout (seconds) eval_max_tokens int 512 Max tokens for evaluation eval_k int 1 Top-k sampling for evaluation eval_temperature float 0.8 Temperature for evaluation num_workers int 1 Number of parallel workers no_wandb bool False Disable Weights & Biases logging wandb_project str \"neural-mirror-es\" W&B project name resume str None Path to checkpoint for resuming Usage Notes: - Meta-ES optimizes the reward function itself through evolution - Population size N must be even for symmetric perturbations - T controls how long each candidate is trained before evaluation - sigma controls exploration; decay helps convergence - Suitable for scenarios where reward design is critical","title":"Meta-ES"},{"location":"PARAMETERS/#backend-selection","text":"AlignTune supports two backends with automatic fallback: Backend Description Availability TRL HuggingFace Transformers RL library Standard, widely supported Unsloth Optimized training with memory efficiency Requires Unsloth installation Backend Priority: - SFT: Unsloth \u2192 TRL - RL: Unsloth \u2192 TRL Setting Backend: # Auto-select best available trainer = create_sft_trainer ( ... , backend = \"auto\" ) # Explicit selection trainer = create_sft_trainer ( ... , backend = \"trl\" ) trainer = create_rl_trainer ( ... , backend = \"unsloth\" , algorithm = \"grpo\" ) Note : When TRL is selected, Unsloth is disabled to prevent interference. GSPO algorithm is TRL-only.","title":"Backend Selection"},{"location":"PARAMETERS/#task-types-sft","text":"Task Type Description INSTRUCTION_FOLLOWING Instruction-response pairs SUPERVISED_FINE_TUNING General supervised training TEXT_CLASSIFICATION Text classification tasks TOKEN_CLASSIFICATION Token-level classification (NER) TEXT_GENERATION General text generation CHAT_COMPLETION Multi-turn chat completion","title":"Task Types (SFT)"},{"location":"PARAMETERS/#convenience-functions","text":"","title":"Convenience Functions"},{"location":"PARAMETERS/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 )","title":"SFT Training"},{"location":"PARAMETERS/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"auto\" , output_dir = \"./output\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" )","title":"RL Training"},{"location":"PARAMETERS/#notes","text":"All parameters with Required must be explicitly provided Parameters with defaults can be omitted to use default values Enum parameters accept both enum values and strings (e.g., \"bf16\" or PrecisionType.BF16 ) For classification tasks, num_labels must be set in ModelConfig Reward model configuration supports three modes: pretrained HF, pretrained local, or custom trained DPO evaluation is optional and controlled by dpo_eval_enabled Sample logging is optional for monitoring generation quality during training","title":"Notes"},{"location":"PARAMETERS/#additional-resources","text":"FinetuneHub Documentation Backend Comparison Guide Algorithm Selection Guide","title":"Additional Resources"},{"location":"cli-reference/","text":"CLI Reference \u00b6 AlignTune provides a comprehensive command-line interface for training and managing models. Overview \u00b6 The CLI provides easy access to all AlignTune functionality without writing Python code. Main Commands \u00b6 aligntune info \u00b6 Display system information and compatibility status. aligntune info aligntune list-backends-cmd \u00b6 List all available backends and their status. aligntune list-backends-cmd Training Commands \u00b6 aligntune train \u00b6 Train a model with specified parameters. # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl # PPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth Training Options \u00b6 Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type (sft, dpo, ppo, etc.) --type dpo --backend Backend (trl, unsloth, auto) --backend auto --epochs Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir Output directory --output-dir ./output Configuration Files \u00b6 The CLI supports YAML configuration files for complex setups: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Examples \u00b6 Quick SFT Training \u00b6 aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --backend auto \\ --epochs 3 \\ --batch-size 4 \\ --learning-rate 5e-5 See Also \u00b6 Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference","title":"CLI Reference"},{"location":"cli-reference/#cli-reference","text":"AlignTune provides a comprehensive command-line interface for training and managing models.","title":"CLI Reference"},{"location":"cli-reference/#overview","text":"The CLI provides easy access to all AlignTune functionality without writing Python code.","title":"Overview"},{"location":"cli-reference/#main-commands","text":"","title":"Main Commands"},{"location":"cli-reference/#aligntune-info","text":"Display system information and compatibility status. aligntune info","title":"aligntune info"},{"location":"cli-reference/#aligntune-list-backends-cmd","text":"List all available backends and their status. aligntune list-backends-cmd","title":"aligntune list-backends-cmd"},{"location":"cli-reference/#training-commands","text":"","title":"Training Commands"},{"location":"cli-reference/#aligntune-train","text":"Train a model with specified parameters. # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl # PPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth","title":"aligntune train"},{"location":"cli-reference/#training-options","text":"Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type (sft, dpo, ppo, etc.) --type dpo --backend Backend (trl, unsloth, auto) --backend auto --epochs Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir Output directory --output-dir ./output","title":"Training Options"},{"location":"cli-reference/#configuration-files","text":"The CLI supports YAML configuration files for complex setups: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Configuration Files"},{"location":"cli-reference/#examples","text":"","title":"Examples"},{"location":"cli-reference/#quick-sft-training","text":"aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --backend auto \\ --epochs 3 \\ --batch-size 4 \\ --learning-rate 5e-5","title":"Quick SFT Training"},{"location":"cli-reference/#see-also","text":"Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference","title":"See Also"},{"location":"unsloth_compatibility/","text":"Unsloth Compatibility Guide \u00b6 This document provides comprehensive information about Unsloth compatibility with AlignTune, including supported versions, known issues, and troubleshooting steps. Overview \u00b6 Unsloth provides faster training with memory optimizations, but requires specific environment configurations. AlignTune includes robust detection and fallback mechanisms to ensure training works even when Unsloth is unavailable. Supported Versions \u00b6 Recommended Combinations \u00b6 PyTorch CUDA Unsloth Status 2.0.0-2.7.0 11.8, 12.1, 12.4 latest Optimal 2.8.0+ 12.8+ latest Known Issues Python Requirements \u00b6 Python 3.8-3.12 CUDA-capable GPU (for GPU training) Known Compatibility Issues \u00b6 1. CUDA Symbol Errors \u00b6 Error : undefined symbol: _ZN3c104cuda9SetDeviceEa Cause : Version incompatibility between PyTorch 2.8.0+ and Unsloth's CUDA extensions. Solutions : - Use PyTorch 2.7.0 or earlier - Update Unsloth: pip install --upgrade unsloth - Use TRL backends instead: --backend trl 2. Flash Attention Issues \u00b6 Error : Flash Attention 2 installation broken Cause : CUDA version incompatibility with Flash Attention. Solutions : - Unsloth automatically falls back to Xformers - Update Flash Attention: pip install --upgrade flash-attn - Use TRL backends: --backend trl 3. Version Mismatches \u00b6 Error : Version incompatibility between dependencies Solutions : - Check compatibility matrix above - Update all dependencies: pip install --upgrade torch unsloth - Use TRL backends: --backend trl Environment Setup \u00b6 Optimal Setup \u00b6 # Install PyTorch with CUDA support pip install torch == 2 .7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Install Unsloth pip install unsloth # Install AlignTune pip install -e . Alternative Setup (if CUDA issues persist) \u00b6 # Use TRL backends instead pip install torch transformers trl pip install -e . Troubleshooting \u00b6 1. Check Environment \u00b6 # Run comprehensive diagnostics aligntune diagnose # Check basic info with verbose output aligntune info --verbose 2. Common Issues and Solutions \u00b6 Unsloth Not Detected \u00b6 # Check if Unsloth is installed python -c \"import unsloth; print('Unsloth available')\" # If error occurs, check CUDA compatibility python -c \"import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')\" CUDA Symbol Errors \u00b6 Solution 1 : Downgrade PyTorch to 2.7.0 Solution 2 : Use TRL backends: --backend trl Solution 3 : Update Unsloth: pip install --upgrade unsloth Flash Attention Issues \u00b6 Unsloth automatically falls back to Xformers No action required, but performance may be slightly reduced 3. Fallback to TRL Backends \u00b6 If Unsloth is not available, AlignTune automatically falls back to TRL backends: # Explicitly use TRL backends aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend trl # Or let AlignTune auto-select aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend auto Performance Comparison \u00b6 Backend Speed Memory Compatibility Unsloth faster 80% less Requires specific setup TRL Standard Standard Universal Legacy Standard Standard Universal Diagnostic Commands \u00b6 Basic Information \u00b6 # Show system status aligntune info # Show detailed diagnostics aligntune info --verbose Comprehensive Diagnostics \u00b6 # Run full environment check aligntune diagnose Backend Selection \u00b6 # List available backends aligntune list-backends # Validate model compatibility aligntune validate --model microsoft/DialoGPT-medium --backend auto Error Messages and Solutions \u00b6 \"Unsloth not available: cuda_symbol_error\" \u00b6 Cause : CUDA version incompatibility Solution : Use TRL backends or fix CUDA setup \"Unsloth not available: flash_attention_error\" \u00b6 Cause : Flash Attention compatibility issues Solution : Unsloth will auto-fallback to Xformers \"Unsloth not available: missing_dependency\" \u00b6 Cause : Unsloth not installed Solution : pip install unsloth or use TRL backends Best Practices \u00b6 Always test with aligntune diagnose before training Use --backend auto to let AlignTune choose the best available backend Check logs for detailed error information Fallback to TRL if Unsloth has issues Update dependencies regularly for best compatibility Getting Help \u00b6 If you encounter issues not covered in this guide: Run aligntune diagnose and share the output Check the logs for detailed error messages Try using TRL backends as a fallback Report issues with full diagnostic output Version History \u00b6 v0.2.0 : Enhanced error detection and diagnostics v0.1.0 : Initial Unsloth support with basic detection For more information, see the main README or run aligntune info --help .","title":"Unsloth Compatibility"},{"location":"unsloth_compatibility/#unsloth-compatibility-guide","text":"This document provides comprehensive information about Unsloth compatibility with AlignTune, including supported versions, known issues, and troubleshooting steps.","title":"Unsloth Compatibility Guide"},{"location":"unsloth_compatibility/#overview","text":"Unsloth provides faster training with memory optimizations, but requires specific environment configurations. AlignTune includes robust detection and fallback mechanisms to ensure training works even when Unsloth is unavailable.","title":"Overview"},{"location":"unsloth_compatibility/#supported-versions","text":"","title":"Supported Versions"},{"location":"unsloth_compatibility/#recommended-combinations","text":"PyTorch CUDA Unsloth Status 2.0.0-2.7.0 11.8, 12.1, 12.4 latest Optimal 2.8.0+ 12.8+ latest Known Issues","title":"Recommended Combinations"},{"location":"unsloth_compatibility/#python-requirements","text":"Python 3.8-3.12 CUDA-capable GPU (for GPU training)","title":"Python Requirements"},{"location":"unsloth_compatibility/#known-compatibility-issues","text":"","title":"Known Compatibility Issues"},{"location":"unsloth_compatibility/#1-cuda-symbol-errors","text":"Error : undefined symbol: _ZN3c104cuda9SetDeviceEa Cause : Version incompatibility between PyTorch 2.8.0+ and Unsloth's CUDA extensions. Solutions : - Use PyTorch 2.7.0 or earlier - Update Unsloth: pip install --upgrade unsloth - Use TRL backends instead: --backend trl","title":"1. CUDA Symbol Errors"},{"location":"unsloth_compatibility/#2-flash-attention-issues","text":"Error : Flash Attention 2 installation broken Cause : CUDA version incompatibility with Flash Attention. Solutions : - Unsloth automatically falls back to Xformers - Update Flash Attention: pip install --upgrade flash-attn - Use TRL backends: --backend trl","title":"2. Flash Attention Issues"},{"location":"unsloth_compatibility/#3-version-mismatches","text":"Error : Version incompatibility between dependencies Solutions : - Check compatibility matrix above - Update all dependencies: pip install --upgrade torch unsloth - Use TRL backends: --backend trl","title":"3. Version Mismatches"},{"location":"unsloth_compatibility/#environment-setup","text":"","title":"Environment Setup"},{"location":"unsloth_compatibility/#optimal-setup","text":"# Install PyTorch with CUDA support pip install torch == 2 .7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Install Unsloth pip install unsloth # Install AlignTune pip install -e .","title":"Optimal Setup"},{"location":"unsloth_compatibility/#alternative-setup-if-cuda-issues-persist","text":"# Use TRL backends instead pip install torch transformers trl pip install -e .","title":"Alternative Setup (if CUDA issues persist)"},{"location":"unsloth_compatibility/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"unsloth_compatibility/#1-check-environment","text":"# Run comprehensive diagnostics aligntune diagnose # Check basic info with verbose output aligntune info --verbose","title":"1. Check Environment"},{"location":"unsloth_compatibility/#2-common-issues-and-solutions","text":"","title":"2. Common Issues and Solutions"},{"location":"unsloth_compatibility/#unsloth-not-detected","text":"# Check if Unsloth is installed python -c \"import unsloth; print('Unsloth available')\" # If error occurs, check CUDA compatibility python -c \"import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')\"","title":"Unsloth Not Detected"},{"location":"unsloth_compatibility/#cuda-symbol-errors","text":"Solution 1 : Downgrade PyTorch to 2.7.0 Solution 2 : Use TRL backends: --backend trl Solution 3 : Update Unsloth: pip install --upgrade unsloth","title":"CUDA Symbol Errors"},{"location":"unsloth_compatibility/#flash-attention-issues","text":"Unsloth automatically falls back to Xformers No action required, but performance may be slightly reduced","title":"Flash Attention Issues"},{"location":"unsloth_compatibility/#3-fallback-to-trl-backends","text":"If Unsloth is not available, AlignTune automatically falls back to TRL backends: # Explicitly use TRL backends aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend trl # Or let AlignTune auto-select aligntune train --model microsoft/DialoGPT-medium --dataset tatsu-lab/alpaca --backend auto","title":"3. Fallback to TRL Backends"},{"location":"unsloth_compatibility/#performance-comparison","text":"Backend Speed Memory Compatibility Unsloth faster 80% less Requires specific setup TRL Standard Standard Universal Legacy Standard Standard Universal","title":"Performance Comparison"},{"location":"unsloth_compatibility/#diagnostic-commands","text":"","title":"Diagnostic Commands"},{"location":"unsloth_compatibility/#basic-information","text":"# Show system status aligntune info # Show detailed diagnostics aligntune info --verbose","title":"Basic Information"},{"location":"unsloth_compatibility/#comprehensive-diagnostics","text":"# Run full environment check aligntune diagnose","title":"Comprehensive Diagnostics"},{"location":"unsloth_compatibility/#backend-selection","text":"# List available backends aligntune list-backends # Validate model compatibility aligntune validate --model microsoft/DialoGPT-medium --backend auto","title":"Backend Selection"},{"location":"unsloth_compatibility/#error-messages-and-solutions","text":"","title":"Error Messages and Solutions"},{"location":"unsloth_compatibility/#unsloth-not-available-cuda_symbol_error","text":"Cause : CUDA version incompatibility Solution : Use TRL backends or fix CUDA setup","title":"\"Unsloth not available: cuda_symbol_error\""},{"location":"unsloth_compatibility/#unsloth-not-available-flash_attention_error","text":"Cause : Flash Attention compatibility issues Solution : Unsloth will auto-fallback to Xformers","title":"\"Unsloth not available: flash_attention_error\""},{"location":"unsloth_compatibility/#unsloth-not-available-missing_dependency","text":"Cause : Unsloth not installed Solution : pip install unsloth or use TRL backends","title":"\"Unsloth not available: missing_dependency\""},{"location":"unsloth_compatibility/#best-practices","text":"Always test with aligntune diagnose before training Use --backend auto to let AlignTune choose the best available backend Check logs for detailed error information Fallback to TRL if Unsloth has issues Update dependencies regularly for best compatibility","title":"Best Practices"},{"location":"unsloth_compatibility/#getting-help","text":"If you encounter issues not covered in this guide: Run aligntune diagnose and share the output Check the logs for detailed error messages Try using TRL backends as a fallback Report issues with full diagnostic output","title":"Getting Help"},{"location":"unsloth_compatibility/#version-history","text":"v0.2.0 : Enhanced error detection and diagnostics v0.1.0 : Initial Unsloth support with basic detection For more information, see the main README or run aligntune info --help .","title":"Version History"},{"location":"advanced/architecture/","text":"Architecture Overview \u00b6 Complete architecture documentation for AlignTune. System Architecture \u00b6 High-Level Overview \u00b6 AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend supports: - SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend supports: - SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO Note: GSPO is a TRL-only algorithm. Complete System Architecture \u00b6 flowchart TD UI[User Interface] --> Factory[Backend Factory Layer] Factory --> TRL[TRL Backends] Factory --> Unsloth[Unsloth Backends] TRL --> Core[Core Training Components] Unsloth --> Core Core --> Rewards[Reward System] Core --> Eval[Evaluation System] Core Components \u00b6 1. Backend Factory \u00b6 The backend factory provides a unified interface for creating trainers: Backend Selection : Automatic or manual backend selection Configuration Management : Unified configuration system Fallback Handling : Automatic fallback to available backends Key Classes: - BackendFactory : Main factory class - BackendConfig : Backend configuration - create_sft_trainer() : SFT trainer creation - create_rl_trainer() : RL trainer creation 2. Configuration System \u00b6 Unified configuration system with type safety: SFT Configuration : SFTConfig , ModelConfig , DatasetConfig , TrainingConfig RL Configuration : UnifiedConfig , algorithm-specific configs Validation : Comprehensive validation with clear error messages 3. Training Backends \u00b6 TRL Backends \u00b6 Pure TRL implementations Maximum compatibility Battle-tested reliability Supports all core algorithms (DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO) Unsloth Backends \u00b6 faster training Memory efficient Optimized kernels Supports DPO, PPO, GRPO, DAPO, Dr. GRPO (not GSPO) 4. Reward System \u00b6 27+ Pre-built Functions : Length, sentiment, safety, etc. Composite Rewards : Combine multiple functions Custom Functions : Extensible reward system Reward Model Training : Train neural reward models 5. Evaluation System \u00b6 Training Evaluation : Automatic during training Standalone Evaluation : Evaluate saved models Benchmark Integration : lm-eval integration Custom Tasks : Task-specific evaluation Design Patterns \u00b6 Factory Pattern \u00b6 Backend selection uses the factory pattern: # Factory creates appropriate trainer trainer = BackendFactory . create_trainer ( config , backend_config ) Strategy Pattern \u00b6 Different backends implement the same interface: # Both backends implement TrainerBase class TRLSFTTrainer ( SFTTrainerBase ): ... class UnslothSFTTrainer ( SFTTrainerBase ): ... Registry Pattern \u00b6 Reward functions use registry pattern: # Register and retrieve functions RewardRegistry . register_reward ( \"custom\" , CustomReward ) func = RewardRegistry . get_reward_function ( \"custom\" ) Data Flow \u00b6 Training Flow \u00b6 1. User creates trainer via factory 2. Factory selects backend 3. Backend loads model and dataset 4. Training loop executes 5. Metrics logged and checkpoints saved 6. Model saved to disk Evaluation Flow \u00b6 1. Load model and evaluation dataset 2. Run inference on dataset 3. Compute metrics 4. Return results Extension Points \u00b6 Custom Backends \u00b6 Create custom backends by implementing trainer base classes: class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): ... def evaluate ( self ): ... Custom Reward Functions \u00b6 Extend reward system with custom functions: class CustomReward ( RewardFunction ): def compute ( self , text : str ) -> float : ... Custom Evaluation Tasks \u00b6 Add custom evaluation tasks: task = EvalTask ( name = \"custom\" , category = TaskCategory . CUSTOM , ... ) Next Steps \u00b6 Custom Backends - Creating custom backends Distributed Training - Distributed training architecture Performance - Performance optimization","title":"Architecture"},{"location":"advanced/architecture/#architecture-overview","text":"Complete architecture documentation for AlignTune.","title":"Architecture Overview"},{"location":"advanced/architecture/#system-architecture","text":"","title":"System Architecture"},{"location":"advanced/architecture/#high-level-overview","text":"AlignTune uses a flexible backend architecture: flowchart TD Factory[Backend Factory] --> TRL[TRL Backend] Factory --> Unsloth[Unsloth Backend] TRL --> TRL_Algos[TRL Algorithms] Unsloth --> Unsloth_Algos[Unsloth Algorithms] TRL Backend supports: - SFT, DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Unsloth Backend supports: - SFT, DPO, PPO, GRPO, DAPO, Dr. GRPO Note: GSPO is a TRL-only algorithm.","title":"High-Level Overview"},{"location":"advanced/architecture/#complete-system-architecture","text":"flowchart TD UI[User Interface] --> Factory[Backend Factory Layer] Factory --> TRL[TRL Backends] Factory --> Unsloth[Unsloth Backends] TRL --> Core[Core Training Components] Unsloth --> Core Core --> Rewards[Reward System] Core --> Eval[Evaluation System]","title":"Complete System Architecture"},{"location":"advanced/architecture/#core-components","text":"","title":"Core Components"},{"location":"advanced/architecture/#1-backend-factory","text":"The backend factory provides a unified interface for creating trainers: Backend Selection : Automatic or manual backend selection Configuration Management : Unified configuration system Fallback Handling : Automatic fallback to available backends Key Classes: - BackendFactory : Main factory class - BackendConfig : Backend configuration - create_sft_trainer() : SFT trainer creation - create_rl_trainer() : RL trainer creation","title":"1. Backend Factory"},{"location":"advanced/architecture/#2-configuration-system","text":"Unified configuration system with type safety: SFT Configuration : SFTConfig , ModelConfig , DatasetConfig , TrainingConfig RL Configuration : UnifiedConfig , algorithm-specific configs Validation : Comprehensive validation with clear error messages","title":"2. Configuration System"},{"location":"advanced/architecture/#3-training-backends","text":"","title":"3. Training Backends"},{"location":"advanced/architecture/#trl-backends","text":"Pure TRL implementations Maximum compatibility Battle-tested reliability Supports all core algorithms (DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO)","title":"TRL Backends"},{"location":"advanced/architecture/#unsloth-backends","text":"faster training Memory efficient Optimized kernels Supports DPO, PPO, GRPO, DAPO, Dr. GRPO (not GSPO)","title":"Unsloth Backends"},{"location":"advanced/architecture/#4-reward-system","text":"27+ Pre-built Functions : Length, sentiment, safety, etc. Composite Rewards : Combine multiple functions Custom Functions : Extensible reward system Reward Model Training : Train neural reward models","title":"4. Reward System"},{"location":"advanced/architecture/#5-evaluation-system","text":"Training Evaluation : Automatic during training Standalone Evaluation : Evaluate saved models Benchmark Integration : lm-eval integration Custom Tasks : Task-specific evaluation","title":"5. Evaluation System"},{"location":"advanced/architecture/#design-patterns","text":"","title":"Design Patterns"},{"location":"advanced/architecture/#factory-pattern","text":"Backend selection uses the factory pattern: # Factory creates appropriate trainer trainer = BackendFactory . create_trainer ( config , backend_config )","title":"Factory Pattern"},{"location":"advanced/architecture/#strategy-pattern","text":"Different backends implement the same interface: # Both backends implement TrainerBase class TRLSFTTrainer ( SFTTrainerBase ): ... class UnslothSFTTrainer ( SFTTrainerBase ): ...","title":"Strategy Pattern"},{"location":"advanced/architecture/#registry-pattern","text":"Reward functions use registry pattern: # Register and retrieve functions RewardRegistry . register_reward ( \"custom\" , CustomReward ) func = RewardRegistry . get_reward_function ( \"custom\" )","title":"Registry Pattern"},{"location":"advanced/architecture/#data-flow","text":"","title":"Data Flow"},{"location":"advanced/architecture/#training-flow","text":"1. User creates trainer via factory 2. Factory selects backend 3. Backend loads model and dataset 4. Training loop executes 5. Metrics logged and checkpoints saved 6. Model saved to disk","title":"Training Flow"},{"location":"advanced/architecture/#evaluation-flow","text":"1. Load model and evaluation dataset 2. Run inference on dataset 3. Compute metrics 4. Return results","title":"Evaluation Flow"},{"location":"advanced/architecture/#extension-points","text":"","title":"Extension Points"},{"location":"advanced/architecture/#custom-backends","text":"Create custom backends by implementing trainer base classes: class CustomSFTTrainer ( SFTTrainerBase ): def train ( self ): ... def evaluate ( self ): ...","title":"Custom Backends"},{"location":"advanced/architecture/#custom-reward-functions","text":"Extend reward system with custom functions: class CustomReward ( RewardFunction ): def compute ( self , text : str ) -> float : ...","title":"Custom Reward Functions"},{"location":"advanced/architecture/#custom-evaluation-tasks","text":"Add custom evaluation tasks: task = EvalTask ( name = \"custom\" , category = TaskCategory . CUSTOM , ... )","title":"Custom Evaluation Tasks"},{"location":"advanced/architecture/#next-steps","text":"Custom Backends - Creating custom backends Distributed Training - Distributed training architecture Performance - Performance optimization","title":"Next Steps"},{"location":"algorithms/dapo/","text":"DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) \u00b6 DAPO is an advanced RLHF algorithm that decouples clipping and dynamic sampling for improved training stability. Overview \u00b6 Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) addresses limitations in traditional PPO by separating the clipping mechanism from dynamic sampling, providing more stable and efficient training. Key Features \u00b6 Decoupled optimization : Separate clipping and sampling mechanisms Dynamic sampling : Adaptive sample generation based on training progress Improved stability : Better gradient behavior than standard PPO Multi-backend support : Available in both TRL and Unsloth backends Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 DAPO improves upon PPO by: Decoupled clipping : Separate value and policy clipping Dynamic sampling : Adjusts sample distribution during training Adaptive optimization : Better handling of reward scaling Configuration Options \u00b6 Parameter Type Default Description clip_range float 0.2 Policy clipping range value_clip_range float 0.2 Value function clipping range dynamic_sampling bool true Enable dynamic sampling sampling_temperature float 1.0 Sampling temperature Advantages \u00b6 More stable training than standard PPO Better performance on complex tasks Reduced hyperparameter sensitivity See Also \u00b6 PPO Algorithm - Base policy optimization GRPO Algorithm - Group-based optimization Algorithms Overview - All supported algorithms","title":"DAPO"},{"location":"algorithms/dapo/#dapo-decouple-clip-and-dynamic-sampling-policy-optimization","text":"DAPO is an advanced RLHF algorithm that decouples clipping and dynamic sampling for improved training stability.","title":"DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization)"},{"location":"algorithms/dapo/#overview","text":"Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) addresses limitations in traditional PPO by separating the clipping mechanism from dynamic sampling, providing more stable and efficient training.","title":"Overview"},{"location":"algorithms/dapo/#key-features","text":"Decoupled optimization : Separate clipping and sampling mechanisms Dynamic sampling : Adaptive sample generation based on training progress Improved stability : Better gradient behavior than standard PPO Multi-backend support : Available in both TRL and Unsloth backends","title":"Key Features"},{"location":"algorithms/dapo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/dapo/#algorithm-details","text":"DAPO improves upon PPO by: Decoupled clipping : Separate value and policy clipping Dynamic sampling : Adjusts sample distribution during training Adaptive optimization : Better handling of reward scaling","title":"Algorithm Details"},{"location":"algorithms/dapo/#configuration-options","text":"Parameter Type Default Description clip_range float 0.2 Policy clipping range value_clip_range float 0.2 Value function clipping range dynamic_sampling bool true Enable dynamic sampling sampling_temperature float 1.0 Sampling temperature","title":"Configuration Options"},{"location":"algorithms/dapo/#advantages","text":"More stable training than standard PPO Better performance on complex tasks Reduced hyperparameter sensitivity","title":"Advantages"},{"location":"algorithms/dapo/#see-also","text":"PPO Algorithm - Base policy optimization GRPO Algorithm - Group-based optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/dpo/","text":"DPO (Direct Preference Optimization) \u00b6 Direct Preference Optimization (DPO) is a popular RLHF algorithm that optimizes language models directly on preference data without requiring a separate reward model. Overview \u00b6 DPO eliminates the need for a reward model by directly optimizing the policy on preference pairs (chosen vs rejected responses). This makes it simpler and faster than PPO-based approaches. Key Features \u00b6 No Reward Model : Works directly with preference pairs Fast Training : Simpler optimization than PPO Both Backends : Supported by TRL and Unsloth Widely Used : Industry-standard for preference learning How It Works \u00b6 DPO optimizes the policy to increase the likelihood of chosen responses while decreasing the likelihood of rejected responses, using a KL divergence constraint to prevent the policy from deviating too far from the reference model. Mathematical Formulation \u00b6 The DPO objective maximizes: L_DPO = E[log \u03c3(\u03b2 * (log \u03c0_\u03b8(y_w | x) - log \u03c0_ref(y_w | x) - log \u03c0_\u03b8(y_l | x) + log \u03c0_ref(y_l | x)))] Where: - \u03c0_\u03b8 : Policy model - \u03c0_ref : Reference model - y_w : Chosen response - y_l : Rejected response - \u03b2 : Temperature parameter Usage \u00b6 Basic DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 1e-6 , beta = 0.1 # DPO temperature parameter ) trainer . train () DPO with Unsloth Backend \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 ) trainer . train () DPO with Custom Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-6 , beta = 0.1 , max_seq_length = 512 , reference_free = False , # Use reference model label_smoothing = 0.0 , loss_type = \"sigmoid\" # DPO loss type ) Configuration Parameters \u00b6 DPO-Specific Parameters \u00b6 Parameter Type Default Description beta float 0.1 DPO temperature parameter (controls KL penalty) reference_free bool False Whether to use reference-free DPO label_smoothing float 0.0 Label smoothing factor loss_type str \"sigmoid\" Loss function type General RL Parameters \u00b6 Parameter Type Default Description num_epochs int 1 Number of training epochs batch_size int 4 Per-device batch size learning_rate float 1e-6 Learning rate max_seq_length int 512 Maximum sequence length Dataset Format \u00b6 DPO requires preference pairs in the following format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know about that topic.\" } Supported Datasets \u00b6 Anthropic/hh-rlhf : Human preference dataset OpenAssistant/oasst1 : OpenAssistant conversations Custom datasets with prompt , chosen , rejected columns Best Practices \u00b6 1. Beta Parameter Tuning \u00b6 The beta parameter controls the strength of the KL penalty: - Low beta (0.01-0.1) : Stronger preference learning, risk of overfitting - Medium beta (0.1-0.5) : Balanced learning - High beta (0.5-1.0) : More conservative, stays closer to reference # Conservative training beta = 0.5 # Aggressive training beta = 0.01 2. Learning Rate \u00b6 DPO typically requires lower learning rates than SFT: - Start with 1e-6 to 5e-6 - Adjust based on loss convergence 3. Batch Size \u00b6 Use batch size 1-4 for smaller models Use batch size 4-8 for larger models Consider gradient accumulation for effective larger batches 4. Number of Epochs \u00b6 Usually 1-3 epochs are sufficient Monitor validation loss to avoid overfitting Evaluation \u00b6 DPO evaluation includes: from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also Common Issues \u00b6 Loss Not Decreasing \u00b6 Solutions : - Reduce learning rate - Increase beta (stronger KL penalty) - Check dataset quality - Verify preference pairs are correct Overfitting \u00b6 Solutions : - Increase beta - Reduce number of epochs - Use more regularization - Increase dataset size Comparison with Other Algorithms \u00b6 Feature DPO PPO GRPO Reward Model No Yes No Training Speed Fast Slow Medium Setup Complexity Low High Medium Preference Data Required Optional Required References \u00b6 Direct Preference Optimization Paper TRL DPO Documentation Next Steps \u00b6 PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"DPO"},{"location":"algorithms/dpo/#dpo-direct-preference-optimization","text":"Direct Preference Optimization (DPO) is a popular RLHF algorithm that optimizes language models directly on preference data without requiring a separate reward model.","title":"DPO (Direct Preference Optimization)"},{"location":"algorithms/dpo/#overview","text":"DPO eliminates the need for a reward model by directly optimizing the policy on preference pairs (chosen vs rejected responses). This makes it simpler and faster than PPO-based approaches.","title":"Overview"},{"location":"algorithms/dpo/#key-features","text":"No Reward Model : Works directly with preference pairs Fast Training : Simpler optimization than PPO Both Backends : Supported by TRL and Unsloth Widely Used : Industry-standard for preference learning","title":"Key Features"},{"location":"algorithms/dpo/#how-it-works","text":"DPO optimizes the policy to increase the likelihood of chosen responses while decreasing the likelihood of rejected responses, using a KL divergence constraint to prevent the policy from deviating too far from the reference model.","title":"How It Works"},{"location":"algorithms/dpo/#mathematical-formulation","text":"The DPO objective maximizes: L_DPO = E[log \u03c3(\u03b2 * (log \u03c0_\u03b8(y_w | x) - log \u03c0_ref(y_w | x) - log \u03c0_\u03b8(y_l | x) + log \u03c0_ref(y_l | x)))] Where: - \u03c0_\u03b8 : Policy model - \u03c0_ref : Reference model - y_w : Chosen response - y_l : Rejected response - \u03b2 : Temperature parameter","title":"Mathematical Formulation"},{"location":"algorithms/dpo/#usage","text":"","title":"Usage"},{"location":"algorithms/dpo/#basic-dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 1e-6 , beta = 0.1 # DPO temperature parameter ) trainer . train ()","title":"Basic DPO Training"},{"location":"algorithms/dpo/#dpo-with-unsloth-backend","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 ) trainer . train ()","title":"DPO with Unsloth Backend"},{"location":"algorithms/dpo/#dpo-with-custom-configuration","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-6 , beta = 0.1 , max_seq_length = 512 , reference_free = False , # Use reference model label_smoothing = 0.0 , loss_type = \"sigmoid\" # DPO loss type )","title":"DPO with Custom Configuration"},{"location":"algorithms/dpo/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"algorithms/dpo/#dpo-specific-parameters","text":"Parameter Type Default Description beta float 0.1 DPO temperature parameter (controls KL penalty) reference_free bool False Whether to use reference-free DPO label_smoothing float 0.0 Label smoothing factor loss_type str \"sigmoid\" Loss function type","title":"DPO-Specific Parameters"},{"location":"algorithms/dpo/#general-rl-parameters","text":"Parameter Type Default Description num_epochs int 1 Number of training epochs batch_size int 4 Per-device batch size learning_rate float 1e-6 Learning rate max_seq_length int 512 Maximum sequence length","title":"General RL Parameters"},{"location":"algorithms/dpo/#dataset-format","text":"DPO requires preference pairs in the following format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know about that topic.\" }","title":"Dataset Format"},{"location":"algorithms/dpo/#supported-datasets","text":"Anthropic/hh-rlhf : Human preference dataset OpenAssistant/oasst1 : OpenAssistant conversations Custom datasets with prompt , chosen , rejected columns","title":"Supported Datasets"},{"location":"algorithms/dpo/#best-practices","text":"","title":"Best Practices"},{"location":"algorithms/dpo/#1-beta-parameter-tuning","text":"The beta parameter controls the strength of the KL penalty: - Low beta (0.01-0.1) : Stronger preference learning, risk of overfitting - Medium beta (0.1-0.5) : Balanced learning - High beta (0.5-1.0) : More conservative, stays closer to reference # Conservative training beta = 0.5 # Aggressive training beta = 0.01","title":"1. Beta Parameter Tuning"},{"location":"algorithms/dpo/#2-learning-rate","text":"DPO typically requires lower learning rates than SFT: - Start with 1e-6 to 5e-6 - Adjust based on loss convergence","title":"2. Learning Rate"},{"location":"algorithms/dpo/#3-batch-size","text":"Use batch size 1-4 for smaller models Use batch size 4-8 for larger models Consider gradient accumulation for effective larger batches","title":"3. Batch Size"},{"location":"algorithms/dpo/#4-number-of-epochs","text":"Usually 1-3 epochs are sufficient Monitor validation loss to avoid overfitting","title":"4. Number of Epochs"},{"location":"algorithms/dpo/#evaluation","text":"DPO evaluation includes: from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also","title":"Evaluation"},{"location":"algorithms/dpo/#common-issues","text":"","title":"Common Issues"},{"location":"algorithms/dpo/#loss-not-decreasing","text":"Solutions : - Reduce learning rate - Increase beta (stronger KL penalty) - Check dataset quality - Verify preference pairs are correct","title":"Loss Not Decreasing"},{"location":"algorithms/dpo/#overfitting","text":"Solutions : - Increase beta - Reduce number of epochs - Use more regularization - Increase dataset size","title":"Overfitting"},{"location":"algorithms/dpo/#comparison-with-other-algorithms","text":"Feature DPO PPO GRPO Reward Model No Yes No Training Speed Fast Slow Medium Setup Complexity Low High Medium Preference Data Required Optional Required","title":"Comparison with Other Algorithms"},{"location":"algorithms/dpo/#references","text":"Direct Preference Optimization Paper TRL DPO Documentation","title":"References"},{"location":"algorithms/dpo/#next-steps","text":"PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"algorithms/dr-grpo/","text":"Dr. GRPO (GRPO Done Right) \u00b6 Dr. GRPO is a corrected and improved version of the original GRPO algorithm. Overview \u00b6 GRPO Done Right (Dr. GRPO) addresses optimization biases identified in the original GRPO implementation, providing more accurate and stable training. Key Features \u00b6 Bias correction : Fixes optimization biases in original GRPO Improved stability : More reliable convergence Enhanced performance : Better results on benchmark tasks Multi-backend support : Available in both TRL and Unsloth Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dr-grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 Dr. GRPO corrects issues in the original GRPO by: Bias correction : Proper normalization of group advantages Improved clipping : Better handling of extreme values Enhanced sampling : More efficient sample utilization Configuration Options \u00b6 Parameter Type Default Description group_size int 8 Number of samples per group bias_correction bool true Enable bias correction clip_range float 0.2 Advantage clipping range Benefits \u00b6 More accurate optimization than original GRPO Better convergence properties Improved performance on complex tasks See Also \u00b6 GRPO Algorithm - Original group optimization Algorithms Overview - All supported algorithms","title":"Dr. GRPO"},{"location":"algorithms/dr-grpo/#dr-grpo-grpo-done-right","text":"Dr. GRPO is a corrected and improved version of the original GRPO algorithm.","title":"Dr. GRPO (GRPO Done Right)"},{"location":"algorithms/dr-grpo/#overview","text":"GRPO Done Right (Dr. GRPO) addresses optimization biases identified in the original GRPO implementation, providing more accurate and stable training.","title":"Overview"},{"location":"algorithms/dr-grpo/#key-features","text":"Bias correction : Fixes optimization biases in original GRPO Improved stability : More reliable convergence Enhanced performance : Better results on benchmark tasks Multi-backend support : Available in both TRL and Unsloth","title":"Key Features"},{"location":"algorithms/dr-grpo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dr-grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/dr-grpo/#algorithm-details","text":"Dr. GRPO corrects issues in the original GRPO by: Bias correction : Proper normalization of group advantages Improved clipping : Better handling of extreme values Enhanced sampling : More efficient sample utilization","title":"Algorithm Details"},{"location":"algorithms/dr-grpo/#configuration-options","text":"Parameter Type Default Description group_size int 8 Number of samples per group bias_correction bool true Enable bias correction clip_range float 0.2 Advantage clipping range","title":"Configuration Options"},{"location":"algorithms/dr-grpo/#benefits","text":"More accurate optimization than original GRPO Better convergence properties Improved performance on complex tasks","title":"Benefits"},{"location":"algorithms/dr-grpo/#see-also","text":"GRPO Algorithm - Original group optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/grpo/","text":"GRPO (Group Relative Policy Optimization) \u00b6 GRPO is an advanced RLHF algorithm that optimizes policies using group-based relative comparisons. Overview \u00b6 Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that performs policy optimization by comparing groups of generated samples rather than individual samples. This approach provides more stable training and better performance on complex tasks. Key Features \u00b6 Group-based optimization : Compares groups of samples for more stable gradients Relative comparisons : Uses relative rewards within groups rather than absolute values Memory efficient : Reduces variance through group normalization Scalable : Works well with large models and complex tasks Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 GRPO optimizes policies by: Group Generation : Generate multiple samples for each prompt Group Scoring : Score all samples in a group Relative Normalization : Normalize rewards within each group Policy Update : Update policy based on group-relative advantages Configuration Options \u00b6 Parameter Type Default Description group_size int 8 Number of samples per group beta float 0.01 KL penalty coefficient cliprange float 0.2 PPO-style clipping range temperature float 0.7 Sampling temperature Best Practices \u00b6 Use larger group sizes (8-16) for better performance Combine with diverse reward functions Monitor group diversity to ensure stable training See Also \u00b6 DPO Algorithm - Direct Preference Optimization PPO Algorithm - Proximal Policy Optimization Algorithms Overview - All supported algorithms","title":"GRPO"},{"location":"algorithms/grpo/#grpo-group-relative-policy-optimization","text":"GRPO is an advanced RLHF algorithm that optimizes policies using group-based relative comparisons.","title":"GRPO (Group Relative Policy Optimization)"},{"location":"algorithms/grpo/#overview","text":"Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that performs policy optimization by comparing groups of generated samples rather than individual samples. This approach provides more stable training and better performance on complex tasks.","title":"Overview"},{"location":"algorithms/grpo/#key-features","text":"Group-based optimization : Compares groups of samples for more stable gradients Relative comparisons : Uses relative rewards within groups rather than absolute values Memory efficient : Reduces variance through group normalization Scalable : Works well with large models and complex tasks","title":"Key Features"},{"location":"algorithms/grpo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/grpo/#algorithm-details","text":"GRPO optimizes policies by: Group Generation : Generate multiple samples for each prompt Group Scoring : Score all samples in a group Relative Normalization : Normalize rewards within each group Policy Update : Update policy based on group-relative advantages","title":"Algorithm Details"},{"location":"algorithms/grpo/#configuration-options","text":"Parameter Type Default Description group_size int 8 Number of samples per group beta float 0.01 KL penalty coefficient cliprange float 0.2 PPO-style clipping range temperature float 0.7 Sampling temperature","title":"Configuration Options"},{"location":"algorithms/grpo/#best-practices","text":"Use larger group sizes (8-16) for better performance Combine with diverse reward functions Monitor group diversity to ensure stable training","title":"Best Practices"},{"location":"algorithms/grpo/#see-also","text":"DPO Algorithm - Direct Preference Optimization PPO Algorithm - Proximal Policy Optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/gspo/","text":"GSPO (Group Sequential Policy Optimization) \u00b6 GSPO is a variant of GRPO that performs sequential optimization within groups. Overview \u00b6 Group Sequential Policy Optimization (GSPO) extends GRPO by performing sequential policy updates within each group, allowing for more fine-grained optimization of complex behaviors. Key Features \u00b6 Sequential updates : Multiple optimization steps per group Fine-grained control : Better handling of complex reward landscapes TRL-only : Currently only available in TRL backend Usage \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only available in TRL num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train () Algorithm Details \u00b6 GSPO performs multiple sequential policy updates within each group, allowing for: Progressive refinement : Each step improves upon the previous Complex optimization : Better handling of multi-objective rewards Stability : Reduced variance through sequential updates Configuration Options \u00b6 Parameter Type Default Description group_size int 4 Number of samples per group seq_steps int 3 Number of sequential steps beta float 0.01 KL penalty coefficient Limitations \u00b6 Currently only available in TRL backend Higher computational cost than standard GRPO May require tuning of sequential parameters See Also \u00b6 GRPO Algorithm - Base group optimization Algorithms Overview - All supported algorithms","title":"GSPO"},{"location":"algorithms/gspo/#gspo-group-sequential-policy-optimization","text":"GSPO is a variant of GRPO that performs sequential optimization within groups.","title":"GSPO (Group Sequential Policy Optimization)"},{"location":"algorithms/gspo/#overview","text":"Group Sequential Policy Optimization (GSPO) extends GRPO by performing sequential policy updates within each group, allowing for more fine-grained optimization of complex behaviors.","title":"Overview"},{"location":"algorithms/gspo/#key-features","text":"Sequential updates : Multiple optimization steps per group Fine-grained control : Better handling of complex reward landscapes TRL-only : Currently only available in TRL backend","title":"Key Features"},{"location":"algorithms/gspo/#usage","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only available in TRL num_epochs = 1 , batch_size = 2 , learning_rate = 1e-6 ) trainer . train ()","title":"Usage"},{"location":"algorithms/gspo/#algorithm-details","text":"GSPO performs multiple sequential policy updates within each group, allowing for: Progressive refinement : Each step improves upon the previous Complex optimization : Better handling of multi-objective rewards Stability : Reduced variance through sequential updates","title":"Algorithm Details"},{"location":"algorithms/gspo/#configuration-options","text":"Parameter Type Default Description group_size int 4 Number of samples per group seq_steps int 3 Number of sequential steps beta float 0.01 KL penalty coefficient","title":"Configuration Options"},{"location":"algorithms/gspo/#limitations","text":"Currently only available in TRL backend Higher computational cost than standard GRPO May require tuning of sequential parameters","title":"Limitations"},{"location":"algorithms/gspo/#see-also","text":"GRPO Algorithm - Base group optimization Algorithms Overview - All supported algorithms","title":"See Also"},{"location":"algorithms/overview/","text":"RL Algorithms Overview \u00b6 AlignTune supports a comprehensive set of Reinforcement Learning algorithms for aligning Large Language Models with human preferences and optimizing for specific objectives. Algorithm Comparison \u00b6 Algorithm TRL Backend Unsloth Backend Reward Model Description DPO Yes Yes No Direct Preference Optimization - No reward model needed PPO Yes Yes Yes Proximal Policy Optimization - Requires reward model GRPO Yes Yes No Group Relative Policy Optimization - Multi-criteria optimization GSPO Yes No No Group Sequential Policy Optimization - Sequential group learning (TRL only) DAPO Yes Yes No Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes No GRPO Done Right - Unbiased GRPO variant Algorithm Selection Guide \u00b6 When to Use DPO \u00b6 Use DPO when: - You have preference data (chosen vs rejected pairs) - You want to avoid training a separate reward model - You need fast training with minimal setup - You're working with preference datasets like Anthropic/hh-rlhf Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) When to Use PPO \u00b6 Use PPO when: - You have a reward model (or want to train one) - You need fine-grained control over reward signals - You're doing online learning with environment interaction - You want to optimize for complex, multi-objective rewards Example : trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" ) When to Use GRPO \u00b6 Use GRPO when: - You want multi-criteria optimization - You're working with group-based preferences - You need to optimize for multiple objectives simultaneously - You want to avoid reward model training Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" ) When to Use GSPO \u00b6 Use GSPO when: - You need sequential group learning - You're working with sequential decision-making tasks - You want TRL backend (Unsloth not supported) Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" # Only TRL supports GSPO ) When to Use DAPO \u00b6 Use DAPO when: - You want to address GRPO limitations - You need decoupled clipping and dynamic sampling - You're working with large-scale RLHF Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" ) When to Use Dr. GRPO \u00b6 Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" ) When to Use Dr. GRPO \u00b6 Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" ) Algorithm Details \u00b6 DPO - Direct Preference Optimization PPO - Proximal Policy Optimization GRPO - Group Relative Policy Optimization GSPO - Group Sequential Policy Optimization DAPO - Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO - GRPO Done Right Quick Reference \u00b6 Algorithm Requirements \u00b6 Algorithm Dataset Format Reward Model Special Requirements DPO Preference pairs Chosen/rejected pairs PPO Prompts Reward model required GRPO Group preferences Group structure GSPO Sequential groups Sequential structure DAPO Group preferences Group structure Dr. GRPO Group preferences Group structure Next Steps \u00b6 DPO Guide - Learn about Direct Preference Optimization PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Overview"},{"location":"algorithms/overview/#rl-algorithms-overview","text":"AlignTune supports a comprehensive set of Reinforcement Learning algorithms for aligning Large Language Models with human preferences and optimizing for specific objectives.","title":"RL Algorithms Overview"},{"location":"algorithms/overview/#algorithm-comparison","text":"Algorithm TRL Backend Unsloth Backend Reward Model Description DPO Yes Yes No Direct Preference Optimization - No reward model needed PPO Yes Yes Yes Proximal Policy Optimization - Requires reward model GRPO Yes Yes No Group Relative Policy Optimization - Multi-criteria optimization GSPO Yes No No Group Sequential Policy Optimization - Sequential group learning (TRL only) DAPO Yes Yes No Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO Yes Yes No GRPO Done Right - Unbiased GRPO variant","title":"Algorithm Comparison"},{"location":"algorithms/overview/#algorithm-selection-guide","text":"","title":"Algorithm Selection Guide"},{"location":"algorithms/overview/#when-to-use-dpo","text":"Use DPO when: - You have preference data (chosen vs rejected pairs) - You want to avoid training a separate reward model - You need fast training with minimal setup - You're working with preference datasets like Anthropic/hh-rlhf Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"When to Use DPO"},{"location":"algorithms/overview/#when-to-use-ppo","text":"Use PPO when: - You have a reward model (or want to train one) - You need fine-grained control over reward signals - You're doing online learning with environment interaction - You want to optimize for complex, multi-objective rewards Example : trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" )","title":"When to Use PPO"},{"location":"algorithms/overview/#when-to-use-grpo","text":"Use GRPO when: - You want multi-criteria optimization - You're working with group-based preferences - You need to optimize for multiple objectives simultaneously - You want to avoid reward model training Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"trl\" )","title":"When to Use GRPO"},{"location":"algorithms/overview/#when-to-use-gspo","text":"Use GSPO when: - You need sequential group learning - You're working with sequential decision-making tasks - You want TRL backend (Unsloth not supported) Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" # Only TRL supports GSPO )","title":"When to Use GSPO"},{"location":"algorithms/overview/#when-to-use-dapo","text":"Use DAPO when: - You want to address GRPO limitations - You need decoupled clipping and dynamic sampling - You're working with large-scale RLHF Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen2-1.5B-Instruct\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dapo\" , backend = \"trl\" )","title":"When to Use DAPO"},{"location":"algorithms/overview/#when-to-use-dr-grpo","text":"Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" )","title":"When to Use Dr. GRPO"},{"location":"algorithms/overview/#when-to-use-dr-grpo_1","text":"Use Dr. GRPO when: - You want an unbiased GRPO variant - You need to correct optimization biases - You're working with group-based preferences Example : trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"drgrpo\" , backend = \"trl\" )","title":"When to Use Dr. GRPO"},{"location":"algorithms/overview/#algorithm-details","text":"DPO - Direct Preference Optimization PPO - Proximal Policy Optimization GRPO - Group Relative Policy Optimization GSPO - Group Sequential Policy Optimization DAPO - Decouple Clip and Dynamic sAmpling Policy Optimization Dr. GRPO - GRPO Done Right","title":"Algorithm Details"},{"location":"algorithms/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"algorithms/overview/#algorithm-requirements","text":"Algorithm Dataset Format Reward Model Special Requirements DPO Preference pairs Chosen/rejected pairs PPO Prompts Reward model required GRPO Group preferences Group structure GSPO Sequential groups Sequential structure DAPO Group preferences Group structure Dr. GRPO Group preferences Group structure","title":"Algorithm Requirements"},{"location":"algorithms/overview/#next-steps","text":"DPO Guide - Learn about Direct Preference Optimization PPO Guide - Learn about Proximal Policy Optimization GRPO Guide - Learn about Group Relative Policy Optimization Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"algorithms/ppo/","text":"PPO (Proximal Policy Optimization) \u00b6 Proximal Policy Optimization (PPO) is a powerful RLHF algorithm that uses a reward model to optimize language model behavior through policy gradient methods. Overview \u00b6 PPO optimizes the policy by generating responses, evaluating them with a reward model, and updating the policy to maximize expected reward while staying close to the reference policy. Key Features \u00b6 Reward Model : Uses neural reward models for fine-grained control Online Learning : Generates and evaluates responses during training Both Backends : Supported by TRL and Unsloth Flexible : Works with custom reward models and functions How It Works \u00b6 PPO alternates between: 1. Rollout Phase : Generate responses from the current policy 2. Evaluation Phase : Score responses with reward model 3. Update Phase : Update policy to maximize reward (with KL constraint) Mathematical Formulation \u00b6 The PPO objective maximizes: L_PPO = E[min(r_t * A_t, clip(r_t, 1-\u03b5, 1+\u03b5) * A_t)] Where: - r_t : Probability ratio between policy and reference - A_t : Advantage estimate - \u03b5 : Clipping parameter Usage \u00b6 Basic PPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , # Required num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO with Unsloth Backend \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO with Custom Reward Model \u00b6 # Train custom reward model first from aligntune.rewards.training import RewardModelTrainer reward_trainer = RewardModelTrainer ( base_model_name = \"Qwen/Qwen3-0.6B\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], composite_weights = [ 0.4 , 0.3 , 0.3 ] ) reward_model_path = reward_trainer . train () # Use in PPO training ppo_trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = reward_model_path , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) PPO with Reward Functions \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_functions = [ \"helpfulness\" , \"safety\" ], reward_function_weights = [ 0.6 , 0.4 ], num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) Configuration Parameters \u00b6 PPO-Specific Parameters \u00b6 Parameter Type Default Description cliprange float 0.2 PPO clipping range kl_coef float 0.1 KL penalty coefficient vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda parameter num_generations int 8 Number of generations per prompt num_ppo_epochs int 4 Number of PPO update epochs Reward Model Parameters \u00b6 Parameter Type Default Description reward_model_name str None HuggingFace reward model ID reward_model_path str None Local reward model path reward_functions list [] List of reward function names reward_function_weights list [] Weights for reward functions Dataset Format \u00b6 PPO requires prompts (not preference pairs): { \"prompt\" : \"What is machine learning?\" , # Responses are generated during training } Supported Datasets \u00b6 Anthropic/hh-rlhf : Can extract prompts OpenAssistant/oasst1 : Conversation prompts Custom datasets with prompt column Best Practices \u00b6 1. Reward Model Selection \u00b6 Use pre-trained models : HuggingFace reward models Train custom models : For domain-specific tasks Use reward functions : For rule-based rewards 2. Clipping Range \u00b6 Conservative (0.1-0.2) : Stable training, slower updates Aggressive (0.2-0.3) : Faster updates, risk of instability 3. KL Coefficient \u00b6 Low (0.01-0.1) : More exploration, risk of divergence High (0.1-0.5) : More conservative, stays close to reference 4. Number of Generations \u00b6 Few (4-8) : Faster training, less exploration Many (8-16) : More exploration, slower training Evaluation \u00b6 PPO evaluation includes: config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also Common Issues \u00b6 Reward Model Not Found \u00b6 Solutions : - Verify reward model name/path - Check HuggingFace authentication - Train custom reward model Training Instability \u00b6 Solutions : - Reduce learning rate - Increase KL coefficient - Reduce clipping range - Use smaller batch size Low Rewards \u00b6 Solutions : - Check reward model quality - Verify reward model is appropriate for task - Adjust reward function weights - Increase number of generations Comparison with Other Algorithms \u00b6 Feature PPO DPO GRPO Reward Model Required Not needed Not needed Training Speed Slow Fast Medium Setup Complexity High Low Medium Reward Control Fine-grained Coarse Medium References \u00b6 Proximal Policy Optimization Paper TRL PPO Documentation Next Steps \u00b6 DPO Guide - Learn about Direct Preference Optimization GRPO Guide - Learn about Group Relative Policy Optimization Reward Model Training - Train custom reward models","title":"PPO"},{"location":"algorithms/ppo/#ppo-proximal-policy-optimization","text":"Proximal Policy Optimization (PPO) is a powerful RLHF algorithm that uses a reward model to optimize language model behavior through policy gradient methods.","title":"PPO (Proximal Policy Optimization)"},{"location":"algorithms/ppo/#overview","text":"PPO optimizes the policy by generating responses, evaluating them with a reward model, and updating the policy to maximize expected reward while staying close to the reference policy.","title":"Overview"},{"location":"algorithms/ppo/#key-features","text":"Reward Model : Uses neural reward models for fine-grained control Online Learning : Generates and evaluates responses during training Both Backends : Supported by TRL and Unsloth Flexible : Works with custom reward models and functions","title":"Key Features"},{"location":"algorithms/ppo/#how-it-works","text":"PPO alternates between: 1. Rollout Phase : Generate responses from the current policy 2. Evaluation Phase : Score responses with reward model 3. Update Phase : Update policy to maximize reward (with KL constraint)","title":"How It Works"},{"location":"algorithms/ppo/#mathematical-formulation","text":"The PPO objective maximizes: L_PPO = E[min(r_t * A_t, clip(r_t, 1-\u03b5, 1+\u03b5) * A_t)] Where: - r_t : Probability ratio between policy and reference - A_t : Advantage estimate - \u03b5 : Clipping parameter","title":"Mathematical Formulation"},{"location":"algorithms/ppo/#usage","text":"","title":"Usage"},{"location":"algorithms/ppo/#basic-ppo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , # Required num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"Basic PPO Training"},{"location":"algorithms/ppo/#ppo-with-unsloth-backend","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO with Unsloth Backend"},{"location":"algorithms/ppo/#ppo-with-custom-reward-model","text":"# Train custom reward model first from aligntune.rewards.training import RewardModelTrainer reward_trainer = RewardModelTrainer ( base_model_name = \"Qwen/Qwen3-0.6B\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], composite_weights = [ 0.4 , 0.3 , 0.3 ] ) reward_model_path = reward_trainer . train () # Use in PPO training ppo_trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = reward_model_path , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 )","title":"PPO with Custom Reward Model"},{"location":"algorithms/ppo/#ppo-with-reward-functions","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_functions = [ \"helpfulness\" , \"safety\" ], reward_function_weights = [ 0.6 , 0.4 ], num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 )","title":"PPO with Reward Functions"},{"location":"algorithms/ppo/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"algorithms/ppo/#ppo-specific-parameters","text":"Parameter Type Default Description cliprange float 0.2 PPO clipping range kl_coef float 0.1 KL penalty coefficient vf_coef float 0.1 Value function coefficient gamma float 1.0 Discount factor lam float 0.95 GAE lambda parameter num_generations int 8 Number of generations per prompt num_ppo_epochs int 4 Number of PPO update epochs","title":"PPO-Specific Parameters"},{"location":"algorithms/ppo/#reward-model-parameters","text":"Parameter Type Default Description reward_model_name str None HuggingFace reward model ID reward_model_path str None Local reward model path reward_functions list [] List of reward function names reward_function_weights list [] Weights for reward functions","title":"Reward Model Parameters"},{"location":"algorithms/ppo/#dataset-format","text":"PPO requires prompts (not preference pairs): { \"prompt\" : \"What is machine learning?\" , # Responses are generated during training }","title":"Dataset Format"},{"location":"algorithms/ppo/#supported-datasets","text":"Anthropic/hh-rlhf : Can extract prompts OpenAssistant/oasst1 : Conversation prompts Custom datasets with prompt column","title":"Supported Datasets"},{"location":"algorithms/ppo/#best-practices","text":"","title":"Best Practices"},{"location":"algorithms/ppo/#1-reward-model-selection","text":"Use pre-trained models : HuggingFace reward models Train custom models : For domain-specific tasks Use reward functions : For rule-based rewards","title":"1. Reward Model Selection"},{"location":"algorithms/ppo/#2-clipping-range","text":"Conservative (0.1-0.2) : Stable training, slower updates Aggressive (0.2-0.3) : Faster updates, risk of instability","title":"2. Clipping Range"},{"location":"algorithms/ppo/#3-kl-coefficient","text":"Low (0.01-0.1) : More exploration, risk of divergence High (0.1-0.5) : More conservative, stays close to reference","title":"3. KL Coefficient"},{"location":"algorithms/ppo/#4-number-of-generations","text":"Few (4-8) : Faster training, less exploration Many (8-16) : More exploration, slower training","title":"4. Number of Generations"},{"location":"algorithms/ppo/#evaluation","text":"PPO evaluation includes: config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) Note : For Base model we don 't need to pass model_path also if model is not trained using lora no need to pass base_model also","title":"Evaluation"},{"location":"algorithms/ppo/#common-issues","text":"","title":"Common Issues"},{"location":"algorithms/ppo/#reward-model-not-found","text":"Solutions : - Verify reward model name/path - Check HuggingFace authentication - Train custom reward model","title":"Reward Model Not Found"},{"location":"algorithms/ppo/#training-instability","text":"Solutions : - Reduce learning rate - Increase KL coefficient - Reduce clipping range - Use smaller batch size","title":"Training Instability"},{"location":"algorithms/ppo/#low-rewards","text":"Solutions : - Check reward model quality - Verify reward model is appropriate for task - Adjust reward function weights - Increase number of generations","title":"Low Rewards"},{"location":"algorithms/ppo/#comparison-with-other-algorithms","text":"Feature PPO DPO GRPO Reward Model Required Not needed Not needed Training Speed Slow Fast Medium Setup Complexity High Low Medium Reward Control Fine-grained Coarse Medium","title":"Comparison with Other Algorithms"},{"location":"algorithms/ppo/#references","text":"Proximal Policy Optimization Paper TRL PPO Documentation","title":"References"},{"location":"algorithms/ppo/#next-steps","text":"DPO Guide - Learn about Direct Preference Optimization GRPO Guide - Learn about Group Relative Policy Optimization Reward Model Training - Train custom reward models","title":"Next Steps"},{"location":"api-reference/api-reference/","text":"AlignTune API Reference \u00b6 Overview \u00b6 This document provides comprehensive documentation for all AlignTune APIs, including detailed parameter descriptions, configuration options, and usage examples. Core Configuration Classes \u00b6 UnifiedConfig \u00b6 The main configuration class that unifies all training parameters. from aligntune import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , # Required: Training algorithm model = ModelConfig ( ... ), # Required: Model configuration datasets = [ DatasetConfig ( ... )], # Required: Dataset configuration(s) train = TrainingConfig ( ... ), # Optional: Training parameters rewards = [ RewardConfig ( ... )], # Optional: Reward functions tasks = [ ... ], # Optional: Task definitions distributed = DistributedConfig ( ... ), # Optional: Distributed training logging = LoggingConfig ( ... ), # Optional: Logging configuration chat_template = \"auto\" , # Optional: Chat template caching = {}, # Optional: Caching configuration ) AlgorithmType \u00b6 Supported training algorithms. from aligntune import AlgorithmType # Available algorithms AlgorithmType . PPO # Proximal Policy Optimization AlgorithmType . DPO # Direct Preference Optimization AlgorithmType . GRPO # Group Relative Policy Optimization AlgorithmType . GSPO # Group Sequential Policy Optimization Model Configuration \u00b6 ModelConfig \u00b6 from aligntune import ModelConfig , PrecisionType model_config = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , # Required: Model name or path sft_path = None , # Optional: Path to SFT model reward_path = None , # Optional: Path to reward model precision = PrecisionType . BF16 , # Optional: Model precision quantization = {}, # Optional: Quantization config attn_implementation = \"auto\" , # Optional: Attention implementation gradient_checkpointing = True , # Optional: Enable gradient checkpointing max_memory = None , # Optional: Memory mapping use_unsloth = False , # Optional: Enable Unsloth max_seq_length = 2048 # Optional: Maximum sequence length ) ModelConfig Parameters \u00b6 Parameter Type Default Description name_or_path str Required HuggingFace model name or local path sft_path str None Path to pre-trained SFT model reward_path str None Path to reward model for PPO precision PrecisionType BF16 Model precision (BF16, FP16, FP32) quantization Dict {} Quantization configuration attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool True Enable gradient checkpointing max_memory Dict None Memory mapping configuration use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length PrecisionType \u00b6 from aligntune import PrecisionType PrecisionType . BF16 # Brain Float 16 (recommended for modern GPUs) PrecisionType . FP16 # Float 16 (good for older GPUs) PrecisionType . FP32 # Float 32 (CPU training) Dataset Configuration \u00b6 DatasetConfig \u00b6 from aligntune import DatasetConfig dataset_config = DatasetConfig ( name = \"Anthropic/hh-rlhf\" , # Required: Dataset name split = \"train\" , # Optional: Dataset split percent = 100 , # Optional: Percentage to use max_samples = None , # Optional: Maximum samples task_type = \"conversation\" , # Optional: Task type weight = 1.0 , # Optional: Dataset weight column_mapping = {}, # Optional: Column mapping preprocessing = {} # Optional: Preprocessing config ) DatasetConfig Parameters \u00b6 Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use percent int 100 Percentage of dataset to use (1-100) max_samples int None Maximum number of samples task_type str \"conversation\" Type of task weight float 1.0 Weight for multi-dataset training column_mapping Dict {} Map dataset columns to expected names preprocessing Dict {} Preprocessing configuration Training Configuration \u00b6 TrainingConfig \u00b6 from aligntune import TrainingConfig training_config = TrainingConfig ( per_device_batch_size = 1 , # Required: Batch size per device gradient_accumulation_steps = 1 , # Optional: Gradient accumulation max_steps = 1000 , # Optional: Maximum training steps eval_interval = 100 , # Optional: Evaluation interval save_interval = 500 , # Optional: Save interval learning_rate = 5e-5 , # Optional: Learning rate weight_decay = 0.01 , # Optional: Weight decay warmup_steps = 100 , # Optional: Warmup steps max_length = 512 , # Optional: Maximum sequence length max_prompt_length = 256 , # Optional: Maximum prompt length temperature = 0.7 , # Optional: Generation temperature beta = 0.1 , # Optional: DPO beta parameter kl_coef = 0.1 , # Optional: PPO KL coefficient cliprange = 0.2 , # Optional: PPO clip range rollout_batch_size = 1 , # Optional: PPO rollout batch size ppo_epochs = 4 , # Optional: PPO epochs vf_coef = 0.1 , # Optional: PPO value function coefficient ent_coef = 0.0 , # Optional: PPO entropy coefficient target_kl = 0.01 , # Optional: PPO target KL init_kl_coef = 0.2 , # Optional: PPO initial KL coefficient adap_kl_ctrl = True , # Optional: PPO adaptive KL control gamma = 1.0 , # Optional: PPO discount factor lam = 0.95 , # Optional: PPO GAE lambda whiten_rewards = False # Optional: PPO reward whitening ) TrainingConfig Parameters \u00b6 Parameter Type Default Description per_device_batch_size int Required Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int 1000 Maximum training steps eval_interval int 100 Steps between evaluations save_interval int 500 Steps between saves learning_rate float 5e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 100 Warmup steps max_length int 512 Maximum sequence length max_prompt_length int 256 Maximum prompt length temperature float 0.7 Generation temperature beta float 0.1 DPO beta parameter kl_coef float 0.1 PPO KL coefficient cliprange float 0.2 PPO clip range rollout_batch_size int 1 PPO rollout batch size ppo_epochs int 4 PPO epochs per update vf_coef float 0.1 PPO value function coefficient ent_coef float 0.0 PPO entropy coefficient target_kl float 0.01 PPO target KL divergence init_kl_coef float 0.2 PPO initial KL coefficient adap_kl_ctrl bool True PPO adaptive KL control gamma float 1.0 PPO discount factor lam float 0.95 PPO GAE lambda whiten_rewards bool False PPO reward whitening Reward Configuration \u00b6 RewardConfig \u00b6 from aligntune import RewardConfig , RewardType reward_config = RewardConfig ( reward_type = RewardType . SAFETY , # Required: Reward type weight = 1.0 , # Optional: Reward weight params = {}, # Optional: Reward parameters model_name = None , # Optional: Model for reward device = \"auto\" , # Optional: Device for reward cache_dir = None # Optional: Cache directory ) RewardConfig Parameters \u00b6 Parameter Type Default Description reward_type RewardType Required Type of reward function weight float 1.0 Weight of this reward params Dict {} Parameters for reward function model_name str None Model name for model-based rewards device str \"auto\" Device for reward computation cache_dir str None Cache directory for models Distributed Configuration \u00b6 DistributedConfig \u00b6 from aligntune import DistributedConfig , BackendType distributed_config = DistributedConfig ( backend = BackendType . SINGLE , # Required: Distributed backend num_processes = 1 , # Optional: Number of processes master_addr = \"localhost\" , # Optional: Master address master_port = 29500 , # Optional: Master port seed = 42 , # Optional: Random seed deepspeed_config = {}, # Optional: DeepSpeed config fsdp_config = {} # Optional: FSDP config ) DistributedConfig Parameters \u00b6 Parameter Type Default Description backend BackendType Required Distributed backend type num_processes int 1 Number of processes master_addr str \"localhost\" Master node address master_port int 29500 Master node port seed int 42 Random seed deepspeed_config Dict {} DeepSpeed configuration fsdp_config Dict {} FSDP configuration BackendType \u00b6 from aligntune import BackendType BackendType . SINGLE # Single GPU/CPU training BackendType . DDP # Distributed Data Parallel BackendType . FSDP # Fully Sharded Data Parallel BackendType . DEEPSPEED # DeepSpeed ZeRO Logging Configuration \u00b6 LoggingConfig \u00b6 from aligntune import LoggingConfig logging_config = LoggingConfig ( output_dir = \"./output\" , # Required: Output directory run_name = \"my_experiment\" , # Optional: Run name loggers = [ \"tensorboard\" ], # Optional: Logging backends level = \"INFO\" , # Optional: Log level save_steps = 500 , # Optional: Save interval eval_steps = 100 , # Optional: Eval interval report_to = [ \"tensorboard\" ], # Optional: Reporting backends logging_dir = \"./logs\" , # Optional: Logging directory disable_tqdm = False , # Optional: Disable progress bars push_to_hub = False , # Optional: Push to HuggingFace Hub hub_model_id = None , # Optional: Hub model ID hub_token = None # Optional: Hub token ) LoggingConfig Parameters \u00b6 Parameter Type Default Description output_dir str Required Output directory for checkpoints run_name str None Name for this training run loggers List[str] [\"tensorboard\"] Logging backends level str \"INFO\" Logging level save_steps int 500 Steps between saves eval_steps int 100 Steps between evaluations report_to List[str] [\"tensorboard\"] Reporting backends logging_dir str \"./logs\" Logging directory disable_tqdm bool False Disable progress bars push_to_hub bool False Push to HuggingFace Hub hub_model_id str None Hub model ID hub_token str None Hub authentication token Backend Factory \u00b6 BackendFactory \u00b6 from aligntune import BackendFactory , BackendConfig , TrainingType , BackendType , RLAlgorithm # Create trainer with specific backend backend_config = BackendConfig ( training_type = TrainingType . RL , backend = BackendType . TRL , algorithm = RLAlgorithm . DPO ) trainer = BackendFactory . create_trainer ( config , backend_config ) Convenience Functions \u00b6 from aligntune import create_sft_trainer , create_rl_trainer # Create SFT trainer sft_trainer = create_sft_trainer ( config , backend = \"trl\" ) # Create RL trainer rl_trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) Configuration Loading \u00b6 ConfigLoader \u00b6 from aligntune import ConfigLoader # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Load from dictionary config = ConfigLoader . load_from_dict ( config_dict ) # Validate configuration ConfigLoader . validate_config ( config ) # Save resolved configuration ConfigLoader . save_resolved_config ( config , output_dir ) Complete Example \u00b6 from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , RewardConfig , RewardType , DistributedConfig , BackendType , LoggingConfig , create_rl_trainer ) # Create complete configuration config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , precision = PrecisionType . BF16 , use_unsloth = True , max_seq_length = 2048 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , split = \"train\" , percent = 10 , max_samples = 1000 ) ], train = TrainingConfig ( per_device_batch_size = 2 , gradient_accumulation_steps = 4 , max_steps = 1000 , learning_rate = 5e-5 , beta = 0.1 , temperature = 0.7 ), rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ) ], distributed = DistributedConfig ( backend = BackendType . SINGLE , seed = 42 ), logging = LoggingConfig ( output_dir = \"./output/dpo_experiment\" , run_name = \"dpo_hh_rlhf\" , loggers = [ \"tensorboard\" , \"wandb\" ] ) ) # Create and train trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) trainer . train () For more examples and use cases, see the Examples Documentation and Reward Functions Reference .","title":"AlignTune API Reference"},{"location":"api-reference/api-reference/#aligntune-api-reference","text":"","title":"AlignTune API Reference"},{"location":"api-reference/api-reference/#overview","text":"This document provides comprehensive documentation for all AlignTune APIs, including detailed parameter descriptions, configuration options, and usage examples.","title":"Overview"},{"location":"api-reference/api-reference/#core-configuration-classes","text":"","title":"Core Configuration Classes"},{"location":"api-reference/api-reference/#unifiedconfig","text":"The main configuration class that unifies all training parameters. from aligntune import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , # Required: Training algorithm model = ModelConfig ( ... ), # Required: Model configuration datasets = [ DatasetConfig ( ... )], # Required: Dataset configuration(s) train = TrainingConfig ( ... ), # Optional: Training parameters rewards = [ RewardConfig ( ... )], # Optional: Reward functions tasks = [ ... ], # Optional: Task definitions distributed = DistributedConfig ( ... ), # Optional: Distributed training logging = LoggingConfig ( ... ), # Optional: Logging configuration chat_template = \"auto\" , # Optional: Chat template caching = {}, # Optional: Caching configuration )","title":"UnifiedConfig"},{"location":"api-reference/api-reference/#algorithmtype","text":"Supported training algorithms. from aligntune import AlgorithmType # Available algorithms AlgorithmType . PPO # Proximal Policy Optimization AlgorithmType . DPO # Direct Preference Optimization AlgorithmType . GRPO # Group Relative Policy Optimization AlgorithmType . GSPO # Group Sequential Policy Optimization","title":"AlgorithmType"},{"location":"api-reference/api-reference/#model-configuration","text":"","title":"Model Configuration"},{"location":"api-reference/api-reference/#modelconfig","text":"from aligntune import ModelConfig , PrecisionType model_config = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , # Required: Model name or path sft_path = None , # Optional: Path to SFT model reward_path = None , # Optional: Path to reward model precision = PrecisionType . BF16 , # Optional: Model precision quantization = {}, # Optional: Quantization config attn_implementation = \"auto\" , # Optional: Attention implementation gradient_checkpointing = True , # Optional: Enable gradient checkpointing max_memory = None , # Optional: Memory mapping use_unsloth = False , # Optional: Enable Unsloth max_seq_length = 2048 # Optional: Maximum sequence length )","title":"ModelConfig"},{"location":"api-reference/api-reference/#modelconfig-parameters","text":"Parameter Type Default Description name_or_path str Required HuggingFace model name or local path sft_path str None Path to pre-trained SFT model reward_path str None Path to reward model for PPO precision PrecisionType BF16 Model precision (BF16, FP16, FP32) quantization Dict {} Quantization configuration attn_implementation str \"auto\" Attention implementation gradient_checkpointing bool True Enable gradient checkpointing max_memory Dict None Memory mapping configuration use_unsloth bool False Enable Unsloth acceleration max_seq_length int 2048 Maximum sequence length","title":"ModelConfig Parameters"},{"location":"api-reference/api-reference/#precisiontype","text":"from aligntune import PrecisionType PrecisionType . BF16 # Brain Float 16 (recommended for modern GPUs) PrecisionType . FP16 # Float 16 (good for older GPUs) PrecisionType . FP32 # Float 32 (CPU training)","title":"PrecisionType"},{"location":"api-reference/api-reference/#dataset-configuration","text":"","title":"Dataset Configuration"},{"location":"api-reference/api-reference/#datasetconfig","text":"from aligntune import DatasetConfig dataset_config = DatasetConfig ( name = \"Anthropic/hh-rlhf\" , # Required: Dataset name split = \"train\" , # Optional: Dataset split percent = 100 , # Optional: Percentage to use max_samples = None , # Optional: Maximum samples task_type = \"conversation\" , # Optional: Task type weight = 1.0 , # Optional: Dataset weight column_mapping = {}, # Optional: Column mapping preprocessing = {} # Optional: Preprocessing config )","title":"DatasetConfig"},{"location":"api-reference/api-reference/#datasetconfig-parameters","text":"Parameter Type Default Description name str Required HuggingFace dataset name or local path split str \"train\" Dataset split to use percent int 100 Percentage of dataset to use (1-100) max_samples int None Maximum number of samples task_type str \"conversation\" Type of task weight float 1.0 Weight for multi-dataset training column_mapping Dict {} Map dataset columns to expected names preprocessing Dict {} Preprocessing configuration","title":"DatasetConfig Parameters"},{"location":"api-reference/api-reference/#training-configuration","text":"","title":"Training Configuration"},{"location":"api-reference/api-reference/#trainingconfig","text":"from aligntune import TrainingConfig training_config = TrainingConfig ( per_device_batch_size = 1 , # Required: Batch size per device gradient_accumulation_steps = 1 , # Optional: Gradient accumulation max_steps = 1000 , # Optional: Maximum training steps eval_interval = 100 , # Optional: Evaluation interval save_interval = 500 , # Optional: Save interval learning_rate = 5e-5 , # Optional: Learning rate weight_decay = 0.01 , # Optional: Weight decay warmup_steps = 100 , # Optional: Warmup steps max_length = 512 , # Optional: Maximum sequence length max_prompt_length = 256 , # Optional: Maximum prompt length temperature = 0.7 , # Optional: Generation temperature beta = 0.1 , # Optional: DPO beta parameter kl_coef = 0.1 , # Optional: PPO KL coefficient cliprange = 0.2 , # Optional: PPO clip range rollout_batch_size = 1 , # Optional: PPO rollout batch size ppo_epochs = 4 , # Optional: PPO epochs vf_coef = 0.1 , # Optional: PPO value function coefficient ent_coef = 0.0 , # Optional: PPO entropy coefficient target_kl = 0.01 , # Optional: PPO target KL init_kl_coef = 0.2 , # Optional: PPO initial KL coefficient adap_kl_ctrl = True , # Optional: PPO adaptive KL control gamma = 1.0 , # Optional: PPO discount factor lam = 0.95 , # Optional: PPO GAE lambda whiten_rewards = False # Optional: PPO reward whitening )","title":"TrainingConfig"},{"location":"api-reference/api-reference/#trainingconfig-parameters","text":"Parameter Type Default Description per_device_batch_size int Required Batch size per device gradient_accumulation_steps int 1 Gradient accumulation steps max_steps int 1000 Maximum training steps eval_interval int 100 Steps between evaluations save_interval int 500 Steps between saves learning_rate float 5e-5 Learning rate weight_decay float 0.01 Weight decay warmup_steps int 100 Warmup steps max_length int 512 Maximum sequence length max_prompt_length int 256 Maximum prompt length temperature float 0.7 Generation temperature beta float 0.1 DPO beta parameter kl_coef float 0.1 PPO KL coefficient cliprange float 0.2 PPO clip range rollout_batch_size int 1 PPO rollout batch size ppo_epochs int 4 PPO epochs per update vf_coef float 0.1 PPO value function coefficient ent_coef float 0.0 PPO entropy coefficient target_kl float 0.01 PPO target KL divergence init_kl_coef float 0.2 PPO initial KL coefficient adap_kl_ctrl bool True PPO adaptive KL control gamma float 1.0 PPO discount factor lam float 0.95 PPO GAE lambda whiten_rewards bool False PPO reward whitening","title":"TrainingConfig Parameters"},{"location":"api-reference/api-reference/#reward-configuration","text":"","title":"Reward Configuration"},{"location":"api-reference/api-reference/#rewardconfig","text":"from aligntune import RewardConfig , RewardType reward_config = RewardConfig ( reward_type = RewardType . SAFETY , # Required: Reward type weight = 1.0 , # Optional: Reward weight params = {}, # Optional: Reward parameters model_name = None , # Optional: Model for reward device = \"auto\" , # Optional: Device for reward cache_dir = None # Optional: Cache directory )","title":"RewardConfig"},{"location":"api-reference/api-reference/#rewardconfig-parameters","text":"Parameter Type Default Description reward_type RewardType Required Type of reward function weight float 1.0 Weight of this reward params Dict {} Parameters for reward function model_name str None Model name for model-based rewards device str \"auto\" Device for reward computation cache_dir str None Cache directory for models","title":"RewardConfig Parameters"},{"location":"api-reference/api-reference/#distributed-configuration","text":"","title":"Distributed Configuration"},{"location":"api-reference/api-reference/#distributedconfig","text":"from aligntune import DistributedConfig , BackendType distributed_config = DistributedConfig ( backend = BackendType . SINGLE , # Required: Distributed backend num_processes = 1 , # Optional: Number of processes master_addr = \"localhost\" , # Optional: Master address master_port = 29500 , # Optional: Master port seed = 42 , # Optional: Random seed deepspeed_config = {}, # Optional: DeepSpeed config fsdp_config = {} # Optional: FSDP config )","title":"DistributedConfig"},{"location":"api-reference/api-reference/#distributedconfig-parameters","text":"Parameter Type Default Description backend BackendType Required Distributed backend type num_processes int 1 Number of processes master_addr str \"localhost\" Master node address master_port int 29500 Master node port seed int 42 Random seed deepspeed_config Dict {} DeepSpeed configuration fsdp_config Dict {} FSDP configuration","title":"DistributedConfig Parameters"},{"location":"api-reference/api-reference/#backendtype","text":"from aligntune import BackendType BackendType . SINGLE # Single GPU/CPU training BackendType . DDP # Distributed Data Parallel BackendType . FSDP # Fully Sharded Data Parallel BackendType . DEEPSPEED # DeepSpeed ZeRO","title":"BackendType"},{"location":"api-reference/api-reference/#logging-configuration","text":"","title":"Logging Configuration"},{"location":"api-reference/api-reference/#loggingconfig","text":"from aligntune import LoggingConfig logging_config = LoggingConfig ( output_dir = \"./output\" , # Required: Output directory run_name = \"my_experiment\" , # Optional: Run name loggers = [ \"tensorboard\" ], # Optional: Logging backends level = \"INFO\" , # Optional: Log level save_steps = 500 , # Optional: Save interval eval_steps = 100 , # Optional: Eval interval report_to = [ \"tensorboard\" ], # Optional: Reporting backends logging_dir = \"./logs\" , # Optional: Logging directory disable_tqdm = False , # Optional: Disable progress bars push_to_hub = False , # Optional: Push to HuggingFace Hub hub_model_id = None , # Optional: Hub model ID hub_token = None # Optional: Hub token )","title":"LoggingConfig"},{"location":"api-reference/api-reference/#loggingconfig-parameters","text":"Parameter Type Default Description output_dir str Required Output directory for checkpoints run_name str None Name for this training run loggers List[str] [\"tensorboard\"] Logging backends level str \"INFO\" Logging level save_steps int 500 Steps between saves eval_steps int 100 Steps between evaluations report_to List[str] [\"tensorboard\"] Reporting backends logging_dir str \"./logs\" Logging directory disable_tqdm bool False Disable progress bars push_to_hub bool False Push to HuggingFace Hub hub_model_id str None Hub model ID hub_token str None Hub authentication token","title":"LoggingConfig Parameters"},{"location":"api-reference/api-reference/#backend-factory","text":"","title":"Backend Factory"},{"location":"api-reference/api-reference/#backendfactory","text":"from aligntune import BackendFactory , BackendConfig , TrainingType , BackendType , RLAlgorithm # Create trainer with specific backend backend_config = BackendConfig ( training_type = TrainingType . RL , backend = BackendType . TRL , algorithm = RLAlgorithm . DPO ) trainer = BackendFactory . create_trainer ( config , backend_config )","title":"BackendFactory"},{"location":"api-reference/api-reference/#convenience-functions","text":"from aligntune import create_sft_trainer , create_rl_trainer # Create SFT trainer sft_trainer = create_sft_trainer ( config , backend = \"trl\" ) # Create RL trainer rl_trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" )","title":"Convenience Functions"},{"location":"api-reference/api-reference/#configuration-loading","text":"","title":"Configuration Loading"},{"location":"api-reference/api-reference/#configloader","text":"from aligntune import ConfigLoader # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Load from dictionary config = ConfigLoader . load_from_dict ( config_dict ) # Validate configuration ConfigLoader . validate_config ( config ) # Save resolved configuration ConfigLoader . save_resolved_config ( config , output_dir )","title":"ConfigLoader"},{"location":"api-reference/api-reference/#complete-example","text":"from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , RewardConfig , RewardType , DistributedConfig , BackendType , LoggingConfig , create_rl_trainer ) # Create complete configuration config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , precision = PrecisionType . BF16 , use_unsloth = True , max_seq_length = 2048 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , split = \"train\" , percent = 10 , max_samples = 1000 ) ], train = TrainingConfig ( per_device_batch_size = 2 , gradient_accumulation_steps = 4 , max_steps = 1000 , learning_rate = 5e-5 , beta = 0.1 , temperature = 0.7 ), rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ) ], distributed = DistributedConfig ( backend = BackendType . SINGLE , seed = 42 ), logging = LoggingConfig ( output_dir = \"./output/dpo_experiment\" , run_name = \"dpo_hh_rlhf\" , loggers = [ \"tensorboard\" , \"wandb\" ] ) ) # Create and train trainer = create_rl_trainer ( config , algorithm = \"dpo\" , backend = \"unsloth\" ) trainer . train () For more examples and use cases, see the Examples Documentation and Reward Functions Reference .","title":"Complete Example"},{"location":"api-reference/backend-factory/","text":"Backend Factory API \u00b6 The Backend Factory is the main entry point for creating trainers in AlignTune. Overview \u00b6 The BackendFactory provides functions to create SFT and RL trainers with automatic backend selection and fallback handling. BackendFactory Class \u00b6 Complete API reference for the BackendFactory class. Factory for creating training backends. Source code in src/aligntune/core/backend_factory.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 class BackendFactory : \"\"\"Factory for creating training backends.\"\"\" # Registry of available backends _backends : Dict [ tuple , Type [ TrainerBase ]] = {} @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) @classmethod def _select_best_backend ( cls , backend_config : BackendConfig ) -> BackendConfig : \"\"\"Select the best available backend with fallback.\"\"\" # Check if requested backend is available if cls . _is_backend_available ( backend_config ): return backend_config # Try fallback backends if backend_config . fallback_enabled : fallback_order = cls . _get_fallback_order ( backend_config ) for fallback_backend in fallback_order : if cls . _is_backend_available ( fallback_backend ): logger . warning ( f \"Requested backend { backend_config . backend } not available, \" f \"falling back to { fallback_backend . backend } \" ) return fallback_backend raise RuntimeError ( f \"No available backend for { backend_config } \" ) @classmethod def _is_backend_available ( cls , backend_config : BackendConfig ) -> bool : \"\"\"Check if a backend is available.\"\"\" key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) trainer_class = cls . _backends . get ( key ) if trainer_class is None : return False # Check if the trainer class can be instantiated try : return trainer_class . is_available () except Exception : return False @classmethod def _get_fallback_order ( cls , backend_config : BackendConfig ) -> list : \"\"\"Get fallback order for backend selection.\"\"\" if backend_config . training_type == TrainingType . SFT : # SFT fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . SFT , BackendType . UNSLOTH ), BackendConfig ( TrainingType . SFT , BackendType . TRL ), ] else : # RL # RL fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . RL , BackendType . UNSLOTH , backend_config . algorithm ), BackendConfig ( TrainingType . RL , BackendType . TRL , backend_config . algorithm ), ] # Remove the original backend from fallbacks fallbacks = [ fb for fb in fallbacks if fb . backend != backend_config . backend ] return fallbacks @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config ) create_trainer ( config , backend_config ) classmethod \u00b6 Create a trainer instance based on backend configuration. Source code in src/aligntune/core/backend_factory.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config ) get_backend_capabilities ( backend_type ) \u00b6 Get capabilities for a specific backend type. Source code in src/aligntune/core/backend_factory.py 426 427 428 429 430 431 432 433 434 435 def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities get_recommended_backend ( training_type , algorithm = None ) classmethod \u00b6 Get recommended backend for given training type and algorithm. Source code in src/aligntune/core/backend_factory.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) is_backend_available ( backend_type ) \u00b6 Check if a specific backend type is available. Source code in src/aligntune/core/backend_factory.py 414 415 416 417 418 419 420 421 422 423 424 def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False list_available_backends () classmethod \u00b6 List all available backends. Source code in src/aligntune/core/backend_factory.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available register_backend ( training_type , backend , algorithm = None , trainer_class = None ) classmethod \u00b6 Register a backend trainer class. Source code in src/aligntune/core/backend_factory.py 325 326 327 328 329 330 331 332 333 334 335 336 @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) options: show_source: true heading_level: 3 Factory Functions \u00b6 create_sft_trainer() \u00b6 Create a Supervised Fine-Tuning trainer. Create SFT trainer with specified backend and parameters. Source code in src/aligntune/core/backend_factory.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def create_sft_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , system_prompt : Optional [ str ] = None , # config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create SFT trainer with specified backend and parameters.\"\"\" if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"sft\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) # Create configuration # Build model config kwargs, only including precision if explicitly provided model_config_kwargs = { \"name_or_path\" : model_name , \"max_seq_length\" : max_seq_length , \"quantization\" : kwargs . get ( 'quantization' , {}), \"use_unsloth\" : kwargs . get ( 'use_unsloth' , False ), \"peft_enabled\" : kwargs . get ( 'use_peft' , kwargs . get ( 'peft_enabled' , False )), \"lora_rank\" : kwargs . get ( 'lora_r' , 16 ), \"lora_alpha\" : kwargs . get ( 'lora_alpha' , 32 ), \"lora_dropout\" : kwargs . get ( 'lora_dropout' , 0.1 ), \"target_modules\" : kwargs . get ( 'lora_target_modules' , None ), \"bias\" : kwargs . get ( 'lora_bias' , 'none' ), \"attn_implementation\" : kwargs . get ( 'attn_implementation' , 'auto' ), \"max_memory\" : kwargs . get ( 'max_memory' ), \"use_gradient_checkpointing\" : kwargs . get ( 'use_gradient_checkpointing' , True ), \"num_labels\" : kwargs . get ( 'num_labels' ), # For classification tasks \"model_init_kwargs\" : kwargs . get ( 'model_init_kwargs' , {}), \"device_map\" : kwargs . get ( 'device_map' , 'auto' ), \"trust_remote_code\" : kwargs . get ( 'trust_remote_code' , False ), } # Only add precision if explicitly provided (otherwise use default from ModelConfig) if 'precision' in kwargs and kwargs . get ( 'precision' ) is not None : # Validate precision value precision_value = kwargs . get ( 'precision' ) if isinstance ( precision_value , str ): # Convert string to PrecisionType enum from .sft.config import PrecisionType try : precision_value = PrecisionType ( precision_value . lower ()) except ValueError : logger . warning ( f \"Invalid precision ' { precision_value } ', using default\" ) precision_value = None if precision_value is not None : model_config_kwargs [ 'precision' ] = precision_value config = SFTConfig ( model = SFTModelConfig ( ** model_config_kwargs ), dataset = SFTDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), subset = kwargs . get ( 'subset' ), # Support dataset config/subset (e.g., for financial_phrasebank) config = kwargs . get ( 'config' ), # Alternative name for subset max_samples = max_samples , percent = kwargs . get ( 'percent' ), column_mapping = kwargs . get ( 'column_mapping' , {}), task_type = kwargs . get ( 'task_type' , 'supervised_fine_tuning' ), system_prompt = system_prompt , # dataset_kwargs=kwargs.get('dataset_kwargs', {}), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), dataset_text_field = kwargs . get ( 'dataset_text_field' , 'text' ), text_column = kwargs . get ( 'text_column' , kwargs . get ( 'dataset_text_field' , 'text' )), # For classification tasks label_column = kwargs . get ( 'label_column' , 'label' ), # For classification tasks # eos_token=kwargs.get('eos_token'), pad_token = kwargs . get ( 'pad_token' ), chat_template = kwargs . get ( 'chat_template' ), preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), ), train = SFTTrainingConfig ( epochs = num_epochs , max_steps = kwargs . get ( 'max_steps' ), per_device_batch_size = batch_size , learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.1 ), weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_steps' , kwargs . get ( 'save_interval' , 500 )), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), fp16 = kwargs . get ( 'fp16' , False ), bf16 = kwargs . get ( 'bf16' , False ), dataloader_num_workers = kwargs . get ( 'dataloader_num_workers' , 0 ), remove_unused_columns = kwargs . get ( 'remove_unused_columns' , False ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), group_by_length = kwargs . get ( 'group_by_length' , True ), dataloader_drop_last = kwargs . get ( 'dataloader_drop_last' , False ), eval_accumulation_steps = kwargs . get ( 'eval_accumulation_steps' ), label_smoothing_factor = kwargs . get ( 'label_smoothing_factor' , 0.0 ), early_stopping_patience = kwargs . get ( 'early_stopping_patience' ), early_stopping_threshold = kwargs . get ( 'early_stopping_threshold' , 0.0 ), load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , True ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' , 'eval_loss' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), use_trl = kwargs . get ( 'use_trl' , False ), dataset_num_proc = kwargs . get ( 'train_dataset_num_proc' ), dataset_kwargs = kwargs . get ( 'train_dataset_kwargs' , {}), packing = kwargs . get ( 'packing' , False ), packing_strategy = kwargs . get ( 'packing_strategy' , 'bfd' ), eval_packing = kwargs . get ( 'eval_packing' ), padding_free = kwargs . get ( 'padding_free' , False ), pad_to_multiple_of = kwargs . get ( 'pad_to_multiple_of' ), completion_only_loss = kwargs . get ( 'completion_only_loss' ), assistant_only_loss = kwargs . get ( 'assistant_only_loss' , False ), loss_type = kwargs . get ( 'loss_type' , 'nll' ), activation_offloading = kwargs . get ( 'activation_offloading' , False ), use_flash_attention_2 = kwargs . get ( 'use_flash_attention_2' ), gradient_checkpointing = kwargs . get ( 'gradient_checkpointing' , False ), gradient_checkpointing_kwargs = kwargs . get ( 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }), extra_params = kwargs , ), logging = SFTLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), log_level = kwargs . get ( 'log_level' , 'INFO' ), log_interval = kwargs . get ( 'logging_steps' , kwargs . get ( 'log_interval' , 10 )), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), eval_strategy = kwargs . get ( 'eval_strategy' , 'steps' ), report_to = kwargs . get ( 'report_to' , \"none\" ), ), evaluation = SFTEvaluationConfig ( compute_perplexity = kwargs . get ( 'compute_perplexity' , True ), compute_rouge = kwargs . get ( 'compute_rouge' , True ), compute_bleu = kwargs . get ( 'compute_bleu' , True ), compute_meteor = kwargs . get ( 'compute_meteor' , False ), compute_bertscore = kwargs . get ( 'compute_bertscore' , False ), compute_semantic_similarity = kwargs . get ( 'compute_semantic_similarity' , False ), compute_codebleu = kwargs . get ( 'compute_codebleu' , False ), max_samples_for_quality_metrics = kwargs . get ( 'max_samples_for_quality_metrics' , 50 ), bertscore_model = kwargs . get ( 'bertscore_model' , 'microsoft/deberta-xlarge-mnli' ), semantic_similarity_model = kwargs . get ( 'semantic_similarity_model' , 'sentence-transformers/all-MiniLM-L6-v2' ) ) ) # Create backend config if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . SFT ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . SFT , backend_type ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 ) trainer . train () create_rl_trainer() \u00b6 Create a Reinforcement Learning trainer. Create RL trainer with specified algorithm and backend. Source code in src/aligntune/core/backend_factory.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def create_rl_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , algorithm : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , max_steps : Optional [ int ] = 100 , # Add max_steps parameter batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , reward_value_model : Optional [ str ] = None , reward_model_name : Optional [ str ] = None , # NEW reward_model_path : Optional [ str ] = None , # NEW: Local reward model path train_custom_reward_model : bool = False , # NEW: Train custom reward model reward_training_texts : Optional [ List [ str ]] = None , # NEW: Training texts for custom model reward_functions : Optional [ List [ str ]] = None , # NEW: Reward functions for custom model reward_function_weights : Optional [ List [ float ]] = None , # NEW: Weights for reward functions reward_training_base_model : Optional [ str ] = None , # NEW: Base model for custom training reward_training_output_dir : Optional [ str ] = None , # NEW: Output dir for custom training reward_value_loading_type : Optional [ str ] = None , reward_model_quantization : Optional [ Dict ] = None , value_model_quantization : Optional [ Dict ] = None , reward_training : Optional [ Dict [ str , Any ]] = None , # NEW: Flexible reward training config reward_device : str = \"auto\" , sample_logging : Optional [ Dict [ str , Any ]] = None , # NEW: Counterfactual GRPO specific parameters boost_factor : float = 2.0 , min_weight : float = 0.5 , max_spans : int = 10 , answer_weight : float = 1.5 , method_name : str = \"counterfactual\" , random_importance : bool = False , invert_importance : bool = False , enable_gradient_conservation : bool = True , weight_debug : bool = False , system_prompt : Optional [ str ] = None , # # NEW: GBMPO-specific parameters gbmpo_divergence_type : Optional [ str ] = None , config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create RL trainer with specified algorithm and backend.\"\"\" # NEW: Load config if provided# Load and parse config if provided if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"rl\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters from merged config model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) algorithm = algorithm or kwargs . pop ( 'algorithm' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) if algorithm is None : raise ValueError ( \"algorithm must be provided either as argument or in config\" ) reward_device = kwargs . pop ( 'reward_device' , reward_device or \"auto\" ) reward_device = reward_device or \"auto\" sample_logging_dict : Dict [ str , Any ] = {} base_sample_logging = sample_logging or kwargs . get ( 'sample_logging_config' ) if base_sample_logging : sample_logging_dict . update ( base_sample_logging ) inline_sample_logging = { \"enabled\" : kwargs . get ( 'enable_sample_logging' ), \"prompts\" : kwargs . get ( 'sample_logging_prompts' ), \"interval_steps\" : kwargs . get ( 'sample_logging_interval_steps' ), \"percent_of_max_steps\" : kwargs . get ( 'sample_logging_percent_of_max_steps' , kwargs . get ( 'sample_logging_percent' )), \"max_new_tokens\" : kwargs . get ( 'sample_logging_max_new_tokens' ), \"temperature\" : kwargs . get ( 'sample_logging_temperature' ), \"top_p\" : kwargs . get ( 'sample_logging_top_p' ), \"num_samples\" : kwargs . get ( 'sample_logging_num_samples' ), } for key , value in inline_sample_logging . items (): if value is not None : sample_logging_dict [ key ] = value cleaned_sample_logging = { k : v for k , v in sample_logging_dict . items () if v is not None } if cleaned_sample_logging : sample_logging_config = SampleLoggingConfig ( ** cleaned_sample_logging ) else : sample_logging_config = SampleLoggingConfig () needs_neural_reward = not any ([ reward_model_path , train_custom_reward_model , reward_functions ]) if reward_model_name is None and needs_neural_reward : reward_model_name = model_name # ============================================ # PPO MODEL CONSISTENCY WARNING # ============================================ if algorithm . lower () == \"ppo\" : print ( \"\u26a0\ufe0f NOTE: For optimal PPO performance, ensure policy_model, reward_model, and value_model are from the same model family\" ) # STRICT VALIDATION: Validate reward model configuration reward_model_source = None source_count = sum ([ reward_model_name is not None , reward_model_path is not None , train_custom_reward_model ]) if source_count > 1 : raise ValueError ( f \"Specify exactly ONE reward source. Found { source_count } : \" f \"reward_model_name= { reward_model_name } , \" f \"reward_model_path= { reward_model_path } , \" f \"train_custom_reward_model= { train_custom_reward_model } \" ) if train_custom_reward_model : # Validate ALL required fields for custom training if not reward_training_texts : raise ValueError ( \"reward_training_texts required when train_custom_reward_model=True\" ) if not reward_functions : raise ValueError ( \"reward_functions required when train_custom_reward_model=True\" ) if not reward_training_base_model : raise ValueError ( \"reward_training_base_model required when train_custom_reward_model=True\" ) if not reward_training_output_dir : raise ValueError ( \"reward_training_output_dir required when train_custom_reward_model=True\" ) # Import required classes from .rl.config import RewardModelTrainingConfig , RewardModelSourceConfig training_config = RewardModelTrainingConfig ( base_model_name = reward_training_base_model , training_texts = reward_training_texts , reward_functions = reward_functions , output_dir = reward_training_output_dir , # Use kwargs for optional training params num_epochs = kwargs . get ( 'reward_training_epochs' , 3 ), learning_rate = kwargs . get ( 'reward_training_lr' , 1e-5 ), batch_size = kwargs . get ( 'reward_training_batch_size' , 8 ), reward_weights = reward_function_weights ) reward_model_source = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) elif reward_model_name : from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = reward_model_name ) elif reward_model_path : # Validate path exists if not Path ( reward_model_path ) . exists (): raise FileNotFoundError ( f \"reward_model_path does not exist: { reward_model_path } \" ) from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = reward_model_path ) # Create reward training config if provided reward_training_config = None if reward_training : from .rl.config import RewardModelTrainingConfig reward_training_config = RewardModelTrainingConfig ( ** reward_training ) # Construct rewards config if reward_functions provided but not in kwargs # rewards_config = kwargs.get('rewards', []) rewards_config = kwargs . get ( 'rewards' ) or [] if not rewards_config and reward_functions and not train_custom_reward_model : weights = reward_function_weights or [ 1.0 ] * len ( reward_functions ) if len ( weights ) != len ( reward_functions ): logger . warning ( \"reward_function_weights length mismatch, using default 1.0\" ) weights = [ 1.0 ] * len ( reward_functions ) for func_name , weight in zip ( reward_functions , weights ): rewards_config . append ({ \"type\" : func_name , \"weight\" : weight , \"params\" : {} }) logger . info ( f \"Constructed rewards config from function list: { len ( rewards_config ) } functions\" ) # FIXED: Added required 'algo' parameter and used 'train' not 'training' if algorithm . lower () == \"counterfact_grpo\" : # Add counterfactual-specific params to kwargs for config creation kwargs . update ({ 'boost_factor' : boost_factor , 'min_weight' : min_weight , 'max_spans' : max_spans , 'answer_weight' : answer_weight , 'method_name' : method_name , 'random_importance' : random_importance , 'invert_importance' : invert_importance , 'enable_gradient_conservation' : enable_gradient_conservation , 'weight_debug' : weight_debug , }) # NEW: Handle GBMPO algorithms - set divergence_type based on algorithm # NEW: Handle GBMPO algorithm variants original_algorithm = algorithm . lower () # Map GBMPO variants to base algorithm and extract divergence type if original_algorithm . startswith ( \"gbmpo\" ): # Extract divergence type from algorithm name if not explicitly provided if gbmpo_divergence_type is None : if original_algorithm == \"gbmpo\" : # Default to l2kl if no variant specified gbmpo_divergence_type = \"l2kl\" else : # Extract from algorithm name: gbmpo_l2kl -> l2kl suffix = original_algorithm . replace ( \"gbmpo_\" , \"\" ) divergence_map = { \"l2\" : \"l2\" , \"l2kl\" : \"l2kl\" , \"probl2\" : \"prob_l2\" , \"probl2kl\" : \"prob_l2kl\" } gbmpo_divergence_type = divergence_map . get ( suffix , \"l2kl\" ) # Normalize to base \"gbmpo\" algorithm algorithm = \"gbmpo\" kwargs [ 'gbmpo_divergence_type' ] = gbmpo_divergence_type logger . info ( f \"GBMPO variant detected: { original_algorithm } -> divergence_type= { gbmpo_divergence_type } \" ) config = UnifiedConfig ( algo = algorithm . lower (), model = RLModelConfig ( name_or_path = model_name , backend = backend if backend != \"auto\" else \"trl\" , max_seq_length = max_seq_length , quantization = kwargs . get ( 'quantization' , {}), gradient_checkpointing = kwargs . get ( 'use_gradient_checkpointing' , False ), precision = kwargs . get ( 'precision' , 'auto' ), reward_value_model = reward_value_model or kwargs . get ( 'reward_value_model' , None ), reward_model_name = reward_model_name , reward_model_source = reward_model_source , reward_value_loading_type = reward_value_loading_type , reward_model_quantization = reward_model_quantization or {}, value_model_quantization = value_model_quantization or {}, use_peft = kwargs . get ( 'use_peft' , True ), lora_r = kwargs . get ( 'lora_r' , 16 ), lora_alpha = kwargs . get ( 'lora_alpha' , 32 ), lora_dropout = kwargs . get ( 'lora_dropout' , 0.05 ), lora_target_modules = kwargs . get ( 'lora_target_modules' , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]), model_init_kwargs = kwargs . get ( 'model_init_kwargs' , {}), ref_model_init_kwargs = kwargs . get ( 'ref_model_init_kwargs' , {}), model_adapter_name = kwargs . get ( 'model_adapter_name' ), ref_adapter_name = kwargs . get ( 'ref_adapter_name' ), force_use_ref_model = kwargs . get ( 'force_use_ref_model' , False ), disable_dropout = kwargs . get ( 'disable_dropout' , True ), use_logits_to_keep = kwargs . get ( 'use_logits_to_keep' , False ), reward_device = reward_device , device_map = kwargs . get ( 'device_map' , 'auto' ), trust_remote_code = kwargs . get ( 'trust_remote_code' , False ), ), datasets = [ RLDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = max_samples , percent = kwargs . get ( 'percent' ), max_eval_samples = kwargs . get ( 'max_eval_samples' , None ), field_mappings = kwargs . get ( 'field_mappings' , {}), column_mapping = kwargs . get ( 'column_mapping' , {}), weight = kwargs . get ( 'dataset_weight' , 1.0 ), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), pad_token = kwargs . get ( 'pad_token' ), label_pad_token_id = kwargs . get ( 'label_pad_token_id' , - 100 ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), padding_free = kwargs . get ( 'padding_free' , False ), precompute_ref_log_probs = kwargs . get ( 'precompute_ref_log_probs' , False ), precompute_ref_batch_size = kwargs . get ( 'precompute_ref_batch_size' ), tools = kwargs . get ( 'tools' ), system_prompt = system_prompt , preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), config_name = kwargs . get ( 'config_name' , None ), ) ], train = RLTrainingConfig ( epochs = num_epochs , max_steps = max_steps , per_device_batch_size = batch_size , per_device_eval_batch_size = kwargs . get ( 'eval_batch_size' , batch_size ), learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), beta = kwargs . get ( 'beta' , 0.1 ), # YAML beta -> config.train.beta kl_coef = kwargs . get ( 'kl_coef' , 0.1 ), num_generations = kwargs . get ( 'num_generations' , batch_size ), cliprange = kwargs . get ( 'cliprange' , 0.2 ), max_length = max_seq_length , use_cache = kwargs . get ( 'use_cache' , True ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 100 ), save_steps = kwargs . get ( 'save_steps' , 500 ), save_total_limit = kwargs . get ( 'save_total_limit' ), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), logging_steps = kwargs . get ( 'logging_steps' , 10 ), eval_steps = kwargs . get ( 'eval_steps' , 100 ), seed = kwargs . get ( 'seed' , 42 ), data_seed = kwargs . get ( 'data_seed' , 47 ), # Match training_script.py mask_truncated_completions = kwargs . get ( 'mask_truncated_completions' , True ), rollout_batch_size = kwargs . get ( 'rollout_batch_size' , 1 ), num_ppo_epochs = kwargs . get ( 'num_ppo_epochs' ), temperature = kwargs . get ( 'temperature' , 0.6 ), top_p = kwargs . get ( 'top_p' , 0.95 ), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), whiten_rewards = kwargs . get ( 'whiten_rewards' , False ), kl_estimator = kwargs . get ( 'kl_estimator' , 'k1' ), vf_coef = kwargs . get ( 'vf_coef' , 0.1 ), cliprange_value = kwargs . get ( 'cliprange_value' , 0.2 ), gamma = kwargs . get ( 'gamma' , 1.0 ), lam = kwargs . get ( 'lam' , 0.95 ), response_length = kwargs . get ( 'response_length' , 128 ), stop_token = kwargs . get ( 'stop_token' , 'eos' ), missing_eos_penalty = kwargs . get ( 'missing_eos_penalty' , 1.0 ), ds3_gather_for_generation = kwargs . get ( 'ds3_gather_for_generation' , True ), generation_kwargs = kwargs . get ( 'generation_kwargs' , {}), max_prompt_length = kwargs . get ( 'max_prompt_length' , 512 ), max_target_length = kwargs . get ( 'max_target_length' ), max_completion_length = kwargs . get ( 'max_completion_length' , 256 ), padding_free = kwargs . get ( 'padding_free' , False ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), loss_type = kwargs . get ( 'loss_type' , 'sigmoid' ), loss_weights = kwargs . get ( 'loss_weights' ), f_divergence_type = kwargs . get ( 'f_divergence_type' , 'reverse_kl' ), f_alpha_divergence_coef = kwargs . get ( 'f_alpha_divergence_coef' , 1.0 ), reference_free = kwargs . get ( 'reference_free' , False ), label_smoothing = kwargs . get ( 'label_smoothing' , 0.0 ), use_weighting = kwargs . get ( 'use_weighting' , False ), rpo_alpha = kwargs . get ( 'rpo_alpha' ), ld_alpha = kwargs . get ( 'ld_alpha' ), discopop_tau = kwargs . get ( 'discopop_tau' , 0.05 ), sync_ref_model = kwargs . get ( 'sync_ref_model' , False ), ref_model_mixup_alpha = kwargs . get ( 'ref_model_mixup_alpha' , 0.6 ), ref_model_sync_steps = kwargs . get ( 'ref_model_sync_steps' , 512 ), use_liger_kernel = kwargs . get ( 'use_liger_kernel' , False ), use_liger_loss = kwargs . get ( 'use_liger_loss' ), # NEW: Counterfactual GRPO specific boost_factor = kwargs . get ( 'boost_factor' , 2.0 ), min_weight = kwargs . get ( 'min_weight' , 0.5 ), max_spans = kwargs . get ( 'max_spans' , 10 ), answer_weight = kwargs . get ( 'answer_weight' , 1.5 ), weighting_mode = kwargs . get ( 'weighting_mode' ), method_name = kwargs . get ( 'method_name' , 'counterfactual' ), random_importance = kwargs . get ( 'random_importance' , False ), invert_importance = kwargs . get ( 'invert_importance' , False ), enable_gradient_conservation = kwargs . get ( 'enable_gradient_conservation' , True ), weight_debug = kwargs . get ( 'weight_debug' , False ), scale_rewards = kwargs . get ( 'scale_rewards' , 'group' ), enable_thinking = kwargs . get ( 'enable_thinking' , False ), fast_inference = kwargs . get ( 'fast_inference' , False ), # Unsloth vLLM fast inference vllm_gpu_memory_utilization = kwargs . get ( 'vllm_gpu_memory_utilization' , 0.7 ), # vLLM GPU memory (0.95 for max speed) gbmpo_l2_coefficient = kwargs . get ( 'gbmpo_l2_coefficient' , 0.0001 ), gbmpo_divergence_type = kwargs . get ( 'gbmpo_divergence_type' ), gbmpo_epsilon = kwargs . get ( 'gbmpo_epsilon' , 0.2 ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.0 ), eval_strategy = kwargs . get ( 'eval_strategy' , 'no' ), logging_strategy = kwargs . get ( 'logging_strategy' , 'steps' ), # BOLT-specific parameters curriculum_enabled = kwargs . get ( 'curriculum_enabled' , False ), curriculum_epsilon = kwargs . get ( 'curriculum_epsilon' , 0.05 ), curriculum_update_freq = kwargs . get ( 'curriculum_update_freq' , 10 ), baseline_enabled = kwargs . get ( 'baseline_enabled' , False ), baseline_rho_min = kwargs . get ( 'baseline_rho_min' , 0.875 ), baseline_rho_max = kwargs . get ( 'baseline_rho_max' , 0.96 ), baseline_D_half = kwargs . get ( 'baseline_D_half' , 0.5 ), baseline_warm_start = kwargs . get ( 'baseline_warm_start' ), use_baseline_advantages = kwargs . get ( 'use_baseline_advantages' , False ), # Meta-ES specific parameters meta_iterations = kwargs . get ( 'meta_iterations' , 15 ), patience = kwargs . get ( 'patience' , 5 ), min_delta = kwargs . get ( 'min_delta' , 0.001 ), init_scale = kwargs . get ( 'init_scale' , 0.01 ), N = kwargs . get ( 'N' , 10 ), T = kwargs . get ( 'T' , 100 ), sigma = kwargs . get ( 'sigma' , 0.01 ), sigma_decay = kwargs . get ( 'sigma_decay' , 0.99 ), alpha = kwargs . get ( 'alpha' , 0.01 ), mirror_coefficient = kwargs . get ( 'mirror_coefficient' , 0.0001 ), debug_mode = kwargs . get ( 'debug_mode' , False ), eval_timeout = kwargs . get ( 'eval_timeout' , 5 ), eval_max_tokens = kwargs . get ( 'eval_max_tokens' , 512 ), eval_k = kwargs . get ( 'eval_k' , 1 ), eval_temperature = kwargs . get ( 'eval_temperature' , 0.8 ), num_workers = kwargs . get ( 'num_workers' , 1 ), no_wandb = kwargs . get ( 'no_wandb' , False ), wandb_project = kwargs . get ( 'wandb_project' , 'neural-mirror-es' ), resume = kwargs . get ( 'resume' ), # DPO evaluation parameters dpo_eval_enabled = kwargs . get ( 'dpo_eval_enabled' , False ), dpo_eval_max_samples = kwargs . get ( 'dpo_eval_max_samples' ), dpo_zero_shot_max_samples = kwargs . get ( 'dpo_zero_shot_max_samples' , 50 ), dpo_few_shot_max_samples = kwargs . get ( 'dpo_few_shot_max_samples' , 30 ), dpo_few_shot_examples_text = kwargs . get ( 'dpo_few_shot_examples_text' ), # Additional checkpoint parameters load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , False ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), # Additional training parameters weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), reward_weights = kwargs . get ( 'reward_weights' ), # GRPO/GSPO specific (if not already present) grpo_alpha = kwargs . get ( 'grpo_alpha' , 0.1 ), grpo_beta = kwargs . get ( 'grpo_beta' , 0.1 ), gspo_gamma = kwargs . get ( 'gspo_gamma' , 0.1 ), gspo_delta = kwargs . get ( 'gspo_delta' , 0.1 ), group_by_length = kwargs . get ( 'group_by_length' , True ), extra_params = kwargs , use_rewards_directly = kwargs . get ( 'use_rewards_directly' , None ), ), logging = RLLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), sample_logging = sample_logging_config , report_to = kwargs . get ( 'report_to' , \"none\" ), ), rewards = rewards_config , chat_template = kwargs . get ( 'chat_template' , 'auto' ), caching = { 'root' : kwargs . get ( 'cache_dir' , 'cache' ), 'enabled' : kwargs . get ( 'caching_enabled' , True ) }, reward_training = reward_training_config ) # Create backend config algorithm_enum = RLAlgorithm ( algorithm . lower ()) if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . RL , algorithm_enum ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . RL , backend_type , algorithm_enum ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () get_backend_status() \u00b6 Get the availability status of all backends. Get current backend status and environment variables. Source code in src/aligntune/core/backend_factory.py 307 308 309 310 311 312 313 314 315 316 def get_backend_status () -> Dict [ str , Any ]: \"\"\"Get current backend status and environment variables.\"\"\" return { \"pure_trl_mode\" : os . environ . get ( 'PURE_TRL_MODE' , '0' ), \"trl_only_mode\" : os . environ . get ( 'TRL_ONLY_MODE' , '0' ), \"disable_unsloth_for_trl\" : os . environ . get ( 'DISABLE_UNSLOTH_FOR_TRL' , '0' ), \"trl_available\" : TRL_AVAILABLE , \"unsloth_available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"current_mode\" : \"TRL-ONLY\" if os . environ . get ( 'PURE_TRL_MODE' ) == '1' else \"UNSLOTH-ENABLED\" } options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl_available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth_available' ] } \" ) list_backends() \u00b6 List all available backends. Source code in src/aligntune/core/backend_factory.py 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 def list_backends () -> Dict [ str , list ]: backends = { \"TRL\" : { \"available\" : TRL_AVAILABLE , \"description\" : \"HuggingFace Transformers Reinforcement Learning library\" , \"status\" : \"\u2705 Available\" if TRL_AVAILABLE else \"\u274c Not Available\" }, \"UNSLOTH\" : { \"available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"description\" : \"Unsloth optimized training with memory efficiency\" , \"status\" : \"\u2705 Available\" if _check_backend_availability ( BackendType . UNSLOTH ) else \"\u274c Not Available\" } } # Print availability status print ( \" \\n \" + \"=\" * 60 ) print ( \"FINETUNEHUB - BACKEND AVAILABILITY\" ) print ( \"=\" * 60 ) for backend_name , info in backends . items (): print ( f \" { backend_name : 10s } : { info [ 'status' ] } \" ) print ( f \" { info [ 'description' ] } \" ) print ( \"=\" * 60 ) print ( \"Note: When TRL is selected, Unsloth is disabled to prevent interference.\" ) print ( \" When Unsloth is selected, TRL-only mode is cleared.\" ) print ( \"=\" * 60 + \" \\n \" ) return backends options: show_source: true heading_level: 3 Enums and Types \u00b6 BackendType Enum \u00b6 Bases: Enum Backend types for training. Source code in src/aligntune/core/backend_factory.py 202 203 204 205 class BackendType ( Enum ): \"\"\"Backend types for training.\"\"\" UNSLOTH = \"unsloth\" # Unsloth backend (fast, memory efficient) TRL = \"trl\" # TRL backend (standard, reliable) options: show_source: true heading_level: 3 TrainingType Enum \u00b6 Bases: Enum Training types supported by AlignTune. Source code in src/aligntune/core/backend_factory.py 195 196 197 198 199 class TrainingType ( Enum ): \"\"\"Training types supported by AlignTune.\"\"\" SFT = \"sft\" # Supervised Fine-Tuning RL = \"rl\" # Reinforcement Learning options: show_source: true heading_level: 3 RLAlgorithm Enum \u00b6 Bases: Enum RL algorithms supported. Source code in src/aligntune/core/backend_factory.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class RLAlgorithm ( Enum ): \"\"\"RL algorithms supported.\"\"\" DPO = \"dpo\" # Direct Preference Optimization PPO = \"ppo\" # Proximal Policy Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective # New COUNTERFACT_GRPO = \"counterfact_grpo\" # NEW: Add GBMPO GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3 BackendConfig Dataclass \u00b6 Configuration for backend selection with task type support. Source code in src/aligntune/core/backend_factory.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 @dataclass class BackendConfig : \"\"\"Configuration for backend selection with task type support.\"\"\" training_type : TrainingType backend : BackendType algorithm : Optional [ RLAlgorithm ] = None # Only for RL task_type : Optional [ TaskType ] = None # NEW: For SFT tasks fallback_enabled : bool = True def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING __post_init__ () \u00b6 Validate configuration. Source code in src/aligntune/core/backend_factory.py 237 238 239 240 241 242 243 244 245 246 247 248 def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING options: show_source: true heading_level: 3 Examples \u00b6 SFT with TRL Backend \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) SFT with Unsloth Backend \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) DPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" ) Error Handling \u00b6 Backend Not Available \u00b6 try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ValueError as e : print ( f \"Backend not available: { e } \" ) # Falls back to TRL automatically Invalid Algorithm \u00b6 try : trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"invalid\" ) except ValueError as e : print ( f \"Invalid algorithm: { e } \" ) Next Steps \u00b6 Configuration Classes - Configuration options Trainers - Trainer methods User Guide - Usage guide","title":"Backend Factory"},{"location":"api-reference/backend-factory/#backend-factory-api","text":"The Backend Factory is the main entry point for creating trainers in AlignTune.","title":"Backend Factory API"},{"location":"api-reference/backend-factory/#overview","text":"The BackendFactory provides functions to create SFT and RL trainers with automatic backend selection and fallback handling.","title":"Overview"},{"location":"api-reference/backend-factory/#backendfactory-class","text":"Complete API reference for the BackendFactory class. Factory for creating training backends. Source code in src/aligntune/core/backend_factory.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 class BackendFactory : \"\"\"Factory for creating training backends.\"\"\" # Registry of available backends _backends : Dict [ tuple , Type [ TrainerBase ]] = {} @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) @classmethod def _select_best_backend ( cls , backend_config : BackendConfig ) -> BackendConfig : \"\"\"Select the best available backend with fallback.\"\"\" # Check if requested backend is available if cls . _is_backend_available ( backend_config ): return backend_config # Try fallback backends if backend_config . fallback_enabled : fallback_order = cls . _get_fallback_order ( backend_config ) for fallback_backend in fallback_order : if cls . _is_backend_available ( fallback_backend ): logger . warning ( f \"Requested backend { backend_config . backend } not available, \" f \"falling back to { fallback_backend . backend } \" ) return fallback_backend raise RuntimeError ( f \"No available backend for { backend_config } \" ) @classmethod def _is_backend_available ( cls , backend_config : BackendConfig ) -> bool : \"\"\"Check if a backend is available.\"\"\" key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) trainer_class = cls . _backends . get ( key ) if trainer_class is None : return False # Check if the trainer class can be instantiated try : return trainer_class . is_available () except Exception : return False @classmethod def _get_fallback_order ( cls , backend_config : BackendConfig ) -> list : \"\"\"Get fallback order for backend selection.\"\"\" if backend_config . training_type == TrainingType . SFT : # SFT fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . SFT , BackendType . UNSLOTH ), BackendConfig ( TrainingType . SFT , BackendType . TRL ), ] else : # RL # RL fallback order: Unsloth -> TRL fallbacks = [ BackendConfig ( TrainingType . RL , BackendType . UNSLOTH , backend_config . algorithm ), BackendConfig ( TrainingType . RL , BackendType . TRL , backend_config . algorithm ), ] # Remove the original backend from fallbacks fallbacks = [ fb for fb in fallbacks if fb . backend != backend_config . backend ] return fallbacks @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" ) @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config )","title":"BackendFactory Class"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.create_trainer","text":"Create a trainer instance based on backend configuration. Source code in src/aligntune/core/backend_factory.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 @classmethod def create_trainer ( cls , config , backend_config : BackendConfig ) -> TrainerBase : \"\"\"Create a trainer instance based on backend configuration.\"\"\" # ==================== CLASSIFICATION ROUTING ==================== # Check for classification tasks - route to ClassificationTrainer if hasattr ( config , 'dataset' ) and hasattr ( config . dataset , 'task_type' ): task_type = config . dataset . task_type # For classification, use ClassificationTrainer (NOT TRL) if task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : from aligntune.backends.trl.sft.Classification_trainer import ClassificationTrainer logger . info ( f \"\u2713 Using ClassificationTrainer for { task_type . value } \" ) return ClassificationTrainer ( config ) except ImportError as e : logger . error ( f \"\u274c ClassificationTrainer import failed: { e } \" ) logger . error ( \"Make sure the file exists at:\" ) logger . error ( \" backends/trl/sft/Classification_trainer.py\" ) raise ImportError ( f \"ClassificationTrainer not found. \" f \"Error: { e } \" ) # ==================== END CLASSIFICATION ROUTING ==================== # Normal backend routing for non-classification tasks key = ( backend_config . training_type , backend_config . backend , backend_config . algorithm ) if key not in cls . _backends : raise ValueError ( f \"No backend registered for { key } \" ) trainer_class = cls . _backends [ key ] # Check if backend is available if not trainer_class . is_available (): if backend_config . fallback_enabled : # Try fallback backends fallback_config = cls . get_recommended_backend ( backend_config . training_type , backend_config . algorithm ) if fallback_config != backend_config : logger . warning ( f \"Backend { backend_config . backend } not available, falling back to { fallback_config . backend } \" ) return cls . create_trainer ( config , fallback_config ) raise RuntimeError ( f \"Backend { backend_config . backend } is not available\" ) return trainer_class ( config )","title":"create_trainer"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.get_backend_capabilities","text":"Get capabilities for a specific backend type. Source code in src/aligntune/core/backend_factory.py 426 427 428 429 430 431 432 433 434 435 def get_backend_capabilities ( self , backend_type : BackendType ) -> List [ str ]: \"\"\"Get capabilities for a specific backend type.\"\"\" capabilities = [] if backend_type == BackendType . TRL and TRL_AVAILABLE : capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] elif backend_type == BackendType . UNSLOTH and ( UNSLOTH_AVAILABLE or _check_unsloth_available ()): capabilities = [ 'sft' , 'dpo' , 'ppo' , 'grpo' , 'gspo' ] return capabilities","title":"get_backend_capabilities"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.get_recommended_backend","text":"Get recommended backend for given training type and algorithm. Source code in src/aligntune/core/backend_factory.py 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 @classmethod def get_recommended_backend ( cls , training_type : TrainingType , algorithm : Optional [ RLAlgorithm ] = None ) -> BackendConfig : \"\"\"Get recommended backend for given training type and algorithm.\"\"\" # print(cls) if training_type == TrainingType . SFT : # For SFT: Unsloth is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend ) if cls . _is_backend_available ( config ): return config else : # RL # For RL: Unsloth+TRL is best, then TRL for backend in [ BackendType . UNSLOTH , BackendType . TRL ]: config = BackendConfig ( training_type , backend , algorithm ) if cls . _is_backend_available ( config ): return config raise RuntimeError ( f \"No available backend for { training_type } { algorithm } \" )","title":"get_recommended_backend"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.is_backend_available","text":"Check if a specific backend type is available. Source code in src/aligntune/core/backend_factory.py 414 415 416 417 418 419 420 421 422 423 424 def is_backend_available ( self , backend_type : BackendType ) -> bool : \"\"\"Check if a specific backend type is available.\"\"\" try : if backend_type == BackendType . TRL : return TRL_AVAILABLE elif backend_type == BackendType . UNSLOTH : return UNSLOTH_AVAILABLE else : return False except Exception : return False","title":"is_backend_available"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.list_available_backends","text":"List all available backends. Source code in src/aligntune/core/backend_factory.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @classmethod def list_available_backends ( cls ) -> Dict [ str , Any ]: \"\"\"List all available backends.\"\"\" available = { \"SFT\" : [], \"RL\" : {} } for ( training_type , backend , algorithm ), trainer_class in cls . _backends . items (): try : if trainer_class . is_available (): if training_type == TrainingType . SFT : available [ \"SFT\" ] . append ( backend . value ) else : # RL if algorithm . value not in available [ \"RL\" ]: available [ \"RL\" ][ algorithm . value ] = [] available [ \"RL\" ][ algorithm . value ] . append ( backend . value ) except Exception : continue return available","title":"list_available_backends"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendFactory.register_backend","text":"Register a backend trainer class. Source code in src/aligntune/core/backend_factory.py 325 326 327 328 329 330 331 332 333 334 335 336 @classmethod def register_backend ( cls , training_type : TrainingType , backend : BackendType , algorithm : Optional [ RLAlgorithm ] = None , trainer_class : Type [ TrainerBase ] = None ): \"\"\"Register a backend trainer class.\"\"\" key = ( training_type , backend , algorithm ) cls . _backends [ key ] = trainer_class logger . debug ( f \"Registered backend: { key } -> { trainer_class . __name__ } \" ) options: show_source: true heading_level: 3","title":"register_backend"},{"location":"api-reference/backend-factory/#factory-functions","text":"","title":"Factory Functions"},{"location":"api-reference/backend-factory/#create_sft_trainer","text":"Create a Supervised Fine-Tuning trainer. Create SFT trainer with specified backend and parameters. Source code in src/aligntune/core/backend_factory.py 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 def create_sft_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , system_prompt : Optional [ str ] = None , # config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create SFT trainer with specified backend and parameters.\"\"\" if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"sft\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) # Create configuration # Build model config kwargs, only including precision if explicitly provided model_config_kwargs = { \"name_or_path\" : model_name , \"max_seq_length\" : max_seq_length , \"quantization\" : kwargs . get ( 'quantization' , {}), \"use_unsloth\" : kwargs . get ( 'use_unsloth' , False ), \"peft_enabled\" : kwargs . get ( 'use_peft' , kwargs . get ( 'peft_enabled' , False )), \"lora_rank\" : kwargs . get ( 'lora_r' , 16 ), \"lora_alpha\" : kwargs . get ( 'lora_alpha' , 32 ), \"lora_dropout\" : kwargs . get ( 'lora_dropout' , 0.1 ), \"target_modules\" : kwargs . get ( 'lora_target_modules' , None ), \"bias\" : kwargs . get ( 'lora_bias' , 'none' ), \"attn_implementation\" : kwargs . get ( 'attn_implementation' , 'auto' ), \"max_memory\" : kwargs . get ( 'max_memory' ), \"use_gradient_checkpointing\" : kwargs . get ( 'use_gradient_checkpointing' , True ), \"num_labels\" : kwargs . get ( 'num_labels' ), # For classification tasks \"model_init_kwargs\" : kwargs . get ( 'model_init_kwargs' , {}), \"device_map\" : kwargs . get ( 'device_map' , 'auto' ), \"trust_remote_code\" : kwargs . get ( 'trust_remote_code' , False ), } # Only add precision if explicitly provided (otherwise use default from ModelConfig) if 'precision' in kwargs and kwargs . get ( 'precision' ) is not None : # Validate precision value precision_value = kwargs . get ( 'precision' ) if isinstance ( precision_value , str ): # Convert string to PrecisionType enum from .sft.config import PrecisionType try : precision_value = PrecisionType ( precision_value . lower ()) except ValueError : logger . warning ( f \"Invalid precision ' { precision_value } ', using default\" ) precision_value = None if precision_value is not None : model_config_kwargs [ 'precision' ] = precision_value config = SFTConfig ( model = SFTModelConfig ( ** model_config_kwargs ), dataset = SFTDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), subset = kwargs . get ( 'subset' ), # Support dataset config/subset (e.g., for financial_phrasebank) config = kwargs . get ( 'config' ), # Alternative name for subset max_samples = max_samples , percent = kwargs . get ( 'percent' ), column_mapping = kwargs . get ( 'column_mapping' , {}), task_type = kwargs . get ( 'task_type' , 'supervised_fine_tuning' ), system_prompt = system_prompt , # dataset_kwargs=kwargs.get('dataset_kwargs', {}), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), dataset_text_field = kwargs . get ( 'dataset_text_field' , 'text' ), text_column = kwargs . get ( 'text_column' , kwargs . get ( 'dataset_text_field' , 'text' )), # For classification tasks label_column = kwargs . get ( 'label_column' , 'label' ), # For classification tasks # eos_token=kwargs.get('eos_token'), pad_token = kwargs . get ( 'pad_token' ), chat_template = kwargs . get ( 'chat_template' ), preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), ), train = SFTTrainingConfig ( epochs = num_epochs , max_steps = kwargs . get ( 'max_steps' ), per_device_batch_size = batch_size , learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.1 ), weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_steps' , kwargs . get ( 'save_interval' , 500 )), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), fp16 = kwargs . get ( 'fp16' , False ), bf16 = kwargs . get ( 'bf16' , False ), dataloader_num_workers = kwargs . get ( 'dataloader_num_workers' , 0 ), remove_unused_columns = kwargs . get ( 'remove_unused_columns' , False ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), group_by_length = kwargs . get ( 'group_by_length' , True ), dataloader_drop_last = kwargs . get ( 'dataloader_drop_last' , False ), eval_accumulation_steps = kwargs . get ( 'eval_accumulation_steps' ), label_smoothing_factor = kwargs . get ( 'label_smoothing_factor' , 0.0 ), early_stopping_patience = kwargs . get ( 'early_stopping_patience' ), early_stopping_threshold = kwargs . get ( 'early_stopping_threshold' , 0.0 ), load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , True ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' , 'eval_loss' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), use_trl = kwargs . get ( 'use_trl' , False ), dataset_num_proc = kwargs . get ( 'train_dataset_num_proc' ), dataset_kwargs = kwargs . get ( 'train_dataset_kwargs' , {}), packing = kwargs . get ( 'packing' , False ), packing_strategy = kwargs . get ( 'packing_strategy' , 'bfd' ), eval_packing = kwargs . get ( 'eval_packing' ), padding_free = kwargs . get ( 'padding_free' , False ), pad_to_multiple_of = kwargs . get ( 'pad_to_multiple_of' ), completion_only_loss = kwargs . get ( 'completion_only_loss' ), assistant_only_loss = kwargs . get ( 'assistant_only_loss' , False ), loss_type = kwargs . get ( 'loss_type' , 'nll' ), activation_offloading = kwargs . get ( 'activation_offloading' , False ), use_flash_attention_2 = kwargs . get ( 'use_flash_attention_2' ), gradient_checkpointing = kwargs . get ( 'gradient_checkpointing' , False ), gradient_checkpointing_kwargs = kwargs . get ( 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }), extra_params = kwargs , ), logging = SFTLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), log_level = kwargs . get ( 'log_level' , 'INFO' ), log_interval = kwargs . get ( 'logging_steps' , kwargs . get ( 'log_interval' , 10 )), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), eval_strategy = kwargs . get ( 'eval_strategy' , 'steps' ), report_to = kwargs . get ( 'report_to' , \"none\" ), ), evaluation = SFTEvaluationConfig ( compute_perplexity = kwargs . get ( 'compute_perplexity' , True ), compute_rouge = kwargs . get ( 'compute_rouge' , True ), compute_bleu = kwargs . get ( 'compute_bleu' , True ), compute_meteor = kwargs . get ( 'compute_meteor' , False ), compute_bertscore = kwargs . get ( 'compute_bertscore' , False ), compute_semantic_similarity = kwargs . get ( 'compute_semantic_similarity' , False ), compute_codebleu = kwargs . get ( 'compute_codebleu' , False ), max_samples_for_quality_metrics = kwargs . get ( 'max_samples_for_quality_metrics' , 50 ), bertscore_model = kwargs . get ( 'bertscore_model' , 'microsoft/deberta-xlarge-mnli' ), semantic_similarity_model = kwargs . get ( 'semantic_similarity_model' , 'sentence-transformers/all-MiniLM-L6-v2' ) ) ) # Create backend config if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . SFT ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . SFT , backend_type ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 ) trainer . train ()","title":"create_sft_trainer()"},{"location":"api-reference/backend-factory/#create_rl_trainer","text":"Create a Reinforcement Learning trainer. Create RL trainer with specified algorithm and backend. Source code in src/aligntune/core/backend_factory.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def create_rl_trainer ( model_name : Optional [ str ] = None , dataset_name : Optional [ str ] = None , algorithm : Optional [ str ] = None , backend : str = \"auto\" , output_dir : str = \"./output\" , num_epochs : int = 3 , max_steps : Optional [ int ] = 100 , # Add max_steps parameter batch_size : int = 4 , learning_rate : float = 2e-4 , max_seq_length : int = 512 , max_samples : Optional [ int ] = None , reward_value_model : Optional [ str ] = None , reward_model_name : Optional [ str ] = None , # NEW reward_model_path : Optional [ str ] = None , # NEW: Local reward model path train_custom_reward_model : bool = False , # NEW: Train custom reward model reward_training_texts : Optional [ List [ str ]] = None , # NEW: Training texts for custom model reward_functions : Optional [ List [ str ]] = None , # NEW: Reward functions for custom model reward_function_weights : Optional [ List [ float ]] = None , # NEW: Weights for reward functions reward_training_base_model : Optional [ str ] = None , # NEW: Base model for custom training reward_training_output_dir : Optional [ str ] = None , # NEW: Output dir for custom training reward_value_loading_type : Optional [ str ] = None , reward_model_quantization : Optional [ Dict ] = None , value_model_quantization : Optional [ Dict ] = None , reward_training : Optional [ Dict [ str , Any ]] = None , # NEW: Flexible reward training config reward_device : str = \"auto\" , sample_logging : Optional [ Dict [ str , Any ]] = None , # NEW: Counterfactual GRPO specific parameters boost_factor : float = 2.0 , min_weight : float = 0.5 , max_spans : int = 10 , answer_weight : float = 1.5 , method_name : str = \"counterfactual\" , random_importance : bool = False , invert_importance : bool = False , enable_gradient_conservation : bool = True , weight_debug : bool = False , system_prompt : Optional [ str ] = None , # # NEW: GBMPO-specific parameters gbmpo_divergence_type : Optional [ str ] = None , config : Optional [ Union [ str , Path , Dict ]] = None , ** kwargs ) -> TrainerBase : \"\"\"Create RL trainer with specified algorithm and backend.\"\"\" # NEW: Load config if provided# Load and parse config if provided if config is not None : if isinstance ( config , ( str , Path )): config_dict = load_config ( config ) # \u2190 Use load_config from config_utils else : config_dict = dict ( config ) # Parse config to unified format parsed_config = parse_config_to_unified ( config_dict , training_type = \"rl\" ) # \u2190 Use the function # Merge: parsed_config as base, kwargs override merged = { ** parsed_config , ** kwargs } kwargs = merged # Extract required parameters from merged config model_name = model_name or kwargs . pop ( 'model_name' , None ) dataset_name = dataset_name or kwargs . pop ( 'dataset_name' , None ) algorithm = algorithm or kwargs . pop ( 'algorithm' , None ) backend = kwargs . pop ( 'backend' , backend ) # Validate required parameters if model_name is None : raise ValueError ( \"model_name must be provided either as argument or in config\" ) if dataset_name is None : raise ValueError ( \"dataset_name must be provided either as argument or in config\" ) if algorithm is None : raise ValueError ( \"algorithm must be provided either as argument or in config\" ) reward_device = kwargs . pop ( 'reward_device' , reward_device or \"auto\" ) reward_device = reward_device or \"auto\" sample_logging_dict : Dict [ str , Any ] = {} base_sample_logging = sample_logging or kwargs . get ( 'sample_logging_config' ) if base_sample_logging : sample_logging_dict . update ( base_sample_logging ) inline_sample_logging = { \"enabled\" : kwargs . get ( 'enable_sample_logging' ), \"prompts\" : kwargs . get ( 'sample_logging_prompts' ), \"interval_steps\" : kwargs . get ( 'sample_logging_interval_steps' ), \"percent_of_max_steps\" : kwargs . get ( 'sample_logging_percent_of_max_steps' , kwargs . get ( 'sample_logging_percent' )), \"max_new_tokens\" : kwargs . get ( 'sample_logging_max_new_tokens' ), \"temperature\" : kwargs . get ( 'sample_logging_temperature' ), \"top_p\" : kwargs . get ( 'sample_logging_top_p' ), \"num_samples\" : kwargs . get ( 'sample_logging_num_samples' ), } for key , value in inline_sample_logging . items (): if value is not None : sample_logging_dict [ key ] = value cleaned_sample_logging = { k : v for k , v in sample_logging_dict . items () if v is not None } if cleaned_sample_logging : sample_logging_config = SampleLoggingConfig ( ** cleaned_sample_logging ) else : sample_logging_config = SampleLoggingConfig () needs_neural_reward = not any ([ reward_model_path , train_custom_reward_model , reward_functions ]) if reward_model_name is None and needs_neural_reward : reward_model_name = model_name # ============================================ # PPO MODEL CONSISTENCY WARNING # ============================================ if algorithm . lower () == \"ppo\" : print ( \"\u26a0\ufe0f NOTE: For optimal PPO performance, ensure policy_model, reward_model, and value_model are from the same model family\" ) # STRICT VALIDATION: Validate reward model configuration reward_model_source = None source_count = sum ([ reward_model_name is not None , reward_model_path is not None , train_custom_reward_model ]) if source_count > 1 : raise ValueError ( f \"Specify exactly ONE reward source. Found { source_count } : \" f \"reward_model_name= { reward_model_name } , \" f \"reward_model_path= { reward_model_path } , \" f \"train_custom_reward_model= { train_custom_reward_model } \" ) if train_custom_reward_model : # Validate ALL required fields for custom training if not reward_training_texts : raise ValueError ( \"reward_training_texts required when train_custom_reward_model=True\" ) if not reward_functions : raise ValueError ( \"reward_functions required when train_custom_reward_model=True\" ) if not reward_training_base_model : raise ValueError ( \"reward_training_base_model required when train_custom_reward_model=True\" ) if not reward_training_output_dir : raise ValueError ( \"reward_training_output_dir required when train_custom_reward_model=True\" ) # Import required classes from .rl.config import RewardModelTrainingConfig , RewardModelSourceConfig training_config = RewardModelTrainingConfig ( base_model_name = reward_training_base_model , training_texts = reward_training_texts , reward_functions = reward_functions , output_dir = reward_training_output_dir , # Use kwargs for optional training params num_epochs = kwargs . get ( 'reward_training_epochs' , 3 ), learning_rate = kwargs . get ( 'reward_training_lr' , 1e-5 ), batch_size = kwargs . get ( 'reward_training_batch_size' , 8 ), reward_weights = reward_function_weights ) reward_model_source = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) elif reward_model_name : from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = reward_model_name ) elif reward_model_path : # Validate path exists if not Path ( reward_model_path ) . exists (): raise FileNotFoundError ( f \"reward_model_path does not exist: { reward_model_path } \" ) from .rl.config import RewardModelSourceConfig reward_model_source = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = reward_model_path ) # Create reward training config if provided reward_training_config = None if reward_training : from .rl.config import RewardModelTrainingConfig reward_training_config = RewardModelTrainingConfig ( ** reward_training ) # Construct rewards config if reward_functions provided but not in kwargs # rewards_config = kwargs.get('rewards', []) rewards_config = kwargs . get ( 'rewards' ) or [] if not rewards_config and reward_functions and not train_custom_reward_model : weights = reward_function_weights or [ 1.0 ] * len ( reward_functions ) if len ( weights ) != len ( reward_functions ): logger . warning ( \"reward_function_weights length mismatch, using default 1.0\" ) weights = [ 1.0 ] * len ( reward_functions ) for func_name , weight in zip ( reward_functions , weights ): rewards_config . append ({ \"type\" : func_name , \"weight\" : weight , \"params\" : {} }) logger . info ( f \"Constructed rewards config from function list: { len ( rewards_config ) } functions\" ) # FIXED: Added required 'algo' parameter and used 'train' not 'training' if algorithm . lower () == \"counterfact_grpo\" : # Add counterfactual-specific params to kwargs for config creation kwargs . update ({ 'boost_factor' : boost_factor , 'min_weight' : min_weight , 'max_spans' : max_spans , 'answer_weight' : answer_weight , 'method_name' : method_name , 'random_importance' : random_importance , 'invert_importance' : invert_importance , 'enable_gradient_conservation' : enable_gradient_conservation , 'weight_debug' : weight_debug , }) # NEW: Handle GBMPO algorithms - set divergence_type based on algorithm # NEW: Handle GBMPO algorithm variants original_algorithm = algorithm . lower () # Map GBMPO variants to base algorithm and extract divergence type if original_algorithm . startswith ( \"gbmpo\" ): # Extract divergence type from algorithm name if not explicitly provided if gbmpo_divergence_type is None : if original_algorithm == \"gbmpo\" : # Default to l2kl if no variant specified gbmpo_divergence_type = \"l2kl\" else : # Extract from algorithm name: gbmpo_l2kl -> l2kl suffix = original_algorithm . replace ( \"gbmpo_\" , \"\" ) divergence_map = { \"l2\" : \"l2\" , \"l2kl\" : \"l2kl\" , \"probl2\" : \"prob_l2\" , \"probl2kl\" : \"prob_l2kl\" } gbmpo_divergence_type = divergence_map . get ( suffix , \"l2kl\" ) # Normalize to base \"gbmpo\" algorithm algorithm = \"gbmpo\" kwargs [ 'gbmpo_divergence_type' ] = gbmpo_divergence_type logger . info ( f \"GBMPO variant detected: { original_algorithm } -> divergence_type= { gbmpo_divergence_type } \" ) config = UnifiedConfig ( algo = algorithm . lower (), model = RLModelConfig ( name_or_path = model_name , backend = backend if backend != \"auto\" else \"trl\" , max_seq_length = max_seq_length , quantization = kwargs . get ( 'quantization' , {}), gradient_checkpointing = kwargs . get ( 'use_gradient_checkpointing' , False ), precision = kwargs . get ( 'precision' , 'auto' ), reward_value_model = reward_value_model or kwargs . get ( 'reward_value_model' , None ), reward_model_name = reward_model_name , reward_model_source = reward_model_source , reward_value_loading_type = reward_value_loading_type , reward_model_quantization = reward_model_quantization or {}, value_model_quantization = value_model_quantization or {}, use_peft = kwargs . get ( 'use_peft' , True ), lora_r = kwargs . get ( 'lora_r' , 16 ), lora_alpha = kwargs . get ( 'lora_alpha' , 32 ), lora_dropout = kwargs . get ( 'lora_dropout' , 0.05 ), lora_target_modules = kwargs . get ( 'lora_target_modules' , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]), model_init_kwargs = kwargs . get ( 'model_init_kwargs' , {}), ref_model_init_kwargs = kwargs . get ( 'ref_model_init_kwargs' , {}), model_adapter_name = kwargs . get ( 'model_adapter_name' ), ref_adapter_name = kwargs . get ( 'ref_adapter_name' ), force_use_ref_model = kwargs . get ( 'force_use_ref_model' , False ), disable_dropout = kwargs . get ( 'disable_dropout' , True ), use_logits_to_keep = kwargs . get ( 'use_logits_to_keep' , False ), reward_device = reward_device , device_map = kwargs . get ( 'device_map' , 'auto' ), trust_remote_code = kwargs . get ( 'trust_remote_code' , False ), ), datasets = [ RLDatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = max_samples , percent = kwargs . get ( 'percent' ), max_eval_samples = kwargs . get ( 'max_eval_samples' , None ), field_mappings = kwargs . get ( 'field_mappings' , {}), column_mapping = kwargs . get ( 'column_mapping' , {}), weight = kwargs . get ( 'dataset_weight' , 1.0 ), dataset_num_proc = kwargs . get ( 'dataset_num_proc' ), pad_token = kwargs . get ( 'pad_token' ), label_pad_token_id = kwargs . get ( 'label_pad_token_id' , - 100 ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), padding_free = kwargs . get ( 'padding_free' , False ), precompute_ref_log_probs = kwargs . get ( 'precompute_ref_log_probs' , False ), precompute_ref_batch_size = kwargs . get ( 'precompute_ref_batch_size' ), tools = kwargs . get ( 'tools' ), system_prompt = system_prompt , preserve_columns = kwargs . get ( 'preserve_columns' ), processing_fn = kwargs . get ( 'processing_fn' ), processing_batched = kwargs . get ( 'processing_batched' , False ), processing_fn_kwargs = kwargs . get ( 'processing_fn_kwargs' , {}), config_name = kwargs . get ( 'config_name' , None ), ) ], train = RLTrainingConfig ( epochs = num_epochs , max_steps = max_steps , per_device_batch_size = batch_size , per_device_eval_batch_size = kwargs . get ( 'eval_batch_size' , batch_size ), learning_rate = learning_rate , gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ), beta = kwargs . get ( 'beta' , 0.1 ), # YAML beta -> config.train.beta kl_coef = kwargs . get ( 'kl_coef' , 0.1 ), num_generations = kwargs . get ( 'num_generations' , batch_size ), cliprange = kwargs . get ( 'cliprange' , 0.2 ), max_length = max_seq_length , use_cache = kwargs . get ( 'use_cache' , True ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 100 ), save_steps = kwargs . get ( 'save_steps' , 500 ), save_total_limit = kwargs . get ( 'save_total_limit' ), save_strategy = kwargs . get ( 'save_strategy' , 'steps' ), logging_steps = kwargs . get ( 'logging_steps' , 10 ), eval_steps = kwargs . get ( 'eval_steps' , 100 ), seed = kwargs . get ( 'seed' , 42 ), data_seed = kwargs . get ( 'data_seed' , 47 ), # Match training_script.py mask_truncated_completions = kwargs . get ( 'mask_truncated_completions' , True ), rollout_batch_size = kwargs . get ( 'rollout_batch_size' , 1 ), num_ppo_epochs = kwargs . get ( 'num_ppo_epochs' ), temperature = kwargs . get ( 'temperature' , 0.6 ), top_p = kwargs . get ( 'top_p' , 0.95 ), max_grad_norm = kwargs . get ( 'max_grad_norm' , 1.0 ), whiten_rewards = kwargs . get ( 'whiten_rewards' , False ), kl_estimator = kwargs . get ( 'kl_estimator' , 'k1' ), vf_coef = kwargs . get ( 'vf_coef' , 0.1 ), cliprange_value = kwargs . get ( 'cliprange_value' , 0.2 ), gamma = kwargs . get ( 'gamma' , 1.0 ), lam = kwargs . get ( 'lam' , 0.95 ), response_length = kwargs . get ( 'response_length' , 128 ), stop_token = kwargs . get ( 'stop_token' , 'eos' ), missing_eos_penalty = kwargs . get ( 'missing_eos_penalty' , 1.0 ), ds3_gather_for_generation = kwargs . get ( 'ds3_gather_for_generation' , True ), generation_kwargs = kwargs . get ( 'generation_kwargs' , {}), max_prompt_length = kwargs . get ( 'max_prompt_length' , 512 ), max_target_length = kwargs . get ( 'max_target_length' ), max_completion_length = kwargs . get ( 'max_completion_length' , 256 ), padding_free = kwargs . get ( 'padding_free' , False ), truncation_mode = kwargs . get ( 'truncation_mode' , 'keep_end' ), loss_type = kwargs . get ( 'loss_type' , 'sigmoid' ), loss_weights = kwargs . get ( 'loss_weights' ), f_divergence_type = kwargs . get ( 'f_divergence_type' , 'reverse_kl' ), f_alpha_divergence_coef = kwargs . get ( 'f_alpha_divergence_coef' , 1.0 ), reference_free = kwargs . get ( 'reference_free' , False ), label_smoothing = kwargs . get ( 'label_smoothing' , 0.0 ), use_weighting = kwargs . get ( 'use_weighting' , False ), rpo_alpha = kwargs . get ( 'rpo_alpha' ), ld_alpha = kwargs . get ( 'ld_alpha' ), discopop_tau = kwargs . get ( 'discopop_tau' , 0.05 ), sync_ref_model = kwargs . get ( 'sync_ref_model' , False ), ref_model_mixup_alpha = kwargs . get ( 'ref_model_mixup_alpha' , 0.6 ), ref_model_sync_steps = kwargs . get ( 'ref_model_sync_steps' , 512 ), use_liger_kernel = kwargs . get ( 'use_liger_kernel' , False ), use_liger_loss = kwargs . get ( 'use_liger_loss' ), # NEW: Counterfactual GRPO specific boost_factor = kwargs . get ( 'boost_factor' , 2.0 ), min_weight = kwargs . get ( 'min_weight' , 0.5 ), max_spans = kwargs . get ( 'max_spans' , 10 ), answer_weight = kwargs . get ( 'answer_weight' , 1.5 ), weighting_mode = kwargs . get ( 'weighting_mode' ), method_name = kwargs . get ( 'method_name' , 'counterfactual' ), random_importance = kwargs . get ( 'random_importance' , False ), invert_importance = kwargs . get ( 'invert_importance' , False ), enable_gradient_conservation = kwargs . get ( 'enable_gradient_conservation' , True ), weight_debug = kwargs . get ( 'weight_debug' , False ), scale_rewards = kwargs . get ( 'scale_rewards' , 'group' ), enable_thinking = kwargs . get ( 'enable_thinking' , False ), fast_inference = kwargs . get ( 'fast_inference' , False ), # Unsloth vLLM fast inference vllm_gpu_memory_utilization = kwargs . get ( 'vllm_gpu_memory_utilization' , 0.7 ), # vLLM GPU memory (0.95 for max speed) gbmpo_l2_coefficient = kwargs . get ( 'gbmpo_l2_coefficient' , 0.0001 ), gbmpo_divergence_type = kwargs . get ( 'gbmpo_divergence_type' ), gbmpo_epsilon = kwargs . get ( 'gbmpo_epsilon' , 0.2 ), optimizer = kwargs . get ( 'optimizer' , 'adamw_torch' ), lr_scheduler = kwargs . get ( 'lr_scheduler' , 'cosine' ), warmup_steps = kwargs . get ( 'warmup_steps' , 0 ), warmup_ratio = kwargs . get ( 'warmup_ratio' , 0.0 ), eval_strategy = kwargs . get ( 'eval_strategy' , 'no' ), logging_strategy = kwargs . get ( 'logging_strategy' , 'steps' ), # BOLT-specific parameters curriculum_enabled = kwargs . get ( 'curriculum_enabled' , False ), curriculum_epsilon = kwargs . get ( 'curriculum_epsilon' , 0.05 ), curriculum_update_freq = kwargs . get ( 'curriculum_update_freq' , 10 ), baseline_enabled = kwargs . get ( 'baseline_enabled' , False ), baseline_rho_min = kwargs . get ( 'baseline_rho_min' , 0.875 ), baseline_rho_max = kwargs . get ( 'baseline_rho_max' , 0.96 ), baseline_D_half = kwargs . get ( 'baseline_D_half' , 0.5 ), baseline_warm_start = kwargs . get ( 'baseline_warm_start' ), use_baseline_advantages = kwargs . get ( 'use_baseline_advantages' , False ), # Meta-ES specific parameters meta_iterations = kwargs . get ( 'meta_iterations' , 15 ), patience = kwargs . get ( 'patience' , 5 ), min_delta = kwargs . get ( 'min_delta' , 0.001 ), init_scale = kwargs . get ( 'init_scale' , 0.01 ), N = kwargs . get ( 'N' , 10 ), T = kwargs . get ( 'T' , 100 ), sigma = kwargs . get ( 'sigma' , 0.01 ), sigma_decay = kwargs . get ( 'sigma_decay' , 0.99 ), alpha = kwargs . get ( 'alpha' , 0.01 ), mirror_coefficient = kwargs . get ( 'mirror_coefficient' , 0.0001 ), debug_mode = kwargs . get ( 'debug_mode' , False ), eval_timeout = kwargs . get ( 'eval_timeout' , 5 ), eval_max_tokens = kwargs . get ( 'eval_max_tokens' , 512 ), eval_k = kwargs . get ( 'eval_k' , 1 ), eval_temperature = kwargs . get ( 'eval_temperature' , 0.8 ), num_workers = kwargs . get ( 'num_workers' , 1 ), no_wandb = kwargs . get ( 'no_wandb' , False ), wandb_project = kwargs . get ( 'wandb_project' , 'neural-mirror-es' ), resume = kwargs . get ( 'resume' ), # DPO evaluation parameters dpo_eval_enabled = kwargs . get ( 'dpo_eval_enabled' , False ), dpo_eval_max_samples = kwargs . get ( 'dpo_eval_max_samples' ), dpo_zero_shot_max_samples = kwargs . get ( 'dpo_zero_shot_max_samples' , 50 ), dpo_few_shot_max_samples = kwargs . get ( 'dpo_few_shot_max_samples' , 30 ), dpo_few_shot_examples_text = kwargs . get ( 'dpo_few_shot_examples_text' ), # Additional checkpoint parameters load_best_model_at_end = kwargs . get ( 'load_best_model_at_end' , False ), metric_for_best_model = kwargs . get ( 'metric_for_best_model' ), greater_is_better = kwargs . get ( 'greater_is_better' , False ), # Additional training parameters weight_decay = kwargs . get ( 'weight_decay' , 0.01 ), reward_weights = kwargs . get ( 'reward_weights' ), # GRPO/GSPO specific (if not already present) grpo_alpha = kwargs . get ( 'grpo_alpha' , 0.1 ), grpo_beta = kwargs . get ( 'grpo_beta' , 0.1 ), gspo_gamma = kwargs . get ( 'gspo_gamma' , 0.1 ), gspo_delta = kwargs . get ( 'gspo_delta' , 0.1 ), group_by_length = kwargs . get ( 'group_by_length' , True ), extra_params = kwargs , use_rewards_directly = kwargs . get ( 'use_rewards_directly' , None ), ), logging = RLLoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' ), loggers = kwargs . get ( 'loggers' , [ \"tensorboard\" ]), sample_logging = sample_logging_config , report_to = kwargs . get ( 'report_to' , \"none\" ), ), rewards = rewards_config , chat_template = kwargs . get ( 'chat_template' , 'auto' ), caching = { 'root' : kwargs . get ( 'cache_dir' , 'cache' ), 'enabled' : kwargs . get ( 'caching_enabled' , True ) }, reward_training = reward_training_config ) # Create backend config algorithm_enum = RLAlgorithm ( algorithm . lower ()) if backend == \"auto\" : backend_config = BackendFactory . get_recommended_backend ( TrainingType . RL , algorithm_enum ) else : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) backend_config = BackendConfig ( TrainingType . RL , backend_type , algorithm_enum ) # Set PURE_TRL_MODE only when TRL backend is being used if backend_config . backend == BackendType . TRL : _disable_unsloth_backend () else : # Clear PURE_TRL_MODE for other backends (especially Unsloth) _enable_unsloth_backend () return BackendFactory . create_trainer ( config , backend_config ) options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train ()","title":"create_rl_trainer()"},{"location":"api-reference/backend-factory/#get_backend_status","text":"Get the availability status of all backends. Get current backend status and environment variables. Source code in src/aligntune/core/backend_factory.py 307 308 309 310 311 312 313 314 315 316 def get_backend_status () -> Dict [ str , Any ]: \"\"\"Get current backend status and environment variables.\"\"\" return { \"pure_trl_mode\" : os . environ . get ( 'PURE_TRL_MODE' , '0' ), \"trl_only_mode\" : os . environ . get ( 'TRL_ONLY_MODE' , '0' ), \"disable_unsloth_for_trl\" : os . environ . get ( 'DISABLE_UNSLOTH_FOR_TRL' , '0' ), \"trl_available\" : TRL_AVAILABLE , \"unsloth_available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"current_mode\" : \"TRL-ONLY\" if os . environ . get ( 'PURE_TRL_MODE' ) == '1' else \"UNSLOTH-ENABLED\" } options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl_available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth_available' ] } \" )","title":"get_backend_status()"},{"location":"api-reference/backend-factory/#list_backends","text":"List all available backends. Source code in src/aligntune/core/backend_factory.py 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 def list_backends () -> Dict [ str , list ]: backends = { \"TRL\" : { \"available\" : TRL_AVAILABLE , \"description\" : \"HuggingFace Transformers Reinforcement Learning library\" , \"status\" : \"\u2705 Available\" if TRL_AVAILABLE else \"\u274c Not Available\" }, \"UNSLOTH\" : { \"available\" : _check_backend_availability ( BackendType . UNSLOTH ), \"description\" : \"Unsloth optimized training with memory efficiency\" , \"status\" : \"\u2705 Available\" if _check_backend_availability ( BackendType . UNSLOTH ) else \"\u274c Not Available\" } } # Print availability status print ( \" \\n \" + \"=\" * 60 ) print ( \"FINETUNEHUB - BACKEND AVAILABILITY\" ) print ( \"=\" * 60 ) for backend_name , info in backends . items (): print ( f \" { backend_name : 10s } : { info [ 'status' ] } \" ) print ( f \" { info [ 'description' ] } \" ) print ( \"=\" * 60 ) print ( \"Note: When TRL is selected, Unsloth is disabled to prevent interference.\" ) print ( \" When Unsloth is selected, TRL-only mode is cleared.\" ) print ( \"=\" * 60 + \" \\n \" ) return backends options: show_source: true heading_level: 3","title":"list_backends()"},{"location":"api-reference/backend-factory/#enums-and-types","text":"","title":"Enums and Types"},{"location":"api-reference/backend-factory/#backendtype-enum","text":"Bases: Enum Backend types for training. Source code in src/aligntune/core/backend_factory.py 202 203 204 205 class BackendType ( Enum ): \"\"\"Backend types for training.\"\"\" UNSLOTH = \"unsloth\" # Unsloth backend (fast, memory efficient) TRL = \"trl\" # TRL backend (standard, reliable) options: show_source: true heading_level: 3","title":"BackendType Enum"},{"location":"api-reference/backend-factory/#trainingtype-enum","text":"Bases: Enum Training types supported by AlignTune. Source code in src/aligntune/core/backend_factory.py 195 196 197 198 199 class TrainingType ( Enum ): \"\"\"Training types supported by AlignTune.\"\"\" SFT = \"sft\" # Supervised Fine-Tuning RL = \"rl\" # Reinforcement Learning options: show_source: true heading_level: 3","title":"TrainingType Enum"},{"location":"api-reference/backend-factory/#rlalgorithm-enum","text":"Bases: Enum RL algorithms supported. Source code in src/aligntune/core/backend_factory.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class RLAlgorithm ( Enum ): \"\"\"RL algorithms supported.\"\"\" DPO = \"dpo\" # Direct Preference Optimization PPO = \"ppo\" # Proximal Policy Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective # New COUNTERFACT_GRPO = \"counterfact_grpo\" # NEW: Add GBMPO GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3","title":"RLAlgorithm Enum"},{"location":"api-reference/backend-factory/#backendconfig-dataclass","text":"Configuration for backend selection with task type support. Source code in src/aligntune/core/backend_factory.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 @dataclass class BackendConfig : \"\"\"Configuration for backend selection with task type support.\"\"\" training_type : TrainingType backend : BackendType algorithm : Optional [ RLAlgorithm ] = None # Only for RL task_type : Optional [ TaskType ] = None # NEW: For SFT tasks fallback_enabled : bool = True def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING","title":"BackendConfig Dataclass"},{"location":"api-reference/backend-factory/#core.backend_factory.BackendConfig.__post_init__","text":"Validate configuration. Source code in src/aligntune/core/backend_factory.py 237 238 239 240 241 242 243 244 245 246 247 248 def __post_init__ ( self ): \"\"\"Validate configuration.\"\"\" if self . training_type == TrainingType . RL and self . algorithm is None : raise ValueError ( \"RL training requires algorithm specification\" ) if self . training_type == TrainingType . SFT and self . algorithm is not None : logger . warning ( \"Algorithm specified for SFT training, ignoring\" ) # NEW: Set default task type for SFT if not specified if self . training_type == TrainingType . SFT and self . task_type is None : logger . info ( \"No task type specified for SFT, using default SUPERVISED_FINE_TUNING\" ) self . task_type = TaskType . SUPERVISED_FINE_TUNING options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/backend-factory/#examples","text":"","title":"Examples"},{"location":"api-reference/backend-factory/#sft-with-trl-backend","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" )","title":"SFT with TRL Backend"},{"location":"api-reference/backend-factory/#sft-with-unsloth-backend","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" )","title":"SFT with Unsloth Backend"},{"location":"api-reference/backend-factory/#dpo-training","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"DPO Training"},{"location":"api-reference/backend-factory/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" )","title":"PPO Training"},{"location":"api-reference/backend-factory/#error-handling","text":"","title":"Error Handling"},{"location":"api-reference/backend-factory/#backend-not-available","text":"try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ValueError as e : print ( f \"Backend not available: { e } \" ) # Falls back to TRL automatically","title":"Backend Not Available"},{"location":"api-reference/backend-factory/#invalid-algorithm","text":"try : trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"invalid\" ) except ValueError as e : print ( f \"Invalid algorithm: { e } \" )","title":"Invalid Algorithm"},{"location":"api-reference/backend-factory/#next-steps","text":"Configuration Classes - Configuration options Trainers - Trainer methods User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/configuration/","text":"Configuration Classes API Reference \u00b6 Complete API reference for AlignTune configuration classes. SFT Configuration \u00b6 SFTConfig \u00b6 Main configuration class for SFT training. Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion Source code in src/aligntune/core/sft/config.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @dataclass class SFTConfig : \"\"\" Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion \"\"\" model : ModelConfig dataset : DatasetConfig train : TrainingConfig = field ( default_factory = TrainingConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) evaluation : EvaluationConfig = field ( default_factory = EvaluationConfig ) def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () def _apply_task_specific_settings ( self ): \"\"\"Apply task-specific configuration settings.\"\"\" task_type = self . dataset . task_type if task_type == TaskType . TEXT_CLASSIFICATION : # Classification tasks should not use Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for text classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 2 # Binary classification default elif task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification also not ideal for Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for token classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 9 # Common for NER elif task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . CHAT_COMPLETION ]: # These tasks benefit from longer sequences if self . model . max_seq_length < 1024 : import logging logger = logging . getLogger ( __name__ ) logger . info ( f \"Task { task_type . value } typically benefits from longer sequences. \" f \"Consider increasing max_seq_length from { self . model . max_seq_length } .\" ) elif task_type == TaskType . TEXT_GENERATION : # Generation tasks can benefit from Unsloth if not self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . info ( \"Text generation tasks can benefit from Unsloth acceleration. \" \"Consider setting use_unsloth=True.\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks __post_init__ () \u00b6 Validate unified configuration and apply task-specific settings. Source code in src/aligntune/core/sft/config.py 341 342 343 344 345 346 347 348 349 350 351 def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () from_dict ( config_dict ) classmethod \u00b6 Create configuration from dictionary. Source code in src/aligntune/core/sft/config.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) get_recommended_backend () \u00b6 Get recommended backend for this task type. Source code in src/aligntune/core/sft/config.py 477 478 479 480 481 482 def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks get_task_type () \u00b6 Get the task type for this configuration. Source code in src/aligntune/core/sft/config.py 452 453 454 def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type is_classification_task () \u00b6 Check if this is a classification task. Source code in src/aligntune/core/sft/config.py 456 457 458 459 460 461 def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] is_generation_task () \u00b6 Check if this is a generation task. Source code in src/aligntune/core/sft/config.py 463 464 465 466 467 468 469 470 def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] supports_unsloth () \u00b6 Check if Unsloth backend is suitable for this task. Source code in src/aligntune/core/sft/config.py 472 473 474 475 def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () to_dict () \u00b6 Convert configuration to dictionary. Source code in src/aligntune/core/sft/config.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" ), train = TrainingConfig ( epochs = 3 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" ) ) ModelConfig (SFT) \u00b6 Model configuration for SFT training. Model configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ModelConfig : \"\"\"Model configuration with task-aware defaults.\"\"\" name_or_path : str precision : PrecisionType = PrecisionType . BF16 quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = True max_memory : Optional [ Dict [ str , str ]] = None use_unsloth : bool = False max_seq_length : int = 2048 peft_enabled : bool = False lora_rank : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.1 target_modules : Optional [ List [ str ]] = None bias : str = \"none\" use_gradient_checkpointing : bool = True # Classification-specific num_labels : Optional [ int ] = None model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) device_map : Optional [ Union [ str , Dict ]] = \"auto\" # Add this line trust_remote_code : bool = True def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) __post_init__ () \u00b6 Validate model configuration. Source code in src/aligntune/core/sft/config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) options: show_source: true heading_level: 3 DatasetConfig (SFT) \u00b6 Dataset configuration for SFT training. Dataset configuration with task-specific field support. Source code in src/aligntune/core/sft/config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @dataclass class DatasetConfig : \"\"\"Dataset configuration with task-specific field support.\"\"\" name : str split : str = \"train\" subset : Optional [ str ] = None # Add this line for dataset config/subset config : Optional [ str ] = None # Alternative name for subset percent : Optional [ float ] = None max_samples : Optional [ int ] = None column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : TaskType = TaskType . SUPERVISED_FINE_TUNING system_prompt : Optional [ str ] = None # Auto-detection auto_detect_fields : bool = False format_type : Optional [ str ] = None # Common fields for all tasks text_column : str = \"text\" # Instruction/SFT specific instruction_column : str = \"instruction\" response_column : str = \"response\" output_column : str = \"output\" input_column : str = \"input\" context_column : str = \"context\" # Classification specific label_column : str = \"label\" # Token classification specific tokens_column : str = \"tokens\" tags_column : str = \"ner_tags\" # Chat specific messages_column : str = \"messages\" # Dataset text field for trainers dataset_text_field : str = \"text\" # Chat template chat_template : Optional [ str ] = None dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None # Processing and Filtering (Added to match RLDatasetConfig and fix factory error) preserve_columns : Optional [ List [ str ]] = None processing_fn : Optional [ Callable ] = None processing_batched : bool = False processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () def _set_task_defaults ( self ): \"\"\"Set task-specific default field names.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : # Ensure we have instruction and response columns if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TEXT_CLASSIFICATION : # Classification needs text and label if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification needs tokens and tags if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . CHAT_COMPLETION : # Chat needs messages field if not self . column_mapping : self . column_mapping = {} __post_init__ () \u00b6 Validate dataset configuration. Source code in src/aligntune/core/sft/config.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () options: show_source: true heading_level: 3 TrainingConfig (SFT) \u00b6 Training configuration for SFT. Training configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @dataclass class TrainingConfig : \"\"\"Training configuration with task-aware defaults.\"\"\" per_device_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None learning_rate : float = 1e-5 weight_decay : float = 0.01 warmup_steps : int = 0 warmup_ratio : float = 0.1 eval_interval : int = 100 save_interval : int = 500 max_grad_norm : float = 1.0 fp16 : bool = False bf16 : bool = False dataloader_num_workers : int = 0 remove_unused_columns : bool = False # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" # Advanced training settings group_by_length : bool = False dataloader_drop_last : bool = False eval_accumulation_steps : Optional [ int ] = None label_smoothing_factor : float = 0.0 # Early stopping early_stopping_patience : Optional [ int ] = None early_stopping_threshold : float = 0.0 # Evaluation load_best_model_at_end : bool = True metric_for_best_model : str = \"eval_loss\" greater_is_better : bool = False # TRL specific use_trl : bool = False dataset_num_proc : Optional [ int ] = None dataset_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Sequence packing and padding packing : bool = False packing_strategy : str = \"bfd\" eval_packing : Optional [ bool ] = None padding_free : bool = False pad_to_multiple_of : Optional [ int ] = None # Loss and masking controls completion_only_loss : Optional [ bool ] = None assistant_only_loss : bool = False loss_type : str = \"nll\" activation_offloading : bool = False use_flash_attention_2 : Optional [ bool ] = None gradient_checkpointing : bool = False # Added gradient_checkpointing_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Added extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) __post_init__ () \u00b6 Validate training configuration. Source code in src/aligntune/core/sft/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) options: show_source: true heading_level: 3 EvaluationConfig (SFT) \u00b6 Evaluation configuration for SFT. Evaluation configuration for SFT models. Source code in src/aligntune/core/sft/config.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @dataclass class EvaluationConfig : \"\"\"Evaluation configuration for SFT models.\"\"\" # Which metrics to compute compute_perplexity : bool = True compute_rouge : bool = True compute_bleu : bool = True compute_meteor : bool = False # Optional: requires nltk compute_bertscore : bool = False # Optional: requires bert-score compute_semantic_similarity : bool = False # Optional: requires sentence-transformers compute_codebleu : bool = False # Optional: for code tasks, requires codebleu # Custom metrics: list of callables taking (prediction, reference, instruction) -> Dict[str, float] custom_metrics : Optional [ List [ Callable [[ str , str , str ], Dict [ str , float ]]]] = None # Evaluation parameters max_samples_for_quality_metrics : int = 50 # Limit samples for faster evaluation bertscore_model : str = \"microsoft/deberta-xlarge-mnli\" # Model for BERTScore semantic_similarity_model : str = \"sentence-transformers/all-MiniLM-L6-v2\" # For semantic similarity def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) __post_init__ () \u00b6 Validate evaluation configuration. Source code in src/aligntune/core/sft/config.py 297 298 299 300 def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) options: show_source: true heading_level: 3 LoggingConfig (SFT) \u00b6 Logging configuration for SFT. Logging configuration. Source code in src/aligntune/core/sft/config.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" output_dir : str = \"./output\" run_name : Optional [ str ] = None loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) log_level : str = \"INFO\" log_interval : int = 10 save_strategy : str = \"steps\" eval_strategy : str = \"steps\" report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) __post_init__ () \u00b6 Validate logging configuration. Source code in src/aligntune/core/sft/config.py 315 316 317 318 319 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3 TaskType Enum (SFT) \u00b6 Task type enumeration for SFT. Bases: Enum Supported SFT task types. Source code in src/aligntune/core/sft/config.py 16 17 18 19 20 21 22 23 class TaskType ( Enum ): \"\"\"Supported SFT task types.\"\"\" INSTRUCTION_FOLLOWING = \"instruction_following\" SUPERVISED_FINE_TUNING = \"supervised_fine_tuning\" TEXT_CLASSIFICATION = \"text_classification\" TOKEN_CLASSIFICATION = \"token_classification\" TEXT_GENERATION = \"text_generation\" CHAT_COMPLETION = \"chat_completion\" options: show_source: true heading_level: 3 PrecisionType Enum \u00b6 Model precision type enumeration. Bases: Enum Model precision types. Source code in src/aligntune/core/sft/config.py 32 33 34 35 36 37 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3 RL Configuration \u00b6 UnifiedConfig \u00b6 Main configuration class for RL training. Unified configuration for RLHF training with no placeholder defaults. Source code in src/aligntune/core/rl/config.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 @dataclass class UnifiedConfig : \"\"\"Unified configuration for RLHF training with no placeholder defaults.\"\"\" algo : AlgorithmType model : ModelConfig datasets : List [ DatasetConfig ] tasks : List [ Dict [ str , Any ]] = field ( default_factory = list ) rewards : List [ RewardConfig ] = field ( default_factory = list ) train : TrainingConfig = field ( default_factory = TrainingConfig ) distributed : DistributedConfig = field ( default_factory = DistributedConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) chat_template : Optional [ str ] = None caching : Dict [ str , Any ] = field ( default_factory = dict ) reward_training : Optional [ RewardModelTrainingConfig ] = None def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} __post_init__ () \u00b6 Validate unified configuration. Source code in src/aligntune/core/rl/config.py 705 706 707 708 709 710 711 def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) to_dict () \u00b6 Convert configuration to dictionary. Source code in src/aligntune/core/rl/config.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" )], train = TrainingConfig ( epochs = 1 , learning_rate = 1e-6 ), logging = LoggingConfig ( output_dir = \"./output\" ) ) ModelConfig (RL) \u00b6 Model configuration for RL training. Model configuration with no default model names. Source code in src/aligntune/core/rl/config.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @dataclass class ModelConfig : \"\"\"Model configuration with no default model names.\"\"\" name_or_path : str backend : str = \"trl\" # Backend: \"trl\" or \"unsloth\" sft_path : Optional [ str ] = None reward_path : Optional [ str ] = None reward_model_name : Optional [ str ] = None # NEW: Separate reward model reward_model_source : Optional [ 'RewardModelSourceConfig' ] = None # NEW: Reward model source configuration precision : PrecisionType = PrecisionType . AUTO quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = False max_memory : Optional [ Dict [ str , str ]] = None device_map : Optional [ Union [ str , Dict [ str , int ]]] = None # Added device_map use_unsloth : bool = False # Enable Unsloth acceleration max_seq_length : int = 2048 # For Unsloth reward_value_model : str = None # Llama model for reward/value (loaded pre-Unsloth to avoid patches) reward_value_loading_type : Optional [ str ] = None # 'unsloth' or 'standard' reward_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) value_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) use_peft : bool = True # Enable PEFT (LoRA) by default lora_r : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.05 lora_target_modules : List [ str ] = field ( default_factory = lambda : [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]) trust_remote_code : bool = True # Trust remote code for model loading # New parameters from model initialization model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) ref_model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) model_adapter_name : Optional [ str ] = None ref_adapter_name : Optional [ str ] = None force_use_ref_model : bool = False disable_dropout : bool = True use_logits_to_keep : bool = False reward_device : str = \"auto\" def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized __post_init__ () \u00b6 Validate model configuration. Source code in src/aligntune/core/rl/config.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized options: show_source: true heading_level: 3 DatasetConfig (RL) \u00b6 Dataset configuration for RL training. Dataset configuration with flexible field mapping support. Source code in src/aligntune/core/rl/config.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @dataclass class DatasetConfig : \"\"\"Dataset configuration with flexible field mapping support.\"\"\" name : str split : str = \"train\" percent : Optional [ float ] = None max_samples : Optional [ int ] = None max_eval_samples : Optional [ int ] = None # Enhanced field mapping system field_mappings : Dict [ str , str ] = field ( default_factory = dict ) format_type : Optional [ str ] = None # \"alpaca\", \"dolly\", \"ultrachat\", \"custom\" auto_detect_fields : bool = True # Legacy support column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : str = \"conversation\" weight : float = 1.0 chat_template : Optional [ str ] = None system_prompt : Optional [ str ] = None # New parameters from dataset processing dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None label_pad_token_id : int = - 100 truncation_mode : str = 'keep_end' padding_free : bool = False precompute_ref_log_probs : bool = False precompute_ref_batch_size : Optional [ int ] = None tools : Optional [ Any ] = None # ADD THESE NEW PARAMETERS preserve_columns : Optional [ List [ str ]] = None # NEW: Columns to preserve processing_fn : Optional [ Any ] = None # NEW: Custom processing function processing_batched : bool = False # NEW: Whether processing is batched processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # NEW: Args for processing_fn config_name : Optional [ str ] = None def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" ) __post_init__ () \u00b6 Validate dataset configuration. Source code in src/aligntune/core/rl/config.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" ) to_dict () \u00b6 Convert to dictionary. Source code in src/aligntune/core/rl/config.py 157 158 159 160 161 162 163 164 165 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 TrainingConfig (RL) \u00b6 Training configuration for RL. Training configuration. Source code in src/aligntune/core/rl/config.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 @dataclass class TrainingConfig : \"\"\"Training configuration.\"\"\" # Basic training parameters per_device_batch_size : int = 1 per_device_eval_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None eval_interval : int = 100 save_interval : int = 500 learning_rate : float = 1e-5 max_grad_norm : float = 1.0 weight_decay : float = 0.01 use_cache : bool = False , # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" warmup_steps : int = 0 # If 0, calculated from warmup_ratio or defaults to 5% of max_steps warmup_ratio : float = 0.0 # Alternative to warmup_steps # PPO/RL-specific parameters rollout_batch_size : int = 1 kl_coef : float = 0.1 cliprange : float = 0.2 cliprange_value : float = 0.2 num_ppo_epochs : Optional [ int ] = None temperature : float = 0.6 whiten_rewards : bool = False kl_estimator : str = 'k1' vf_coef : float = 0.1 gamma : float = 1.0 lam : float = 0.95 # Generation parameters response_length : int = 128 stop_token : str = 'eos' missing_eos_penalty : float = 1.0 ds3_gather_for_generation : bool = True generation_kwargs : dict = None # Length parameters max_length : int = 1024 max_prompt_length : int = 512 max_target_length : Optional [ int ] = None max_completion_length : int = 256 # Generation sampling parameters top_p : float = 0.95 # Processing parameters padding_free : bool = False truncation_mode : str = 'keep_end' # DPO-specific parameters beta : float = 0.1 loss_type : str = None loss_weights : Optional [ dict ] = None f_divergence_type : str = 'reverse_kl' f_alpha_divergence_coef : float = 1.0 reference_free : bool = False label_smoothing : float = 0.0 use_weighting : bool = False # Algorithm-specific parameters rpo_alpha : Optional [ float ] = None ld_alpha : Optional [ float ] = None discopop_tau : float = 0.05 # Reference model parameters sync_ref_model : bool = False ref_model_mixup_alpha : float = 0.6 ref_model_sync_steps : int = 512 # GRPO-specific parameters grpo_alpha : float = 0.1 grpo_beta : float = 0.1 # GSPO-specific parameters gspo_gamma : float = 0.1 gspo_delta : float = 0.1 # Evaluation parameters eval_steps : Optional [ int ] = 100 eval_strategy : str = \"no\" # DPO-specific evaluation options dpo_eval_enabled : bool = False dpo_eval_max_samples : Optional [ int ] = None dpo_zero_shot_max_samples : int = 50 dpo_few_shot_max_samples : int = 30 dpo_few_shot_examples_text : Optional [ str ] = None # Checkpointing parameters save_steps : int = 500 save_strategy : str = \"steps\" save_total_limit : Optional [ int ] = None load_best_model_at_end : bool = False metric_for_best_model : Optional [ str ] = None greater_is_better : bool = False # Logging parameters logging_steps : int = 10 logging_strategy : str = \"steps\" # Generation parameters (GRPO-specific) num_generations : Optional [ int ] = None mask_truncated_completions : bool = True scale_rewards : str = \"group\" reward_weights : Optional [ List [ float ]] = None enable_thinking : bool = False # Qwen3 thinking mode fast_inference : bool = False # Unsloth vLLM fast inference (2-3x faster generation) vllm_gpu_memory_utilization : float = 0.7 # vLLM GPU memory (0.95 for max speed with spare VRAM) # Seed parameters seed : Optional [ int ] = 42 data_seed : Optional [ int ] = 47 # Match training_script.py for fair comparison # Meta-ES specific meta_iterations : int = 15 patience : int = 5 min_delta : float = 0.001 init_scale : float = 0.01 N : int = 10 # Population size (must be even) T : int = 100 # Training steps per evaluation sigma : float = 0.01 # ES noise std sigma_decay : float = 0.99 # ES sigma decay per iteration alpha : float = 0.01 # ES learning rate mirror_coefficient : float = 0.0001 debug_mode : bool = False eval_timeout : int = 5 eval_max_tokens : int = 512 eval_k : int = 1 eval_temperature : float = 0.8 num_workers : int = 1 no_wandb : bool = False wandb_project : str = \"neural-mirror-es\" resume : Optional [ str ] = None use_rewards_directly : Optional [ List [ callable ]] = None # ============================================================================ # NEW: Counterfactual GRPO-specific parameters # ============================================================================ boost_factor : float = 2.0 \"\"\"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 \"\"\" min_weight : float = 0.5 \"\"\"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 \"\"\" max_spans : int = 10 \"\"\"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 \"\"\" answer_weight : float = 1.5 \"\"\"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 \"\"\" weighting_mode : Optional [ str ] = None \"\"\"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. \"\"\" method_name : str = \"counterfactual\" \"\"\"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) \"\"\" random_importance : bool = False \"\"\"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. \"\"\" invert_importance : bool = False \"\"\"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. \"\"\" enable_gradient_conservation : bool = True \"\"\"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) \"\"\" weight_debug : bool = False \"\"\"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). \"\"\" # ============================================================================ # NEW: GBMPO-specific parameters (Generalized Bregman Mirror Descent) # ============================================================================ gbmpo_l2_coefficient : float = 0.0001 \"\"\"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 \"\"\" gbmpo_divergence_type : Optional [ str ] = None \"\"\"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. \"\"\" gbmpo_epsilon : float = 0.2 \"\"\"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 \"\"\" # Performance optimization use_liger_kernel : bool = False use_liger_loss : Optional [ bool ] = None # BOLT-specific parameters (Baseline-Optimized Learning Technique) curriculum_enabled : bool = False # Enable uncertainty-based sampling curriculum_epsilon : float = 0.05 # Floor for sampling weights curriculum_update_freq : int = 10 # Steps between weight updates baseline_enabled : bool = False # Enable persistent baseline tracking baseline_rho_min : float = 0.875 # Min forgetting factor (fast adaptation) baseline_rho_max : float = 0.96 # Max forgetting factor (slow adaptation) baseline_D_half : float = 0.5 # KL half-life for adaptive forgetting baseline_warm_start : Optional [ str ] = None # Path to JSON/PKL for warm-start use_baseline_advantages : bool = False # Use A = r - v\u0302(x) vs group mean group_by_length : bool = True extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) answer_weight = 1.5 class-attribute instance-attribute \u00b6 Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 boost_factor = 2.0 class-attribute instance-attribute \u00b6 Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 enable_gradient_conservation = True class-attribute instance-attribute \u00b6 Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) gbmpo_divergence_type = None class-attribute instance-attribute \u00b6 Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. gbmpo_epsilon = 0.2 class-attribute instance-attribute \u00b6 PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 gbmpo_l2_coefficient = 0.0001 class-attribute instance-attribute \u00b6 L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 invert_importance = False class-attribute instance-attribute \u00b6 Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. max_spans = 10 class-attribute instance-attribute \u00b6 Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 method_name = 'counterfactual' class-attribute instance-attribute \u00b6 Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) min_weight = 0.5 class-attribute instance-attribute \u00b6 Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 random_importance = False class-attribute instance-attribute \u00b6 Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. weight_debug = False class-attribute instance-attribute \u00b6 Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). weighting_mode = None class-attribute instance-attribute \u00b6 Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. __post_init__ () \u00b6 Validate training configuration. Source code in src/aligntune/core/rl/config.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) options: show_source: true heading_level: 3 RewardConfig \u00b6 Reward function configuration. Reward function configuration. Source code in src/aligntune/core/rl/config.py 182 183 184 185 186 187 188 189 190 @dataclass class RewardConfig : \"\"\"Reward function configuration.\"\"\" type : str weight : float = 1.0 params : Dict [ str , Any ] = field ( default_factory = dict ) shield : bool = False clip : Optional [ float ] = None normalize : bool = False options: show_source: true heading_level: 3 RewardModelSourceConfig \u00b6 Reward model source configuration. Reward model source - EXACTLY ONE must be specified. Source code in src/aligntune/core/rl/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 @dataclass class RewardModelSourceConfig : \"\"\"Reward model source - EXACTLY ONE must be specified.\"\"\" source_type : str # Must be set explicitly model_name : Optional [ str ] = None model_path : Optional [ str ] = None training_config : Optional [ RewardModelTrainingConfig ] = None fine_tune_with_rewards : bool = False # NEW: Enable hybrid mode to fine-tune pretrained with reward functions def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result __post_init__ () \u00b6 Validate source configuration with strict mutual exclusivity. Source code in src/aligntune/core/rl/config.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) to_dict () \u00b6 Convert to dictionary. Source code in src/aligntune/core/rl/config.py 288 289 290 291 292 293 294 295 296 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 RewardModelTrainingConfig \u00b6 Reward model training configuration. Configuration for reward model training - ALL fields validated. Source code in src/aligntune/core/rl/config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @dataclass class RewardModelTrainingConfig : \"\"\"Configuration for reward model training - ALL fields validated.\"\"\" base_model_name : str # NO default - must be explicit training_texts : List [ str ] # NO default - must provide data reward_functions : List [ str ] # NO default - must specify output_dir : str # NO default - must specify where to save # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training params with JUSTIFIED defaults num_epochs : int = 3 # Standard for reward model training learning_rate : float = 1e-5 # Standard LM fine-tuning rate batch_size : int = 8 # Balanced for memory/speed gradient_accumulation_steps : int = 4 # For effective batch size 32 max_length : int = 512 # Standard transformer length def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) __post_init__ () \u00b6 Validate ALL fields with strict validation. Source code in src/aligntune/core/rl/config.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) options: show_source: true heading_level: 3 LoggingConfig (RL) \u00b6 Logging configuration for RL. Logging configuration. Source code in src/aligntune/core/rl/config.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) run_name : Optional [ str ] = None output_dir : str = \"./output\" log_level : str = \"INFO\" sample_logging : SampleLoggingConfig = field ( default_factory = SampleLoggingConfig ) report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) __post_init__ () \u00b6 Validate logging configuration. Source code in src/aligntune/core/rl/config.py 658 659 660 661 662 663 664 665 666 667 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3 SampleLoggingConfig \u00b6 Sample logging configuration for qualitative generation. Configuration for qualitative sample logging. Source code in src/aligntune/core/rl/config.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 @dataclass class SampleLoggingConfig : \"\"\"Configuration for qualitative sample logging.\"\"\" enabled : bool = False prompts : Optional [ List [ str ]] = None interval_steps : Optional [ int ] = None percent_of_max_steps : Optional [ float ] = None max_new_tokens : int = 80 temperature : float = 0.6 top_p : float = 0.9 num_samples : int = 3 def __post_init__ ( self ): if self . interval_steps is not None and self . interval_steps <= 0 : raise ValueError ( \"sample_logging.interval_steps must be positive when set\" ) if self . percent_of_max_steps is not None : if self . percent_of_max_steps <= 0 or self . percent_of_max_steps > 1 : raise ValueError ( \"sample_logging.percent_of_max_steps must be in (0, 1]\" ) if self . max_new_tokens <= 0 : raise ValueError ( \"sample_logging.max_new_tokens must be positive\" ) if not ( 0 < self . temperature ): raise ValueError ( \"sample_logging.temperature must be positive\" ) if not ( 0 < self . top_p <= 1 ): raise ValueError ( \"sample_logging.top_p must be in (0, 1]\" ) if self . num_samples <= 0 : raise ValueError ( \"sample_logging.num_samples must be positive\" ) options: show_source: true heading_level: 3 DistributedConfig \u00b6 Distributed training configuration. Distributed training configuration. Source code in src/aligntune/core/rl/config.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 @dataclass class DistributedConfig : \"\"\"Distributed training configuration.\"\"\" backend : BackendType = BackendType . SINGLE fsdp_config : Dict [ str , Any ] = field ( default_factory = dict ) deepspeed_config : Dict [ str , Any ] = field ( default_factory = dict ) nodes : int = 1 gpus_per_node : int = 1 seed : int = 42 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) __post_init__ () \u00b6 Validate distributed configuration. Source code in src/aligntune/core/rl/config.py 605 606 607 608 609 610 611 612 613 614 615 616 617 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) options: show_source: true heading_level: 3 AlgorithmType Enum \u00b6 RL algorithm type enumeration. Bases: Enum Supported RLHF algorithms. Source code in src/aligntune/core/rl/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 class AlgorithmType ( Enum ): \"\"\"Supported RLHF algorithms.\"\"\" PPO = \"ppo\" DPO = \"dpo\" GRPO = \"grpo\" GSPO = \"gspo\" COUNTERFACT_GRPO = \"counterfact_grpo\" GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3 PrecisionType Enum (RL) \u00b6 Model precision type enumeration for RL. Bases: Enum Model precision types. Source code in src/aligntune/core/rl/config.py 30 31 32 33 34 35 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3 BackendType Enum (RL) \u00b6 Distributed training backend enumeration. Bases: Enum Distributed training backends. Source code in src/aligntune/core/rl/config.py 38 39 40 41 42 43 class BackendType ( Enum ): \"\"\"Distributed training backends.\"\"\" SINGLE = \"single\" DDP = \"ddp\" FSDP = \"fsdp\" DEEPSPEED = \"deepspeed\" options: show_source: true heading_level: 3 Configuration Loaders \u00b6 SFTConfigLoader \u00b6 Configuration loader for SFT configs. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config load_from_yaml ( path ) staticmethod \u00b6 Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config save_config ( config , path ) staticmethod \u00b6 Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) validate_config ( config ) staticmethod \u00b6 Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 ConfigLoader (RL) \u00b6 Configuration loader for RL configs. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) create_config_from_cli_args ( ** kwargs ) staticmethod \u00b6 Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) load_from_yaml ( path ) staticmethod \u00b6 Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) parse_dataset_spec ( spec ) staticmethod \u00b6 Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) parse_reward_spec ( spec ) staticmethod \u00b6 Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) save_resolved_config ( config , output_dir ) staticmethod \u00b6 Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) validate_config ( config ) staticmethod \u00b6 Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Helper Functions \u00b6 create_instruction_following_config() \u00b6 Create configuration for instruction following. Create configuration for instruction following tasks. Source code in src/aligntune/core/sft/config.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def create_instruction_following_config ( model_name : str , dataset_name : str , output_dir : str = \"./output/instruction\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for instruction following tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 1024 ), use_unsloth = kwargs . get ( 'use_unsloth' , True ), peft_enabled = kwargs . get ( 'peft_enabled' , True ), quantization = kwargs . get ( 'quantization' , { \"load_in_4bit\" : True }) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . INSTRUCTION_FOLLOWING , instruction_column = kwargs . get ( 'instruction_column' , 'instruction' ), response_column = kwargs . get ( 'response_column' , 'output' ), context_column = kwargs . get ( 'context_column' , 'input' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 4 ), learning_rate = kwargs . get ( 'learning_rate' , 2e-4 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'instruction_following' ) ) ) options: show_source: true heading_level: 3 create_text_classification_config() \u00b6 Create configuration for text classification. Create configuration for text classification tasks. Source code in src/aligntune/core/sft/config.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def create_text_classification_config ( model_name : str , dataset_name : str , num_labels : int , output_dir : str = \"./output/classification\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for text classification tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 512 ), use_unsloth = False , # Not recommended for classification peft_enabled = kwargs . get ( 'peft_enabled' , True ), num_labels = num_labels , quantization = kwargs . get ( 'quantization' , {}) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . TEXT_CLASSIFICATION , text_column = kwargs . get ( 'text_column' , 'text' ), label_column = kwargs . get ( 'label_column' , 'label' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 8 ), learning_rate = kwargs . get ( 'learning_rate' , 5e-5 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'text_classification' ) ) ) options: show_source: true heading_level: 3 Usage Examples \u00b6 Creating SFT Configuration \u00b6 from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" , precision = \"bf16\" , max_seq_length = 512 ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" , max_samples = 1000 , task_type = \"instruction_following\" ), train = TrainingConfig ( epochs = 3 , per_device_batch_size = 4 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_sft_training\" ) ) Creating RL Configuration \u00b6 from aligntune.core.rl.config import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig ) config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , use_peft = True , lora_r = 16 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , max_samples = 1000 ) ], train = TrainingConfig ( epochs = 1 , per_device_batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 # DPO parameter ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_dpo_training\" ) ) Loading from YAML \u00b6 from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) Next Steps \u00b6 Backend Factory - Trainer creation Trainers - Trainer classes User Guide - Usage guide","title":"Configuration Classes"},{"location":"api-reference/configuration/#configuration-classes-api-reference","text":"Complete API reference for AlignTune configuration classes.","title":"Configuration Classes API Reference"},{"location":"api-reference/configuration/#sft-configuration","text":"","title":"SFT Configuration"},{"location":"api-reference/configuration/#sftconfig","text":"Main configuration class for SFT training. Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion Source code in src/aligntune/core/sft/config.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @dataclass class SFTConfig : \"\"\" Unified configuration for SFT training with complete task type support. This config supports all task types: - Instruction Following - Supervised Fine-Tuning - Text Classification - Token Classification - Text Generation - Chat Completion \"\"\" model : ModelConfig dataset : DatasetConfig train : TrainingConfig = field ( default_factory = TrainingConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) evaluation : EvaluationConfig = field ( default_factory = EvaluationConfig ) def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings () def _apply_task_specific_settings ( self ): \"\"\"Apply task-specific configuration settings.\"\"\" task_type = self . dataset . task_type if task_type == TaskType . TEXT_CLASSIFICATION : # Classification tasks should not use Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for text classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 2 # Binary classification default elif task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification also not ideal for Unsloth if self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"Unsloth is not recommended for token classification. \" \"Consider using TRL backend.\" ) # Ensure num_labels is set if self . model . num_labels is None : self . model . num_labels = 9 # Common for NER elif task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . CHAT_COMPLETION ]: # These tasks benefit from longer sequences if self . model . max_seq_length < 1024 : import logging logger = logging . getLogger ( __name__ ) logger . info ( f \"Task { task_type . value } typically benefits from longer sequences. \" f \"Consider increasing max_seq_length from { self . model . max_seq_length } .\" ) elif task_type == TaskType . TEXT_GENERATION : # Generation tasks can benefit from Unsloth if not self . model . use_unsloth : import logging logger = logging . getLogger ( __name__ ) logger . info ( \"Text generation tasks can benefit from Unsloth acceleration. \" \"Consider setting use_unsloth=True.\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config ) def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task () def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks","title":"SFTConfig"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.__post_init__","text":"Validate unified configuration and apply task-specific settings. Source code in src/aligntune/core/sft/config.py 341 342 343 344 345 346 347 348 349 350 351 def __post_init__ ( self ): \"\"\"Validate unified configuration and apply task-specific settings.\"\"\" # Validate required fields if not self . model . name_or_path : raise ValueError ( \"Model name_or_path is required\" ) if not self . dataset . name : raise ValueError ( \"Dataset name is required\" ) # Apply task-specific validation and defaults self . _apply_task_specific_settings ()","title":"__post_init__"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.from_dict","text":"Create configuration from dictionary. Source code in src/aligntune/core/sft/config.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ]) -> 'SFTConfig' : \"\"\"Create configuration from dictionary.\"\"\" # Extract nested configs model_dict = config_dict . get ( 'model' , {}) dataset_dict = config_dict . get ( 'dataset' , {}) train_dict = config_dict . get ( 'train' , {}) logging_dict = config_dict . get ( 'logging' , {}) # Convert enums if 'precision' in model_dict and isinstance ( model_dict [ 'precision' ], str ): model_dict [ 'precision' ] = PrecisionType ( model_dict [ 'precision' ]) if 'task_type' in dataset_dict and isinstance ( dataset_dict [ 'task_type' ], str ): dataset_dict [ 'task_type' ] = TaskType ( dataset_dict [ 'task_type' ]) # Create config objects model_config = ModelConfig ( ** model_dict ) dataset_config = DatasetConfig ( ** dataset_dict ) train_config = TrainingConfig ( ** train_dict ) logging_config = LoggingConfig ( ** logging_dict ) return cls ( model = model_config , dataset = dataset_config , train = train_config , logging = logging_config )","title":"from_dict"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.get_recommended_backend","text":"Get recommended backend for this task type. Source code in src/aligntune/core/sft/config.py 477 478 479 480 481 482 def get_recommended_backend ( self ) -> str : \"\"\"Get recommended backend for this task type.\"\"\" if self . is_classification_task (): return \"trl\" # TRL for classification else : return \"unsloth\" # Unsloth for generation tasks","title":"get_recommended_backend"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.get_task_type","text":"Get the task type for this configuration. Source code in src/aligntune/core/sft/config.py 452 453 454 def get_task_type ( self ) -> TaskType : \"\"\"Get the task type for this configuration.\"\"\" return self . dataset . task_type","title":"get_task_type"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.is_classification_task","text":"Check if this is a classification task. Source code in src/aligntune/core/sft/config.py 456 457 458 459 460 461 def is_classification_task ( self ) -> bool : \"\"\"Check if this is a classification task.\"\"\" return self . dataset . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]","title":"is_classification_task"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.is_generation_task","text":"Check if this is a generation task. Source code in src/aligntune/core/sft/config.py 463 464 465 466 467 468 469 470 def is_generation_task ( self ) -> bool : \"\"\"Check if this is a generation task.\"\"\" return self . dataset . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]","title":"is_generation_task"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.supports_unsloth","text":"Check if Unsloth backend is suitable for this task. Source code in src/aligntune/core/sft/config.py 472 473 474 475 def supports_unsloth ( self ) -> bool : \"\"\"Check if Unsloth backend is suitable for this task.\"\"\" # Unsloth works best with generation tasks return self . is_generation_task ()","title":"supports_unsloth"},{"location":"api-reference/configuration/#core.sft.config.SFTConfig.to_dict","text":"Convert configuration to dictionary. Source code in src/aligntune/core/sft/config.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value elif hasattr ( field_value , '__dict__' ): nested_dict = {} for nested_name , nested_value in field_value . __dict__ . items (): if isinstance ( nested_value , Enum ): nested_dict [ nested_name ] = nested_value . value else : nested_dict [ nested_name ] = nested_value result [ field_name ] = nested_dict else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" ), train = TrainingConfig ( epochs = 3 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" ) )","title":"to_dict"},{"location":"api-reference/configuration/#modelconfig-sft","text":"Model configuration for SFT training. Model configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 @dataclass class ModelConfig : \"\"\"Model configuration with task-aware defaults.\"\"\" name_or_path : str precision : PrecisionType = PrecisionType . BF16 quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = True max_memory : Optional [ Dict [ str , str ]] = None use_unsloth : bool = False max_seq_length : int = 2048 peft_enabled : bool = False lora_rank : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.1 target_modules : Optional [ List [ str ]] = None bias : str = \"none\" use_gradient_checkpointing : bool = True # Classification-specific num_labels : Optional [ int ] = None model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) device_map : Optional [ Union [ str , Dict ]] = \"auto\" # Add this line trust_remote_code : bool = True def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" )","title":"ModelConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.ModelConfig.__post_init__","text":"Validate model configuration. Source code in src/aligntune/core/sft/config.py 72 73 74 75 76 77 78 79 80 81 82 83 84 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" if not self . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) if not isinstance ( self . precision , PrecisionType ): if self . precision is None : # Use default if None is explicitly passed self . precision = PrecisionType . BF16 elif isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#datasetconfig-sft","text":"Dataset configuration for SFT training. Dataset configuration with task-specific field support. Source code in src/aligntune/core/sft/config.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @dataclass class DatasetConfig : \"\"\"Dataset configuration with task-specific field support.\"\"\" name : str split : str = \"train\" subset : Optional [ str ] = None # Add this line for dataset config/subset config : Optional [ str ] = None # Alternative name for subset percent : Optional [ float ] = None max_samples : Optional [ int ] = None column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : TaskType = TaskType . SUPERVISED_FINE_TUNING system_prompt : Optional [ str ] = None # Auto-detection auto_detect_fields : bool = False format_type : Optional [ str ] = None # Common fields for all tasks text_column : str = \"text\" # Instruction/SFT specific instruction_column : str = \"instruction\" response_column : str = \"response\" output_column : str = \"output\" input_column : str = \"input\" context_column : str = \"context\" # Classification specific label_column : str = \"label\" # Token classification specific tokens_column : str = \"tokens\" tags_column : str = \"ner_tags\" # Chat specific messages_column : str = \"messages\" # Dataset text field for trainers dataset_text_field : str = \"text\" # Chat template chat_template : Optional [ str ] = None dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None # Processing and Filtering (Added to match RLDatasetConfig and fix factory error) preserve_columns : Optional [ List [ str ]] = None processing_fn : Optional [ Callable ] = None processing_batched : bool = False processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () def _set_task_defaults ( self ): \"\"\"Set task-specific default field names.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : # Ensure we have instruction and response columns if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TEXT_CLASSIFICATION : # Classification needs text and label if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Token classification needs tokens and tags if not self . column_mapping : self . column_mapping = {} elif self . task_type == TaskType . CHAT_COMPLETION : # Chat needs messages field if not self . column_mapping : self . column_mapping = {}","title":"DatasetConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.DatasetConfig.__post_init__","text":"Validate dataset configuration. Source code in src/aligntune/core/sft/config.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if not isinstance ( self . task_type , TaskType ): if isinstance ( self . task_type , str ): self . task_type = TaskType ( self . task_type ) else : raise ValueError ( f \"Invalid task type: { self . task_type } \" ) # Normalize subset/config (they're the same thing) if self . config and not self . subset : self . subset = self . config elif self . subset and not self . config : self . config = self . subset # Set task-specific defaults self . _set_task_defaults () options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#trainingconfig-sft","text":"Training configuration for SFT. Training configuration with task-aware defaults. Source code in src/aligntune/core/sft/config.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 @dataclass class TrainingConfig : \"\"\"Training configuration with task-aware defaults.\"\"\" per_device_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None learning_rate : float = 1e-5 weight_decay : float = 0.01 warmup_steps : int = 0 warmup_ratio : float = 0.1 eval_interval : int = 100 save_interval : int = 500 max_grad_norm : float = 1.0 fp16 : bool = False bf16 : bool = False dataloader_num_workers : int = 0 remove_unused_columns : bool = False # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" # Advanced training settings group_by_length : bool = False dataloader_drop_last : bool = False eval_accumulation_steps : Optional [ int ] = None label_smoothing_factor : float = 0.0 # Early stopping early_stopping_patience : Optional [ int ] = None early_stopping_threshold : float = 0.0 # Evaluation load_best_model_at_end : bool = True metric_for_best_model : str = \"eval_loss\" greater_is_better : bool = False # TRL specific use_trl : bool = False dataset_num_proc : Optional [ int ] = None dataset_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Sequence packing and padding packing : bool = False packing_strategy : str = \"bfd\" eval_packing : Optional [ bool ] = None padding_free : bool = False pad_to_multiple_of : Optional [ int ] = None # Loss and masking controls completion_only_loss : Optional [ bool ] = None assistant_only_loss : bool = False loss_type : str = \"nll\" activation_offloading : bool = False use_flash_attention_2 : Optional [ bool ] = None gradient_checkpointing : bool = False # Added gradient_checkpointing_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # Added extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" )","title":"TrainingConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.TrainingConfig.__post_init__","text":"Validate training configuration. Source code in src/aligntune/core/sft/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) # Set default epochs if max_steps not specified if self . epochs is None and self . max_steps is None : self . epochs = 3 if self . dataset_num_proc is not None and self . dataset_num_proc <= 0 : raise ValueError ( \"dataset_num_proc must be positive\" ) if self . pad_to_multiple_of is not None and self . pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of must be positive when set\" ) if self . packing_strategy not in { \"bfd\" , \"wrapped\" }: raise ValueError ( \"packing_strategy must be 'bfd' or 'wrapped'\" ) if self . loss_type not in { \"nll\" , \"dft\" }: raise ValueError ( \"loss_type must be 'nll' or 'dft'\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#evaluationconfig-sft","text":"Evaluation configuration for SFT. Evaluation configuration for SFT models. Source code in src/aligntune/core/sft/config.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 @dataclass class EvaluationConfig : \"\"\"Evaluation configuration for SFT models.\"\"\" # Which metrics to compute compute_perplexity : bool = True compute_rouge : bool = True compute_bleu : bool = True compute_meteor : bool = False # Optional: requires nltk compute_bertscore : bool = False # Optional: requires bert-score compute_semantic_similarity : bool = False # Optional: requires sentence-transformers compute_codebleu : bool = False # Optional: for code tasks, requires codebleu # Custom metrics: list of callables taking (prediction, reference, instruction) -> Dict[str, float] custom_metrics : Optional [ List [ Callable [[ str , str , str ], Dict [ str , float ]]]] = None # Evaluation parameters max_samples_for_quality_metrics : int = 50 # Limit samples for faster evaluation bertscore_model : str = \"microsoft/deberta-xlarge-mnli\" # Model for BERTScore semantic_similarity_model : str = \"sentence-transformers/all-MiniLM-L6-v2\" # For semantic similarity def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" )","title":"EvaluationConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.EvaluationConfig.__post_init__","text":"Validate evaluation configuration. Source code in src/aligntune/core/sft/config.py 297 298 299 300 def __post_init__ ( self ): \"\"\"Validate evaluation configuration.\"\"\" if self . max_samples_for_quality_metrics < 1 : raise ValueError ( \"max_samples_for_quality_metrics must be >= 1\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#loggingconfig-sft","text":"Logging configuration for SFT. Logging configuration. Source code in src/aligntune/core/sft/config.py 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" output_dir : str = \"./output\" run_name : Optional [ str ] = None loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) log_level : str = \"INFO\" log_interval : int = 10 save_strategy : str = \"steps\" eval_strategy : str = \"steps\" report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" )","title":"LoggingConfig (SFT)"},{"location":"api-reference/configuration/#core.sft.config.LoggingConfig.__post_init__","text":"Validate logging configuration. Source code in src/aligntune/core/sft/config.py 315 316 317 318 319 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#tasktype-enum-sft","text":"Task type enumeration for SFT. Bases: Enum Supported SFT task types. Source code in src/aligntune/core/sft/config.py 16 17 18 19 20 21 22 23 class TaskType ( Enum ): \"\"\"Supported SFT task types.\"\"\" INSTRUCTION_FOLLOWING = \"instruction_following\" SUPERVISED_FINE_TUNING = \"supervised_fine_tuning\" TEXT_CLASSIFICATION = \"text_classification\" TOKEN_CLASSIFICATION = \"token_classification\" TEXT_GENERATION = \"text_generation\" CHAT_COMPLETION = \"chat_completion\" options: show_source: true heading_level: 3","title":"TaskType Enum (SFT)"},{"location":"api-reference/configuration/#precisiontype-enum","text":"Model precision type enumeration. Bases: Enum Model precision types. Source code in src/aligntune/core/sft/config.py 32 33 34 35 36 37 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3","title":"PrecisionType Enum"},{"location":"api-reference/configuration/#rl-configuration","text":"","title":"RL Configuration"},{"location":"api-reference/configuration/#unifiedconfig","text":"Main configuration class for RL training. Unified configuration for RLHF training with no placeholder defaults. Source code in src/aligntune/core/rl/config.py 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 @dataclass class UnifiedConfig : \"\"\"Unified configuration for RLHF training with no placeholder defaults.\"\"\" algo : AlgorithmType model : ModelConfig datasets : List [ DatasetConfig ] tasks : List [ Dict [ str , Any ]] = field ( default_factory = list ) rewards : List [ RewardConfig ] = field ( default_factory = list ) train : TrainingConfig = field ( default_factory = TrainingConfig ) distributed : DistributedConfig = field ( default_factory = DistributedConfig ) logging : LoggingConfig = field ( default_factory = LoggingConfig ) chat_template : Optional [ str ] = None caching : Dict [ str , Any ] = field ( default_factory = dict ) reward_training : Optional [ RewardModelTrainingConfig ] = None def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()}","title":"UnifiedConfig"},{"location":"api-reference/configuration/#core.rl.config.UnifiedConfig.__post_init__","text":"Validate unified configuration. Source code in src/aligntune/core/rl/config.py 705 706 707 708 709 710 711 def __post_init__ ( self ): \"\"\"Validate unified configuration.\"\"\" if not isinstance ( self . algo , AlgorithmType ): if isinstance ( self . algo , str ): self . algo = AlgorithmType ( self . algo ) else : raise ValueError ( f \"Invalid algorithm type: { self . algo } \" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.UnifiedConfig.to_dict","text":"Convert configuration to dictionary. Source code in src/aligntune/core/rl/config.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert configuration to dictionary.\"\"\" def _serialize ( value ): if isinstance ( value , Enum ): return value . value if hasattr ( value , \"to_dict\" ): return value . to_dict () if isinstance ( value , list ): return [ _serialize ( item ) for item in value ] if isinstance ( value , dict ): return { k : _serialize ( v ) for k , v in value . items ()} if hasattr ( value , \"__dict__\" ): return { k : _serialize ( v ) for k , v in value . __dict__ . items ()} return value return { field_name : _serialize ( field_value ) for field_name , field_value in self . __dict__ . items ()} options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config import UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" )], train = TrainingConfig ( epochs = 1 , learning_rate = 1e-6 ), logging = LoggingConfig ( output_dir = \"./output\" ) )","title":"to_dict"},{"location":"api-reference/configuration/#modelconfig-rl","text":"Model configuration for RL training. Model configuration with no default model names. Source code in src/aligntune/core/rl/config.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @dataclass class ModelConfig : \"\"\"Model configuration with no default model names.\"\"\" name_or_path : str backend : str = \"trl\" # Backend: \"trl\" or \"unsloth\" sft_path : Optional [ str ] = None reward_path : Optional [ str ] = None reward_model_name : Optional [ str ] = None # NEW: Separate reward model reward_model_source : Optional [ 'RewardModelSourceConfig' ] = None # NEW: Reward model source configuration precision : PrecisionType = PrecisionType . AUTO quantization : Dict [ str , Any ] = field ( default_factory = dict ) attn_implementation : str = \"auto\" gradient_checkpointing : bool = False max_memory : Optional [ Dict [ str , str ]] = None device_map : Optional [ Union [ str , Dict [ str , int ]]] = None # Added device_map use_unsloth : bool = False # Enable Unsloth acceleration max_seq_length : int = 2048 # For Unsloth reward_value_model : str = None # Llama model for reward/value (loaded pre-Unsloth to avoid patches) reward_value_loading_type : Optional [ str ] = None # 'unsloth' or 'standard' reward_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) value_model_quantization : Dict [ str , Any ] = field ( default_factory = dict ) use_peft : bool = True # Enable PEFT (LoRA) by default lora_r : int = 16 lora_alpha : int = 32 lora_dropout : float = 0.05 lora_target_modules : List [ str ] = field ( default_factory = lambda : [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ]) trust_remote_code : bool = True # Trust remote code for model loading # New parameters from model initialization model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) ref_model_init_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) model_adapter_name : Optional [ str ] = None ref_adapter_name : Optional [ str ] = None force_use_ref_model : bool = False disable_dropout : bool = True use_logits_to_keep : bool = False reward_device : str = \"auto\" def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized","title":"ModelConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.ModelConfig.__post_init__","text":"Validate model configuration. Source code in src/aligntune/core/rl/config.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __post_init__ ( self ): \"\"\"Validate model configuration.\"\"\" # Validate backend if self . backend not in [ \"trl\" , \"unsloth\" ]: raise ValueError ( f \"backend must be 'trl' or 'unsloth', got ' { self . backend } '\" ) # Validate precision if not isinstance ( self . precision , PrecisionType ): if isinstance ( self . precision , str ): self . precision = PrecisionType ( self . precision ) else : raise ValueError ( f \"Invalid precision type: { self . precision } \" ) # Set default device_map if not specified if self . device_map is None : self . device_map = \"auto\" # Normalize reward_device if not self . reward_device : self . reward_device = \"auto\" else : normalized = self . reward_device . lower () if normalized not in { \"auto\" , \"cpu\" , \"cuda\" }: raise ValueError ( f \"reward_device must be 'auto', 'cpu', or 'cuda', got ' { self . reward_device } '\" ) self . reward_device = normalized options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#datasetconfig-rl","text":"Dataset configuration for RL training. Dataset configuration with flexible field mapping support. Source code in src/aligntune/core/rl/config.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @dataclass class DatasetConfig : \"\"\"Dataset configuration with flexible field mapping support.\"\"\" name : str split : str = \"train\" percent : Optional [ float ] = None max_samples : Optional [ int ] = None max_eval_samples : Optional [ int ] = None # Enhanced field mapping system field_mappings : Dict [ str , str ] = field ( default_factory = dict ) format_type : Optional [ str ] = None # \"alpaca\", \"dolly\", \"ultrachat\", \"custom\" auto_detect_fields : bool = True # Legacy support column_mapping : Dict [ str , str ] = field ( default_factory = dict ) task_type : str = \"conversation\" weight : float = 1.0 chat_template : Optional [ str ] = None system_prompt : Optional [ str ] = None # New parameters from dataset processing dataset_num_proc : Optional [ int ] = None pad_token : Optional [ str ] = None label_pad_token_id : int = - 100 truncation_mode : str = 'keep_end' padding_free : bool = False precompute_ref_log_probs : bool = False precompute_ref_batch_size : Optional [ int ] = None tools : Optional [ Any ] = None # ADD THESE NEW PARAMETERS preserve_columns : Optional [ List [ str ]] = None # NEW: Columns to preserve processing_fn : Optional [ Any ] = None # NEW: Custom processing function processing_batched : bool = False # NEW: Whether processing is batched processing_fn_kwargs : Dict [ str , Any ] = field ( default_factory = dict ) # NEW: Args for processing_fn config_name : Optional [ str ] = None def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" )","title":"DatasetConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.DatasetConfig.__post_init__","text":"Validate dataset configuration. Source code in src/aligntune/core/rl/config.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def __post_init__ ( self ): \"\"\"Validate dataset configuration.\"\"\" if not self . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) if self . percent is not None and ( self . percent <= 0 or self . percent > 100 ): raise ValueError ( \"Dataset percent must be between 0 and 100\" ) if self . max_samples is not None and self . max_samples <= 0 : raise ValueError ( \"max_samples must be positive\" ) if self . weight <= 0 : raise ValueError ( \"Dataset weight must be positive\" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.DatasetConfig.to_dict","text":"Convert to dictionary. Source code in src/aligntune/core/rl/config.py 157 158 159 160 161 162 163 164 165 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3","title":"to_dict"},{"location":"api-reference/configuration/#trainingconfig-rl","text":"Training configuration for RL. Training configuration. Source code in src/aligntune/core/rl/config.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 @dataclass class TrainingConfig : \"\"\"Training configuration.\"\"\" # Basic training parameters per_device_batch_size : int = 1 per_device_eval_batch_size : int = 1 gradient_accumulation_steps : int = 1 max_steps : Optional [ int ] = None epochs : Optional [ int ] = None eval_interval : int = 100 save_interval : int = 500 learning_rate : float = 1e-5 max_grad_norm : float = 1.0 weight_decay : float = 0.01 use_cache : bool = False , # Optimizer and scheduler optimizer : str = \"adamw_torch\" lr_scheduler : str = \"cosine\" warmup_steps : int = 0 # If 0, calculated from warmup_ratio or defaults to 5% of max_steps warmup_ratio : float = 0.0 # Alternative to warmup_steps # PPO/RL-specific parameters rollout_batch_size : int = 1 kl_coef : float = 0.1 cliprange : float = 0.2 cliprange_value : float = 0.2 num_ppo_epochs : Optional [ int ] = None temperature : float = 0.6 whiten_rewards : bool = False kl_estimator : str = 'k1' vf_coef : float = 0.1 gamma : float = 1.0 lam : float = 0.95 # Generation parameters response_length : int = 128 stop_token : str = 'eos' missing_eos_penalty : float = 1.0 ds3_gather_for_generation : bool = True generation_kwargs : dict = None # Length parameters max_length : int = 1024 max_prompt_length : int = 512 max_target_length : Optional [ int ] = None max_completion_length : int = 256 # Generation sampling parameters top_p : float = 0.95 # Processing parameters padding_free : bool = False truncation_mode : str = 'keep_end' # DPO-specific parameters beta : float = 0.1 loss_type : str = None loss_weights : Optional [ dict ] = None f_divergence_type : str = 'reverse_kl' f_alpha_divergence_coef : float = 1.0 reference_free : bool = False label_smoothing : float = 0.0 use_weighting : bool = False # Algorithm-specific parameters rpo_alpha : Optional [ float ] = None ld_alpha : Optional [ float ] = None discopop_tau : float = 0.05 # Reference model parameters sync_ref_model : bool = False ref_model_mixup_alpha : float = 0.6 ref_model_sync_steps : int = 512 # GRPO-specific parameters grpo_alpha : float = 0.1 grpo_beta : float = 0.1 # GSPO-specific parameters gspo_gamma : float = 0.1 gspo_delta : float = 0.1 # Evaluation parameters eval_steps : Optional [ int ] = 100 eval_strategy : str = \"no\" # DPO-specific evaluation options dpo_eval_enabled : bool = False dpo_eval_max_samples : Optional [ int ] = None dpo_zero_shot_max_samples : int = 50 dpo_few_shot_max_samples : int = 30 dpo_few_shot_examples_text : Optional [ str ] = None # Checkpointing parameters save_steps : int = 500 save_strategy : str = \"steps\" save_total_limit : Optional [ int ] = None load_best_model_at_end : bool = False metric_for_best_model : Optional [ str ] = None greater_is_better : bool = False # Logging parameters logging_steps : int = 10 logging_strategy : str = \"steps\" # Generation parameters (GRPO-specific) num_generations : Optional [ int ] = None mask_truncated_completions : bool = True scale_rewards : str = \"group\" reward_weights : Optional [ List [ float ]] = None enable_thinking : bool = False # Qwen3 thinking mode fast_inference : bool = False # Unsloth vLLM fast inference (2-3x faster generation) vllm_gpu_memory_utilization : float = 0.7 # vLLM GPU memory (0.95 for max speed with spare VRAM) # Seed parameters seed : Optional [ int ] = 42 data_seed : Optional [ int ] = 47 # Match training_script.py for fair comparison # Meta-ES specific meta_iterations : int = 15 patience : int = 5 min_delta : float = 0.001 init_scale : float = 0.01 N : int = 10 # Population size (must be even) T : int = 100 # Training steps per evaluation sigma : float = 0.01 # ES noise std sigma_decay : float = 0.99 # ES sigma decay per iteration alpha : float = 0.01 # ES learning rate mirror_coefficient : float = 0.0001 debug_mode : bool = False eval_timeout : int = 5 eval_max_tokens : int = 512 eval_k : int = 1 eval_temperature : float = 0.8 num_workers : int = 1 no_wandb : bool = False wandb_project : str = \"neural-mirror-es\" resume : Optional [ str ] = None use_rewards_directly : Optional [ List [ callable ]] = None # ============================================================================ # NEW: Counterfactual GRPO-specific parameters # ============================================================================ boost_factor : float = 2.0 \"\"\"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0 \"\"\" min_weight : float = 0.5 \"\"\"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5 \"\"\" max_spans : int = 10 \"\"\"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20 \"\"\" answer_weight : float = 1.5 \"\"\"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5 \"\"\" weighting_mode : Optional [ str ] = None \"\"\"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags. \"\"\" method_name : str = \"counterfactual\" \"\"\"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation) \"\"\" random_importance : bool = False \"\"\"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random. \"\"\" invert_importance : bool = False \"\"\"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters. \"\"\" enable_gradient_conservation : bool = True \"\"\"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion) \"\"\" weight_debug : bool = False \"\"\"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs). \"\"\" # ============================================================================ # NEW: GBMPO-specific parameters (Generalized Bregman Mirror Descent) # ============================================================================ gbmpo_l2_coefficient : float = 0.0001 \"\"\"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005 \"\"\" gbmpo_divergence_type : Optional [ str ] = None \"\"\"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms. \"\"\" gbmpo_epsilon : float = 0.2 \"\"\"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3 \"\"\" # Performance optimization use_liger_kernel : bool = False use_liger_loss : Optional [ bool ] = None # BOLT-specific parameters (Baseline-Optimized Learning Technique) curriculum_enabled : bool = False # Enable uncertainty-based sampling curriculum_epsilon : float = 0.05 # Floor for sampling weights curriculum_update_freq : int = 10 # Steps between weight updates baseline_enabled : bool = False # Enable persistent baseline tracking baseline_rho_min : float = 0.875 # Min forgetting factor (fast adaptation) baseline_rho_max : float = 0.96 # Max forgetting factor (slow adaptation) baseline_D_half : float = 0.5 # KL half-life for adaptive forgetting baseline_warm_start : Optional [ str ] = None # Path to JSON/PKL for warm-start use_baseline_advantages : bool = False # Use A = r - v\u0302(x) vs group mean group_by_length : bool = True extra_params : Dict [ str , Any ] = field ( default_factory = dict ) def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" )","title":"TrainingConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.answer_weight","text":"Weight multiplier for answer sections in reasoning tasks. Emphasizes the final answer/conclusion in math/reasoning problems. Typical range: 1.0-2.5","title":"answer_weight"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.boost_factor","text":"Importance weight boost factor for counterfactual tokens. Higher values give more weight to important tokens identified by the counterfactual method. Typical range: 1.5-3.0","title":"boost_factor"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.enable_gradient_conservation","text":"Enable gradient conservation to maintain gradient flow. When True, normalizes weights to preserve total gradient magnitude. Recommended: True (prevents gradient vanishing/explosion)","title":"enable_gradient_conservation"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_divergence_type","text":"Type of divergence for GBMPO algorithms. Options: - \"l2\": L2 norm regularization in log-space - \"l2kl\": Dual L2 + KL divergence - \"prob_l2\": L2 norm in probability space - \"prob_l2kl\": Dual probability-space L2 + KL Required when using GBMPO algorithms.","title":"gbmpo_divergence_type"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_epsilon","text":"PPO-style clipping parameter for GBMPO. Used for advantage clipping in policy updates. Typical range: 0.1-0.3","title":"gbmpo_epsilon"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.gbmpo_l2_coefficient","text":"L2 regularization coefficient for GBMPO variants. Controls the strength of L2 regularization in log-space (L2/L2KL) or probability-space (ProbL2/ProbL2KL) variants. Typical range: 0.00001-0.001 - Math tasks (GSM8K): 0.0001 - Code tasks (MBPP): 0.0001-0.0005","title":"gbmpo_l2_coefficient"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.invert_importance","text":"Invert importance weights for ablation studies. When True, flips the importance weights (important becomes unimportant). Used to validate that weighting direction matters.","title":"invert_importance"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.max_spans","text":"Maximum number of counterfactual spans to detect per sequence. Limits the number of important token regions identified. Typical range: 5-20","title":"max_spans"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.method_name","text":"Weighting method to use. Options: - \"counterfactual\": Standard counterfactual importance weighting - \"uniform\": Equal weights for all tokens (baseline) - \"random\": Random importance weights (ablation) - \"inverse\": Inverted importance weights (ablation)","title":"method_name"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.min_weight","text":"Minimum importance weight to prevent underflow. Ensures no token weight drops below this value. Typical range: 0.1-0.5","title":"min_weight"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.random_importance","text":"Use random importance weights for ablation studies. When True, assigns random weights instead of computed counterfactual weights. Used to validate that counterfactual weighting improves over random.","title":"random_importance"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.weight_debug","text":"Enable detailed logging of importance weights for debugging. When True, logs weight statistics, span detection, and per-token weights. Use for development/debugging only (generates verbose logs).","title":"weight_debug"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.weighting_mode","text":"Unified weighting mode selector. Options: counterfactual, random, inverted, vanilla. If set, overrides random_importance and invert_importance flags.","title":"weighting_mode"},{"location":"api-reference/configuration/#core.rl.config.TrainingConfig.__post_init__","text":"Validate training configuration. Source code in src/aligntune/core/rl/config.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __post_init__ ( self ): \"\"\"Validate training configuration.\"\"\" if self . per_device_batch_size <= 0 : raise ValueError ( \"per_device_batch_size must be positive\" ) if self . gradient_accumulation_steps <= 0 : raise ValueError ( \"gradient_accumulation_steps must be positive\" ) # if self.max_steps is not None and self.max_steps <= 0: # raise ValueError(\"max_steps must be positive\") if self . epochs is not None and self . epochs <= 0 : raise ValueError ( \"epochs must be positive\" ) if self . max_steps is None and self . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if self . eval_interval <= 0 : raise ValueError ( \"eval_interval must be positive\" ) if self . save_interval <= 0 : raise ValueError ( \"save_interval must be positive\" ) if self . rollout_batch_size <= 0 : raise ValueError ( \"rollout_batch_size must be positive\" ) if self . kl_coef < 0 : raise ValueError ( \"kl_coef must be non-negative\" ) if self . cliprange <= 0 : raise ValueError ( \"cliprange must be positive\" ) if self . learning_rate <= 0 : raise ValueError ( \"learning_rate must be positive\" ) if self . max_grad_norm < 0 : raise ValueError ( \"max_grad_norm must be non-negative\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#rewardconfig","text":"Reward function configuration. Reward function configuration. Source code in src/aligntune/core/rl/config.py 182 183 184 185 186 187 188 189 190 @dataclass class RewardConfig : \"\"\"Reward function configuration.\"\"\" type : str weight : float = 1.0 params : Dict [ str , Any ] = field ( default_factory = dict ) shield : bool = False clip : Optional [ float ] = None normalize : bool = False options: show_source: true heading_level: 3","title":"RewardConfig"},{"location":"api-reference/configuration/#rewardmodelsourceconfig","text":"Reward model source configuration. Reward model source - EXACTLY ONE must be specified. Source code in src/aligntune/core/rl/config.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 @dataclass class RewardModelSourceConfig : \"\"\"Reward model source - EXACTLY ONE must be specified.\"\"\" source_type : str # Must be set explicitly model_name : Optional [ str ] = None model_path : Optional [ str ] = None training_config : Optional [ RewardModelTrainingConfig ] = None fine_tune_with_rewards : bool = False # NEW: Enable hybrid mode to fine-tune pretrained with reward functions def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result","title":"RewardModelSourceConfig"},{"location":"api-reference/configuration/#core.rl.config.RewardModelSourceConfig.__post_init__","text":"Validate source configuration with strict mutual exclusivity. Source code in src/aligntune/core/rl/config.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def __post_init__ ( self ): \"\"\"Validate source configuration with strict mutual exclusivity.\"\"\" # Validate source_type valid_types = [ \"pretrained_hf\" , \"pretrained_local\" , \"custom_trained\" ] if self . source_type not in valid_types : raise ValueError ( f \"source_type must be one of { valid_types } , got ' { self . source_type } '\" ) # Validate exactly one source sources = [ self . model_name , self . model_path , self . training_config ] source_count = sum ( x is not None for x in sources ) if source_count != 1 : raise ValueError ( f \"Exactly ONE source must be specified. \" f \"Found { source_count } : \" f \"model_name= { self . model_name } , \" f \"model_path= { self . model_path } , \" f \"training_config= { self . training_config is not None } \" ) # Validate source matches type if self . source_type == \"pretrained_hf\" and not self . model_name : raise ValueError ( \"source_type='pretrained_hf' requires model_name\" ) if self . source_type == \"pretrained_local\" and not self . model_path : raise ValueError ( \"source_type='pretrained_local' requires model_path\" ) if self . source_type == \"custom_trained\" and not self . training_config : raise ValueError ( \"source_type='custom_trained' requires training_config\" )","title":"__post_init__"},{"location":"api-reference/configuration/#core.rl.config.RewardModelSourceConfig.to_dict","text":"Convert to dictionary. Source code in src/aligntune/core/rl/config.py 288 289 290 291 292 293 294 295 296 def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert to dictionary.\"\"\" result = {} for field_name , field_value in self . __dict__ . items (): if isinstance ( field_value , Enum ): result [ field_name ] = field_value . value else : result [ field_name ] = field_value return result options: show_source: true heading_level: 3","title":"to_dict"},{"location":"api-reference/configuration/#rewardmodeltrainingconfig","text":"Reward model training configuration. Configuration for reward model training - ALL fields validated. Source code in src/aligntune/core/rl/config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @dataclass class RewardModelTrainingConfig : \"\"\"Configuration for reward model training - ALL fields validated.\"\"\" base_model_name : str # NO default - must be explicit training_texts : List [ str ] # NO default - must provide data reward_functions : List [ str ] # NO default - must specify output_dir : str # NO default - must specify where to save # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training params with JUSTIFIED defaults num_epochs : int = 3 # Standard for reward model training learning_rate : float = 1e-5 # Standard LM fine-tuning rate batch_size : int = 8 # Balanced for memory/speed gradient_accumulation_steps : int = 4 # For effective batch size 32 max_length : int = 512 # Standard transformer length def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" )","title":"RewardModelTrainingConfig"},{"location":"api-reference/configuration/#core.rl.config.RewardModelTrainingConfig.__post_init__","text":"Validate ALL fields with strict validation. Source code in src/aligntune/core/rl/config.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __post_init__ ( self ): \"\"\"Validate ALL fields with strict validation.\"\"\" # Validate required fields if not self . base_model_name : raise ValueError ( \"base_model_name cannot be empty\" ) if not self . training_texts : raise ValueError ( \"training_texts cannot be empty\" ) if not isinstance ( self . training_texts , list ): raise TypeError ( f \"training_texts must be list, got { type ( self . training_texts ) } \" ) if len ( self . training_texts ) < 10 : raise ValueError ( f \"Need at least 10 training texts, got { len ( self . training_texts ) } \" ) if not self . reward_functions : raise ValueError ( \"reward_functions cannot be empty\" ) if not self . output_dir : raise ValueError ( \"output_dir cannot be empty\" ) # Validate optional fields if provided if self . reference_texts and len ( self . reference_texts ) != len ( self . training_texts ): raise ValueError ( \"reference_texts length must match training_texts\" ) if self . reward_weights : if len ( self . reward_weights ) != len ( self . reward_functions ): raise ValueError ( \"reward_weights length must match reward_functions\" ) if not all ( w > 0 for w in self . reward_weights ): raise ValueError ( \"All reward_weights must be positive\" ) # Validate training params if self . num_epochs <= 0 : raise ValueError ( f \"num_epochs must be positive, got { self . num_epochs } \" ) if self . learning_rate <= 0 : raise ValueError ( f \"learning_rate must be positive, got { self . learning_rate } \" ) if self . batch_size <= 0 : raise ValueError ( f \"batch_size must be positive, got { self . batch_size } \" ) if self . max_length <= 0 : raise ValueError ( f \"max_length must be positive, got { self . max_length } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#loggingconfig-rl","text":"Logging configuration for RL. Logging configuration. Source code in src/aligntune/core/rl/config.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 @dataclass class LoggingConfig : \"\"\"Logging configuration.\"\"\" loggers : List [ str ] = field ( default_factory = lambda : [ \"tensorboard\" ]) run_name : Optional [ str ] = None output_dir : str = \"./output\" log_level : str = \"INFO\" sample_logging : SampleLoggingConfig = field ( default_factory = SampleLoggingConfig ) report_to : str = \"none\" def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" )","title":"LoggingConfig (RL)"},{"location":"api-reference/configuration/#core.rl.config.LoggingConfig.__post_init__","text":"Validate logging configuration. Source code in src/aligntune/core/rl/config.py 658 659 660 661 662 663 664 665 666 667 def __post_init__ ( self ): \"\"\"Validate logging configuration.\"\"\" valid_loggers = { \"tensorboard\" , \"wandb\" } for logger in self . loggers : if logger not in valid_loggers : raise ValueError ( f \"Invalid logger: { logger } . Must be one of { valid_loggers } \" ) valid_levels = { \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" } if self . log_level . upper () not in valid_levels : raise ValueError ( f \"Invalid log level: { self . log_level } . Must be one of { valid_levels } \" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#sampleloggingconfig","text":"Sample logging configuration for qualitative generation. Configuration for qualitative sample logging. Source code in src/aligntune/core/rl/config.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 @dataclass class SampleLoggingConfig : \"\"\"Configuration for qualitative sample logging.\"\"\" enabled : bool = False prompts : Optional [ List [ str ]] = None interval_steps : Optional [ int ] = None percent_of_max_steps : Optional [ float ] = None max_new_tokens : int = 80 temperature : float = 0.6 top_p : float = 0.9 num_samples : int = 3 def __post_init__ ( self ): if self . interval_steps is not None and self . interval_steps <= 0 : raise ValueError ( \"sample_logging.interval_steps must be positive when set\" ) if self . percent_of_max_steps is not None : if self . percent_of_max_steps <= 0 or self . percent_of_max_steps > 1 : raise ValueError ( \"sample_logging.percent_of_max_steps must be in (0, 1]\" ) if self . max_new_tokens <= 0 : raise ValueError ( \"sample_logging.max_new_tokens must be positive\" ) if not ( 0 < self . temperature ): raise ValueError ( \"sample_logging.temperature must be positive\" ) if not ( 0 < self . top_p <= 1 ): raise ValueError ( \"sample_logging.top_p must be in (0, 1]\" ) if self . num_samples <= 0 : raise ValueError ( \"sample_logging.num_samples must be positive\" ) options: show_source: true heading_level: 3","title":"SampleLoggingConfig"},{"location":"api-reference/configuration/#distributedconfig","text":"Distributed training configuration. Distributed training configuration. Source code in src/aligntune/core/rl/config.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 @dataclass class DistributedConfig : \"\"\"Distributed training configuration.\"\"\" backend : BackendType = BackendType . SINGLE fsdp_config : Dict [ str , Any ] = field ( default_factory = dict ) deepspeed_config : Dict [ str , Any ] = field ( default_factory = dict ) nodes : int = 1 gpus_per_node : int = 1 seed : int = 42 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" )","title":"DistributedConfig"},{"location":"api-reference/configuration/#core.rl.config.DistributedConfig.__post_init__","text":"Validate distributed configuration. Source code in src/aligntune/core/rl/config.py 605 606 607 608 609 610 611 612 613 614 615 616 617 def __post_init__ ( self ): \"\"\"Validate distributed configuration.\"\"\" if not isinstance ( self . backend , BackendType ): if isinstance ( self . backend , str ): self . backend = BackendType ( self . backend ) else : raise ValueError ( f \"Invalid backend type: { self . backend } \" ) if self . nodes <= 0 : raise ValueError ( \"nodes must be positive\" ) if self . gpus_per_node <= 0 : raise ValueError ( \"gpus_per_node must be positive\" ) options: show_source: true heading_level: 3","title":"__post_init__"},{"location":"api-reference/configuration/#algorithmtype-enum","text":"RL algorithm type enumeration. Bases: Enum Supported RLHF algorithms. Source code in src/aligntune/core/rl/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 class AlgorithmType ( Enum ): \"\"\"Supported RLHF algorithms.\"\"\" PPO = \"ppo\" DPO = \"dpo\" GRPO = \"grpo\" GSPO = \"gspo\" COUNTERFACT_GRPO = \"counterfact_grpo\" GBMPO = \"gbmpo\" DRGRPO = \"drgrpo\" DAPO = \"dapo\" BOLT = \"bolt\" # Baseline-Optimized Learning Technique NMGRPO = \"nmgrpo\" METAES = \"metaes\" options: show_source: true heading_level: 3","title":"AlgorithmType Enum"},{"location":"api-reference/configuration/#precisiontype-enum-rl","text":"Model precision type enumeration for RL. Bases: Enum Model precision types. Source code in src/aligntune/core/rl/config.py 30 31 32 33 34 35 class PrecisionType ( Enum ): \"\"\"Model precision types.\"\"\" BF16 = \"bf16\" FP16 = \"fp16\" FP32 = \"fp32\" AUTO = \"auto\" # ADD THIS options: show_source: true heading_level: 3","title":"PrecisionType Enum (RL)"},{"location":"api-reference/configuration/#backendtype-enum-rl","text":"Distributed training backend enumeration. Bases: Enum Distributed training backends. Source code in src/aligntune/core/rl/config.py 38 39 40 41 42 43 class BackendType ( Enum ): \"\"\"Distributed training backends.\"\"\" SINGLE = \"single\" DDP = \"ddp\" FSDP = \"fsdp\" DEEPSPEED = \"deepspeed\" options: show_source: true heading_level: 3","title":"BackendType Enum (RL)"},{"location":"api-reference/configuration/#configuration-loaders","text":"","title":"Configuration Loaders"},{"location":"api-reference/configuration/#sftconfigloader","text":"Configuration loader for SFT configs. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"SFTConfigLoader"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.load_from_dict","text":"Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_dict"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.load_from_yaml","text":"Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_yaml"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/configuration/#core.sft.config_loader.SFTConfigLoader.validate_config","text":"Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3","title":"validate_config"},{"location":"api-reference/configuration/#configloader-rl","text":"Configuration loader for RL configs. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"ConfigLoader (RL)"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.create_config_from_cli_args","text":"Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"create_config_from_cli_args"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.load_from_dict","text":"Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_dict"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.load_from_yaml","text":"Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_yaml"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.parse_dataset_spec","text":"Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params )","title":"parse_dataset_spec"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.parse_reward_spec","text":"Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params )","title":"parse_reward_spec"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.save_resolved_config","text":"Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False )","title":"save_resolved_config"},{"location":"api-reference/configuration/#core.rl.config_loader.ConfigLoader.validate_config","text":"Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3","title":"validate_config"},{"location":"api-reference/configuration/#helper-functions","text":"","title":"Helper Functions"},{"location":"api-reference/configuration/#create_instruction_following_config","text":"Create configuration for instruction following. Create configuration for instruction following tasks. Source code in src/aligntune/core/sft/config.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def create_instruction_following_config ( model_name : str , dataset_name : str , output_dir : str = \"./output/instruction\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for instruction following tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 1024 ), use_unsloth = kwargs . get ( 'use_unsloth' , True ), peft_enabled = kwargs . get ( 'peft_enabled' , True ), quantization = kwargs . get ( 'quantization' , { \"load_in_4bit\" : True }) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . INSTRUCTION_FOLLOWING , instruction_column = kwargs . get ( 'instruction_column' , 'instruction' ), response_column = kwargs . get ( 'response_column' , 'output' ), context_column = kwargs . get ( 'context_column' , 'input' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 4 ), learning_rate = kwargs . get ( 'learning_rate' , 2e-4 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'instruction_following' ) ) ) options: show_source: true heading_level: 3","title":"create_instruction_following_config()"},{"location":"api-reference/configuration/#create_text_classification_config","text":"Create configuration for text classification. Create configuration for text classification tasks. Source code in src/aligntune/core/sft/config.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 def create_text_classification_config ( model_name : str , dataset_name : str , num_labels : int , output_dir : str = \"./output/classification\" , ** kwargs ) -> SFTConfig : \"\"\"Create configuration for text classification tasks.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = model_name , max_seq_length = kwargs . get ( 'max_seq_length' , 512 ), use_unsloth = False , # Not recommended for classification peft_enabled = kwargs . get ( 'peft_enabled' , True ), num_labels = num_labels , quantization = kwargs . get ( 'quantization' , {}) ), dataset = DatasetConfig ( name = dataset_name , split = kwargs . get ( 'split' , 'train' ), max_samples = kwargs . get ( 'max_samples' ), task_type = TaskType . TEXT_CLASSIFICATION , text_column = kwargs . get ( 'text_column' , 'text' ), label_column = kwargs . get ( 'label_column' , 'label' ) ), train = TrainingConfig ( epochs = kwargs . get ( 'epochs' , 3 ), per_device_batch_size = kwargs . get ( 'batch_size' , 8 ), learning_rate = kwargs . get ( 'learning_rate' , 5e-5 ), gradient_accumulation_steps = kwargs . get ( 'gradient_accumulation_steps' , 1 ) ), logging = LoggingConfig ( output_dir = output_dir , run_name = kwargs . get ( 'run_name' , 'text_classification' ) ) ) options: show_source: true heading_level: 3","title":"create_text_classification_config()"},{"location":"api-reference/configuration/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/configuration/#creating-sft-configuration","text":"from aligntune.core.sft.config import SFTConfig , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig config = SFTConfig ( model = ModelConfig ( name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\" , precision = \"bf16\" , max_seq_length = 512 ), dataset = DatasetConfig ( name = \"tatsu-lab/alpaca\" , max_samples = 1000 , task_type = \"instruction_following\" ), train = TrainingConfig ( epochs = 3 , per_device_batch_size = 4 , learning_rate = 5e-5 ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_sft_training\" ) )","title":"Creating SFT Configuration"},{"location":"api-reference/configuration/#creating-rl-configuration","text":"from aligntune.core.rl.config import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , LoggingConfig ) config = UnifiedConfig ( algo = AlgorithmType . DPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-medium\" , use_peft = True , lora_r = 16 ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , max_samples = 1000 ) ], train = TrainingConfig ( epochs = 1 , per_device_batch_size = 1 , learning_rate = 1e-6 , beta = 0.1 # DPO parameter ), logging = LoggingConfig ( output_dir = \"./output\" , run_name = \"my_dpo_training\" ) )","title":"Creating RL Configuration"},{"location":"api-reference/configuration/#loading-from-yaml","text":"from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" )","title":"Loading from YAML"},{"location":"api-reference/configuration/#next-steps","text":"Backend Factory - Trainer creation Trainers - Trainer classes User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/core/","text":"Core API Reference \u00b6 Complete API reference for AlignTune core functions and utilities. Overview \u00b6 This page covers core utility functions, error classes, and helper functions. For main API functions, see Backend Factory . Configuration Loaders \u00b6 SFTConfigLoader \u00b6 Load SFT configuration from YAML files. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config load_from_yaml ( path ) staticmethod \u00b6 Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config save_config ( config , path ) staticmethod \u00b6 Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" ) validate_config ( config ) staticmethod \u00b6 Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config_loader import SFTConfigLoader config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader (RL) \u00b6 Load RL configuration from YAML files. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) create_config_from_cli_args ( ** kwargs ) staticmethod \u00b6 Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) ) load_from_dict ( data ) staticmethod \u00b6 Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) load_from_yaml ( path ) staticmethod \u00b6 Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) parse_dataset_spec ( spec ) staticmethod \u00b6 Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) parse_reward_spec ( spec ) staticmethod \u00b6 Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) save_resolved_config ( config , output_dir ) staticmethod \u00b6 Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) validate_config ( config ) staticmethod \u00b6 Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"ppo_config.yaml\" ) Trainer Factories \u00b6 SFTTrainerFactory \u00b6 Factory for creating SFT trainers. Factory for creating SFT trainers - delegates to Backend Factory. Source code in src/aligntune/core/sft/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class SFTTrainerFactory : \"\"\"Factory for creating SFT trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) create_trainer ( config ) classmethod \u00b6 Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config SFTConfig SFT configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/sft/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) options: show_source: true heading_level: 3 TrainerFactory (RL) \u00b6 Factory for creating RL trainers. Factory for creating RLHF trainers - delegates to Backend Factory. Source code in src/aligntune/core/rl/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class TrainerFactory : \"\"\"Factory for creating RLHF trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) create_trainer ( config ) classmethod \u00b6 Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config UnifiedConfig Unified configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/rl/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) options: show_source: true heading_level: 3 Error Classes \u00b6 AlignTuneError \u00b6 Base exception for AlignTune errors. Bases: Exception Base exception class for AlignTune errors. Source code in src/aligntune/utils/errors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AlignTuneError ( Exception ): \"\"\"Base exception class for AlignTune errors.\"\"\" def __init__ ( self , message : str , error_code : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . message = message self . error_code = error_code self . suggestions = suggestions or [] super () . __init__ ( self . message ) def __str__ ( self ): return self . format_error () def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) format_error () \u00b6 Format error message with suggestions. Source code in src/aligntune/utils/errors.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) options: show_source: true heading_level: 3 ConfigurationError \u00b6 Configuration-related errors. Bases: AlignTuneError Configuration-related errors. Source code in src/aligntune/utils/errors.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class ConfigurationError ( AlignTuneError ): \"\"\"Configuration-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field error_code = \"CONFIG_ERROR\" if not suggestions : suggestions = self . _generate_config_suggestions ( message , field ) super () . __init__ ( message , error_code , suggestions ) def _generate_config_suggestions ( self , message : str , field : Optional [ str ]) -> List [ str ]: \"\"\"Generate configuration-specific suggestions.\"\"\" suggestions = [] if \"optimizer\" in message . lower (): suggestions . extend ([ \"Check available optimizers with 'aligntune recipes list'\" , \"Use 'adamw_torch' for general training\" , \"Use 'adamw_8bit' for memory-efficient training\" ]) if \"scheduler\" in message . lower (): suggestions . extend ([ \"Check available schedulers with 'aligntune recipes list'\" , \"Use 'cosine' for most training scenarios\" , \"Use 'linear' for fine-tuning tasks\" ]) if \"model\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Ensure model name is correct (e.g., 'meta-llama/Meta-Llama-3-8B')\" , \"Run 'huggingface-cli login' if using private models\" , \"Check model availability at https://huggingface.co/models\" ]) if \"dataset\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Check dataset name is correct (e.g., 'tatsu-lab/alpaca')\" , \"Verify dataset exists at https://huggingface.co/datasets\" ]) if \"memory\" in message . lower (): suggestions . extend ([ \"Reduce batch size\" , \"Enable gradient checkpointing\" , \"Use 4-bit quantization\" , \"Reduce model max length\" ]) if field : suggestions . append ( f \"Check the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3 TrainingError \u00b6 Training-related errors. Bases: AlignTuneError Training-related errors. Source code in src/aligntune/utils/errors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TrainingError ( AlignTuneError ): \"\"\"Training-related errors.\"\"\" def __init__ ( self , message : str , stage : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . stage = stage error_code = \"TRAINING_ERROR\" if not suggestions : suggestions = self . _generate_training_suggestions ( message , stage ) super () . __init__ ( message , error_code , suggestions ) def _generate_training_suggestions ( self , message : str , stage : Optional [ str ]) -> List [ str ]: \"\"\"Generate training-specific suggestions.\"\"\" suggestions = [] if \"cuda\" in message . lower () or \"gpu\" in message . lower (): suggestions . extend ([ \"Check GPU memory availability with 'nvidia-smi'\" , \"Reduce batch size or enable gradient accumulation\" , \"Enable gradient checkpointing\" , \"Use mixed precision (bf16/fp16)\" ]) if \"out of memory\" in message . lower (): suggestions . extend ([ \"Reduce per_device_batch_size\" , \"Increase gradient_accumulation_steps\" , \"Enable gradient_checkpointing\" , \"Use 4-bit quantization\" , \"Reduce max_seq_length\" ]) if \"nan\" in message . lower () or \"inf\" in message . lower (): suggestions . extend ([ \"Reduce learning rate\" , \"Enable gradient clipping\" , \"Check input data quality\" , \"Use more stable optimizer (AdamW instead of Lion)\" ]) if \"loss\" in message . lower () and \"not decreasing\" in message . lower (): suggestions . extend ([ \"Increase learning rate\" , \"Check data quality and preprocessing\" , \"Try different optimizer\" , \"Adjust batch size\" ]) if stage : suggestions . append ( f \"Error occurred during { stage } stage\" ) return suggestions options: show_source: true heading_level: 3 EnvironmentError \u00b6 Environment-related errors. Bases: AlignTuneError Environment and dependency errors. Source code in src/aligntune/utils/errors.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class EnvironmentError ( AlignTuneError ): \"\"\"Environment and dependency errors.\"\"\" def __init__ ( self , message : str , dependency : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . dependency = dependency error_code = \"ENV_ERROR\" if not suggestions : suggestions = self . _generate_env_suggestions ( message , dependency ) super () . __init__ ( message , error_code , suggestions ) def _generate_env_suggestions ( self , message : str , dependency : Optional [ str ]) -> List [ str ]: \"\"\"Generate environment-specific suggestions.\"\"\" suggestions = [] if \"torch\" in message . lower () or \"cuda\" in message . lower (): suggestions . extend ([ \"Install PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" , \"Check CUDA version compatibility\" , \"Update GPU drivers\" ]) if \"transformers\" in message . lower (): suggestions . extend ([ \"Install transformers: pip install transformers\" , \"Update to latest version: pip install --upgrade transformers\" ]) if \"trl\" in message . lower (): suggestions . extend ([ \"Install TRL: pip install trl\" , \"Update to latest version: pip install --upgrade trl\" ]) if \"bitsandbytes\" in message . lower (): suggestions . extend ([ \"Install bitsandbytes: pip install bitsandbytes\" , \"For CUDA issues, try: pip install bitsandbytes --index-url https://download.pytorch.org/whl/cu118\" ]) if \"unsloth\" in message . lower (): suggestions . extend ([ \"Install unsloth: pip install unsloth\" , \"Check CUDA compatibility for your GPU\" ]) if dependency : suggestions . append ( f \"Install missing dependency: pip install { dependency } \" ) return suggestions options: show_source: true heading_level: 3 ValidationError \u00b6 Validation-related errors. Bases: AlignTuneError Validation-related errors. Source code in src/aligntune/utils/errors.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class ValidationError ( AlignTuneError ): \"\"\"Validation-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , value : Optional [ Any ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field self . value = value error_code = \"VALIDATION_ERROR\" if not suggestions : suggestions = self . _generate_validation_suggestions ( message , field , value ) super () . __init__ ( message , error_code , suggestions ) def _generate_validation_suggestions ( self , message : str , field : Optional [ str ], value : Optional [ Any ]) -> List [ str ]: \"\"\"Generate validation-specific suggestions.\"\"\" suggestions = [] if \"positive\" in message . lower () and field : suggestions . append ( f \"Set { field } to a positive value (current: { value } )\" ) if \"required\" in message . lower () and field : suggestions . append ( f \"Provide a value for the required field ' { field } '\" ) if \"invalid\" in message . lower (): suggestions . append ( \"Check the allowed values in the documentation\" ) if field : suggestions . append ( f \"Fix the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3 Utility Functions \u00b6 validate_backend_selection() \u00b6 Validate backend selection and provide helpful errors. Validate backend selection and provide helpful error messages. Source code in src/aligntune/core/backend_factory.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def validate_backend_selection ( backend : str , training_type : str = \"RL\" ) -> BackendType : \"\"\"Validate backend selection and provide helpful error messages.\"\"\" try : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) except ValueError : available_backends = [ bt . value for bt in BackendType ] raise ValueError ( f \"Invalid backend ' { backend } '. Available backends: { available_backends } \" ) # Check availability if not _check_backend_availability ( backend_type ): if backend_type == BackendType . TRL : raise ImportError ( \"TRL backend not available. Install with: pip install trl \\n \" \"Alternatively, use Unsloth backend: --backend unsloth\" ) elif backend_type == BackendType . UNSLOTH : raise ImportError ( \"Unsloth backend not available. Install with: pip install unsloth \\n \" \"Alternatively, use TRL backend: --backend trl\" ) return backend_type options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import validate_backend_selection backend_type = validate_backend_selection ( \"trl\" , \"SFT\" ) See Also \u00b6 Backend Factory - Main API for creating trainers Configuration Classes - Configuration API Trainers - Trainer API","title":"Core API"},{"location":"api-reference/core/#core-api-reference","text":"Complete API reference for AlignTune core functions and utilities.","title":"Core API Reference"},{"location":"api-reference/core/#overview","text":"This page covers core utility functions, error classes, and helper functions. For main API functions, see Backend Factory .","title":"Overview"},{"location":"api-reference/core/#configuration-loaders","text":"","title":"Configuration Loaders"},{"location":"api-reference/core/#sftconfigloader","text":"Load SFT configuration from YAML files. Loader for SFT configurations. Source code in src/aligntune/core/sft/config_loader.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 class SFTConfigLoader : \"\"\"Loader for SFT configurations.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Convert dictionary to SFTConfig object.\"\"\" # Convert model config model_data = data . get ( \"model\" , {}) # Store backend separately since ModelConfig doesn't have it backend = model_data . get ( \"backend\" , \"auto\" ) model_config = ModelConfig ( name_or_path = model_data [ \"name_or_path\" ], precision = PrecisionType ( model_data . get ( \"precision\" , \"bf16\" )), quantization = model_data . get ( \"quantization\" , {}), attn_implementation = model_data . get ( \"attn_implementation\" , \"auto\" ), gradient_checkpointing = model_data . get ( \"gradient_checkpointing\" , True ), max_memory = model_data . get ( \"max_memory\" ), use_unsloth = model_data . get ( \"use_unsloth\" , False ), max_seq_length = model_data . get ( \"max_seq_length\" , 2048 ), peft_enabled = model_data . get ( \"peft_enabled\" , False ), lora_rank = model_data . get ( \"lora_rank\" , 16 ), lora_alpha = model_data . get ( \"lora_alpha\" , 32 ), lora_dropout = model_data . get ( \"lora_dropout\" , 0.1 ), target_modules = model_data . get ( \"target_modules\" ) ) # Attach backend to model_config for trainer factory to access model_config . backend = backend # Convert dataset config dataset_data = data . get ( \"dataset\" , {}) dataset_config = DatasetConfig ( name = dataset_data [ \"name\" ], split = dataset_data . get ( \"split\" , \"train\" ), subset = dataset_data . get ( \"subset\" ), config = dataset_data . get ( \"config\" ), percent = dataset_data . get ( \"percent\" ), max_samples = dataset_data . get ( \"max_samples\" ), column_mapping = dataset_data . get ( \"column_mapping\" , {}), task_type = TaskType ( dataset_data . get ( \"task_type\" , \"supervised_fine_tuning\" )) ) # Convert training config train_data = data . get ( \"train\" , {}) training_config = TrainingConfig ( per_device_batch_size = train_data . get ( \"per_device_batch_size\" , 1 ), gradient_accumulation_steps = train_data . get ( \"gradient_accumulation_steps\" , 1 ), max_steps = train_data . get ( \"max_steps\" ), epochs = train_data . get ( \"epochs\" ), learning_rate = float ( train_data . get ( \"learning_rate\" , 1e-5 )), weight_decay = float ( train_data . get ( \"weight_decay\" , 0.01 )), warmup_steps = train_data . get ( \"warmup_steps\" , 0 ), eval_interval = train_data . get ( \"eval_interval\" , 100 ), save_interval = train_data . get ( \"save_interval\" , 500 ), max_grad_norm = float ( train_data . get ( \"max_grad_norm\" , 1.0 )), fp16 = train_data . get ( \"fp16\" , False ), bf16 = train_data . get ( \"bf16\" , False ), dataloader_num_workers = train_data . get ( \"dataloader_num_workers\" , 0 ), remove_unused_columns = train_data . get ( \"remove_unused_columns\" , False ), dataset_num_proc = train_data . get ( \"dataset_num_proc\" ), dataset_kwargs = train_data . get ( \"dataset_kwargs\" , {}), packing = train_data . get ( \"packing\" , False ), packing_strategy = train_data . get ( \"packing_strategy\" , \"bfd\" ), eval_packing = train_data . get ( \"eval_packing\" ), padding_free = train_data . get ( \"padding_free\" , False ), pad_to_multiple_of = train_data . get ( \"pad_to_multiple_of\" ), completion_only_loss = train_data . get ( \"completion_only_loss\" ), assistant_only_loss = train_data . get ( \"assistant_only_loss\" , False ), loss_type = train_data . get ( \"loss_type\" , \"nll\" ), activation_offloading = train_data . get ( \"activation_offloading\" , False ), use_flash_attention_2 = train_data . get ( \"use_flash_attention_2\" ) ) # Convert logging config logging_data = data . get ( \"logging\" , {}) logging_config = LoggingConfig ( output_dir = logging_data . get ( \"output_dir\" , \"./output\" ), run_name = logging_data . get ( \"run_name\" ), loggers = logging_data . get ( \"loggers\" , [ \"tensorboard\" ]), log_level = logging_data . get ( \"log_level\" , \"INFO\" ), log_interval = logging_data . get ( \"log_interval\" , 10 ), save_strategy = logging_data . get ( \"save_strategy\" , \"steps\" ), eval_strategy = logging_data . get ( \"eval_strategy\" , \"steps\" ) ) # Convert evaluation config eval_data = data . get ( \"evaluation\" , {}) evaluation_config = EvaluationConfig ( compute_perplexity = eval_data . get ( \"compute_perplexity\" , True ), compute_rouge = eval_data . get ( \"compute_rouge\" , True ), compute_bleu = eval_data . get ( \"compute_bleu\" , True ), compute_meteor = eval_data . get ( \"compute_meteor\" , False ), compute_bertscore = eval_data . get ( \"compute_bertscore\" , False ), compute_semantic_similarity = eval_data . get ( \"compute_semantic_similarity\" , False ), compute_codebleu = eval_data . get ( \"compute_codebleu\" , False ), max_samples_for_quality_metrics = eval_data . get ( \"max_samples_for_quality_metrics\" , 50 ), bertscore_model = eval_data . get ( \"bertscore_model\" , \"microsoft/deberta-xlarge-mnli\" ), semantic_similarity_model = eval_data . get ( \"semantic_similarity_model\" , \"sentence-transformers/all-MiniLM-L6-v2\" ) ) # Store custom evaluation fields as attributes (not in EvaluationConfig dataclass) evaluation_config . enabled = eval_data . get ( \"enabled\" , False ) evaluation_config . pre_training = eval_data . get ( \"pre_training\" , False ) evaluation_config . post_training = eval_data . get ( \"post_training\" , False ) evaluation_config . eval_dataset = eval_data . get ( \"eval_dataset\" , {}) evaluation_config . metrics = eval_data . get ( \"metrics\" , []) # Create main config return SFTConfig ( model = model_config , dataset = dataset_config , train = training_config , logging = logging_config , evaluation = evaluation_config ) @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"SFTConfigLoader"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.load_from_dict","text":"Load configuration from a dictionary. Source code in src/aligntune/core/sft/config_loader.py 31 32 33 34 35 36 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> SFTConfig : \"\"\"Load configuration from a dictionary.\"\"\" config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_dict"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.load_from_yaml","text":"Load configuration from a YAML file. Source code in src/aligntune/core/sft/config_loader.py 21 22 23 24 25 26 27 28 29 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> SFTConfig : \"\"\"Load configuration from a YAML file.\"\"\" with open ( path , 'r' ) as f : data = yaml . safe_load ( f ) config = SFTConfigLoader . _dict_to_config ( data ) SFTConfigLoader . validate_config ( config ) return config","title":"load_from_yaml"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/core/sft/config_loader.py 168 169 170 171 172 173 174 @staticmethod def save_config ( config : SFTConfig , path : Union [ str , Path ]) -> None : \"\"\"Save configuration to YAML file.\"\"\" with open ( path , 'w' ) as f : yaml . dump ( config . to_dict (), f , default_flow_style = False , sort_keys = False ) logger . info ( f \"Configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/core/#core.sft.config_loader.SFTConfigLoader.validate_config","text":"Validate configuration. Source code in src/aligntune/core/sft/config_loader.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @staticmethod def validate_config ( config : SFTConfig ) -> None : \"\"\"Validate configuration.\"\"\" # Validate model name if not config . model . name_or_path : raise ValueError ( \"Model name_or_path is required and cannot be empty\" ) # Validate dataset name if not config . dataset . name : raise ValueError ( \"Dataset name is required and cannot be empty\" ) # Validate training parameters if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) logger . info ( \"Configuration validation passed\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.sft.config_loader import SFTConfigLoader config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" )","title":"validate_config"},{"location":"api-reference/core/#configloader-rl","text":"Load RL configuration from YAML files. Load and validate unified configurations with no placeholder defaults. Source code in src/aligntune/core/rl/config_loader.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 class ConfigLoader : \"\"\"Load and validate unified configurations with no placeholder defaults.\"\"\" @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data ) @staticmethod def _dict_to_config ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Convert dictionary to UnifiedConfig with validation.\"\"\" # Validate required fields required_fields = [ 'algo' , 'model' , 'datasets' ] for field in required_fields : if field not in data : raise ValueError ( f \"Missing required field: { field } \" ) # Convert nested dictionaries to config objects model_config = ModelConfig ( ** data [ 'model' ]) # Convert datasets datasets = [] for ds_data in data [ 'datasets' ]: datasets . append ( DatasetConfig ( ** ds_data )) # Convert rewards rewards = [] for reward_data in data . get ( 'rewards' , []): rewards . append ( RewardConfig ( ** reward_data )) # Convert other configs with type conversion train_data = data . get ( 'train' , {}) if 'learning_rate' in train_data and isinstance ( train_data [ 'learning_rate' ], str ): train_data [ 'learning_rate' ] = float ( train_data [ 'learning_rate' ]) train_config = TrainingConfig ( ** train_data ) distributed_data = data . get ( 'distributed' , {}) if 'seed' in distributed_data and isinstance ( distributed_data [ 'seed' ], str ): distributed_data [ 'seed' ] = int ( distributed_data [ 'seed' ]) distributed_config = DistributedConfig ( ** distributed_data ) logging_config = LoggingConfig ( ** data . get ( 'logging' , {})) return UnifiedConfig ( algo = AlgorithmType ( data [ 'algo' ]), model = model_config , datasets = datasets , tasks = data . get ( 'tasks' , []), rewards = rewards , train = train_config , distributed = distributed_config , logging = logging_config , chat_template = data . get ( 'chat_template' ), caching = data . get ( 'caching' , {}) ) @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params ) @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params ) @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False ) @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"ConfigLoader (RL)"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.create_config_from_cli_args","text":"Create configuration from CLI arguments. Source code in src/aligntune/core/rl/config_loader.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 @staticmethod def create_config_from_cli_args ( ** kwargs ) -> UnifiedConfig : \"\"\"Create configuration from CLI arguments.\"\"\" # Parse dataset specifications datasets = [] for ds_spec in kwargs . get ( 'dataset' , []): datasets . append ( ConfigLoader . parse_dataset_spec ( ds_spec )) # Parse reward specifications rewards = [] for reward_spec in kwargs . get ( 'rewards' , []): rewards . append ( ConfigLoader . parse_reward_spec ( reward_spec )) # Create configuration return UnifiedConfig ( algo = AlgorithmType ( kwargs [ 'algo' ]), model = ModelConfig ( name_or_path = kwargs [ 'model' ], precision = PrecisionType ( kwargs . get ( 'precision' , 'bf16' )), gradient_checkpointing = kwargs . get ( 'grad_checkpointing' , True ) ), datasets = datasets , rewards = rewards , train = TrainingConfig ( per_device_batch_size = kwargs . get ( 'train_batch' , 1 ), gradient_accumulation_steps = kwargs . get ( 'accum' , 1 ), max_steps = kwargs . get ( 'max_steps' ), epochs = kwargs . get ( 'epochs' ), eval_interval = kwargs . get ( 'eval_interval' , 100 ), save_interval = kwargs . get ( 'save_interval' , 500 ), rollout_batch_size = kwargs . get ( 'rollout_bs' , 1 ) ), distributed = DistributedConfig ( backend = BackendType ( kwargs . get ( 'backend' , 'single' )), seed = kwargs . get ( 'seed' , 42 ) ), logging = LoggingConfig ( loggers = kwargs . get ( 'loggers' , [ 'tensorboard' ]), output_dir = kwargs . get ( 'output_dir' , './output' ) ), chat_template = kwargs . get ( 'chat_template' ) )","title":"create_config_from_cli_args"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.load_from_dict","text":"Load configuration from dictionary. Source code in src/aligntune/core/rl/config_loader.py 44 45 46 47 48 49 50 @staticmethod def load_from_dict ( data : Dict [ str , Any ]) -> UnifiedConfig : \"\"\"Load configuration from dictionary.\"\"\" if not isinstance ( data , dict ): raise ValueError ( f \"Configuration data must be a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_dict"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.load_from_yaml","text":"Load configuration from YAML file. Source code in src/aligntune/core/rl/config_loader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @staticmethod def load_from_yaml ( path : Union [ str , Path ]) -> UnifiedConfig : \"\"\"Load configuration from YAML file.\"\"\" path = Path ( path ) if not path . exists (): raise FileNotFoundError ( f \"Configuration file not found: { path } \" ) with open ( path , 'r' , encoding = 'utf-8' ) as f : data = yaml . safe_load ( f ) if not isinstance ( data , dict ): raise ValueError ( f \"Configuration file must contain a dictionary, got { type ( data ) } \" ) return ConfigLoader . _dict_to_config ( data )","title":"load_from_yaml"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.parse_dataset_spec","text":"Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" Source code in src/aligntune/core/rl/config_loader.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @staticmethod def parse_dataset_spec ( spec : str ) -> DatasetConfig : \"\"\" Parse dataset specification string. Format: name[:split][#percent=N|max=N][?map.key=value] Example: \"Anthropic/hh-rlhf:train#percent=25?map.prompt=prompt\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Dataset specification must be a non-empty string\" ) # Split query parameters parts = spec . split ( '?' ) main_part = parts [ 0 ] query_params = {} if len ( parts ) > 1 : for param in parts [ 1 ] . split ( '&' ): if '=' in param : key , value = param . split ( '=' , 1 ) # Handle map. prefix if key . startswith ( 'map.' ): key = key [ 4 :] # Remove 'map.' prefix query_params [ key ] = value # Parse main part if '#' in main_part : name_split , size_spec = main_part . split ( '#' , 1 ) else : name_split = main_part size_spec = None if ':' in name_split : name , split = name_split . split ( ':' , 1 ) else : name = name_split split = \"train\" # Parse size specification percent = None max_samples = None if size_spec : if size_spec . startswith ( 'percent=' ): percent = float ( size_spec . split ( '=' )[ 1 ]) elif size_spec . startswith ( 'max=' ): max_samples = int ( size_spec . split ( '=' )[ 1 ]) else : raise ValueError ( f \"Invalid size specification: { size_spec } \" ) return DatasetConfig ( name = name , split = split , percent = percent , max_samples = max_samples , column_mapping = query_params )","title":"parse_dataset_spec"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.parse_reward_spec","text":"Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" Source code in src/aligntune/core/rl/config_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @staticmethod def parse_reward_spec ( spec : str ) -> RewardConfig : \"\"\" Parse reward specification string. Format: type:weight:param1=value1:param2=value2 Example: \"numeric_math:weight=1.0:tolerance=1e-6\" \"\"\" if not spec or not isinstance ( spec , str ): raise ValueError ( \"Reward specification must be a non-empty string\" ) parts = spec . split ( ':' ) if len ( parts ) < 1 : raise ValueError ( \"Reward specification must include at least the type\" ) reward_type = parts [ 0 ] weight = 1.0 params = {} for part in parts [ 1 :]: if '=' in part : key , value = part . split ( '=' , 1 ) if key == 'weight' : weight = float ( value ) else : # Try to parse as number or boolean, otherwise keep as string try : if value . lower () in ( 'true' , 'false' ): params [ key ] = value . lower () == 'true' elif 'e' in value . lower () or '.' in value : params [ key ] = float ( value ) else : params [ key ] = int ( value ) except ValueError : params [ key ] = value else : # Handle special cases like \"shield:safety:strict=true\" if part == \"shield\" and len ( parts ) > parts . index ( part ) + 1 : # This is a shield specification shield_type = parts [ parts . index ( part ) + 1 ] params [ \"shield_type\" ] = shield_type # Look for additional shield parameters for i in range ( parts . index ( part ) + 2 , len ( parts )): if '=' in parts [ i ]: key , value = parts [ i ] . split ( '=' , 1 ) params [ f \"shield_ { key } \" ] = value return RewardConfig ( type = reward_type , weight = weight , params = params )","title":"parse_reward_spec"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.save_resolved_config","text":"Save resolved configuration to output directory. Source code in src/aligntune/core/rl/config_loader.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def save_resolved_config ( config : UnifiedConfig , output_dir : Union [ str , Path ]) -> None : \"\"\"Save resolved configuration to output directory.\"\"\" output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) resolved_config = config . to_dict () # Add metadata resolved_config [ \"_metadata\" ] = { \"created_at\" : str ( Path () . cwd ()), \"config_version\" : \"1.0\" , \"resolved\" : True } config_path = output_dir / \"config.resolved.yaml\" with open ( config_path , 'w' , encoding = 'utf-8' ) as f : yaml . dump ( resolved_config , f , default_flow_style = False , indent = 2 , allow_unicode = True , sort_keys = False )","title":"save_resolved_config"},{"location":"api-reference/core/#core.rl.config_loader.ConfigLoader.validate_config","text":"Validate configuration for consistency and completeness. Source code in src/aligntune/core/rl/config_loader.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 @staticmethod def validate_config ( config : UnifiedConfig ) -> None : \"\"\"Validate configuration for consistency and completeness.\"\"\" # Validate model path exists or is downloadable if not config . model . name_or_path : raise ValueError ( \"Model name_or_path cannot be empty\" ) # Validate at least one dataset is specified if not config . datasets : raise ValueError ( \"At least one dataset must be specified\" ) # Validate dataset names are not empty for dataset in config . datasets : if not dataset . name : raise ValueError ( \"All dataset names must be non-empty\" ) # Validate reward functions are registered (basic check) for reward in config . rewards : if not reward . type : raise ValueError ( \"All reward types must be non-empty\" ) # Validate distributed configuration if config . distributed . backend . value == \"deepspeed\" : if not config . distributed . deepspeed_config : raise ValueError ( \"DeepSpeed backend requires deepspeed_config\" ) # Validate memory requirements are reasonable if config . train . per_device_batch_size * config . train . gradient_accumulation_steps > 1000 : raise ValueError ( \"Effective batch size too large (>1000)\" ) # Validate that either max_steps or epochs is specified if config . train . max_steps is None and config . train . epochs is None : raise ValueError ( \"Either max_steps or epochs must be specified\" ) if config . train . max_steps is not None and config . train . epochs is not None : raise ValueError ( \"Cannot specify both max_steps and epochs\" ) options: show_source: true heading_level: 3 Example : from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"ppo_config.yaml\" )","title":"validate_config"},{"location":"api-reference/core/#trainer-factories","text":"","title":"Trainer Factories"},{"location":"api-reference/core/#sfttrainerfactory","text":"Factory for creating SFT trainers. Factory for creating SFT trainers - delegates to Backend Factory. Source code in src/aligntune/core/sft/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class SFTTrainerFactory : \"\"\"Factory for creating SFT trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , )","title":"SFTTrainerFactory"},{"location":"api-reference/core/#core.sft.trainer_factory.SFTTrainerFactory.create_trainer","text":"Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config SFTConfig SFT configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/sft/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @classmethod def create_trainer ( cls , config : SFTConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: SFT configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_sft_trainer # Extract backend from config, default to automatic selection backend = \"auto\" if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend # If use_unsloth is explicitly False, use TRL backend if hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is False : backend = \"trl\" elif hasattr ( config . model , 'use_unsloth' ) and config . model . use_unsloth is True : backend = \"unsloth\" backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory # Extract optional parameters with defaults logging_steps = getattr ( config . train , 'logging_steps' , getattr ( config . logging , 'log_interval' , 10 )) save_steps = getattr ( config . train , 'save_steps' , config . train . save_interval ) max_steps = getattr ( config . train , 'max_steps' , None ) return create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , warmup_steps = config . train . warmup_steps , logging_steps = logging_steps , save_steps = save_steps , max_steps = max_steps , task_type = config . dataset . task_type . value if hasattr ( config . dataset . task_type , 'value' ) else str ( config . dataset . task_type ), column_mapping = config . dataset . column_mapping , max_samples = config . dataset . max_samples , use_peft = config . model . peft_enabled , lora_r = config . model . lora_rank , lora_alpha = config . model . lora_alpha , lora_dropout = config . model . lora_dropout , lora_target_modules = getattr ( config . model , 'target_modules' , None ), bf16 = config . train . bf16 , gradient_checkpointing = config . model . gradient_checkpointing , ) options: show_source: true heading_level: 3","title":"create_trainer"},{"location":"api-reference/core/#trainerfactory-rl","text":"Factory for creating RL trainers. Factory for creating RLHF trainers - delegates to Backend Factory. Source code in src/aligntune/core/rl/trainer_factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class TrainerFactory : \"\"\"Factory for creating RLHF trainers - delegates to Backend Factory.\"\"\" @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , )","title":"TrainerFactory (RL)"},{"location":"api-reference/core/#core.rl.trainer_factory.TrainerFactory.create_trainer","text":"Create a trainer by delegating to Backend Factory. Parameters: Name Type Description Default config UnifiedConfig Unified configuration required Returns: Type Description Any Trainer instance from Backend Factory Source code in src/aligntune/core/rl/trainer_factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @classmethod def create_trainer ( cls , config : UnifiedConfig ) -> Any : \"\"\" Create a trainer by delegating to Backend Factory. Args: config: Unified configuration Returns: Trainer instance from Backend Factory \"\"\" # Import here to avoid circular imports from ..backend_factory import create_rl_trainer , BackendType # Extract backend from config - default to TRL for safety backend = BackendType . TRL if hasattr ( config , 'backend' ): backend = config . backend elif hasattr ( config . model , 'backend' ): backend = config . model . backend backend_value = backend . value if hasattr ( backend , 'value' ) else backend # Delegate to Backend Factory return create_rl_trainer ( model_name = config . model . name_or_path , dataset_name = config . datasets [ 0 ] . name , # Use first dataset algorithm = config . algo . value , backend = backend_value , output_dir = config . logging . output_dir , num_epochs = config . train . epochs , max_steps = config . train . max_steps , # Add max_steps batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , gradient_accumulation_steps = config . train . gradient_accumulation_steps , # Use available attributes from TrainingConfig eval_interval = config . train . eval_interval , save_interval = config . train . save_interval , # Add reward model configuration reward_model_name = config . model . reward_model_name if hasattr ( config . model , 'reward_model_name' ) else None , ) options: show_source: true heading_level: 3","title":"create_trainer"},{"location":"api-reference/core/#error-classes","text":"","title":"Error Classes"},{"location":"api-reference/core/#aligntuneerror","text":"Base exception for AlignTune errors. Bases: Exception Base exception class for AlignTune errors. Source code in src/aligntune/utils/errors.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AlignTuneError ( Exception ): \"\"\"Base exception class for AlignTune errors.\"\"\" def __init__ ( self , message : str , error_code : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . message = message self . error_code = error_code self . suggestions = suggestions or [] super () . __init__ ( self . message ) def __str__ ( self ): return self . format_error () def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines )","title":"AlignTuneError"},{"location":"api-reference/core/#utils.errors.AlignTuneError.format_error","text":"Format error message with suggestions. Source code in src/aligntune/utils/errors.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def format_error ( self ) -> str : \"\"\"Format error message with suggestions.\"\"\" lines = [ f \"\u274c { self . message } \" ] if self . error_code : lines . append ( f \" Error Code: { self . error_code } \" ) if self . suggestions : lines . append ( \" \ud83d\udca1 Suggestions:\" ) for suggestion in self . suggestions : lines . append ( f \" \u2022 { suggestion } \" ) return \" \\n \" . join ( lines ) options: show_source: true heading_level: 3","title":"format_error"},{"location":"api-reference/core/#configurationerror","text":"Configuration-related errors. Bases: AlignTuneError Configuration-related errors. Source code in src/aligntune/utils/errors.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class ConfigurationError ( AlignTuneError ): \"\"\"Configuration-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field error_code = \"CONFIG_ERROR\" if not suggestions : suggestions = self . _generate_config_suggestions ( message , field ) super () . __init__ ( message , error_code , suggestions ) def _generate_config_suggestions ( self , message : str , field : Optional [ str ]) -> List [ str ]: \"\"\"Generate configuration-specific suggestions.\"\"\" suggestions = [] if \"optimizer\" in message . lower (): suggestions . extend ([ \"Check available optimizers with 'aligntune recipes list'\" , \"Use 'adamw_torch' for general training\" , \"Use 'adamw_8bit' for memory-efficient training\" ]) if \"scheduler\" in message . lower (): suggestions . extend ([ \"Check available schedulers with 'aligntune recipes list'\" , \"Use 'cosine' for most training scenarios\" , \"Use 'linear' for fine-tuning tasks\" ]) if \"model\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Ensure model name is correct (e.g., 'meta-llama/Meta-Llama-3-8B')\" , \"Run 'huggingface-cli login' if using private models\" , \"Check model availability at https://huggingface.co/models\" ]) if \"dataset\" in message . lower () and \"not found\" in message . lower (): suggestions . extend ([ \"Check dataset name is correct (e.g., 'tatsu-lab/alpaca')\" , \"Verify dataset exists at https://huggingface.co/datasets\" ]) if \"memory\" in message . lower (): suggestions . extend ([ \"Reduce batch size\" , \"Enable gradient checkpointing\" , \"Use 4-bit quantization\" , \"Reduce model max length\" ]) if field : suggestions . append ( f \"Check the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3","title":"ConfigurationError"},{"location":"api-reference/core/#trainingerror","text":"Training-related errors. Bases: AlignTuneError Training-related errors. Source code in src/aligntune/utils/errors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class TrainingError ( AlignTuneError ): \"\"\"Training-related errors.\"\"\" def __init__ ( self , message : str , stage : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . stage = stage error_code = \"TRAINING_ERROR\" if not suggestions : suggestions = self . _generate_training_suggestions ( message , stage ) super () . __init__ ( message , error_code , suggestions ) def _generate_training_suggestions ( self , message : str , stage : Optional [ str ]) -> List [ str ]: \"\"\"Generate training-specific suggestions.\"\"\" suggestions = [] if \"cuda\" in message . lower () or \"gpu\" in message . lower (): suggestions . extend ([ \"Check GPU memory availability with 'nvidia-smi'\" , \"Reduce batch size or enable gradient accumulation\" , \"Enable gradient checkpointing\" , \"Use mixed precision (bf16/fp16)\" ]) if \"out of memory\" in message . lower (): suggestions . extend ([ \"Reduce per_device_batch_size\" , \"Increase gradient_accumulation_steps\" , \"Enable gradient_checkpointing\" , \"Use 4-bit quantization\" , \"Reduce max_seq_length\" ]) if \"nan\" in message . lower () or \"inf\" in message . lower (): suggestions . extend ([ \"Reduce learning rate\" , \"Enable gradient clipping\" , \"Check input data quality\" , \"Use more stable optimizer (AdamW instead of Lion)\" ]) if \"loss\" in message . lower () and \"not decreasing\" in message . lower (): suggestions . extend ([ \"Increase learning rate\" , \"Check data quality and preprocessing\" , \"Try different optimizer\" , \"Adjust batch size\" ]) if stage : suggestions . append ( f \"Error occurred during { stage } stage\" ) return suggestions options: show_source: true heading_level: 3","title":"TrainingError"},{"location":"api-reference/core/#environmenterror","text":"Environment-related errors. Bases: AlignTuneError Environment and dependency errors. Source code in src/aligntune/utils/errors.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class EnvironmentError ( AlignTuneError ): \"\"\"Environment and dependency errors.\"\"\" def __init__ ( self , message : str , dependency : Optional [ str ] = None , suggestions : Optional [ List [ str ]] = None ): self . dependency = dependency error_code = \"ENV_ERROR\" if not suggestions : suggestions = self . _generate_env_suggestions ( message , dependency ) super () . __init__ ( message , error_code , suggestions ) def _generate_env_suggestions ( self , message : str , dependency : Optional [ str ]) -> List [ str ]: \"\"\"Generate environment-specific suggestions.\"\"\" suggestions = [] if \"torch\" in message . lower () or \"cuda\" in message . lower (): suggestions . extend ([ \"Install PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" , \"Check CUDA version compatibility\" , \"Update GPU drivers\" ]) if \"transformers\" in message . lower (): suggestions . extend ([ \"Install transformers: pip install transformers\" , \"Update to latest version: pip install --upgrade transformers\" ]) if \"trl\" in message . lower (): suggestions . extend ([ \"Install TRL: pip install trl\" , \"Update to latest version: pip install --upgrade trl\" ]) if \"bitsandbytes\" in message . lower (): suggestions . extend ([ \"Install bitsandbytes: pip install bitsandbytes\" , \"For CUDA issues, try: pip install bitsandbytes --index-url https://download.pytorch.org/whl/cu118\" ]) if \"unsloth\" in message . lower (): suggestions . extend ([ \"Install unsloth: pip install unsloth\" , \"Check CUDA compatibility for your GPU\" ]) if dependency : suggestions . append ( f \"Install missing dependency: pip install { dependency } \" ) return suggestions options: show_source: true heading_level: 3","title":"EnvironmentError"},{"location":"api-reference/core/#validationerror","text":"Validation-related errors. Bases: AlignTuneError Validation-related errors. Source code in src/aligntune/utils/errors.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class ValidationError ( AlignTuneError ): \"\"\"Validation-related errors.\"\"\" def __init__ ( self , message : str , field : Optional [ str ] = None , value : Optional [ Any ] = None , suggestions : Optional [ List [ str ]] = None ): self . field = field self . value = value error_code = \"VALIDATION_ERROR\" if not suggestions : suggestions = self . _generate_validation_suggestions ( message , field , value ) super () . __init__ ( message , error_code , suggestions ) def _generate_validation_suggestions ( self , message : str , field : Optional [ str ], value : Optional [ Any ]) -> List [ str ]: \"\"\"Generate validation-specific suggestions.\"\"\" suggestions = [] if \"positive\" in message . lower () and field : suggestions . append ( f \"Set { field } to a positive value (current: { value } )\" ) if \"required\" in message . lower () and field : suggestions . append ( f \"Provide a value for the required field ' { field } '\" ) if \"invalid\" in message . lower (): suggestions . append ( \"Check the allowed values in the documentation\" ) if field : suggestions . append ( f \"Fix the ' { field } ' field in your configuration\" ) return suggestions options: show_source: true heading_level: 3","title":"ValidationError"},{"location":"api-reference/core/#utility-functions","text":"","title":"Utility Functions"},{"location":"api-reference/core/#validate_backend_selection","text":"Validate backend selection and provide helpful errors. Validate backend selection and provide helpful error messages. Source code in src/aligntune/core/backend_factory.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def validate_backend_selection ( backend : str , training_type : str = \"RL\" ) -> BackendType : \"\"\"Validate backend selection and provide helpful error messages.\"\"\" try : if hasattr ( backend , 'value' ): # BackendType enum backend_type = backend else : # string backend_type = BackendType ( backend . lower ()) except ValueError : available_backends = [ bt . value for bt in BackendType ] raise ValueError ( f \"Invalid backend ' { backend } '. Available backends: { available_backends } \" ) # Check availability if not _check_backend_availability ( backend_type ): if backend_type == BackendType . TRL : raise ImportError ( \"TRL backend not available. Install with: pip install trl \\n \" \"Alternatively, use Unsloth backend: --backend unsloth\" ) elif backend_type == BackendType . UNSLOTH : raise ImportError ( \"Unsloth backend not available. Install with: pip install unsloth \\n \" \"Alternatively, use TRL backend: --backend trl\" ) return backend_type options: show_source: true heading_level: 3 Example : from aligntune.core.backend_factory import validate_backend_selection backend_type = validate_backend_selection ( \"trl\" , \"SFT\" )","title":"validate_backend_selection()"},{"location":"api-reference/core/#see-also","text":"Backend Factory - Main API for creating trainers Configuration Classes - Configuration API Trainers - Trainer API","title":"See Also"},{"location":"api-reference/overview/","text":"API Reference Overview \u00b6 AlignTune provides a comprehensive API for fine-tuning models. This section covers all available APIs. Core APIs \u00b6 Backend Factory \u00b6 The main entry point for creating trainers: from aligntune.core.backend_factory import ( create_sft_trainer , create_rl_trainer , BackendType , get_backend_status ) Functions : - create_sft_trainer() - Create SFT trainer - create_rl_trainer() - Create RL trainer - get_backend_status() - Check backend availability See Backend Factory for details. Configuration Classes \u00b6 Configuration classes for all training parameters: from aligntune.core.sft.config import SFTConfig , TaskType from aligntune.core.rl.config import UnifiedConfig , AlgorithmType See Configuration Classes for details. Trainers \u00b6 Base trainer classes with common methods: from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.rl.trainer_base import TrainerBase Common Methods : - train() - Start training - evaluate() - Evaluate model - save_model() - Save model - load_checkpoint() - Load checkpoint - predict() - Generate predictions - push_to_hub() - Push to HuggingFace Hub See Trainers for details. Quick Reference \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () API Documentation \u00b6 Unified API - Unified RLHF API system Core API - Core functions and utilities Backend Factory - Trainer creation Configuration Classes - All configuration options Trainers - Trainer classes and methods Reward Functions Reference - Complete reward functions list Reward Model Training Reference - Reward model system architecture Next Steps \u00b6 Getting Started - Quick start guide User Guide - Detailed usage guide Examples - Code examples","title":"Overview"},{"location":"api-reference/overview/#api-reference-overview","text":"AlignTune provides a comprehensive API for fine-tuning models. This section covers all available APIs.","title":"API Reference Overview"},{"location":"api-reference/overview/#core-apis","text":"","title":"Core APIs"},{"location":"api-reference/overview/#backend-factory","text":"The main entry point for creating trainers: from aligntune.core.backend_factory import ( create_sft_trainer , create_rl_trainer , BackendType , get_backend_status ) Functions : - create_sft_trainer() - Create SFT trainer - create_rl_trainer() - Create RL trainer - get_backend_status() - Check backend availability See Backend Factory for details.","title":"Backend Factory"},{"location":"api-reference/overview/#configuration-classes","text":"Configuration classes for all training parameters: from aligntune.core.sft.config import SFTConfig , TaskType from aligntune.core.rl.config import UnifiedConfig , AlgorithmType See Configuration Classes for details.","title":"Configuration Classes"},{"location":"api-reference/overview/#trainers","text":"Base trainer classes with common methods: from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.rl.trainer_base import TrainerBase Common Methods : - train() - Start training - evaluate() - Evaluate model - save_model() - Save model - load_checkpoint() - Load checkpoint - predict() - Generate predictions - push_to_hub() - Push to HuggingFace Hub See Trainers for details.","title":"Trainers"},{"location":"api-reference/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"api-reference/overview/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train ()","title":"SFT Training"},{"location":"api-reference/overview/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train ()","title":"RL Training"},{"location":"api-reference/overview/#api-documentation","text":"Unified API - Unified RLHF API system Core API - Core functions and utilities Backend Factory - Trainer creation Configuration Classes - All configuration options Trainers - Trainer classes and methods Reward Functions Reference - Complete reward functions list Reward Model Training Reference - Reward model system architecture","title":"API Documentation"},{"location":"api-reference/overview/#next-steps","text":"Getting Started - Quick start guide User Guide - Detailed usage guide Examples - Code examples","title":"Next Steps"},{"location":"api-reference/reward-functions-reference/","text":"AlignTune Reward Functions Documentation \u00b6 Overview \u00b6 AlignTune provides 27+ prebuilt reward functions covering all major aspects of text generation quality, safety, and task-specific performance. These reward functions can be used individually or combined to create sophisticated reward landscapes for RLHF training. Available Reward Functions \u00b6 Basic Quality Rewards \u00b6 1. Length Reward ( length ) \u00b6 Purpose : Encourages appropriate response length Parameters : min_length , max_length Use Case : Preventing too short or too long responses Example : {\"type\": \"length\", \"params\": {\"min_length\": 10, \"max_length\": 500}} 2. Coherence Reward ( coherence ) \u00b6 Purpose : Measures logical flow and coherence of text Parameters : None Use Case : Ensuring responses make logical sense Example : {\"type\": \"coherence\", \"weight\": 1.0} 3. Fluency Reward ( fluency ) \u00b6 Purpose : Measures grammatical correctness and fluency Parameters : None Use Case : Ensuring grammatically correct responses Example : {\"type\": \"fluency\", \"weight\": 0.8} Task-Specific Rewards \u00b6 4. Sentiment Reward ( sentiment ) \u00b6 Purpose : Encourages specific sentiment (positive, negative, neutral) Parameters : target_sentiment Use Case : Customer service, emotional support Example : {\"type\": \"sentiment\", \"params\": {\"target_sentiment\": \"positive\"}} 5. Safety Reward ( safety ) \u00b6 Purpose : Detects and penalizes harmful content Parameters : strict (bool) Use Case : Content moderation, safe AI Example : {\"type\": \"safety\", \"params\": {\"strict\": true}} 6. Factuality Reward ( factuality ) \u00b6 Purpose : Measures factual accuracy Parameters : None Use Case : Information retrieval, Q&A systems Example : {\"type\": \"factuality\", \"weight\": 1.2} 7. Bias Reward ( bias ) \u00b6 Purpose : Detects and penalizes biased content Parameters : None Use Case : Fair AI, unbiased responses Example : {\"type\": \"bias\", \"weight\": 0.5} Generation Quality Metrics \u00b6 8. BLEU Reward ( bleu ) \u00b6 Purpose : Measures n-gram overlap with reference Parameters : n_gram , smooth Use Case : Translation, summarization Example : {\"type\": \"bleu\", \"params\": {\"n_gram\": 4}} 9. ROUGE Reward ( rouge ) \u00b6 Purpose : Measures recall-oriented understudy for gisting evaluation Parameters : rouge_type (rouge-1, rouge-2, rouge-l) Use Case : Summarization, text generation Example : {\"type\": \"rouge\", \"params\": {\"rouge_type\": \"rouge-l\"}} 10. METEOR Reward ( meteor ) \u00b6 Purpose : Advanced translation evaluation metric Parameters : None Use Case : Translation quality Example : {\"type\": \"meteor\", \"weight\": 0.7} 11. BERTScore Reward ( bertscore ) \u00b6 Purpose : Contextual embedding-based similarity Parameters : model_name Use Case : Semantic similarity, paraphrase detection Example : {\"type\": \"bertscore\", \"params\": {\"model_name\": \"microsoft/DialoGPT-medium\"}} Code Quality Rewards \u00b6 12. Code Syntax Reward ( code_syntax ) \u00b6 Purpose : Validates code syntax correctness Parameters : language Use Case : Code generation, programming assistance Example : {\"type\": \"code_syntax\", \"params\": {\"language\": \"python\"}} 13. Code Execution Reward ( code_execution ) \u00b6 Purpose : Tests if code runs without errors Parameters : timeout , safe_mode Use Case : Code generation, programming education Example : {\"type\": \"code_execution\", \"params\": {\"timeout\": 5, \"safe_mode\": true}} 14. Code Completeness Reward ( code_completeness ) \u00b6 Purpose : Measures code completeness and structure Parameters : None Use Case : Code generation quality Example : {\"type\": \"code_completeness\", \"weight\": 1.0} Math and Reasoning Rewards \u00b6 15. Math Correctness Reward ( math_correctness ) \u00b6 Purpose : Validates mathematical correctness Parameters : tolerance Use Case : Math problem solving, scientific computation Example : {\"type\": \"math_correctness\", \"params\": {\"tolerance\": 1e-6}} 16. Logical Consistency Reward ( logical_consistency ) \u00b6 Purpose : Measures logical consistency in reasoning Parameters : None Use Case : Reasoning tasks, logical problem solving Example : {\"type\": \"logical_consistency\", \"weight\": 1.0} 17. Commonsense Reward ( commonsense ) \u00b6 Purpose : Evaluates commonsense reasoning Parameters : None Use Case : General reasoning, everyday knowledge Example : {\"type\": \"commonsense\", \"weight\": 0.8} Specialized Rewards \u00b6 18. Hallucination Reward ( hallucination ) \u00b6 Purpose : Detects and penalizes hallucinated information Parameters : None Use Case : Factual accuracy, information retrieval Example : {\"type\": \"hallucination\", \"weight\": 1.5} 19. Toxicity Reward ( toxicity ) \u00b6 Purpose : Detects toxic, harmful, or offensive content Parameters : threshold Use Case : Content moderation, safe AI Example : {\"type\": \"toxicity\", \"params\": {\"threshold\": 0.5}} 20. Politeness Reward ( politeness ) \u00b6 Purpose : Encourages polite and respectful language Parameters : None Use Case : Customer service, professional communication Example : {\"type\": \"politeness\", \"weight\": 0.7} 21. Helpfulness Reward ( helpfulness ) \u00b6 Purpose : Measures how helpful and informative the response is Parameters : None Use Case : Q&A systems, customer support Example : {\"type\": \"helpfulness\", \"weight\": 1.0} 22. Honesty Reward ( honesty ) \u00b6 Purpose : Encourages honest and truthful responses Parameters : None Use Case : Trustworthy AI, factual accuracy Example : {\"type\": \"honesty\", \"weight\": 1.2} Multi-Modal Rewards (Future) \u00b6 23. Image Relevance Reward ( image_relevance ) \u00b6 Purpose : Measures relevance to provided images Parameters : None Use Case : Multi-modal tasks, image captioning Example : {\"type\": \"image_relevance\", \"weight\": 1.0} 24. Audio Quality Reward ( audio_quality ) \u00b6 Purpose : Measures audio quality and clarity Parameters : None Use Case : Speech generation, audio processing Example : {\"type\": \"audio_quality\", \"weight\": 1.0} Usage Examples \u00b6 Basic Configuration \u00b6 # Single reward function rewards : - type : \"length\" weight : 1.0 params : min_length : 10 max_length : 500 Multiple Reward Functions \u00b6 # Multiple reward functions with different weights rewards : - type : \"length\" weight : 0.5 params : min_length : 20 max_length : 300 - type : \"safety\" weight : 2.0 params : strict : true - type : \"helpfulness\" weight : 1.5 - type : \"coherence\" weight : 1.0 Task-Specific Configurations \u00b6 Customer Support Bot \u00b6 rewards : - type : \"politeness\" weight : 1.5 - type : \"helpfulness\" weight : 2.0 - type : \"safety\" weight : 1.0 - type : \"length\" weight : 0.5 params : min_length : 50 max_length : 200 Code Generation Assistant \u00b6 rewards : - type : \"code_syntax\" weight : 2.0 params : language : \"python\" - type : \"code_execution\" weight : 1.5 params : timeout : 5 safe_mode : true - type : \"code_completeness\" weight : 1.0 - type : \"helpfulness\" weight : 0.8 Math Problem Solver \u00b6 rewards : - type : \"math_correctness\" weight : 3.0 params : tolerance : 1e-6 - type : \"logical_consistency\" weight : 1.5 - type : \"coherence\" weight : 1.0 - type : \"length\" weight : 0.3 params : min_length : 10 max_length : 1000 Content Moderation System \u00b6 rewards : - type : \"safety\" weight : 3.0 params : strict : true - type : \"toxicity\" weight : 2.5 params : threshold : 0.3 - type : \"bias\" weight : 2.0 - type : \"factuality\" weight : 1.5 Advanced Configuration \u00b6 Custom Reward Weights \u00b6 from aligntune import RewardConfig , RewardType # Create custom reward configuration rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ), RewardConfig ( reward_type = RewardType . LENGTH , weight = 0.5 , params = { \"min_length\" : 20 , \"max_length\" : 300 } ) ] Dynamic Reward Weighting \u00b6 # Example with dynamic weighting based on task rewards : - type : \"safety\" weight : 2.0 # High priority for safety - type : \"helpfulness\" weight : 1.5 # Medium-high priority - type : \"coherence\" weight : 1.0 # Standard priority - type : \"length\" weight : 0.3 # Low priority Reward Function Performance \u00b6 Computational Efficiency \u00b6 Fast : Length, Coherence, Fluency, Sentiment Medium : Safety, Toxicity, Bias, Politeness Slow : BERTScore, Code Execution, Math Correctness Memory Usage \u00b6 Low : Basic rewards (Length, Coherence, etc.) Medium : Task-specific rewards (Safety, Sentiment, etc.) High : Model-based rewards (BERTScore, Code Execution, etc.) Accuracy vs Speed Trade-offs \u00b6 High Accuracy : BERTScore, Math Correctness, Code Execution Balanced : Safety, Toxicity, Helpfulness Fast Approximation : Length, Coherence, Fluency Best Practices \u00b6 Reward Selection \u00b6 Start Simple : Begin with basic rewards (length, coherence, safety) Add Task-Specific : Include rewards relevant to your use case Balance Weights : Ensure no single reward dominates Monitor Performance : Track reward trends during training Weight Tuning \u00b6 Safety First : Always include safety rewards with high weight Task Relevance : Higher weights for task-specific rewards Quality Balance : Balance between different quality aspects Iterative Tuning : Adjust weights based on training results Performance Optimization \u00b6 Use Fast Rewards : Prefer fast rewards for real-time applications Batch Processing : Process multiple texts together when possible Caching : Cache model outputs for repeated computations Device Selection : Use appropriate devices (CPU/GPU) for each reward Extending Reward Functions \u00b6 Custom Reward Functions \u00b6 from aligntune import RewardFunction , RewardType class CustomReward ( RewardFunction ): def __init__ ( self , config : RewardConfig ): super () . __init__ ( config ) # Initialize your custom reward logic def compute ( self , text : str , reference : Optional [ str ] = None , ** kwargs ) -> float : # Implement your custom reward computation return 0.8 # Return reward score between 0 and 1 # Register custom reward from aligntune import RewardRegistry RewardRegistry . register_reward ( \"custom\" , CustomReward ) Combining Rewards \u00b6 def combined_reward ( text : str , rewards : List [ RewardFunction ]) -> float : \"\"\"Combine multiple reward functions.\"\"\" total_score = 0.0 total_weight = 0.0 for reward_func in rewards : score = reward_func . compute ( text ) weight = reward_func . config . weight total_score += score * weight total_weight += weight return total_score / total_weight if total_weight > 0 else 0.0 Monitoring and Debugging \u00b6 Reward Tracking \u00b6 Monitor individual reward scores during training Track reward trends and convergence Identify which rewards are most influential Debug reward computation issues Common Issues \u00b6 Reward Collapse : All rewards converge to similar values Reward Instability : High variance in reward scores Reward Conflicts : Conflicting objectives between rewards Performance Bottlenecks : Slow reward computation Debugging Tools \u00b6 Reward score logging and visualization Individual reward breakdown analysis Performance profiling for slow rewards A/B testing for reward configurations AlignTune provides a comprehensive suite of reward functions to create sophisticated reward landscapes for any RLHF training scenario. Choose the right combination of rewards for your specific use case and achieve optimal model performance!","title":"AlignTune Reward Functions Documentation"},{"location":"api-reference/reward-functions-reference/#aligntune-reward-functions-documentation","text":"","title":"AlignTune Reward Functions Documentation"},{"location":"api-reference/reward-functions-reference/#overview","text":"AlignTune provides 27+ prebuilt reward functions covering all major aspects of text generation quality, safety, and task-specific performance. These reward functions can be used individually or combined to create sophisticated reward landscapes for RLHF training.","title":"Overview"},{"location":"api-reference/reward-functions-reference/#available-reward-functions","text":"","title":"Available Reward Functions"},{"location":"api-reference/reward-functions-reference/#basic-quality-rewards","text":"","title":"Basic Quality Rewards"},{"location":"api-reference/reward-functions-reference/#1-length-reward-length","text":"Purpose : Encourages appropriate response length Parameters : min_length , max_length Use Case : Preventing too short or too long responses Example : {\"type\": \"length\", \"params\": {\"min_length\": 10, \"max_length\": 500}}","title":"1. Length Reward (length)"},{"location":"api-reference/reward-functions-reference/#2-coherence-reward-coherence","text":"Purpose : Measures logical flow and coherence of text Parameters : None Use Case : Ensuring responses make logical sense Example : {\"type\": \"coherence\", \"weight\": 1.0}","title":"2. Coherence Reward (coherence)"},{"location":"api-reference/reward-functions-reference/#3-fluency-reward-fluency","text":"Purpose : Measures grammatical correctness and fluency Parameters : None Use Case : Ensuring grammatically correct responses Example : {\"type\": \"fluency\", \"weight\": 0.8}","title":"3. Fluency Reward (fluency)"},{"location":"api-reference/reward-functions-reference/#task-specific-rewards","text":"","title":"Task-Specific Rewards"},{"location":"api-reference/reward-functions-reference/#4-sentiment-reward-sentiment","text":"Purpose : Encourages specific sentiment (positive, negative, neutral) Parameters : target_sentiment Use Case : Customer service, emotional support Example : {\"type\": \"sentiment\", \"params\": {\"target_sentiment\": \"positive\"}}","title":"4. Sentiment Reward (sentiment)"},{"location":"api-reference/reward-functions-reference/#5-safety-reward-safety","text":"Purpose : Detects and penalizes harmful content Parameters : strict (bool) Use Case : Content moderation, safe AI Example : {\"type\": \"safety\", \"params\": {\"strict\": true}}","title":"5. Safety Reward (safety)"},{"location":"api-reference/reward-functions-reference/#6-factuality-reward-factuality","text":"Purpose : Measures factual accuracy Parameters : None Use Case : Information retrieval, Q&A systems Example : {\"type\": \"factuality\", \"weight\": 1.2}","title":"6. Factuality Reward (factuality)"},{"location":"api-reference/reward-functions-reference/#7-bias-reward-bias","text":"Purpose : Detects and penalizes biased content Parameters : None Use Case : Fair AI, unbiased responses Example : {\"type\": \"bias\", \"weight\": 0.5}","title":"7. Bias Reward (bias)"},{"location":"api-reference/reward-functions-reference/#generation-quality-metrics","text":"","title":"Generation Quality Metrics"},{"location":"api-reference/reward-functions-reference/#8-bleu-reward-bleu","text":"Purpose : Measures n-gram overlap with reference Parameters : n_gram , smooth Use Case : Translation, summarization Example : {\"type\": \"bleu\", \"params\": {\"n_gram\": 4}}","title":"8. BLEU Reward (bleu)"},{"location":"api-reference/reward-functions-reference/#9-rouge-reward-rouge","text":"Purpose : Measures recall-oriented understudy for gisting evaluation Parameters : rouge_type (rouge-1, rouge-2, rouge-l) Use Case : Summarization, text generation Example : {\"type\": \"rouge\", \"params\": {\"rouge_type\": \"rouge-l\"}}","title":"9. ROUGE Reward (rouge)"},{"location":"api-reference/reward-functions-reference/#10-meteor-reward-meteor","text":"Purpose : Advanced translation evaluation metric Parameters : None Use Case : Translation quality Example : {\"type\": \"meteor\", \"weight\": 0.7}","title":"10. METEOR Reward (meteor)"},{"location":"api-reference/reward-functions-reference/#11-bertscore-reward-bertscore","text":"Purpose : Contextual embedding-based similarity Parameters : model_name Use Case : Semantic similarity, paraphrase detection Example : {\"type\": \"bertscore\", \"params\": {\"model_name\": \"microsoft/DialoGPT-medium\"}}","title":"11. BERTScore Reward (bertscore)"},{"location":"api-reference/reward-functions-reference/#code-quality-rewards","text":"","title":"Code Quality Rewards"},{"location":"api-reference/reward-functions-reference/#12-code-syntax-reward-code_syntax","text":"Purpose : Validates code syntax correctness Parameters : language Use Case : Code generation, programming assistance Example : {\"type\": \"code_syntax\", \"params\": {\"language\": \"python\"}}","title":"12. Code Syntax Reward (code_syntax)"},{"location":"api-reference/reward-functions-reference/#13-code-execution-reward-code_execution","text":"Purpose : Tests if code runs without errors Parameters : timeout , safe_mode Use Case : Code generation, programming education Example : {\"type\": \"code_execution\", \"params\": {\"timeout\": 5, \"safe_mode\": true}}","title":"13. Code Execution Reward (code_execution)"},{"location":"api-reference/reward-functions-reference/#14-code-completeness-reward-code_completeness","text":"Purpose : Measures code completeness and structure Parameters : None Use Case : Code generation quality Example : {\"type\": \"code_completeness\", \"weight\": 1.0}","title":"14. Code Completeness Reward (code_completeness)"},{"location":"api-reference/reward-functions-reference/#math-and-reasoning-rewards","text":"","title":"Math and Reasoning Rewards"},{"location":"api-reference/reward-functions-reference/#15-math-correctness-reward-math_correctness","text":"Purpose : Validates mathematical correctness Parameters : tolerance Use Case : Math problem solving, scientific computation Example : {\"type\": \"math_correctness\", \"params\": {\"tolerance\": 1e-6}}","title":"15. Math Correctness Reward (math_correctness)"},{"location":"api-reference/reward-functions-reference/#16-logical-consistency-reward-logical_consistency","text":"Purpose : Measures logical consistency in reasoning Parameters : None Use Case : Reasoning tasks, logical problem solving Example : {\"type\": \"logical_consistency\", \"weight\": 1.0}","title":"16. Logical Consistency Reward (logical_consistency)"},{"location":"api-reference/reward-functions-reference/#17-commonsense-reward-commonsense","text":"Purpose : Evaluates commonsense reasoning Parameters : None Use Case : General reasoning, everyday knowledge Example : {\"type\": \"commonsense\", \"weight\": 0.8}","title":"17. Commonsense Reward (commonsense)"},{"location":"api-reference/reward-functions-reference/#specialized-rewards","text":"","title":"Specialized Rewards"},{"location":"api-reference/reward-functions-reference/#18-hallucination-reward-hallucination","text":"Purpose : Detects and penalizes hallucinated information Parameters : None Use Case : Factual accuracy, information retrieval Example : {\"type\": \"hallucination\", \"weight\": 1.5}","title":"18. Hallucination Reward (hallucination)"},{"location":"api-reference/reward-functions-reference/#19-toxicity-reward-toxicity","text":"Purpose : Detects toxic, harmful, or offensive content Parameters : threshold Use Case : Content moderation, safe AI Example : {\"type\": \"toxicity\", \"params\": {\"threshold\": 0.5}}","title":"19. Toxicity Reward (toxicity)"},{"location":"api-reference/reward-functions-reference/#20-politeness-reward-politeness","text":"Purpose : Encourages polite and respectful language Parameters : None Use Case : Customer service, professional communication Example : {\"type\": \"politeness\", \"weight\": 0.7}","title":"20. Politeness Reward (politeness)"},{"location":"api-reference/reward-functions-reference/#21-helpfulness-reward-helpfulness","text":"Purpose : Measures how helpful and informative the response is Parameters : None Use Case : Q&A systems, customer support Example : {\"type\": \"helpfulness\", \"weight\": 1.0}","title":"21. Helpfulness Reward (helpfulness)"},{"location":"api-reference/reward-functions-reference/#22-honesty-reward-honesty","text":"Purpose : Encourages honest and truthful responses Parameters : None Use Case : Trustworthy AI, factual accuracy Example : {\"type\": \"honesty\", \"weight\": 1.2}","title":"22. Honesty Reward (honesty)"},{"location":"api-reference/reward-functions-reference/#multi-modal-rewards-future","text":"","title":"Multi-Modal Rewards (Future)"},{"location":"api-reference/reward-functions-reference/#23-image-relevance-reward-image_relevance","text":"Purpose : Measures relevance to provided images Parameters : None Use Case : Multi-modal tasks, image captioning Example : {\"type\": \"image_relevance\", \"weight\": 1.0}","title":"23. Image Relevance Reward (image_relevance)"},{"location":"api-reference/reward-functions-reference/#24-audio-quality-reward-audio_quality","text":"Purpose : Measures audio quality and clarity Parameters : None Use Case : Speech generation, audio processing Example : {\"type\": \"audio_quality\", \"weight\": 1.0}","title":"24. Audio Quality Reward (audio_quality)"},{"location":"api-reference/reward-functions-reference/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/reward-functions-reference/#basic-configuration","text":"# Single reward function rewards : - type : \"length\" weight : 1.0 params : min_length : 10 max_length : 500","title":"Basic Configuration"},{"location":"api-reference/reward-functions-reference/#multiple-reward-functions","text":"# Multiple reward functions with different weights rewards : - type : \"length\" weight : 0.5 params : min_length : 20 max_length : 300 - type : \"safety\" weight : 2.0 params : strict : true - type : \"helpfulness\" weight : 1.5 - type : \"coherence\" weight : 1.0","title":"Multiple Reward Functions"},{"location":"api-reference/reward-functions-reference/#task-specific-configurations","text":"","title":"Task-Specific Configurations"},{"location":"api-reference/reward-functions-reference/#customer-support-bot","text":"rewards : - type : \"politeness\" weight : 1.5 - type : \"helpfulness\" weight : 2.0 - type : \"safety\" weight : 1.0 - type : \"length\" weight : 0.5 params : min_length : 50 max_length : 200","title":"Customer Support Bot"},{"location":"api-reference/reward-functions-reference/#code-generation-assistant","text":"rewards : - type : \"code_syntax\" weight : 2.0 params : language : \"python\" - type : \"code_execution\" weight : 1.5 params : timeout : 5 safe_mode : true - type : \"code_completeness\" weight : 1.0 - type : \"helpfulness\" weight : 0.8","title":"Code Generation Assistant"},{"location":"api-reference/reward-functions-reference/#math-problem-solver","text":"rewards : - type : \"math_correctness\" weight : 3.0 params : tolerance : 1e-6 - type : \"logical_consistency\" weight : 1.5 - type : \"coherence\" weight : 1.0 - type : \"length\" weight : 0.3 params : min_length : 10 max_length : 1000","title":"Math Problem Solver"},{"location":"api-reference/reward-functions-reference/#content-moderation-system","text":"rewards : - type : \"safety\" weight : 3.0 params : strict : true - type : \"toxicity\" weight : 2.5 params : threshold : 0.3 - type : \"bias\" weight : 2.0 - type : \"factuality\" weight : 1.5","title":"Content Moderation System"},{"location":"api-reference/reward-functions-reference/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"api-reference/reward-functions-reference/#custom-reward-weights","text":"from aligntune import RewardConfig , RewardType # Create custom reward configuration rewards = [ RewardConfig ( reward_type = RewardType . SAFETY , weight = 2.0 , params = { \"strict\" : True } ), RewardConfig ( reward_type = RewardType . HELPFULNESS , weight = 1.5 ), RewardConfig ( reward_type = RewardType . LENGTH , weight = 0.5 , params = { \"min_length\" : 20 , \"max_length\" : 300 } ) ]","title":"Custom Reward Weights"},{"location":"api-reference/reward-functions-reference/#dynamic-reward-weighting","text":"# Example with dynamic weighting based on task rewards : - type : \"safety\" weight : 2.0 # High priority for safety - type : \"helpfulness\" weight : 1.5 # Medium-high priority - type : \"coherence\" weight : 1.0 # Standard priority - type : \"length\" weight : 0.3 # Low priority","title":"Dynamic Reward Weighting"},{"location":"api-reference/reward-functions-reference/#reward-function-performance","text":"","title":"Reward Function Performance"},{"location":"api-reference/reward-functions-reference/#computational-efficiency","text":"Fast : Length, Coherence, Fluency, Sentiment Medium : Safety, Toxicity, Bias, Politeness Slow : BERTScore, Code Execution, Math Correctness","title":"Computational Efficiency"},{"location":"api-reference/reward-functions-reference/#memory-usage","text":"Low : Basic rewards (Length, Coherence, etc.) Medium : Task-specific rewards (Safety, Sentiment, etc.) High : Model-based rewards (BERTScore, Code Execution, etc.)","title":"Memory Usage"},{"location":"api-reference/reward-functions-reference/#accuracy-vs-speed-trade-offs","text":"High Accuracy : BERTScore, Math Correctness, Code Execution Balanced : Safety, Toxicity, Helpfulness Fast Approximation : Length, Coherence, Fluency","title":"Accuracy vs Speed Trade-offs"},{"location":"api-reference/reward-functions-reference/#best-practices","text":"","title":"Best Practices"},{"location":"api-reference/reward-functions-reference/#reward-selection","text":"Start Simple : Begin with basic rewards (length, coherence, safety) Add Task-Specific : Include rewards relevant to your use case Balance Weights : Ensure no single reward dominates Monitor Performance : Track reward trends during training","title":"Reward Selection"},{"location":"api-reference/reward-functions-reference/#weight-tuning","text":"Safety First : Always include safety rewards with high weight Task Relevance : Higher weights for task-specific rewards Quality Balance : Balance between different quality aspects Iterative Tuning : Adjust weights based on training results","title":"Weight Tuning"},{"location":"api-reference/reward-functions-reference/#performance-optimization","text":"Use Fast Rewards : Prefer fast rewards for real-time applications Batch Processing : Process multiple texts together when possible Caching : Cache model outputs for repeated computations Device Selection : Use appropriate devices (CPU/GPU) for each reward","title":"Performance Optimization"},{"location":"api-reference/reward-functions-reference/#extending-reward-functions","text":"","title":"Extending Reward Functions"},{"location":"api-reference/reward-functions-reference/#custom-reward-functions","text":"from aligntune import RewardFunction , RewardType class CustomReward ( RewardFunction ): def __init__ ( self , config : RewardConfig ): super () . __init__ ( config ) # Initialize your custom reward logic def compute ( self , text : str , reference : Optional [ str ] = None , ** kwargs ) -> float : # Implement your custom reward computation return 0.8 # Return reward score between 0 and 1 # Register custom reward from aligntune import RewardRegistry RewardRegistry . register_reward ( \"custom\" , CustomReward )","title":"Custom Reward Functions"},{"location":"api-reference/reward-functions-reference/#combining-rewards","text":"def combined_reward ( text : str , rewards : List [ RewardFunction ]) -> float : \"\"\"Combine multiple reward functions.\"\"\" total_score = 0.0 total_weight = 0.0 for reward_func in rewards : score = reward_func . compute ( text ) weight = reward_func . config . weight total_score += score * weight total_weight += weight return total_score / total_weight if total_weight > 0 else 0.0","title":"Combining Rewards"},{"location":"api-reference/reward-functions-reference/#monitoring-and-debugging","text":"","title":"Monitoring and Debugging"},{"location":"api-reference/reward-functions-reference/#reward-tracking","text":"Monitor individual reward scores during training Track reward trends and convergence Identify which rewards are most influential Debug reward computation issues","title":"Reward Tracking"},{"location":"api-reference/reward-functions-reference/#common-issues","text":"Reward Collapse : All rewards converge to similar values Reward Instability : High variance in reward scores Reward Conflicts : Conflicting objectives between rewards Performance Bottlenecks : Slow reward computation","title":"Common Issues"},{"location":"api-reference/reward-functions-reference/#debugging-tools","text":"Reward score logging and visualization Individual reward breakdown analysis Performance profiling for slow rewards A/B testing for reward configurations AlignTune provides a comprehensive suite of reward functions to create sophisticated reward landscapes for any RLHF training scenario. Choose the right combination of rewards for your specific use case and achieve optimal model performance!","title":"Debugging Tools"},{"location":"api-reference/reward-model-training-reference/","text":"Reward Model Training System \u00b6 Overview \u00b6 The Reward Model Training System provides comprehensive capabilities for training neural reward models from rule-based reward functions and integrating them into PPO training pipelines. This system enables scalable reward model training with strict validation, no fallbacks, and production-ready implementations. Architecture \u00b6 Core Components \u00b6 RewardModelTrainer : Main training class using TRL's RewardTrainer RewardModelDataset : PyTorch Dataset for reward training data RewardModelValidator : Strict validation with no fallbacks RewardModelLoader : Loads HF Hub, local, or custom trained models RewardModelTrainingConfig : Configuration for training parameters RewardModelSourceConfig : Configuration for reward model sources Integration Points \u00b6 PPO Integration : Both Unsloth and TRL PPO backends support reward model loading Reward Functions : Uses existing CompositeReward system for training data generation Configuration : Integrates with existing dataclass-based config system Validation : Strict validation throughout with clear error messages Usage Modes \u00b6 1. Standalone Reward Model Training \u00b6 Train reward models independently without PPO: from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func ], composite_weights = [ 0.5 , 0.5 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) 2. PPO with Custom Reward Model Training \u00b6 Train custom reward models as part of PPO training: from aligntune.core.backend_factory import create_rl_trainer # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , max_samples = 100 ) 3. PPO with Pre-trained Reward Models \u00b6 Use existing reward models from HuggingFace Hub or local paths: # HuggingFace Hub model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" , # ... other config ) # Local model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_path = \"./reward_models/my_custom_model\" , # ... other config ) Configuration \u00b6 RewardModelTrainingConfig \u00b6 Configuration for reward model training with strict validation: @dataclass class RewardModelTrainingConfig : base_model_name : str # Required - no default training_texts : List [ str ] # Required - no default reward_functions : List [ str ] # Required - no default output_dir : str # Required - no default # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training parameters with justified defaults num_epochs : int = 3 learning_rate : float = 1e-5 batch_size : int = 8 gradient_accumulation_steps : int = 4 max_length : int = 512 RewardModelSourceConfig \u00b6 Configuration for reward model source with strict mutual exclusivity: @dataclass class RewardModelSourceConfig : source_type : str # \"pretrained_hf\", \"pretrained_local\", \"custom_trained\" model_name : Optional [ str ] = None # For HF Hub model_path : Optional [ str ] = None # For local training_config : Optional [ RewardModelTrainingConfig ] = None # For custom Validation \u00b6 Strict Validation Rules \u00b6 Exactly One Source : Must specify exactly one reward source (HF Hub, local, or custom) No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Consistent Lengths : Reference texts must match training texts length Positive Weights : All reward weights must be positive Error Handling \u00b6 No Fallbacks : System fails fast with clear error messages No Silent Failures : All errors are logged and raised Comprehensive Validation : Every input is validated before processing Clear Messages : Error messages include actionable information Examples \u00b6 Complete Custom Training Example \u00b6 import logging from aligntune.core.backend_factory import create_rl_trainer logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def load_training_texts (): \"\"\"Load your training texts here.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure about this, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" , # ... more texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # Training parameters num_epochs = 1 , batch_size = 1 , max_samples = 100 , output_dir = \"./output/ppo_custom_reward\" ) # Setup and train trainer . setup_data () trainer . setup_trainer () results = trainer . train () logger . info ( f \"Training completed: { results } \" ) if __name__ == \"__main__\" : main () Standalone Training Example \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry def standalone_training (): # Get reward functions registry = RewardRegistry () reward_functions = [ registry . get_reward_function ( \"length\" ), registry . get_reward_function ( \"sentiment\" ), registry . get_reward_function ( \"safety\" ) ] # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = reward_functions , composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data texts = [ \"Your training texts here...\" ] * 20 training_data = trainer . generate_training_data ( texts ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/standalone\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Model trained and saved to: { model_path } \" ) if __name__ == \"__main__\" : standalone_training () Supported Reward Functions \u00b6 The system supports all reward functions available in the registry: Length : Text length rewards Sentiment : Sentiment analysis rewards Safety : Safety and toxicity rewards Coherence : Text coherence rewards BLEU : BLEU score rewards ROUGE : ROUGE score rewards Math Correctness : Mathematical reasoning rewards Code Syntax : Code quality rewards Toxicity : Toxicity detection rewards Troubleshooting \u00b6 Common Issues \u00b6 \"Need at least 10 training texts\" Solution: Provide at least 10 training texts Check: len(training_texts) >= 10 \"Missing reward functions\" Solution: Use functions available in registry Check: RewardRegistry().list_reward_functions() \"Multiple reward sources specified\" Solution: Specify exactly one source (HF Hub, local, or custom) Check: Only one of reward_model_name , reward_model_path , train_custom_reward_model should be True \"reward_model_path does not exist\" Solution: Ensure local path exists and contains model files Check: Path exists and contains config.json and model files \"Model not accessible on HuggingFace Hub\" Solution: Check model name and internet connection Check: huggingface_hub.model_info(model_name) Debug Mode \u00b6 Enable debug logging for detailed information: import logging logging . basicConfig ( level = logging . DEBUG ) Validation Helpers \u00b6 Use validation helpers to check configuration: from aligntune.rewards.training import RewardModelValidator # Validate training config RewardModelValidator . validate_training_config ( config ) # Validate reward source RewardModelValidator . validate_reward_source ( source_config ) # Validate HF model access RewardModelValidator . validate_hf_model_access ( \"model-name\" ) # Validate local path RewardModelValidator . validate_local_path ( \"/path/to/model\" ) Performance Considerations \u00b6 Memory Usage \u00b6 Batch Processing : Use appropriate batch sizes for your GPU memory Gradient Accumulation : Use gradient accumulation for effective larger batch sizes Model Quantization : Consider 4-bit quantization for memory efficiency Training Speed \u00b6 Batch Size : Larger batches generally train faster Gradient Accumulation : Use gradient accumulation to simulate larger batches Model Size : Smaller base models train faster but may have lower quality Scalability \u00b6 Distributed Training : Use multiple GPUs for large-scale training Data Parallelism : Process multiple samples in parallel Model Parallelism : Split large models across devices Best Practices \u00b6 Data Quality : Use high-quality training texts for better reward models Function Selection : Choose reward functions relevant to your task Weight Tuning : Experiment with different reward function weights Validation : Always validate your configuration before training Monitoring : Monitor training progress and adjust parameters as needed Testing : Test trained models before using in production API Reference \u00b6 RewardModelTrainer \u00b6 class RewardModelTrainer : def __init__ ( self , base_model_name : str , reward_functions : List [ RewardFunction ], composite_weights : List [ float ]) def generate_training_data ( self , texts : List [ str ], references : Optional [ List [ str ]] = None , batch_size : int = 32 ) -> RewardModelDataset def train_reward_model ( self , training_data : RewardModelDataset , output_dir : str , ** kwargs ) -> str def create_scalable_pipeline ( self , training_texts : List [ str ], output_dir : str , ** kwargs ) -> str RewardModelDataset \u00b6 class RewardModelDataset ( Dataset ): def __init__ ( self , texts : List [ str ], reward_scores : List [ float ], tokenizer : AutoTokenizer , max_length : int = 512 ) def __len__ ( self ) -> int def __getitem__ ( self , idx : int ) -> Dict [ str , torch . Tensor ] RewardModelValidator \u00b6 class RewardModelValidator : @staticmethod def validate_reward_source ( source_config : RewardModelSourceConfig ) -> None @staticmethod def validate_training_config ( config : RewardModelTrainingConfig ) -> None @staticmethod def validate_hf_model_access ( model_name : str ) -> None @staticmethod def validate_local_path ( path : str ) -> None RewardModelLoader \u00b6 class RewardModelLoader : def load_from_huggingface ( self , model_name : str , ** kwargs ) -> AutoModelForSequenceClassification def load_from_local ( self , model_path : str , ** kwargs ) -> AutoModelForSequenceClassification Contributing \u00b6 When contributing to the reward model training system: Follow Validation : Always use strict validation with no fallbacks Add Tests : Include comprehensive tests for new features Document Changes : Update documentation for any API changes Error Handling : Provide clear, actionable error messages Performance : Consider performance implications of changes License \u00b6 This reward model training system is part of the AlignTune project and follows the same license terms.","title":"Reward Model Training System"},{"location":"api-reference/reward-model-training-reference/#reward-model-training-system","text":"","title":"Reward Model Training System"},{"location":"api-reference/reward-model-training-reference/#overview","text":"The Reward Model Training System provides comprehensive capabilities for training neural reward models from rule-based reward functions and integrating them into PPO training pipelines. This system enables scalable reward model training with strict validation, no fallbacks, and production-ready implementations.","title":"Overview"},{"location":"api-reference/reward-model-training-reference/#architecture","text":"","title":"Architecture"},{"location":"api-reference/reward-model-training-reference/#core-components","text":"RewardModelTrainer : Main training class using TRL's RewardTrainer RewardModelDataset : PyTorch Dataset for reward training data RewardModelValidator : Strict validation with no fallbacks RewardModelLoader : Loads HF Hub, local, or custom trained models RewardModelTrainingConfig : Configuration for training parameters RewardModelSourceConfig : Configuration for reward model sources","title":"Core Components"},{"location":"api-reference/reward-model-training-reference/#integration-points","text":"PPO Integration : Both Unsloth and TRL PPO backends support reward model loading Reward Functions : Uses existing CompositeReward system for training data generation Configuration : Integrates with existing dataclass-based config system Validation : Strict validation throughout with clear error messages","title":"Integration Points"},{"location":"api-reference/reward-model-training-reference/#usage-modes","text":"","title":"Usage Modes"},{"location":"api-reference/reward-model-training-reference/#1-standalone-reward-model-training","text":"Train reward models independently without PPO: from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func ], composite_weights = [ 0.5 , 0.5 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 )","title":"1. Standalone Reward Model Training"},{"location":"api-reference/reward-model-training-reference/#2-ppo-with-custom-reward-model-training","text":"Train custom reward models as part of PPO training: from aligntune.core.backend_factory import create_rl_trainer # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , max_samples = 100 )","title":"2. PPO with Custom Reward Model Training"},{"location":"api-reference/reward-model-training-reference/#3-ppo-with-pre-trained-reward-models","text":"Use existing reward models from HuggingFace Hub or local paths: # HuggingFace Hub model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\" , # ... other config ) # Local model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_model_path = \"./reward_models/my_custom_model\" , # ... other config )","title":"3. PPO with Pre-trained Reward Models"},{"location":"api-reference/reward-model-training-reference/#configuration","text":"","title":"Configuration"},{"location":"api-reference/reward-model-training-reference/#rewardmodeltrainingconfig","text":"Configuration for reward model training with strict validation: @dataclass class RewardModelTrainingConfig : base_model_name : str # Required - no default training_texts : List [ str ] # Required - no default reward_functions : List [ str ] # Required - no default output_dir : str # Required - no default # Optional but validated if provided reference_texts : Optional [ List [ str ]] = None reward_weights : Optional [ List [ float ]] = None # Training parameters with justified defaults num_epochs : int = 3 learning_rate : float = 1e-5 batch_size : int = 8 gradient_accumulation_steps : int = 4 max_length : int = 512","title":"RewardModelTrainingConfig"},{"location":"api-reference/reward-model-training-reference/#rewardmodelsourceconfig","text":"Configuration for reward model source with strict mutual exclusivity: @dataclass class RewardModelSourceConfig : source_type : str # \"pretrained_hf\", \"pretrained_local\", \"custom_trained\" model_name : Optional [ str ] = None # For HF Hub model_path : Optional [ str ] = None # For local training_config : Optional [ RewardModelTrainingConfig ] = None # For custom","title":"RewardModelSourceConfig"},{"location":"api-reference/reward-model-training-reference/#validation","text":"","title":"Validation"},{"location":"api-reference/reward-model-training-reference/#strict-validation-rules","text":"Exactly One Source : Must specify exactly one reward source (HF Hub, local, or custom) No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Consistent Lengths : Reference texts must match training texts length Positive Weights : All reward weights must be positive","title":"Strict Validation Rules"},{"location":"api-reference/reward-model-training-reference/#error-handling","text":"No Fallbacks : System fails fast with clear error messages No Silent Failures : All errors are logged and raised Comprehensive Validation : Every input is validated before processing Clear Messages : Error messages include actionable information","title":"Error Handling"},{"location":"api-reference/reward-model-training-reference/#examples","text":"","title":"Examples"},{"location":"api-reference/reward-model-training-reference/#complete-custom-training-example","text":"import logging from aligntune.core.backend_factory import create_rl_trainer logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def load_training_texts (): \"\"\"Load your training texts here.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure about this, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" , # ... more texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # Training parameters num_epochs = 1 , batch_size = 1 , max_samples = 100 , output_dir = \"./output/ppo_custom_reward\" ) # Setup and train trainer . setup_data () trainer . setup_trainer () results = trainer . train () logger . info ( f \"Training completed: { results } \" ) if __name__ == \"__main__\" : main ()","title":"Complete Custom Training Example"},{"location":"api-reference/reward-model-training-reference/#standalone-training-example","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry def standalone_training (): # Get reward functions registry = RewardRegistry () reward_functions = [ registry . get_reward_function ( \"length\" ), registry . get_reward_function ( \"sentiment\" ), registry . get_reward_function ( \"safety\" ) ] # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = reward_functions , composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data texts = [ \"Your training texts here...\" ] * 20 training_data = trainer . generate_training_data ( texts ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/standalone\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Model trained and saved to: { model_path } \" ) if __name__ == \"__main__\" : standalone_training ()","title":"Standalone Training Example"},{"location":"api-reference/reward-model-training-reference/#supported-reward-functions","text":"The system supports all reward functions available in the registry: Length : Text length rewards Sentiment : Sentiment analysis rewards Safety : Safety and toxicity rewards Coherence : Text coherence rewards BLEU : BLEU score rewards ROUGE : ROUGE score rewards Math Correctness : Mathematical reasoning rewards Code Syntax : Code quality rewards Toxicity : Toxicity detection rewards","title":"Supported Reward Functions"},{"location":"api-reference/reward-model-training-reference/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api-reference/reward-model-training-reference/#common-issues","text":"\"Need at least 10 training texts\" Solution: Provide at least 10 training texts Check: len(training_texts) >= 10 \"Missing reward functions\" Solution: Use functions available in registry Check: RewardRegistry().list_reward_functions() \"Multiple reward sources specified\" Solution: Specify exactly one source (HF Hub, local, or custom) Check: Only one of reward_model_name , reward_model_path , train_custom_reward_model should be True \"reward_model_path does not exist\" Solution: Ensure local path exists and contains model files Check: Path exists and contains config.json and model files \"Model not accessible on HuggingFace Hub\" Solution: Check model name and internet connection Check: huggingface_hub.model_info(model_name)","title":"Common Issues"},{"location":"api-reference/reward-model-training-reference/#debug-mode","text":"Enable debug logging for detailed information: import logging logging . basicConfig ( level = logging . DEBUG )","title":"Debug Mode"},{"location":"api-reference/reward-model-training-reference/#validation-helpers","text":"Use validation helpers to check configuration: from aligntune.rewards.training import RewardModelValidator # Validate training config RewardModelValidator . validate_training_config ( config ) # Validate reward source RewardModelValidator . validate_reward_source ( source_config ) # Validate HF model access RewardModelValidator . validate_hf_model_access ( \"model-name\" ) # Validate local path RewardModelValidator . validate_local_path ( \"/path/to/model\" )","title":"Validation Helpers"},{"location":"api-reference/reward-model-training-reference/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"api-reference/reward-model-training-reference/#memory-usage","text":"Batch Processing : Use appropriate batch sizes for your GPU memory Gradient Accumulation : Use gradient accumulation for effective larger batch sizes Model Quantization : Consider 4-bit quantization for memory efficiency","title":"Memory Usage"},{"location":"api-reference/reward-model-training-reference/#training-speed","text":"Batch Size : Larger batches generally train faster Gradient Accumulation : Use gradient accumulation to simulate larger batches Model Size : Smaller base models train faster but may have lower quality","title":"Training Speed"},{"location":"api-reference/reward-model-training-reference/#scalability","text":"Distributed Training : Use multiple GPUs for large-scale training Data Parallelism : Process multiple samples in parallel Model Parallelism : Split large models across devices","title":"Scalability"},{"location":"api-reference/reward-model-training-reference/#best-practices","text":"Data Quality : Use high-quality training texts for better reward models Function Selection : Choose reward functions relevant to your task Weight Tuning : Experiment with different reward function weights Validation : Always validate your configuration before training Monitoring : Monitor training progress and adjust parameters as needed Testing : Test trained models before using in production","title":"Best Practices"},{"location":"api-reference/reward-model-training-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/reward-model-training-reference/#rewardmodeltrainer","text":"class RewardModelTrainer : def __init__ ( self , base_model_name : str , reward_functions : List [ RewardFunction ], composite_weights : List [ float ]) def generate_training_data ( self , texts : List [ str ], references : Optional [ List [ str ]] = None , batch_size : int = 32 ) -> RewardModelDataset def train_reward_model ( self , training_data : RewardModelDataset , output_dir : str , ** kwargs ) -> str def create_scalable_pipeline ( self , training_texts : List [ str ], output_dir : str , ** kwargs ) -> str","title":"RewardModelTrainer"},{"location":"api-reference/reward-model-training-reference/#rewardmodeldataset","text":"class RewardModelDataset ( Dataset ): def __init__ ( self , texts : List [ str ], reward_scores : List [ float ], tokenizer : AutoTokenizer , max_length : int = 512 ) def __len__ ( self ) -> int def __getitem__ ( self , idx : int ) -> Dict [ str , torch . Tensor ]","title":"RewardModelDataset"},{"location":"api-reference/reward-model-training-reference/#rewardmodelvalidator","text":"class RewardModelValidator : @staticmethod def validate_reward_source ( source_config : RewardModelSourceConfig ) -> None @staticmethod def validate_training_config ( config : RewardModelTrainingConfig ) -> None @staticmethod def validate_hf_model_access ( model_name : str ) -> None @staticmethod def validate_local_path ( path : str ) -> None","title":"RewardModelValidator"},{"location":"api-reference/reward-model-training-reference/#rewardmodelloader","text":"class RewardModelLoader : def load_from_huggingface ( self , model_name : str , ** kwargs ) -> AutoModelForSequenceClassification def load_from_local ( self , model_path : str , ** kwargs ) -> AutoModelForSequenceClassification","title":"RewardModelLoader"},{"location":"api-reference/reward-model-training-reference/#contributing","text":"When contributing to the reward model training system: Follow Validation : Always use strict validation with no fallbacks Add Tests : Include comprehensive tests for new features Document Changes : Update documentation for any API changes Error Handling : Provide clear, actionable error messages Performance : Consider performance implications of changes","title":"Contributing"},{"location":"api-reference/reward-model-training-reference/#license","text":"This reward model training system is part of the AlignTune project and follows the same license terms.","title":"License"},{"location":"api-reference/trainers/","text":"Trainers API Reference \u00b6 Complete API reference for AlignTune trainer classes. Base Trainers \u00b6 TrainerBase (RL) \u00b6 Abstract base trainer for RL training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. Source code in src/aligntune/core/rl/trainer_base.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 class TrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. \"\"\" def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = False, # metrics: Optional[List] = None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) # metric_key_prefix: Prefix for metric keys # use_custom_evaluator: If True, use BaseEvaluator/RLEvaluator. If False, use native. # metrics: Metrics to compute (only for custom evaluator, overrides setup) # **kwargs: Additional evaluation arguments (e.g., reference_model, reward_model for RL) # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Route to appropriate evaluator # if use_custom_evaluator: # eval_metrics = self._evaluate_with_custom(eval_dataset, metrics, **kwargs) # else: # eval_metrics = self._evaluate_native(eval_dataset, **kwargs) # # Apply metric key prefix # if metric_key_prefix and eval_metrics: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # if eval_metrics: # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def _evaluate_with_custom( # self, # eval_dataset, # metrics: Optional[List], # **kwargs # ) -> Dict[str, float]: # \"\"\"Evaluate using BaseEvaluator or RLEvaluator.\"\"\" # # Auto-setup if not configured # if self.custom_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\", metrics=metrics) # # Use provided dataset or fall back to configured dataset # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # # RL Evaluation # if isinstance(self.custom_evaluator, RLEvaluator): # # Get reference model (defaults to policy model if not provided) # reference_model = kwargs.pop('reference_model', self.model) # reward_model = kwargs.pop('reward_model', None) # return self.custom_evaluator.evaluate_rl( # policy_model=self.model, # reference_model=reference_model, # tokenizer=self.tokenizer, # dataset=dataset, # reward_model=reward_model, # **kwargs # ) # # Base Evaluation (SFT) # else: # task_name = kwargs.pop('task_name', 'text_generation') # return self.custom_evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # task_name=task_name, # **kwargs # ) # def _evaluate_native( # self, # eval_dataset=None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Use UnifiedEvaluator (native evaluation). # Subclasses can override this to use their trainer-specific evaluation. # \"\"\" # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None def _evaluate_with_custom_evaluator ( self , eval_dataset = None , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Custom evaluation using unified framework (BaseEvaluator + RLEvaluator).\"\"\" dataset = eval_dataset or self . eval_dataset if not dataset : logger . warning ( \"No eval dataset available for custom evaluation\" ) return {} eval_results = {} max_samples = kwargs . get ( 'max_samples' , None ) # 1. BaseEvaluator: Text generation metrics (ROUGE, BLEU, Perplexity) if self . base_evaluator : try : logger . info ( \"Running BaseEvaluator for text generation metrics...\" ) base_metrics = self . base_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = kwargs . get ( 'task_name' , 'text_generation' ), max_samples = max_samples ) eval_results . update ( base_metrics ) logger . info ( f \"BaseEvaluator completed: { list ( base_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"BaseEvaluator failed: { e } \" ) logger . debug ( \"BaseEvaluator error details:\" , exc_info = True ) # 2. RLEvaluator: RL-specific metrics (KL Divergence, Reward Accuracy) if self . rl_evaluator : try : logger . info ( \"Running RLEvaluator for RL-specific metrics...\" ) # Get reference model and reward model from kwargs or use defaults reference_model = kwargs . get ( 'reference_model' , self . model ) reward_model = kwargs . get ( 'reward_model' , getattr ( self , '_combined_reward_function' , None )) # Run RL evaluation rl_metrics = self . rl_evaluator . evaluate_rl ( policy_model = self . model , reference_model = reference_model , tokenizer = self . tokenizer , dataset = dataset , reward_model = reward_model , max_samples = max_samples ) eval_results . update ( rl_metrics ) logger . info ( f \"RLEvaluator completed: { list ( rl_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"RLEvaluator failed: { e } \" ) logger . debug ( \"RLEvaluator error details:\" , exc_info = True ) if not eval_results : logger . warning ( \"No evaluation metrics were computed successfully\" ) return eval_results def _evaluate_native ( self , eval_dataset = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Native evaluation - uses UnifiedEvaluator or trainer's built-in evaluate. Subclasses can override this to use their trainer-specific evaluation. \"\"\" dataset = eval_dataset or self . eval_dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Try trainer's built-in evaluate first (if exists) if hasattr ( self , 'trainer' ) and self . trainer and hasattr ( self . trainer , 'evaluate' ): try : return self . trainer . evaluate () except Exception as e : logger . warning ( f \"Trainer evaluation failed: { e } \" ) # Fallback to UnifiedEvaluator # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) return {} def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False __enter__ () \u00b6 Context manager entry. Source code in src/aligntune/core/rl/trainer_base.py 911 912 913 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) \u00b6 Context manager exit. Source code in src/aligntune/core/rl/trainer_base.py 915 916 917 918 919 920 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False __init__ ( config , callbacks = None ) \u00b6 Initialize trainer with configuration. Source code in src/aligntune/core/rl/trainer_base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) cleanup () \u00b6 Cleanup resources. Source code in src/aligntune/core/rl/trainer_base.py 904 905 906 907 908 909 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) create_data_loader () \u00b6 Create data loader (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 699 700 701 def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) evaluate ( eval_dataset = None , metric_key_prefix = 'eval' , use_custom_evaluator = False , metrics = None , ** kwargs ) \u00b6 Evaluate the trained model with flexible evaluation strategies. Parameters: Name Type Description Default eval_dataset Dataset to evaluate on (uses self.eval_dataset if None) None metric_key_prefix str Prefix for metric names in results 'eval' use_custom_evaluator bool If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation False metrics Optional [ List ] Optional list of additional metrics to compute None **kwargs Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics with the specified prefix Source code in src/aligntune/core/rl/trainer_base.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results get_next_batch () \u00b6 Get next batch from data loader. Source code in src/aligntune/core/rl/trainer_base.py 687 688 689 690 691 692 693 694 695 696 697 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) get_sample_outputs () \u00b6 Get sample outputs for logging (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 715 716 717 def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] load_checkpoint ( checkpoint_path ) \u00b6 Load checkpoint and broadcast to all ranks. Source code in src/aligntune/core/rl/trainer_base.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) on_eval ( metrics ) \u00b6 Hook for evaluation operations. Source code in src/aligntune/core/rl/trainer_base.py 709 710 711 712 713 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () on_step ( step , metrics ) \u00b6 Hook for step-level operations. Source code in src/aligntune/core/rl/trainer_base.py 703 704 705 706 707 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) predict ( inputs , max_new_tokens = 100 , temperature = 1.0 , top_p = 0.9 , do_sample = True , ** kwargs ) \u00b6 Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/rl/trainer_base.py 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions push_to_hub ( repo_id , private = False , token = None , commit_message = 'Upload fine-tuned model' , ** kwargs ) \u00b6 Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/rl/trainer_base.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url save_checkpoint () \u00b6 Save checkpoint (rank-0 only). Source code in src/aligntune/core/rl/trainer_base.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) setup_custom_evaluator ( evaluator_type = 'auto' , metrics = None ) \u00b6 Setup custom evaluators (BaseEvaluator/RLEvaluator). Parameters: Name Type Description Default evaluator_type str \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) 'auto' metrics Optional [ List ] Optional list of metrics to use (overrides defaults) None Source code in src/aligntune/core/rl/trainer_base.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None setup_data () abstractmethod \u00b6 Setup datasets and data loaders. Source code in src/aligntune/core/rl/trainer_base.py 111 112 113 114 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass setup_model () abstractmethod \u00b6 Setup model, tokenizer, and optimization. Source code in src/aligntune/core/rl/trainer_base.py 106 107 108 109 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass setup_rewards () abstractmethod \u00b6 Setup reward functions and evaluators. Source code in src/aligntune/core/rl/trainer_base.py 116 117 118 119 @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass train () \u00b6 Main training loop with hooks. Source code in src/aligntune/core/rl/trainer_base.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) train_step ( batch ) abstractmethod \u00b6 Execute single training step. Source code in src/aligntune/core/rl/trainer_base.py 121 122 123 124 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.rl.trainer_base import TrainerBase from aligntune.core.rl.config import UnifiedConfig # TrainerBase is abstract - use concrete implementations # See Backend Trainers below SFTTrainerBase \u00b6 Abstract base trainer for SFT training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. Source code in src/aligntune/core/sft/trainer_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class SFTTrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. \"\"\" def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics def _evaluate_with_custom ( self , eval_dataset , metrics : Optional [ List ], ** kwargs ) -> Dict [ str , float ]: \"\"\"Evaluate using BaseEvaluator (SFT only - no RL metrics).\"\"\" # Auto-setup if not configured if self . custom_evaluator is None : self . setup_custom_evaluator ( evaluator_type = \"base\" , metrics = metrics ) # Use provided dataset or fall back to configured dataset dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Base Evaluation (SFT) task_name = kwargs . pop ( 'task_name' , 'text_generation' ) return self . custom_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = task_name , ** kwargs ) def _evaluate_native ( self , eval_dataset = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Use SFTEvaluator (native evaluation).\"\"\" dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} return self . evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , config = self . config , ** kwargs ) def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader __init__ ( config , callbacks = None ) \u00b6 Initialize trainer with configuration. Source code in src/aligntune/core/sft/trainer_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) cleanup () \u00b6 Cleanup resources. Source code in src/aligntune/core/sft/trainer_base.py 592 593 594 595 596 597 598 599 600 601 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader create_data_loader () \u00b6 Create data loader - to be implemented by subclasses. Source code in src/aligntune/core/sft/trainer_base.py 395 396 397 def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) evaluate ( eval_dataset = None , metric_key_prefix = 'eval' , use_custom_evaluator = False , metrics = None , ** kwargs ) \u00b6 Run evaluation and return metrics. Parameters: Name Type Description Default eval_dataset Dataset to evaluate on (defaults to self.eval_dataset) None metric_key_prefix str Prefix for metric keys 'eval' use_custom_evaluator bool If True, use BaseEvaluator. If False, use native SFTEvaluator. False metrics Optional [ List ] Metrics to compute (only for custom evaluator, overrides setup) None **kwargs Additional evaluation arguments {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics Source code in src/aligntune/core/sft/trainer_base.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics get_next_batch () \u00b6 Get next batch from data loader. Source code in src/aligntune/core/sft/trainer_base.py 383 384 385 386 387 388 389 390 391 392 393 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) load_checkpoint ( checkpoint_path ) \u00b6 Load checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) on_eval ( metrics ) \u00b6 Hook called after each evaluation. Source code in src/aligntune/core/sft/trainer_base.py 403 404 405 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass on_step ( step , metrics ) \u00b6 Hook called after each training step. Source code in src/aligntune/core/sft/trainer_base.py 399 400 401 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass predict ( inputs , max_new_tokens = 100 , temperature = 1.0 , top_p = 0.9 , do_sample = True , ** kwargs ) \u00b6 Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/sft/trainer_base.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions push_to_hub ( repo_id , private = False , token = None , commit_message = 'Upload fine-tuned model' , ** kwargs ) \u00b6 Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/sft/trainer_base.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url save_checkpoint () \u00b6 Save checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) setup_data () abstractmethod \u00b6 Setup datasets and data loaders. Source code in src/aligntune/core/sft/trainer_base.py 99 100 101 102 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass setup_model () abstractmethod \u00b6 Setup model, tokenizer, and optimization. Source code in src/aligntune/core/sft/trainer_base.py 94 95 96 97 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass train () \u00b6 Main training loop with hooks. Source code in src/aligntune/core/sft/trainer_base.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) train_step ( batch ) abstractmethod \u00b6 Execute single training step. Source code in src/aligntune/core/sft/trainer_base.py 104 105 106 107 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.sft.config import SFTConfig # SFTTrainerBase is abstract - use concrete implementations # See Backend Trainers below TrainingState \u00b6 Training state tracking dataclass. Training state tracking. Source code in src/aligntune/core/rl/trainer_base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TrainingState : \"\"\"Training state tracking.\"\"\" step : int = 0 epoch : int = 0 best_metric : float = 0.0 checkpoint_path : Optional [ str ] = None start_time : float = field ( default_factory = time . time ) last_eval_time : float = field ( default_factory = time . time ) last_save_time : float = field ( default_factory = time . time ) def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time get_elapsed_time () \u00b6 Get elapsed training time in seconds. Source code in src/aligntune/core/rl/trainer_base.py 60 61 62 def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time update_best_metric ( metric ) \u00b6 Update best metric if improved. Source code in src/aligntune/core/rl/trainer_base.py 53 54 55 56 57 58 def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False update_epoch ( epoch ) \u00b6 Update current epoch. Source code in src/aligntune/core/rl/trainer_base.py 49 50 51 def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch update_step ( step ) \u00b6 Update current step. Source code in src/aligntune/core/rl/trainer_base.py 45 46 47 def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step options: show_source: true heading_level: 3 Backend Trainers \u00b6 TRL Backends \u00b6 TRLSFTTrainer \u00b6 TRL backend for Supervised Fine-Tuning. Bases: SFTTrainerBase SFT trainer using pure TRL SFTTrainer with comprehensive task type support. Source code in src/aligntune/backends/trl/sft/sft.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 class TRLSFTTrainer ( SFTTrainerBase ): \"\"\"SFT trainer using pure TRL SFTTrainer with comprehensive task type support.\"\"\" # All supported task types SUPPORTED_TASKS = [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION , TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def __init__ ( self , config ): super () . __init__ ( config ) self . config = config self . task_type = self . _get_task_type () self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . dataset = None self . training_history = [] self . logging_manager = None self . evaluator = None self . eval_dataset = None self . eval_dataset = None # Already exists - no need to add self . custom_evaluator = None # ADD THIS LINE (for BaseEvaluator) # Validate task type self . _validate_task_type () logger . info ( f \"Initialized TRLSFTTrainer for task: { self . task_type . value } \" ) def _get_task_type ( self ) -> TaskType : \"\"\"Extract task type from config.\"\"\" if hasattr ( self . config , 'dataset' ) and hasattr ( self . config . dataset , 'task_type' ): task_type = self . config . dataset . task_type elif hasattr ( self . config , 'train' ) and hasattr ( self . config . train , 'task_type' ): task_type = self . config . train . task_type else : task_type = TaskType . SUPERVISED_FINE_TUNING if isinstance ( task_type , str ): task_type = TaskType ( task_type . lower ()) return task_type def _validate_task_type ( self ): \"\"\"Validate that the task type is supported.\"\"\" if self . task_type not in self . SUPPORTED_TASKS : raise ValueError ( f \"Task type { self . task_type . value } is not supported by TRL backend. \" f \"Supported tasks: { [ t . value for t in self . SUPPORTED_TASKS ] } \" ) @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) return True except ImportError : return False # Replace lines 465-476 in setup_model: def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers with task type awareness.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT model: { self . config . model . name_or_path } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) try : from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) except ImportError as e : raise ImportError ( \"Transformers not available.\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model based on task type logger . info ( f \"Loading model for task: { self . task_type . value } \" ) if self . task_type == TaskType . TEXT_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 2 ) logger . info ( f \"Loading classification model with { num_labels } labels\" ) self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 9 ) logger . info ( f \"Loading token classification model with { num_labels } labels\" ) self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) else : # Causal LM for generation tasks logger . info ( \"Loading causal language model\" ) # Handle quantization configuration quantization_config = None quantization_dict = getattr ( self . config . model , 'quantization' , {}) if quantization_dict . get ( 'load_in_4bit' , False ) or quantization_dict . get ( 'load_in_8bit' , False ): try : from transformers import BitsAndBytesConfig load_in_4bit = quantization_dict . get ( 'load_in_4bit' , False ) load_in_8bit = quantization_dict . get ( 'load_in_8bit' , False ) if load_in_4bit : compute_dtype_str = quantization_dict . get ( 'bnb_4bit_compute_dtype' , 'bfloat16' ) compute_dtype = torch . bfloat16 if compute_dtype_str == 'bfloat16' else torch . float16 quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_compute_dtype = compute_dtype , bnb_4bit_quant_type = quantization_dict . get ( 'bnb_4bit_quant_type' , 'nf4' ), bnb_4bit_use_double_quant = quantization_dict . get ( 'bnb_4bit_use_double_quant' , False ), ) logger . info ( \"\u2705 4-bit quantization enabled for memory optimization\" ) elif load_in_8bit : quantization_config = BitsAndBytesConfig ( load_in_8bit = True ) logger . info ( \"\u2705 8-bit quantization enabled for memory optimization\" ) except ImportError : logger . warning ( \"\u26a0\ufe0f BitsAndBytes not available. Install with: pip install bitsandbytes\" ) quantization_config = None # Get max_memory from config if specified max_memory = getattr ( self . config . model , 'max_memory' , None ) model_kwargs = { 'torch_dtype' : dtype , 'trust_remote_code' : True , 'device_map' : 'auto' if torch . cuda . is_available () else None , 'low_cpu_mem_usage' : True , } # Add max_memory if specified if max_memory : model_kwargs [ 'max_memory' ] = max_memory if quantization_config : model_kwargs [ 'quantization_config' ] = quantization_config logger . info ( f \"Quantization config: { quantization_config } \" ) else : # Only set these if not using quantization model_kwargs [ 'load_in_4bit' ] = False model_kwargs [ 'load_in_8bit' ] = False logger . info ( \"No quantization configured - loading full precision model\" ) self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Move to GPU only if not using device_map='auto' (which handles it automatically) if not quantization_config and torch . cuda . is_available () and model_kwargs . get ( 'device_map' ) != 'auto' : self . model = self . model . cuda () logger . info ( \"Model moved to GPU\" ) elif quantization_config : logger . info ( \"Model loaded with quantization (device_map='auto')\" ) # CRITICAL: When using quantization, PEFT adapters MUST be enabled # TRL SFTTrainer will handle this, but we need to ensure PEFT is enabled in config if not getattr ( self . config . model , 'peft_enabled' , False ): logger . warning ( \"\u26a0\ufe0f Quantization requires PEFT adapters. Enabling PEFT automatically.\" ) self . config . model . peft_enabled = True # Setup tokenizer for specific tasks self . _setup_tokenizer_for_task () logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) def _setup_tokenizer_for_task ( self ): \"\"\"Setup tokenizer with task-specific configurations.\"\"\" # If a custom chat template (jinja) is provided in config, set it so # `tokenizer.apply_chat_template(...)` can be used during SFT formatting. cfg_template = getattr ( self . config . dataset , \"chat_template\" , None ) if cfg_template and isinstance ( cfg_template , str ): if ( \"{%\" in cfg_template or \"{{\" in cfg_template ) and ( not hasattr ( self . tokenizer , \"chat_template\" ) or self . tokenizer . chat_template is None ): try : self . tokenizer . chat_template = cfg_template logger . info ( \"Applied custom chat_template from config to tokenizer\" ) except Exception as e : logger . debug ( f \"Failed to set tokenizer.chat_template from config: { e } \" ) if self . task_type == TaskType . CHAT_COMPLETION : if not hasattr ( self . tokenizer , 'chat_template' ) or self . tokenizer . chat_template is None : self . tokenizer . chat_template = ( \"{ % f or message in messages %}\" \"{{ message['role'] }}: {{ message['content'] }} \\n \" \"{ % e ndfor %}Assistant:\" ) logger . info ( \"Added chat template to tokenizer\" ) elif self . task_type == TaskType . INSTRUCTION_FOLLOWING : # If the tokenizer already supports chat templating, prefer using that # instead of introducing ChatML-style tokens. has_chat_template = ( hasattr ( self . tokenizer , \"apply_chat_template\" ) and hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None ) if not has_chat_template : special_tokens = [ \"<|im_start|>\" , \"<|im_end|>\" ] num_added = self . tokenizer . add_special_tokens ({ 'additional_special_tokens' : special_tokens }) if num_added > 0 : self . model . resize_token_embeddings ( len ( self . tokenizer )) logger . info ( f \"Added { num_added } special tokens for instruction following\" ) def _apply_chat_template_safe ( self , messages : List [ Dict [ str , str ]], add_generation_prompt : bool = False ) -> Optional [ str ]: \"\"\"Apply tokenizer chat template with broad compatibility.\"\"\" if not self . tokenizer or not hasattr ( self . tokenizer , \"apply_chat_template\" ): return None try : return self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = add_generation_prompt , ) except TypeError : # Some tokenizers don't support add_generation_prompt or other kwargs. try : return self . tokenizer . apply_chat_template ( messages , tokenize = False ) except Exception : return None except Exception : return None def _format_with_chat_template ( self , examples : Dict [ str , Any ], dataset_cfg ) -> Dict [ str , List [ str ]]: \"\"\" Build a `text` field using the model's chat template when available. Falls back to legacy string formatting if templating fails. \"\"\" system_prompt = getattr ( dataset_cfg , \"system_prompt\" , None ) texts : List [ str ] = [] def _prepend_system_if_missing ( msgs : List [ Dict [ str , str ]]) -> List [ Dict [ str , str ]]: if system_prompt and ( not msgs or msgs [ 0 ] . get ( \"role\" ) != \"system\" ): return [{ \"role\" : \"system\" , \"content\" : system_prompt }] + msgs return msgs if self . task_type == TaskType . CHAT_COMPLETION : messages_field = dataset_cfg . messages_column if hasattr ( dataset_cfg , \"messages_column\" ) else \"messages\" for messages in examples . get ( messages_field , []): if isinstance ( messages , list ): msgs = _prepend_system_if_missing ( messages ) templated = self . _apply_chat_template_safe ( msgs , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) continue # Fallback: plain \"role: content\" formatting conversation = [ f \" { m . get ( 'role' , 'user' ) } : { m . get ( 'content' , '' ) } \" for m in msgs ] texts . append ( \" \\n \" . join ( conversation )) else : texts . append ( str ( messages ) if messages else \"\" ) return { \"text\" : texts } if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING ]: # Determine columns (prefer config, fall back to common names) instr_col = getattr ( dataset_cfg , \"instruction_column\" , \"instruction\" ) resp_col = getattr ( dataset_cfg , \"response_column\" , \"response\" ) ctx_col = getattr ( dataset_cfg , \"context_column\" , \"context\" ) input_col = getattr ( dataset_cfg , \"input_column\" , \"input\" ) instructions = examples . get ( instr_col , examples . get ( \"instruction\" , [])) # response could be in response/output/completion depending on dataset responses = examples . get ( resp_col , examples . get ( \"response\" , examples . get ( \"output\" , []))) n = min ( len ( instructions ), len ( responses )) for i in range ( n ): instruction = instructions [ i ] response = responses [ i ] context = \"\" if ctx_col in examples and i < len ( examples . get ( ctx_col , [])): context = examples . get ( ctx_col , [ \"\" ])[ i ] or \"\" input_text = \"\" if input_col in examples and i < len ( examples . get ( input_col , [])): input_text = examples . get ( input_col , [ \"\" ])[ i ] or \"\" sys_parts : List [ str ] = [] if system_prompt : sys_parts . append ( str ( system_prompt ) . strip ()) if context and str ( context ) . strip (): sys_parts . append ( f \"Context: { str ( context ) . strip () } \" ) messages : List [ Dict [ str , str ]] = [] if sys_parts : messages . append ({ \"role\" : \"system\" , \"content\" : \" \\n\\n \" . join ( sys_parts )}) user_content = str ( instruction ) if instruction is not None else \"\" if input_text and str ( input_text ) . strip (): user_content = f \" { user_content } \\n\\n { str ( input_text ) . strip () } \" messages . append ({ \"role\" : \"user\" , \"content\" : user_content }) messages . append ({ \"role\" : \"assistant\" , \"content\" : str ( response ) if response is not None else \"\" }) templated = self . _apply_chat_template_safe ( messages , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) else : # Fallback to legacy formatting (inline; avoid recomputing full batch) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : if context and str ( context ) . strip (): texts . append ( f \"<|im_start|>system \\n Context: { str ( context ) . strip () } <|im_end|> \\n \" f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : if user_content and response is not None : texts . append ( f \"### Instruction: \\n { user_content } \\n\\n ### Response: \\n { str ( response ) } \" ) else : texts . append ( str ( examples . get ( \"text\" , [ \"\" ])[ i ]) if i < len ( examples . get ( \"text\" , [ \"\" ])) else \"\" ) if not texts : texts = [ \"[NO_VALID_TEXT]\" ] return { \"text\" : texts } # Default fallback (shouldn't be hit for tasks we call this for) return { \"text\" : examples . get ( \"text\" , [ \"[NO_VALID_TEXT]\" ])} def setup_data ( self ) -> None : \"\"\"Setup datasets for SFT training with task-aware formatting.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT datasets: { self . config . dataset . name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Initialize dataset cache cache_root = getattr ( self . config , 'caching' , {}) . get ( \"root\" , \"./cache\" ) if hasattr ( self . config , 'caching' ) else \"./cache\" self . dataset_cache = DatasetCache ( cache_root = cache_root ) logger . info ( f \"Dataset cache initialized at: { self . dataset_cache . cache_root } \" ) # Load dataset logger . info ( \"Loading dataset...\" ) dataset_config = self . config . dataset from datasets import load_dataset # Handle dataset config/subset load_kwargs = { 'split' : dataset_config . split or \"train\" } dataset_name = dataset_config . name dataset_subset = None if hasattr ( dataset_config , 'subset' ) and dataset_config . subset : dataset_subset = dataset_config . subset elif hasattr ( dataset_config , 'config' ) and dataset_config . config : dataset_subset = dataset_config . config # Check if dataset has a registered custom loader if dataset_name in DatasetRegistry . list_loaders (): logger . info ( f \"Using registered dataset loader: { dataset_name } \" ) dataset = DatasetRegistry . load_dataset ( name = dataset_name , split = dataset_config . split or \"train\" , max_samples = dataset_config . max_samples , ** { k : v for k , v in dataset_config . __dict__ . items () if k not in [ 'name' , 'split' , 'max_samples' ] and v is not None } ) else : # Load dataset try : if dataset_subset : dataset = load_dataset ( dataset_name , dataset_subset , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ValueError as e : if \"Config name is missing\" in str ( e ): raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' to your DatasetConfig.\" ) raise logger . info ( f \"Original dataset size: { len ( dataset ) } samples\" ) # \ud83d\udd39 CRITICAL FIX: Shuffle dataset BEFORE selecting samples # This ensures balanced class distribution for classification tasks if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . info ( \"Shuffling dataset to ensure balanced sampling...\" ) dataset = dataset . shuffle ( seed = 42 ) # Apply dataset size limits if dataset_config . percent and dataset_config . percent < 1.0 : dataset_size = int ( len ( dataset ) * dataset_config . percent ) dataset = dataset . select ( range ( dataset_size )) logger . info ( f \"Using { dataset_config . percent * 100 } % of dataset: { dataset_size } samples\" ) elif dataset_config . max_samples : dataset = dataset . select ( range ( min ( dataset_config . max_samples , len ( dataset )))) logger . info ( f \"Using max_samples limit: { len ( dataset ) } samples\" ) # Apply column mappings if needed if hasattr ( dataset_config , 'column_mapping' ) and dataset_config . column_mapping : valid_mappings = { k : v for k , v in dataset_config . column_mapping . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , dataset_config ) # Create evaluation split try : test_size = 0.1 if len ( dataset ) >= 1000 : test_size = 0.1 elif len ( dataset ) >= 200 : test_size = 0.1 else : test_size = min ( 0.2 , max ( 0.1 , 50 / max ( 1 , len ( dataset )))) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 , shuffle = True ) self . dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Created eval split. Train: { len ( self . dataset ) } | Eval: { len ( self . eval_dataset ) } \" ) except Exception as e : self . dataset = dataset self . eval_dataset = None logger . warning ( f \"Could not create eval split: { str ( e ) } \" ) logger . info ( \"=\" * 80 ) logger . info ( f \"TRL SFT dataset setup completed: { len ( self . dataset ) } samples\" ) logger . info ( \"=\" * 80 ) def _format_dataset_for_task ( self , dataset , dataset_config ): \"\"\"Format dataset based on task type.\"\"\" logger . info ( f \"Formatting dataset for task: { self . task_type . value } \" ) # Select appropriate formatter can_template = self . tokenizer is not None and hasattr ( self . tokenizer , \"apply_chat_template\" ) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_instruction_following elif self . task_type == TaskType . SUPERVISED_FINE_TUNING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_supervised_fine_tuning elif self . task_type == TaskType . TEXT_GENERATION : formatter = TaskFormatter . format_text_generation elif self . task_type == TaskType . CHAT_COMPLETION : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_chat_completion elif self . task_type == TaskType . TEXT_CLASSIFICATION : formatter = TaskFormatter . format_classification elif self . task_type == TaskType . TOKEN_CLASSIFICATION : formatter = TaskFormatter . format_token_classification else : raise ValueError ( f \"Unsupported task type: { self . task_type } \" ) # Apply formatting try : # For classification, we need to keep the columns AND rename properly if self . task_type == TaskType . TEXT_CLASSIFICATION : # Don't remove columns, just map them formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = [ col for col in dataset . column_names if col not in [ 'text' , 'label' , 'labels' ]], desc = f \"Formatting for { self . task_type . value } \" ) # NOW remove the old 'label' column if it exists (keep only 'labels') if 'label' in formatted_dataset . column_names and 'labels' in formatted_dataset . column_names : formatted_dataset = formatted_dataset . remove_columns ([ 'label' ]) logger . info ( \"Removed duplicate 'label' column, keeping 'labels'\" ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = [], desc = f \"Formatting for { self . task_type . value } \" ) else : # For generation tasks, remove all columns formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = dataset . column_names , desc = f \"Formatting for { self . task_type . value } \" ) # Verify sample = formatted_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) if 'text' in sample : logger . info ( f \"Sample text (first 200 chars): { sample [ 'text' ][: 200 ] } \" ) if 'labels' in sample : logger . info ( f \"Sample label: { sample [ 'labels' ] } \" ) # CRITICAL: For classification, check label distribution if self . task_type == TaskType . TEXT_CLASSIFICATION : labels_sample = [ formatted_dataset [ i ][ 'labels' ] for i in range ( min ( 100 , len ( formatted_dataset )))] unique_labels = set ( labels_sample ) label_counts = { label : labels_sample . count ( label ) for label in unique_labels } logger . info ( f \"\u2713 Label distribution (first 100): { label_counts } \" ) logger . info ( f \"\u2713 Unique labels: { unique_labels } \" ) if len ( unique_labels ) == 1 : logger . error ( f \"\u274c ERROR: All labels are { list ( unique_labels )[ 0 ] } ! Dataset is corrupted!\" ) raise ValueError ( \"Dataset has only one label class!\" ) return formatted_dataset except Exception as e : logger . error ( f \"Error formatting dataset: { e } \" ) raise def setup_rewards ( self ) -> None : \"\"\"SFT doesn't use reward functions.\"\"\" logger . info ( \"SFT training doesn't use reward functions\" ) self . reward_functions = [] def train ( self ) -> Dict [ str , Any ]: \"\"\"Train using TRL SFTTrainer with task-aware setup.\"\"\" logger . info ( f \"Starting TRL SFT training for task: { self . task_type . value } \" ) # Setup model and data if not already done if self . model is None : self . setup_model () if self . dataset is None : self . setup_data () try : from trl import SFTTrainer from transformers import TrainingArguments , Trainer from transformers import DataCollatorWithPadding , DataCollatorForTokenClassification import evaluate except ImportError as e : raise ImportError ( \"TRL required for SFT training. Install with: pip install trl\" ) from e # Create training arguments # Get optimizer and scheduler configurations from ....core.optimization import get_optimizer_for_config , get_scheduler_for_config # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate or 2e-4 , self . config . train . weight_decay or 0.01 ) # Calculate max_steps - use config value or calculate from epochs max_steps = self . config . train . max_steps if max_steps is None : # Calculate from epochs if max_steps not specified epochs = getattr ( self . config . train , 'epochs' , 3 ) or 3 dataset_size = len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 1000 batch_size = self . config . train . per_device_batch_size or 2 grad_accum = self . config . train . gradient_accumulation_steps or 4 max_steps = ( dataset_size * epochs ) // ( batch_size * grad_accum ) max_steps = max ( max_steps , 10 ) # Minimum 10 steps logger . info ( f \"Calculated max_steps= { max_steps } from { epochs } epochs, { dataset_size } samples\" ) # Calculate warmup steps warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" # Exclude lr and weight_decay since TrainingArguments already sets them filtered = { k : v for k , v in kwargs_dict . items () if k not in [ 'lr' , 'learning_rate' , 'weight_decay' ]} if not filtered : return None return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in filtered . items ()) optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) # print(lr_scheduler_kwargs_str) # print(scheduler_config) filtered_scheduler_kwargs = { k : v for k , v in scheduler_config [ 'lr_scheduler_kwargs' ] . items () if k not in [ 'num_training_steps' , 'num_warmup_steps' ] } # Build TrainingArguments kwargs, conditionally including optim_args training_kwargs = { \"output_dir\" : self . config . logging . output_dir , \"max_steps\" : max_steps , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size or 2 , \"gradient_accumulation_steps\" : self . config . train . gradient_accumulation_steps or 4 , \"learning_rate\" : self . config . train . learning_rate or 2e-4 , \"weight_decay\" : self . config . train . weight_decay or 0.01 , \"warmup_steps\" : warmup_steps , \"logging_steps\" : getattr ( self . config . logging , 'log_interval' , 10 ), \"save_steps\" : getattr ( self . config . train , 'save_interval' , 500 ), \"eval_steps\" : getattr ( self . config . train , 'eval_interval' , 500 ), \"eval_strategy\" : \"no\" , \"save_strategy\" : \"steps\" , \"load_best_model_at_end\" : False , \"report_to\" : [], \"run_name\" : self . config . logging . run_name , \"seed\" : 42 , ** precision_args , \"dataloader_num_workers\" : 0 , \"remove_unused_columns\" : False , \"optim\" : optimizer_name , \"lr_scheduler_type\" : scheduler_name , \"lr_scheduler_kwargs\" : filtered_scheduler_kwargs , } # Only include optim_args if it's not None/empty and valid. # For 8-bit optimizers (adamw_8bit, paged_adamw_8bit, etc.), bitsandbytes handles kwargs internally, # so we skip optim_args to avoid Transformers parsing issues. # Also skip if the string doesn't contain \"=\" (would cause parsing errors). should_skip_optim_args = ( not optim_args_str or # None or empty string not optim_args_str . strip () or # Whitespace only \"=\" not in optim_args_str or # Invalid format (no key=value pairs) any ( x in optimizer_name . lower () for x in [ \"8bit\" , \"8_bit\" , \"bnb\" , \"paged\" ]) # 8-bit optimizers ) if not should_skip_optim_args : training_kwargs [ \"optim_args\" ] = optim_args_str training_args = TrainingArguments ( ** training_kwargs ) # Create trainer based on task type if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: self . _setup_classification_trainer ( training_args ) else : # Generation tasks use SFTTrainer # Setup PEFT config if enabled (required for quantized models) peft_config = None if getattr ( self . config . model , 'peft_enabled' , False ): try : from peft import LoraConfig # Get LoRA parameters from config lora_r = getattr ( self . config . model , 'lora_rank' , 16 ) lora_alpha = getattr ( self . config . model , 'lora_alpha' , 32 ) lora_dropout = getattr ( self . config . model , 'lora_dropout' , 0.1 ) target_modules = getattr ( self . config . model , 'target_modules' , None ) # If target_modules not specified, use default for Llama models if target_modules is None : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) logger . info ( f \"\u2705 PEFT LoRA config created: r= { lora_r } , alpha= { lora_alpha } , modules= { target_modules } \" ) except ImportError : logger . warning ( \"\u26a0\ufe0f PEFT not available. Install with: pip install peft\" ) peft_config = None # Generation tasks use SFTTrainer self . trainer = SFTTrainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , peft_config = peft_config , # Pass PEFT config to SFTTrainer ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( f \"Starting TRL SFT Training - Task: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Log training metrics if self . logging_manager and hasattr ( train_result , 'metrics' ): self . logging_manager . log_metrics ( train_result . metrics ) # Save model self . trainer . save_model () # Add to training history self . training_history . append ({ 'timestamp' : time . time (), 'duration' : training_duration , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'task_type' : self . task_type . value , 'model_path' : self . config . logging . output_dir , }) logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT training completed successfully!\" ) logger . info ( \"=\" * 80 ) # Return training results return { 'training_time' : training_duration , 'final_loss' : getattr ( train_result , 'train_loss' , 0.0 ), 'model_path' : self . config . logging . output_dir , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'epochs' : getattr ( train_result , 'epoch' , 0 ), 'metrics' : getattr ( train_result , 'metrics' , {}), 'task_type' : self . task_type . value } def _setup_classification_trainer ( self , training_args ): \"\"\"Setup trainer for classification tasks - FIXED VERSION.\"\"\" from transformers import Trainer , DataCollatorWithPadding import evaluate if self . task_type == TaskType . TEXT_CLASSIFICATION : # CRITICAL: Verify dataset has both text and labels BEFORE tokenization logger . info ( f \"Dataset columns before tokenization: { self . dataset . column_names } \" ) logger . info ( f \"Sample before tokenization: { self . dataset [ 0 ] } \" ) # Ensure we have the required columns if 'text' not in self . dataset . column_names or 'labels' not in self . dataset . column_names : raise ValueError ( f \"Dataset must have 'text' and 'labels' columns. \" f \"Found: { self . dataset . column_names } \" ) def tokenize_function ( examples ): \"\"\"Tokenize text and preserve labels - FIXED\"\"\" # Get texts - handle both list and single values texts = examples [ 'text' ] if not isinstance ( texts , list ): texts = [ texts ] # Clean texts texts = [ str ( t ) if t is not None else \"\" for t in texts ] # Tokenize WITHOUT max_length padding here tokenized = self . tokenizer ( texts , truncation = True , padding = False , # FIX: Don't pad here, let collator handle it max_length = self . config . model . max_seq_length , return_tensors = None ) # Get labels - handle both list and single values labels = examples [ 'labels' ] if not isinstance ( labels , list ): labels = [ labels ] # FIX: Ensure labels are integers and valid try : tokenized [ 'labels' ] = [ int ( label ) for label in labels ] except ( ValueError , TypeError ) as e : logger . error ( f \"Label conversion error: { e } \" ) logger . error ( f \"Problematic labels: { labels } \" ) raise return tokenized # Store original column names original_train_cols = self . dataset . column_names . copy () # Tokenize train dataset - REMOVE original columns logger . info ( \"Tokenizing train dataset...\" ) self . dataset = self . dataset . map ( tokenize_function , batched = True , remove_columns = original_train_cols , desc = \"Tokenizing train dataset\" ) # Tokenize eval dataset if it exists if self . eval_dataset : original_eval_cols = self . eval_dataset . column_names . copy () logger . info ( \"Tokenizing eval dataset...\" ) self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = original_eval_cols , desc = \"Tokenizing eval dataset\" ) # Verify tokenization logger . info ( f \"\u2713 Train dataset columns after tokenization: { self . dataset . column_names } \" ) sample = self . dataset [ 0 ] logger . info ( f \"\u2713 Sample keys: { list ( sample . keys ()) } \" ) logger . info ( f \"\u2713 Sample input_ids length: { len ( sample [ 'input_ids' ]) } \" ) logger . info ( f \"\u2713 Sample label: { sample [ 'labels' ] } \" ) logger . info ( f \"\u2713 Sample label type: { type ( sample [ 'labels' ]) } \" ) # FIX: Verify label distribution and validity sample_size = min ( 100 , len ( self . dataset )) all_labels = [ self . dataset [ i ][ 'labels' ] for i in range ( sample_size )] unique_labels = set ( all_labels ) label_counts = { label : all_labels . count ( label ) for label in unique_labels } logger . info ( f \"\u2713 First { sample_size } labels distribution: { label_counts } \" ) logger . info ( f \"\u2713 Unique labels: { unique_labels } \" ) logger . info ( f \"\u2713 Expected labels: { set ( range ( self . config . model . num_labels )) } \" ) # Validate labels expected_labels = set ( range ( self . config . model . num_labels )) if not unique_labels . issubset ( expected_labels ): raise ValueError ( f \"Found unexpected labels: { unique_labels - expected_labels } . \" f \"Expected labels in range [0, { self . config . model . num_labels } )\" ) if len ( unique_labels ) == 1 : logger . error ( f \"\u274c ERROR: All labels are { list ( unique_labels )[ 0 ] } !\" ) raise ValueError ( \"Dataset has only one label class after tokenization!\" ) # FIX: Use DataCollatorWithPadding that handles padding dynamically data_collator = DataCollatorWithPadding ( tokenizer = self . tokenizer , padding = True , # Pad dynamically to longest in batch max_length = self . config . model . max_seq_length , pad_to_multiple_of = 8 # Optimize for GPU ) logger . info ( \"\u2713 Data collator created with dynamic padding\" ) def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics\"\"\" predictions , labels = eval_pred # Handle logits if len ( predictions . shape ) > 1 : predictions = np . argmax ( predictions , axis = 1 ) # Log predictions distribution unique_preds = np . unique ( predictions ) unique_labels = np . unique ( labels ) logger . info ( f \"Predictions distribution: { np . bincount ( predictions ) } \" ) logger . info ( f \"Labels distribution: { np . bincount ( labels . astype ( int )) } \" ) metrics = {} # Accuracy accuracy_metric = evaluate . load ( \"accuracy\" ) metrics . update ( accuracy_metric . compute ( predictions = predictions , references = labels )) # F1 score try : f1_metric = evaluate . load ( \"f1\" ) metrics . update ( f1_metric . compute ( predictions = predictions , references = labels , average = 'weighted' )) except Exception as e : logger . warning ( f \"Could not compute F1: { e } \" ) return metrics # Create trainer self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , data_collator = data_collator , compute_metrics = compute_metrics ) logger . info ( \"\u2713 Classification trainer created successfully\" ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Tokenize for token classification def tokenize_and_align_labels ( examples ): tokenized = self . tokenizer ( examples [ 'tokens' ], truncation = True , is_split_into_words = True , max_length = self . config . model . max_seq_length ) labels = [] for i , label in enumerate ( examples [ 'labels' ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : label_ids . append ( - 100 ) elif word_idx != previous_word_idx : label_ids . append ( label [ word_idx ]) else : label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ \"labels\" ] = labels return tokenized self . dataset = self . dataset . map ( tokenize_and_align_labels , batched = True ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_and_align_labels , batched = True ) data_collator = DataCollatorForTokenClassification ( tokenizer = self . tokenizer ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred predictions = np . argmax ( predictions , axis = 2 ) true_predictions = [ [ p for ( p , l ) in zip ( pred , label ) if l != - 100 ] for pred , label in zip ( predictions , labels ) ] true_labels = [ [ l for ( p , l ) in zip ( pred , label ) if l != - 100 ] for pred , label in zip ( predictions , labels ) ] flat_preds = [ item for sublist in true_predictions for item in sublist ] flat_labels = [ item for sublist in true_labels for item in sublist ] accuracy = evaluate . load ( \"accuracy\" ) return accuracy . compute ( predictions = flat_preds , references = flat_labels ) self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , data_collator = data_collator , compute_metrics = compute_metrics ) def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model and tokenizer. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : output_dir = Path ( self . config . logging . output_dir ) / \"sft_model\" else : output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) if hasattr ( self , 'trainer' ) and self . trainer is not None : self . trainer . save_model ( output_dir ) logger . info ( f \"Model saved to { output_dir } \" ) else : logger . warning ( \"No trainer available to save model\" ) if hasattr ( self , 'tokenizer' ) and self . tokenizer is not None : self . tokenizer . save_pretrained ( output_dir ) logger . info ( f \"Tokenizer saved to { output_dir } \" ) else : logger . warning ( \"No tokenizer available to save\" ) # Save task type info config_path = output_dir / \"training_config.yaml\" config_dict = { 'task_type' : self . task_type . value , 'model_name' : self . config . model . name_or_path } with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f ) logger . info ( f \"Training config saved to { config_path } \" ) # Store for push_to_hub self . _last_save_path = str ( output_dir ) return str ( output_dir ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute training step (handled by TRL trainer).\"\"\" return { \"loss\" : 0.0 } def create_data_loader ( self ) -> DataLoader : \"\"\"Create data loader for training.\"\"\" if self . dataset is None : raise RuntimeError ( \"Dataset not loaded. Call setup_data() first.\" ) from torch.utils.data import DataLoader return DataLoader ( self . dataset , batch_size = self . config . train . per_device_batch_size , shuffle = True , collate_fn = self . _collate_fn ) def _collate_fn ( self , batch ): \"\"\"Collate function for data loader.\"\"\" if not batch : return {} # Extract text from batch texts = [] for item in batch : if isinstance ( item , dict ): if \"text\" in item : texts . append ( item [ \"text\" ]) elif \"input_ids\" in item : return item else : for key , value in item . items (): if isinstance ( value , str ) and len ( value . strip ()) > 0 : texts . append ( value ) break elif isinstance ( item , str ): texts . append ( item ) if not texts : raise ValueError ( \"No valid text found in batch\" ) return self . tokenizer ( texts , padding = True , truncation = True , max_length = self . config . model . max_seq_length , return_tensors = \"pt\" ) def get_sample_outputs ( self ) -> list : \"\"\"Get sample outputs for logging.\"\"\" if not hasattr ( self , 'dataset' ) or self . dataset is None : return [] # Classification tasks don't generate samples if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: return [] sample_outputs = [] for i in range ( min ( 3 , len ( self . dataset ))): sample = self . dataset [ i ] if \"text\" in sample : text = sample [ \"text\" ] elif \"messages\" in sample : messages = sample [ \"messages\" ] text = self . tokenizer . apply_chat_template ( messages , tokenize = False ) else : text = str ( sample . get ( \"input\" , \"\" )) inputs = self . tokenizer ( text , return_tensors = \"pt\" , truncation = True , max_length = 512 ) with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 50 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), pad_token_id = self . tokenizer . eos_token_id ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) sample_outputs . append ({ \"input\" : text [: 100 ] + \"...\" if len ( text ) > 100 else text , \"output\" : response , \"model\" : \"trl_sft\" , \"task_type\" : self . task_type . value }) return sample_outputs def cleanup ( self ) -> None : \"\"\"Cleanup TRL resources.\"\"\" super () . cleanup () if self . model is not None : del self . model torch . cuda . empty_cache () logger . info ( \"TRL SFT trainer cleanup completed\" ) def evaluate ( self , eval_config : Optional [ Dict ] = None , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the model performance with enhanced metrics.\"\"\" logger . info ( f \"Running TRL SFT evaluation for task: { self . task_type . value } \" ) if not self . trainer : raise ValueError ( \"Training not setup. Call train() first.\" ) if eval_dataset : self . eval_dataset = eval_dataset # Run evaluation eval_results = {} if hasattr ( self . trainer , 'evaluate' ): if getattr ( self , 'eval_dataset' , None ) is not None : eval_results = self . trainer . evaluate () else : logger . warning ( \"No eval_dataset available; skipping evaluation.\" ) if self . logging_manager : self . logging_manager . log_metrics ( eval_results ) # Add task type to results eval_results [ 'task_type' ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator for generation tasks if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]: try : from aligntune.core.sft.evaluator import SFTEvaluator if self . model and self . tokenizer and hasattr ( self , 'eval_dataset' ) and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) # Generate qualitative samples for generation tasks if self . task_type not in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : samples = self . _generate_samples () eval_results [ 'qualitative_samples' ] = samples except Exception as e : logger . warning ( f \"Could not generate qualitative samples: { str ( e ) } \" ) eval_results [ 'qualitative_samples' ] = [] logger . info ( f \"TRL SFT evaluation completed: { len ( eval_results ) } metrics computed\" ) return eval_results def _generate_samples ( self ) -> List [ Dict [ str , str ]]: \"\"\"Generate qualitative samples for manual inspection.\"\"\" samples = [] self . model . eval () # Task-specific prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : sample_prompts = [ \"<|im_start|>user \\n What is artificial intelligence?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain machine learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Write a Python function.<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : sample_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Explain quantum computing. \\n Assistant:\" ] else : sample_prompts = [ \"The future of artificial intelligence is\" , \"In a world where technology advances rapidly,\" , \"The most important skill for the 21st century is\" ] for i , prompt in enumerate ( sample_prompts [: 3 ]): try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): generated = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), top_p = 0.9 , repetition_penalty = 1.1 , pad_token_id = self . tokenizer . eos_token_id , ) generated_text = self . tokenizer . decode ( generated [ 0 ], skip_special_tokens = True ) response = generated_text [ len ( prompt ):] . strip () samples . append ({ 'prompt' : prompt , 'generated_response' : response , 'task_type' : self . task_type . value }) logger . info ( f \"Generated sample { i + 1 } /3\" ) except Exception as e : logger . warning ( f \"Error generating sample { i } : { str ( e ) } \" ) samples . append ({ 'prompt' : prompt , 'generated_response' : f \"Error: { str ( e ) } \" , 'task_type' : self . task_type . value }) return samples def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Public method to generate samples.\"\"\" if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Sample generation not supported for classification tasks\" ) return [] try : samples = self . _generate_samples () # Return limited number and ensure all have 'response' key return [ s for s in samples [: num_samples ] if 'generated_response' in s or 'response' in s ] except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] def run_zero_shot_evaluation ( self , test_prompts = None ) -> Dict [ str , Any ]: \"\"\"Run zero-shot evaluation before training with enhanced metrics.\"\"\" logger . info ( f \"Running zero-shot evaluation for task: { self . task_type . value } \" ) # Classification tasks don't support zero-shot evaluation if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Zero-shot evaluation not supported for classification tasks\" ) return {} if test_prompts is None : # Task-specific default prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : test_prompts = [ \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain ML.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : test_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Tell me about AI. \\n Assistant:\" ] else : test_prompts = [ \"What is artificial intelligence?\" , \"Explain machine learning briefly.\" , \"How does a computer work?\" ] results = [] self . model . eval () for prompt in test_prompts : try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = False , pad_token_id = self . tokenizer . eos_token_id , temperature = 0.7 , top_p = 0.9 ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () results . append ({ 'prompt' : prompt , 'response' : response , 'response_length' : len ( response . split ()), 'coherence_score' : self . _calculate_coherence_score ( response ) }) except Exception as e : logger . warning ( f \"Zero-shot evaluation failed for prompt: { e } \" ) results . append ({ 'prompt' : prompt , 'response' : f \"Error: { str ( e ) } \" , 'response_length' : 0 , 'coherence_score' : 0.0 }) # Calculate aggregate metrics avg_length = sum ( r [ 'response_length' ] for r in results ) / len ( results ) if results else 0 avg_coherence = sum ( r [ 'coherence_score' ] for r in results ) / len ( results ) if results else 0 successful_responses = len ([ r for r in results if not r [ 'response' ] . startswith ( 'Error' )]) zero_shot_metrics = { 'task_type' : self . task_type . value , 'zero_shot_results' : results , 'avg_response_length' : avg_length , 'avg_coherence_score' : avg_coherence , 'successful_responses' : successful_responses , 'success_rate' : successful_responses / len ( results ) if results else 0 } if self . logging_manager : self . logging_manager . log_metrics ({ 'zero_shot_avg_length' : avg_length , 'zero_shot_coherence' : avg_coherence , 'zero_shot_success_rate' : successful_responses / len ( results ) if results else 0 }) logger . info ( f \"Zero-shot evaluation completed. Success rate: { successful_responses } / { len ( results ) } \" ) return zero_shot_metrics def _calculate_coherence_score ( self , text : str ) -> float : \"\"\"Calculate a simple coherence score for generated text.\"\"\" if not text or len ( text . strip ()) == 0 : return 0.0 words = text . split () if len ( words ) == 0 : return 0.0 # Check for repetition unique_words = len ( set ( words )) repetition_penalty = unique_words / len ( words ) # Check for reasonable length length_score = min ( 1.0 , len ( words ) / 20.0 ) if len ( words ) < 20 else max ( 0.5 , 40.0 / len ( words )) # Simple readability check sentences = text . split ( '.' ) avg_sentence_length = sum ( len ( s . split ()) for s in sentences ) / len ( sentences ) if sentences else 0 readability_score = 1.0 if 5 <= avg_sentence_length <= 25 else 0.7 # Combine scores coherence_score = ( repetition_penalty + length_score + readability_score ) / 3.0 return min ( 1.0 , max ( 0.0 , coherence_score )) def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get enhanced training statistics and information.\"\"\" stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'dataset_name' : self . config . dataset . name , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'learning_rate' : self . config . train . learning_rate , 'batch_size' : self . config . train . per_device_batch_size , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : self . config . model . precision if hasattr ( self . config . model , 'precision' ) else 'auto' , }, 'dataset_info' : { 'train_size' : len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 0 , 'val_size' : len ( self . eval_dataset ) if hasattr ( self , 'eval_dataset' ) and self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save enhanced configuration to YAML file.\"\"\" config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'max_seq_length' : self . config . model . max_seq_length , 'learning_rate' : self . config . train . learning_rate , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'batch_size' : self . config . train . per_device_batch_size , 'dataset_name' : self . config . dataset . name , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : str ( self . config . model . precision ) if hasattr ( self . config . model , 'precision' ) else 'auto' , } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL SFT configuration saved to { path } \" ) def run_experiment ( self ): \"\"\"Run a complete experiment with enhanced features.\"\"\" logger . info ( f \"Starting TRL SFT experiment: { self . config . logging . run_name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) try : # Setup model and data self . setup_model () self . setup_data () # Run zero-shot evaluation if configured zero_shot_results = None if hasattr ( self . config , 'evaluation' ) and getattr ( self . config . evaluation , 'run_zero_shot' , False ): logger . info ( \"Running pre-training zero-shot evaluation...\" ) zero_shot_results = self . run_zero_shot_evaluation () # Start training logger . info ( \"Starting training...\" ) start_time = time . time () train_results = self . train () end_time = time . time () training_duration = end_time - start_time # Evaluate after training logger . info ( \"Running post-training evaluation...\" ) eval_results = self . evaluate () # Combine results results = { 'mode' : 'train_and_eval' , 'task_type' : self . task_type . value , 'train_results' : train_results , 'eval_results' : eval_results , 'training_stats' : self . get_training_stats (), 'config' : { 'model' : self . config . model . __dict__ if hasattr ( self . config . model , '__dict__' ) else str ( self . config . model ), 'dataset' : self . config . dataset . __dict__ if hasattr ( self . config . dataset , '__dict__' ) else str ( self . config . dataset ), 'train' : self . config . train . __dict__ if hasattr ( self . config . train , '__dict__' ) else str ( self . config . train ), } } # Add zero-shot results if available if zero_shot_results : results [ 'zero_shot_evaluation' ] = zero_shot_results # Save results output_dir = Path ( self . config . logging . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) results_path = output_dir / \"experiment_results.yaml\" with open ( results_path , 'w' ) as f : yaml . dump ( results , f , default_flow_style = False ) logger . info ( f \"TRL SFT experiment completed. Results saved to { results_path } \" ) return results except Exception as e : logger . error ( f \"TRL SFT experiment failed: { e } \" ) import traceback logger . error ( traceback . format_exc ()) raise finally : if self . logging_manager : self . logging_manager . finish () cleanup () \u00b6 Cleanup TRL resources. Source code in src/aligntune/backends/trl/sft/sft.py 1276 1277 1278 1279 1280 1281 1282 1283 1284 def cleanup ( self ) -> None : \"\"\"Cleanup TRL resources.\"\"\" super () . cleanup () if self . model is not None : del self . model torch . cuda . empty_cache () logger . info ( \"TRL SFT trainer cleanup completed\" ) create_data_loader () \u00b6 Create data loader for training. Source code in src/aligntune/backends/trl/sft/sft.py 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 def create_data_loader ( self ) -> DataLoader : \"\"\"Create data loader for training.\"\"\" if self . dataset is None : raise RuntimeError ( \"Dataset not loaded. Call setup_data() first.\" ) from torch.utils.data import DataLoader return DataLoader ( self . dataset , batch_size = self . config . train . per_device_batch_size , shuffle = True , collate_fn = self . _collate_fn ) evaluate ( eval_config = None , eval_dataset = None ) \u00b6 Evaluate the model performance with enhanced metrics. Source code in src/aligntune/backends/trl/sft/sft.py 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 def evaluate ( self , eval_config : Optional [ Dict ] = None , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the model performance with enhanced metrics.\"\"\" logger . info ( f \"Running TRL SFT evaluation for task: { self . task_type . value } \" ) if not self . trainer : raise ValueError ( \"Training not setup. Call train() first.\" ) if eval_dataset : self . eval_dataset = eval_dataset # Run evaluation eval_results = {} if hasattr ( self . trainer , 'evaluate' ): if getattr ( self , 'eval_dataset' , None ) is not None : eval_results = self . trainer . evaluate () else : logger . warning ( \"No eval_dataset available; skipping evaluation.\" ) if self . logging_manager : self . logging_manager . log_metrics ( eval_results ) # Add task type to results eval_results [ 'task_type' ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator for generation tasks if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]: try : from aligntune.core.sft.evaluator import SFTEvaluator if self . model and self . tokenizer and hasattr ( self , 'eval_dataset' ) and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) # Generate qualitative samples for generation tasks if self . task_type not in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : samples = self . _generate_samples () eval_results [ 'qualitative_samples' ] = samples except Exception as e : logger . warning ( f \"Could not generate qualitative samples: { str ( e ) } \" ) eval_results [ 'qualitative_samples' ] = [] logger . info ( f \"TRL SFT evaluation completed: { len ( eval_results ) } metrics computed\" ) return eval_results generate_samples ( num_samples = 5 ) \u00b6 Public method to generate samples. Source code in src/aligntune/backends/trl/sft/sft.py 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Public method to generate samples.\"\"\" if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Sample generation not supported for classification tasks\" ) return [] try : samples = self . _generate_samples () # Return limited number and ensure all have 'response' key return [ s for s in samples [: num_samples ] if 'generated_response' in s or 'response' in s ] except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] get_sample_outputs () \u00b6 Get sample outputs for logging. Source code in src/aligntune/backends/trl/sft/sft.py 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 def get_sample_outputs ( self ) -> list : \"\"\"Get sample outputs for logging.\"\"\" if not hasattr ( self , 'dataset' ) or self . dataset is None : return [] # Classification tasks don't generate samples if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: return [] sample_outputs = [] for i in range ( min ( 3 , len ( self . dataset ))): sample = self . dataset [ i ] if \"text\" in sample : text = sample [ \"text\" ] elif \"messages\" in sample : messages = sample [ \"messages\" ] text = self . tokenizer . apply_chat_template ( messages , tokenize = False ) else : text = str ( sample . get ( \"input\" , \"\" )) inputs = self . tokenizer ( text , return_tensors = \"pt\" , truncation = True , max_length = 512 ) with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 50 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), pad_token_id = self . tokenizer . eos_token_id ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) sample_outputs . append ({ \"input\" : text [: 100 ] + \"...\" if len ( text ) > 100 else text , \"output\" : response , \"model\" : \"trl_sft\" , \"task_type\" : self . task_type . value }) return sample_outputs get_training_stats () \u00b6 Get enhanced training statistics and information. Source code in src/aligntune/backends/trl/sft/sft.py 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get enhanced training statistics and information.\"\"\" stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'dataset_name' : self . config . dataset . name , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'learning_rate' : self . config . train . learning_rate , 'batch_size' : self . config . train . per_device_batch_size , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : self . config . model . precision if hasattr ( self . config . model , 'precision' ) else 'auto' , }, 'dataset_info' : { 'train_size' : len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 0 , 'val_size' : len ( self . eval_dataset ) if hasattr ( self , 'eval_dataset' ) and self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats is_available () classmethod \u00b6 Check if TRL is available. Source code in src/aligntune/backends/trl/sft/sft.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) return True except ImportError : return False run_experiment () \u00b6 Run a complete experiment with enhanced features. Source code in src/aligntune/backends/trl/sft/sft.py 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 def run_experiment ( self ): \"\"\"Run a complete experiment with enhanced features.\"\"\" logger . info ( f \"Starting TRL SFT experiment: { self . config . logging . run_name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) try : # Setup model and data self . setup_model () self . setup_data () # Run zero-shot evaluation if configured zero_shot_results = None if hasattr ( self . config , 'evaluation' ) and getattr ( self . config . evaluation , 'run_zero_shot' , False ): logger . info ( \"Running pre-training zero-shot evaluation...\" ) zero_shot_results = self . run_zero_shot_evaluation () # Start training logger . info ( \"Starting training...\" ) start_time = time . time () train_results = self . train () end_time = time . time () training_duration = end_time - start_time # Evaluate after training logger . info ( \"Running post-training evaluation...\" ) eval_results = self . evaluate () # Combine results results = { 'mode' : 'train_and_eval' , 'task_type' : self . task_type . value , 'train_results' : train_results , 'eval_results' : eval_results , 'training_stats' : self . get_training_stats (), 'config' : { 'model' : self . config . model . __dict__ if hasattr ( self . config . model , '__dict__' ) else str ( self . config . model ), 'dataset' : self . config . dataset . __dict__ if hasattr ( self . config . dataset , '__dict__' ) else str ( self . config . dataset ), 'train' : self . config . train . __dict__ if hasattr ( self . config . train , '__dict__' ) else str ( self . config . train ), } } # Add zero-shot results if available if zero_shot_results : results [ 'zero_shot_evaluation' ] = zero_shot_results # Save results output_dir = Path ( self . config . logging . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) results_path = output_dir / \"experiment_results.yaml\" with open ( results_path , 'w' ) as f : yaml . dump ( results , f , default_flow_style = False ) logger . info ( f \"TRL SFT experiment completed. Results saved to { results_path } \" ) return results except Exception as e : logger . error ( f \"TRL SFT experiment failed: { e } \" ) import traceback logger . error ( traceback . format_exc ()) raise finally : if self . logging_manager : self . logging_manager . finish () run_zero_shot_evaluation ( test_prompts = None ) \u00b6 Run zero-shot evaluation before training with enhanced metrics. Source code in src/aligntune/backends/trl/sft/sft.py 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 def run_zero_shot_evaluation ( self , test_prompts = None ) -> Dict [ str , Any ]: \"\"\"Run zero-shot evaluation before training with enhanced metrics.\"\"\" logger . info ( f \"Running zero-shot evaluation for task: { self . task_type . value } \" ) # Classification tasks don't support zero-shot evaluation if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Zero-shot evaluation not supported for classification tasks\" ) return {} if test_prompts is None : # Task-specific default prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : test_prompts = [ \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain ML.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : test_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Tell me about AI. \\n Assistant:\" ] else : test_prompts = [ \"What is artificial intelligence?\" , \"Explain machine learning briefly.\" , \"How does a computer work?\" ] results = [] self . model . eval () for prompt in test_prompts : try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = False , pad_token_id = self . tokenizer . eos_token_id , temperature = 0.7 , top_p = 0.9 ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () results . append ({ 'prompt' : prompt , 'response' : response , 'response_length' : len ( response . split ()), 'coherence_score' : self . _calculate_coherence_score ( response ) }) except Exception as e : logger . warning ( f \"Zero-shot evaluation failed for prompt: { e } \" ) results . append ({ 'prompt' : prompt , 'response' : f \"Error: { str ( e ) } \" , 'response_length' : 0 , 'coherence_score' : 0.0 }) # Calculate aggregate metrics avg_length = sum ( r [ 'response_length' ] for r in results ) / len ( results ) if results else 0 avg_coherence = sum ( r [ 'coherence_score' ] for r in results ) / len ( results ) if results else 0 successful_responses = len ([ r for r in results if not r [ 'response' ] . startswith ( 'Error' )]) zero_shot_metrics = { 'task_type' : self . task_type . value , 'zero_shot_results' : results , 'avg_response_length' : avg_length , 'avg_coherence_score' : avg_coherence , 'successful_responses' : successful_responses , 'success_rate' : successful_responses / len ( results ) if results else 0 } if self . logging_manager : self . logging_manager . log_metrics ({ 'zero_shot_avg_length' : avg_length , 'zero_shot_coherence' : avg_coherence , 'zero_shot_success_rate' : successful_responses / len ( results ) if results else 0 }) logger . info ( f \"Zero-shot evaluation completed. Success rate: { successful_responses } / { len ( results ) } \" ) return zero_shot_metrics save_config ( path ) \u00b6 Save enhanced configuration to YAML file. Source code in src/aligntune/backends/trl/sft/sft.py 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 def save_config ( self , path : str ): \"\"\"Save enhanced configuration to YAML file.\"\"\" config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'max_seq_length' : self . config . model . max_seq_length , 'learning_rate' : self . config . train . learning_rate , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'batch_size' : self . config . train . per_device_batch_size , 'dataset_name' : self . config . dataset . name , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : str ( self . config . model . precision ) if hasattr ( self . config . model , 'precision' ) else 'auto' , } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL SFT configuration saved to { path } \" ) save_model ( output_dir = None ) \u00b6 Save the trained model and tokenizer. Parameters: Name Type Description Default output_dir Optional [ str ] Directory to save to (defaults to config output_dir) None Returns: Type Description str Path where model was saved Source code in src/aligntune/backends/trl/sft/sft.py 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model and tokenizer. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : output_dir = Path ( self . config . logging . output_dir ) / \"sft_model\" else : output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) if hasattr ( self , 'trainer' ) and self . trainer is not None : self . trainer . save_model ( output_dir ) logger . info ( f \"Model saved to { output_dir } \" ) else : logger . warning ( \"No trainer available to save model\" ) if hasattr ( self , 'tokenizer' ) and self . tokenizer is not None : self . tokenizer . save_pretrained ( output_dir ) logger . info ( f \"Tokenizer saved to { output_dir } \" ) else : logger . warning ( \"No tokenizer available to save\" ) # Save task type info config_path = output_dir / \"training_config.yaml\" config_dict = { 'task_type' : self . task_type . value , 'model_name' : self . config . model . name_or_path } with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f ) logger . info ( f \"Training config saved to { config_path } \" ) # Store for push_to_hub self . _last_save_path = str ( output_dir ) return str ( output_dir ) setup_data () \u00b6 Setup datasets for SFT training with task-aware formatting. Source code in src/aligntune/backends/trl/sft/sft.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def setup_data ( self ) -> None : \"\"\"Setup datasets for SFT training with task-aware formatting.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT datasets: { self . config . dataset . name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Initialize dataset cache cache_root = getattr ( self . config , 'caching' , {}) . get ( \"root\" , \"./cache\" ) if hasattr ( self . config , 'caching' ) else \"./cache\" self . dataset_cache = DatasetCache ( cache_root = cache_root ) logger . info ( f \"Dataset cache initialized at: { self . dataset_cache . cache_root } \" ) # Load dataset logger . info ( \"Loading dataset...\" ) dataset_config = self . config . dataset from datasets import load_dataset # Handle dataset config/subset load_kwargs = { 'split' : dataset_config . split or \"train\" } dataset_name = dataset_config . name dataset_subset = None if hasattr ( dataset_config , 'subset' ) and dataset_config . subset : dataset_subset = dataset_config . subset elif hasattr ( dataset_config , 'config' ) and dataset_config . config : dataset_subset = dataset_config . config # Check if dataset has a registered custom loader if dataset_name in DatasetRegistry . list_loaders (): logger . info ( f \"Using registered dataset loader: { dataset_name } \" ) dataset = DatasetRegistry . load_dataset ( name = dataset_name , split = dataset_config . split or \"train\" , max_samples = dataset_config . max_samples , ** { k : v for k , v in dataset_config . __dict__ . items () if k not in [ 'name' , 'split' , 'max_samples' ] and v is not None } ) else : # Load dataset try : if dataset_subset : dataset = load_dataset ( dataset_name , dataset_subset , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ValueError as e : if \"Config name is missing\" in str ( e ): raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' to your DatasetConfig.\" ) raise logger . info ( f \"Original dataset size: { len ( dataset ) } samples\" ) # \ud83d\udd39 CRITICAL FIX: Shuffle dataset BEFORE selecting samples # This ensures balanced class distribution for classification tasks if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . info ( \"Shuffling dataset to ensure balanced sampling...\" ) dataset = dataset . shuffle ( seed = 42 ) # Apply dataset size limits if dataset_config . percent and dataset_config . percent < 1.0 : dataset_size = int ( len ( dataset ) * dataset_config . percent ) dataset = dataset . select ( range ( dataset_size )) logger . info ( f \"Using { dataset_config . percent * 100 } % of dataset: { dataset_size } samples\" ) elif dataset_config . max_samples : dataset = dataset . select ( range ( min ( dataset_config . max_samples , len ( dataset )))) logger . info ( f \"Using max_samples limit: { len ( dataset ) } samples\" ) # Apply column mappings if needed if hasattr ( dataset_config , 'column_mapping' ) and dataset_config . column_mapping : valid_mappings = { k : v for k , v in dataset_config . column_mapping . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , dataset_config ) # Create evaluation split try : test_size = 0.1 if len ( dataset ) >= 1000 : test_size = 0.1 elif len ( dataset ) >= 200 : test_size = 0.1 else : test_size = min ( 0.2 , max ( 0.1 , 50 / max ( 1 , len ( dataset )))) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 , shuffle = True ) self . dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Created eval split. Train: { len ( self . dataset ) } | Eval: { len ( self . eval_dataset ) } \" ) except Exception as e : self . dataset = dataset self . eval_dataset = None logger . warning ( f \"Could not create eval split: { str ( e ) } \" ) logger . info ( \"=\" * 80 ) logger . info ( f \"TRL SFT dataset setup completed: { len ( self . dataset ) } samples\" ) logger . info ( \"=\" * 80 ) setup_model () \u00b6 Setup model using standard Transformers with task type awareness. Source code in src/aligntune/backends/trl/sft/sft.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers with task type awareness.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT model: { self . config . model . name_or_path } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) try : from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) except ImportError as e : raise ImportError ( \"Transformers not available.\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model based on task type logger . info ( f \"Loading model for task: { self . task_type . value } \" ) if self . task_type == TaskType . TEXT_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 2 ) logger . info ( f \"Loading classification model with { num_labels } labels\" ) self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 9 ) logger . info ( f \"Loading token classification model with { num_labels } labels\" ) self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) else : # Causal LM for generation tasks logger . info ( \"Loading causal language model\" ) # Handle quantization configuration quantization_config = None quantization_dict = getattr ( self . config . model , 'quantization' , {}) if quantization_dict . get ( 'load_in_4bit' , False ) or quantization_dict . get ( 'load_in_8bit' , False ): try : from transformers import BitsAndBytesConfig load_in_4bit = quantization_dict . get ( 'load_in_4bit' , False ) load_in_8bit = quantization_dict . get ( 'load_in_8bit' , False ) if load_in_4bit : compute_dtype_str = quantization_dict . get ( 'bnb_4bit_compute_dtype' , 'bfloat16' ) compute_dtype = torch . bfloat16 if compute_dtype_str == 'bfloat16' else torch . float16 quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_compute_dtype = compute_dtype , bnb_4bit_quant_type = quantization_dict . get ( 'bnb_4bit_quant_type' , 'nf4' ), bnb_4bit_use_double_quant = quantization_dict . get ( 'bnb_4bit_use_double_quant' , False ), ) logger . info ( \"\u2705 4-bit quantization enabled for memory optimization\" ) elif load_in_8bit : quantization_config = BitsAndBytesConfig ( load_in_8bit = True ) logger . info ( \"\u2705 8-bit quantization enabled for memory optimization\" ) except ImportError : logger . warning ( \"\u26a0\ufe0f BitsAndBytes not available. Install with: pip install bitsandbytes\" ) quantization_config = None # Get max_memory from config if specified max_memory = getattr ( self . config . model , 'max_memory' , None ) model_kwargs = { 'torch_dtype' : dtype , 'trust_remote_code' : True , 'device_map' : 'auto' if torch . cuda . is_available () else None , 'low_cpu_mem_usage' : True , } # Add max_memory if specified if max_memory : model_kwargs [ 'max_memory' ] = max_memory if quantization_config : model_kwargs [ 'quantization_config' ] = quantization_config logger . info ( f \"Quantization config: { quantization_config } \" ) else : # Only set these if not using quantization model_kwargs [ 'load_in_4bit' ] = False model_kwargs [ 'load_in_8bit' ] = False logger . info ( \"No quantization configured - loading full precision model\" ) self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Move to GPU only if not using device_map='auto' (which handles it automatically) if not quantization_config and torch . cuda . is_available () and model_kwargs . get ( 'device_map' ) != 'auto' : self . model = self . model . cuda () logger . info ( \"Model moved to GPU\" ) elif quantization_config : logger . info ( \"Model loaded with quantization (device_map='auto')\" ) # CRITICAL: When using quantization, PEFT adapters MUST be enabled # TRL SFTTrainer will handle this, but we need to ensure PEFT is enabled in config if not getattr ( self . config . model , 'peft_enabled' , False ): logger . warning ( \"\u26a0\ufe0f Quantization requires PEFT adapters. Enabling PEFT automatically.\" ) self . config . model . peft_enabled = True # Setup tokenizer for specific tasks self . _setup_tokenizer_for_task () logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) setup_rewards () \u00b6 SFT doesn't use reward functions. Source code in src/aligntune/backends/trl/sft/sft.py 699 700 701 702 def setup_rewards ( self ) -> None : \"\"\"SFT doesn't use reward functions.\"\"\" logger . info ( \"SFT training doesn't use reward functions\" ) self . reward_functions = [] train () \u00b6 Train using TRL SFTTrainer with task-aware setup. Source code in src/aligntune/backends/trl/sft/sft.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def train ( self ) -> Dict [ str , Any ]: \"\"\"Train using TRL SFTTrainer with task-aware setup.\"\"\" logger . info ( f \"Starting TRL SFT training for task: { self . task_type . value } \" ) # Setup model and data if not already done if self . model is None : self . setup_model () if self . dataset is None : self . setup_data () try : from trl import SFTTrainer from transformers import TrainingArguments , Trainer from transformers import DataCollatorWithPadding , DataCollatorForTokenClassification import evaluate except ImportError as e : raise ImportError ( \"TRL required for SFT training. Install with: pip install trl\" ) from e # Create training arguments # Get optimizer and scheduler configurations from ....core.optimization import get_optimizer_for_config , get_scheduler_for_config # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate or 2e-4 , self . config . train . weight_decay or 0.01 ) # Calculate max_steps - use config value or calculate from epochs max_steps = self . config . train . max_steps if max_steps is None : # Calculate from epochs if max_steps not specified epochs = getattr ( self . config . train , 'epochs' , 3 ) or 3 dataset_size = len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 1000 batch_size = self . config . train . per_device_batch_size or 2 grad_accum = self . config . train . gradient_accumulation_steps or 4 max_steps = ( dataset_size * epochs ) // ( batch_size * grad_accum ) max_steps = max ( max_steps , 10 ) # Minimum 10 steps logger . info ( f \"Calculated max_steps= { max_steps } from { epochs } epochs, { dataset_size } samples\" ) # Calculate warmup steps warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" # Exclude lr and weight_decay since TrainingArguments already sets them filtered = { k : v for k , v in kwargs_dict . items () if k not in [ 'lr' , 'learning_rate' , 'weight_decay' ]} if not filtered : return None return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in filtered . items ()) optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) # print(lr_scheduler_kwargs_str) # print(scheduler_config) filtered_scheduler_kwargs = { k : v for k , v in scheduler_config [ 'lr_scheduler_kwargs' ] . items () if k not in [ 'num_training_steps' , 'num_warmup_steps' ] } # Build TrainingArguments kwargs, conditionally including optim_args training_kwargs = { \"output_dir\" : self . config . logging . output_dir , \"max_steps\" : max_steps , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size or 2 , \"gradient_accumulation_steps\" : self . config . train . gradient_accumulation_steps or 4 , \"learning_rate\" : self . config . train . learning_rate or 2e-4 , \"weight_decay\" : self . config . train . weight_decay or 0.01 , \"warmup_steps\" : warmup_steps , \"logging_steps\" : getattr ( self . config . logging , 'log_interval' , 10 ), \"save_steps\" : getattr ( self . config . train , 'save_interval' , 500 ), \"eval_steps\" : getattr ( self . config . train , 'eval_interval' , 500 ), \"eval_strategy\" : \"no\" , \"save_strategy\" : \"steps\" , \"load_best_model_at_end\" : False , \"report_to\" : [], \"run_name\" : self . config . logging . run_name , \"seed\" : 42 , ** precision_args , \"dataloader_num_workers\" : 0 , \"remove_unused_columns\" : False , \"optim\" : optimizer_name , \"lr_scheduler_type\" : scheduler_name , \"lr_scheduler_kwargs\" : filtered_scheduler_kwargs , } # Only include optim_args if it's not None/empty and valid. # For 8-bit optimizers (adamw_8bit, paged_adamw_8bit, etc.), bitsandbytes handles kwargs internally, # so we skip optim_args to avoid Transformers parsing issues. # Also skip if the string doesn't contain \"=\" (would cause parsing errors). should_skip_optim_args = ( not optim_args_str or # None or empty string not optim_args_str . strip () or # Whitespace only \"=\" not in optim_args_str or # Invalid format (no key=value pairs) any ( x in optimizer_name . lower () for x in [ \"8bit\" , \"8_bit\" , \"bnb\" , \"paged\" ]) # 8-bit optimizers ) if not should_skip_optim_args : training_kwargs [ \"optim_args\" ] = optim_args_str training_args = TrainingArguments ( ** training_kwargs ) # Create trainer based on task type if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: self . _setup_classification_trainer ( training_args ) else : # Generation tasks use SFTTrainer # Setup PEFT config if enabled (required for quantized models) peft_config = None if getattr ( self . config . model , 'peft_enabled' , False ): try : from peft import LoraConfig # Get LoRA parameters from config lora_r = getattr ( self . config . model , 'lora_rank' , 16 ) lora_alpha = getattr ( self . config . model , 'lora_alpha' , 32 ) lora_dropout = getattr ( self . config . model , 'lora_dropout' , 0.1 ) target_modules = getattr ( self . config . model , 'target_modules' , None ) # If target_modules not specified, use default for Llama models if target_modules is None : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) logger . info ( f \"\u2705 PEFT LoRA config created: r= { lora_r } , alpha= { lora_alpha } , modules= { target_modules } \" ) except ImportError : logger . warning ( \"\u26a0\ufe0f PEFT not available. Install with: pip install peft\" ) peft_config = None # Generation tasks use SFTTrainer self . trainer = SFTTrainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , peft_config = peft_config , # Pass PEFT config to SFTTrainer ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( f \"Starting TRL SFT Training - Task: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Log training metrics if self . logging_manager and hasattr ( train_result , 'metrics' ): self . logging_manager . log_metrics ( train_result . metrics ) # Save model self . trainer . save_model () # Add to training history self . training_history . append ({ 'timestamp' : time . time (), 'duration' : training_duration , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'task_type' : self . task_type . value , 'model_path' : self . config . logging . output_dir , }) logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT training completed successfully!\" ) logger . info ( \"=\" * 80 ) # Return training results return { 'training_time' : training_duration , 'final_loss' : getattr ( train_result , 'train_loss' , 0.0 ), 'model_path' : self . config . logging . output_dir , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'epochs' : getattr ( train_result , 'epoch' , 0 ), 'metrics' : getattr ( train_result , 'metrics' , {}), 'task_type' : self . task_type . value } train_step ( batch ) \u00b6 Execute training step (handled by TRL trainer). Source code in src/aligntune/backends/trl/sft/sft.py 1183 1184 1185 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute training step (handled by TRL trainer).\"\"\" return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.sft.sft import TRLSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = TRLSFTTrainer ( config ) trainer . train () TRLDPOTrainer \u00b6 TRL backend for Direct Preference Optimization. Bases: TrainerBase TRL-based DPO trainer using TRL's DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 class TRLDPOTrainer ( TrainerBase ): \"\"\"TRL-based DPO trainer using TRL's DPOTrainer.\"\"\" def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize TRL DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . dataset_cache = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL DPO trainer is available.\"\"\" try : from trl import DPOTrainer , DPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default # Required abstract methods def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO (uses preferences).\"\"\" logger . info ( \"DPO uses preference pairs instead of explicit rewards\" ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Single training step - handled internally by TRL DPOTrainer.\"\"\" if not self . trainer : raise RuntimeError ( \"Trainer not initialized. Call train() first.\" ) # The actual training loop is handled by self.trainer.train() return {} def setup_model ( self ) -> None : \"\"\"Setup model and tokenizer for DPO.\"\"\" try : from transformers import AutoModelForCausalLM , AutoTokenizer , BitsAndBytesConfig logger . info ( f \"Setting up TRL DPO model: { self . config . model . name_or_path } \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL DPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Determine dtype dtype = None if self . config . model . precision . value == \"bf16\" : dtype = torch . bfloat16 elif self . config . model . precision . value == \"fp16\" : dtype = torch . float16 elif self . config . model . precision . value == \"fp32\" : dtype = torch . float32 # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Setup quantization config quantization_config = None use_quantization = False if self . config . model . quantization : if self . config . model . quantization . get ( \"load_in_4bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_quant_type = \"nf4\" , bnb_4bit_compute_dtype = dtype or torch . bfloat16 , bnb_4bit_use_double_quant = True , ) use_quantization = True logger . info ( \"Using 4-bit quantization with BitsAndBytesConfig\" ) elif self . config . model . quantization . get ( \"load_in_8bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_8bit = True , ) use_quantization = True logger . info ( \"Using 8-bit quantization with BitsAndBytesConfig\" ) # Load main model model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : model_kwargs [ \"quantization_config\" ] = quantization_config self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # ---- Extract LoRA config safely ---- lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) # ---- Apply LoRA ---- from peft import LoraConfig , get_peft_model if use_quantization : from peft import prepare_model_for_kbit_training self . model = prepare_model_for_kbit_training ( self . model ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to quantized model\" ) elif getattr ( self . config . model , \"use_peft\" , False ): peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to model\" ) # Create reference model (frozen copy) # For quantized models, reference model uses same quantization but no LoRA ref_model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : ref_model_kwargs [ \"quantization_config\" ] = quantization_config self . reference_model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** ref_model_kwargs ) logger . info ( \"TRL DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup TRL DPO model: { e } \" ) raise def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer.\"\"\" try : from trl import DPOTrainer , DPOConfig logger . info ( \"Setting up TRL DPOTrainer\" ) # Create DPO configuration # Get optimizer and scheduler configurations from aligntune.core.optimization import get_optimizer_for_config , get_scheduler_for_config # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate , self . config . train . weight_decay or 0.01 ) # Calculate warmup steps num_train_epochs = self . config . train . epochs or 1 # Estimate max_steps for scheduler if not provided max_steps = getattr ( self . config . train , 'max_steps' , None ) if max_steps is None and hasattr ( self . train_dataset , '__len__' ): # Rough estimation: steps = epochs * (dataset_size / batch_size / accumulation) dataset_size = len ( self . train_dataset ) batch_size = self . config . train . per_device_batch_size or 1 accumulation = self . config . train . gradient_accumulation_steps or 1 effective_batch_size = batch_size * accumulation max_steps = int ( num_train_epochs * dataset_size / effective_batch_size ) else : max_steps = 1000 # fallback warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) # print( scheduler_config) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in kwargs_dict . items ()) if kwargs_dict else None optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = None ) # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Get max_steps from config (if set, it overrides num_train_epochs) config_max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = None ) dpo_config = DPOConfig ( output_dir = self . config . logging . output_dir , num_train_epochs = num_train_epochs , max_steps = config_max_steps if config_max_steps else - 1 , # -1 means use num_train_epochs per_device_train_batch_size = self . config . train . per_device_batch_size , per_device_eval_batch_size = self . config . train . per_device_batch_size , # ADD THIS gradient_accumulation_steps = self . config . train . gradient_accumulation_steps , learning_rate = self . config . train . learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Optimizer and scheduler lr_scheduler_type = scheduler_name , optim = optimizer_name , weight_decay = optimizer_config [ 'optimizer_kwargs' ] . get ( 'weight_decay' , 0.01 ), optim_args = optim_args_str , # Precision ** precision_args , # Data handling dataloader_pin_memory = False , remove_unused_columns = False , # DPO-specific parameters beta = self . config . train . beta , loss_type = getattr ( self . config . train , 'loss_type' , 'sigmoid' ), max_length = self . config . model . max_seq_length , max_prompt_length = getattr ( self . config . train , 'max_prompt_length' , self . config . model . max_seq_length // 2 ), truncation_mode = getattr ( self . config . train , 'truncation_mode' , 'keep_end' ), precompute_ref_log_probs = getattr ( self . config . train , 'precompute_ref_log_probs' , False ), # Logging configuration report_to = report_to if report_to else ( self . config . logging . loggers if self . config . logging . loggers else []), ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , processing_class = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting TRL DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"TRL DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 ) } except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained model.\"\"\" # try: # if not self.trainer or not self.eval_dataset: # logger.warning(\"No trainer or evaluation dataset available\") # return {} # logger.info(\"Running DPO evaluation\") # # Run evaluation # eval_result = self.trainer.evaluate() # logger.info(\"DPO evaluation completed\") # return { # \"eval_loss\": eval_result.get(\"eval_loss\", 0), # \"eval_metrics\": eval_result # } # except Exception as e: # logger.error(f\"DPO evaluation failed: {e}\") # return {} # def evaluate(self, eval_dataset=None, metric_key_prefix: str = \"eval\",use_custom_evaluator=False, **kwargs) -> Dict[str, float]: # # Setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise __init__ ( config ) \u00b6 Initialize TRL DPO trainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize TRL DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . dataset_cache = None self . dataset_dict = None is_available () classmethod \u00b6 Check if TRL DPO trainer is available. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 39 40 41 42 43 44 45 46 47 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL DPO trainer is available.\"\"\" try : from trl import DPOTrainer , DPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False load_model ( path ) \u00b6 Load a trained model. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise save_model ( path = None ) \u00b6 Save model. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise setup_data () \u00b6 Setup data - delegates to setup_dataset. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 62 63 64 def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () setup_dataset () \u00b6 Setup and prepare preference dataset for DPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise setup_model () \u00b6 Setup model and tokenizer for DPO. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def setup_model ( self ) -> None : \"\"\"Setup model and tokenizer for DPO.\"\"\" try : from transformers import AutoModelForCausalLM , AutoTokenizer , BitsAndBytesConfig logger . info ( f \"Setting up TRL DPO model: { self . config . model . name_or_path } \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL DPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Determine dtype dtype = None if self . config . model . precision . value == \"bf16\" : dtype = torch . bfloat16 elif self . config . model . precision . value == \"fp16\" : dtype = torch . float16 elif self . config . model . precision . value == \"fp32\" : dtype = torch . float32 # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Setup quantization config quantization_config = None use_quantization = False if self . config . model . quantization : if self . config . model . quantization . get ( \"load_in_4bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_quant_type = \"nf4\" , bnb_4bit_compute_dtype = dtype or torch . bfloat16 , bnb_4bit_use_double_quant = True , ) use_quantization = True logger . info ( \"Using 4-bit quantization with BitsAndBytesConfig\" ) elif self . config . model . quantization . get ( \"load_in_8bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_8bit = True , ) use_quantization = True logger . info ( \"Using 8-bit quantization with BitsAndBytesConfig\" ) # Load main model model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : model_kwargs [ \"quantization_config\" ] = quantization_config self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # ---- Extract LoRA config safely ---- lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) # ---- Apply LoRA ---- from peft import LoraConfig , get_peft_model if use_quantization : from peft import prepare_model_for_kbit_training self . model = prepare_model_for_kbit_training ( self . model ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to quantized model\" ) elif getattr ( self . config . model , \"use_peft\" , False ): peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to model\" ) # Create reference model (frozen copy) # For quantized models, reference model uses same quantization but no LoRA ref_model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : ref_model_kwargs [ \"quantization_config\" ] = quantization_config self . reference_model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** ref_model_kwargs ) logger . info ( \"TRL DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup TRL DPO model: { e } \" ) raise setup_rewards () \u00b6 Setup rewards - not used in DPO (uses preferences). Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 66 67 68 def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO (uses preferences).\"\"\" logger . info ( \"DPO uses preference pairs instead of explicit rewards\" ) setup_trainer () \u00b6 Setup TRL DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer.\"\"\" try : from trl import DPOTrainer , DPOConfig logger . info ( \"Setting up TRL DPOTrainer\" ) # Create DPO configuration # Get optimizer and scheduler configurations from aligntune.core.optimization import get_optimizer_for_config , get_scheduler_for_config # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate , self . config . train . weight_decay or 0.01 ) # Calculate warmup steps num_train_epochs = self . config . train . epochs or 1 # Estimate max_steps for scheduler if not provided max_steps = getattr ( self . config . train , 'max_steps' , None ) if max_steps is None and hasattr ( self . train_dataset , '__len__' ): # Rough estimation: steps = epochs * (dataset_size / batch_size / accumulation) dataset_size = len ( self . train_dataset ) batch_size = self . config . train . per_device_batch_size or 1 accumulation = self . config . train . gradient_accumulation_steps or 1 effective_batch_size = batch_size * accumulation max_steps = int ( num_train_epochs * dataset_size / effective_batch_size ) else : max_steps = 1000 # fallback warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) # print( scheduler_config) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in kwargs_dict . items ()) if kwargs_dict else None optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = None ) # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Get max_steps from config (if set, it overrides num_train_epochs) config_max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = None ) dpo_config = DPOConfig ( output_dir = self . config . logging . output_dir , num_train_epochs = num_train_epochs , max_steps = config_max_steps if config_max_steps else - 1 , # -1 means use num_train_epochs per_device_train_batch_size = self . config . train . per_device_batch_size , per_device_eval_batch_size = self . config . train . per_device_batch_size , # ADD THIS gradient_accumulation_steps = self . config . train . gradient_accumulation_steps , learning_rate = self . config . train . learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Optimizer and scheduler lr_scheduler_type = scheduler_name , optim = optimizer_name , weight_decay = optimizer_config [ 'optimizer_kwargs' ] . get ( 'weight_decay' , 0.01 ), optim_args = optim_args_str , # Precision ** precision_args , # Data handling dataloader_pin_memory = False , remove_unused_columns = False , # DPO-specific parameters beta = self . config . train . beta , loss_type = getattr ( self . config . train , 'loss_type' , 'sigmoid' ), max_length = self . config . model . max_seq_length , max_prompt_length = getattr ( self . config . train , 'max_prompt_length' , self . config . model . max_seq_length // 2 ), truncation_mode = getattr ( self . config . train , 'truncation_mode' , 'keep_end' ), precompute_ref_log_probs = getattr ( self . config . train , 'precompute_ref_log_probs' , False ), # Logging configuration report_to = report_to if report_to else ( self . config . logging . loggers if self . config . logging . loggers else []), ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , processing_class = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise train () \u00b6 Run DPO training. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting TRL DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"TRL DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 ) } except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise train_step ( batch ) \u00b6 Single training step - handled internally by TRL DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 70 71 72 73 74 75 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Single training step - handled internally by TRL DPOTrainer.\"\"\" if not self . trainer : raise RuntimeError ( \"Trainer not initialized. Call train() first.\" ) # The actual training loop is handled by self.trainer.train() return {} options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.rl.dpo.dpo import TRLDPOTrainer from aligntune.core.rl.config import UnifiedConfig , AlgorithmType config = UnifiedConfig ( algo = AlgorithmType . DPO , ... ) trainer = TRLDPOTrainer ( config ) trainer . train () TRLPPOTrainer \u00b6 TRL backend for Proximal Policy Optimization. Bases: TrainerBase PPO trainer using pure TRL PPOTrainer with math, code, and enhanced rewards. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 class TRLPPOTrainer ( TrainerBase ): \"\"\"PPO trainer using pure TRL PPOTrainer with math, code, and enhanced rewards.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . policy_model = None self . ref_model = None self . reward_model = None self . value_model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . train_dataset = None self . eval_dataset = None self . reward_functions = [] self . training_history = [] self . logging_manager = None self . custom_evaluator = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import PPOTrainer , PPOConfig from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup models (policy, reference, reward, value) using standard Transformers.\"\"\" # Extract model config values safely policy_model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) # Check if integrated reward training is configured if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training is not None ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) else : # Extract from config as normal reward_model_name = self . _get_config_value ( self . config . model , 'reward_model_name' , 'reward_path' , default = None ) value_model_name = self . _get_config_value ( self . config . model , 'reward_value_model' , default = None ) sft_model_path = self . _get_config_value ( self . config . model , 'sft_path' , default = None ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL PPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Quantization settings load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # Auto-detect quantization from model name if not load_in_4bit and not load_in_8bit : model_name_lower = policy_model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { policy_model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { policy_model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL PPO models\" ) logger . info ( f \"Policy model: { policy_model_name } \" ) logger . info ( f \"SFT path: { sft_model_path or 'Same as policy' } \" ) logger . info ( f \"Reward model: { reward_model_name or 'Same as value model' } \" ) logger . info ( f \"Value model: { value_model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( policy_model_name , trust_remote_code = trust_remote_code , ) if self . tokenizer . padding_side != \"left\" : self . tokenizer . padding_side = \"left\" # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Common model kwargs base_model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified for policy model policy_model_kwargs = base_model_kwargs . copy () if load_in_4bit : logger . info ( \"Loading policy model with 4-bit quantization...\" ) policy_model_kwargs [ \"load_in_4bit\" ] = True policy_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 policy_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" policy_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading policy model with 8-bit quantization...\" ) policy_model_kwargs [ \"load_in_8bit\" ] = True # Load policy model (from SFT path if provided, else base model) logger . info ( \"Loading policy model...\" ) policy_path = sft_model_path or policy_model_name self . policy_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** policy_model_kwargs ) # Apply PEFT if specified if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration to policy model...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing policy model for k-bit training...\" ) self . policy_model = prepare_model_for_kbit_training ( self . policy_model ) # Extract PEFT config values lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . policy_model = get_peft_model ( self . policy_model , peft_config ) self . policy_model . print_trainable_parameters () logger . info ( \"PEFT adapters applied to policy model successfully\" ) # Reference model is None when using PEFT (TRL handles this) self . ref_model = None logger . info ( \"Reference model: None (PEFT mode - TRL will handle reference)\" ) else : # Load reference model (frozen copy of SFT model) logger . info ( \"Loading reference model (frozen copy)...\" ) self . ref_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** base_model_kwargs ) self . ref_model . eval () for param in self . ref_model . parameters (): param . requires_grad = False logger . info ( \"Reference model loaded and frozen\" ) # Load reward and value models logger . info ( \"Loading reward and value models...\" ) # Reward model quantization settings reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , default = {}) reward_load_4bit = reward_quant . get ( 'load_in_4bit' , False ) reward_load_8bit = reward_quant . get ( 'load_in_8bit' , False ) reward_model_kwargs = base_model_kwargs . copy () if reward_load_4bit : logger . info ( \"Loading reward model with 4-bit quantization...\" ) reward_model_kwargs [ \"load_in_4bit\" ] = True reward_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 reward_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" reward_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif reward_load_8bit : logger . info ( \"Loading reward model with 8-bit quantization...\" ) reward_model_kwargs [ \"load_in_8bit\" ] = True # Value model quantization settings value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , default = {}) value_load_4bit = value_quant . get ( 'load_in_4bit' , False ) value_load_8bit = value_quant . get ( 'load_in_8bit' , False ) value_model_kwargs = base_model_kwargs . copy () if value_load_4bit : logger . info ( \"Loading value model with 4-bit quantization...\" ) value_model_kwargs [ \"load_in_4bit\" ] = True value_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 value_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" value_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif value_load_8bit : logger . info ( \"Loading value model with 8-bit quantization...\" ) value_model_kwargs [ \"load_in_8bit\" ] = True # Load reward model (or use value model if not specified) try : reward_path = reward_model_name or value_model_name self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_path , num_labels = 1 , ** reward_model_kwargs ) logger . info ( f \"Reward model loaded from: { reward_path } \" ) except : self . reward_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** reward_model_kwargs ) # Load value model try : self . value_model = AutoModelForSequenceClassification . from_pretrained ( value_model_name , num_labels = 1 , ** value_model_kwargs ) logger . info ( f \"Value model loaded from: { value_model_name } \" ) except : self . value_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** value_model_kwargs ) def _train_custom_reward_model ( self ) -> str : \"\"\"Train a custom reward model using the reward_training config.\"\"\" from aligntune.rewards import RewardModelTrainer from aligntune.rewards.factory import create_reward_functions reward_config = self . config . reward_training # Validate required fields if not hasattr ( reward_config , 'base_model_name' ) or not reward_config . base_model_name : raise ValueError ( \"reward_training.base_model_name is required\" ) if not hasattr ( reward_config , 'output_dir' ) or not reward_config . output_dir : raise ValueError ( \"reward_training.output_dir is required\" ) if not hasattr ( reward_config , 'reward_functions' ) or not reward_config . reward_functions : raise ValueError ( \"reward_training.reward_functions is required\" ) if not hasattr ( reward_config , 'training_texts' ) or not reward_config . training_texts : raise ValueError ( \"reward_training.training_texts is required (minimum 10 samples)\" ) logger . info ( f \"Training reward model with base: { reward_config . base_model_name } \" ) logger . info ( f \"Output directory: { reward_config . output_dir } \" ) logger . info ( f \"Training samples: { len ( reward_config . training_texts ) } \" ) logger . info ( f \"Reward functions: { reward_config . reward_functions } \" ) # Create reward functions from config # reward_functions is a List[str] according to config from aligntune.rewards.registry import RewardRegistry reward_func_list = [] for rf_name in reward_config . reward_functions : try : rf = RewardRegistry . get_reward_function ( rf_name ) reward_func_list . append ( rf ) except Exception as e : logger . warning ( f \"Could not load reward function ' { rf_name } ': { e } \" ) if not reward_func_list : raise ValueError ( \"No valid reward functions could be loaded\" ) # Get composite weights if provided composite_weights = None if hasattr ( reward_config , 'reward_weights' ) and reward_config . reward_weights : composite_weights = reward_config . reward_weights else : composite_weights = [ 1.0 ] * len ( reward_func_list ) # Create reward model trainer trainer = RewardModelTrainer ( base_model_name = reward_config . base_model_name , reward_functions = reward_func_list , composite_weights = composite_weights ) # Generate training dataset from training_texts logger . info ( \"Generating training data from reward functions...\" ) training_data = trainer . generate_training_data ( texts = reward_config . training_texts , references = reward_config . reference_texts if hasattr ( reward_config , 'reference_texts' ) else None , batch_size = reward_config . batch_size ) logger . info ( f \"Generated { len ( training_data ) } training examples\" ) # Train the reward model logger . info ( \"Starting reward model training...\" ) model_path = trainer . train_reward_model ( training_data = training_data , output_dir = reward_config . output_dir , num_epochs = reward_config . num_epochs , learning_rate = reward_config . learning_rate , batch_size = reward_config . batch_size , gradient_accumulation_steps = reward_config . gradient_accumulation_steps ) logger . info ( f \"\u2705 Custom reward model training completed: { model_path } \" ) return model_path def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task (prompt-only format) from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict [ \"train\" ] self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # Apply sampling if specified if max_samples : logger . info ( f \"Limiting train dataset to { max_samples } samples\" ) self . train_dataset = self . train_dataset . select ( range ( min ( max_samples , len ( self . train_dataset )))) elif percent and percent < 100 : num_samples = int ( len ( self . train_dataset ) * percent / 100 ) logger . info ( f \"Using { percent } % of dataset ( { num_samples } samples)\" ) self . train_dataset = self . train_dataset . select ( range ( num_samples )) if self . eval_dataset and max_samples : eval_max = max_samples // 10 # Use 10% for eval self . eval_dataset = self . eval_dataset . select ( range ( min ( eval_max , len ( self . eval_dataset )))) logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO print ( \"Tokenizing dataset...\" ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" # Get prompt column (DataManager ensures consistent naming) prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) # Tokenize tokenized = self . tokenizer ( prompts , padding = False , # Keep False like TRL example truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] # Calculate lengths for each sequence in the batch lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length (like TRL example) logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure (like TRL example checks) assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for PPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments including prompts, references, etc. \"\"\" if not completions : return [] batch_rewards = [] # Extract batch data from kwargs test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # Check multiple possible reference column names references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'ground_truth' , 'response' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] break if references is None : references = [ None ] * len ( completions ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): try : reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : reward = reward_func ( completion ) except TypeError : try : reward = reward_func ( completion , reference = reference ) except TypeError : try : reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_trainer ( self ) -> None : \"\"\"Set up the PPO trainer with all configurations.\"\"\" logger . info ( \"Setting up TRL PPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.01 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # PPO-specific parameters kl_coef = self . _get_config_value ( self . config . train , 'kl_coef' , default = 0.1 ) cliprange = self . _get_config_value ( self . config . train , 'cliprange' , default = 0.2 ) cliprange_value = self . _get_config_value ( self . config . train , 'cliprange_value' , default = 0.2 ) vf_coef = self . _get_config_value ( self . config . train , 'vf_coef' , default = 0.1 ) gamma = self . _get_config_value ( self . config . train , 'gamma' , default = 1.0 ) lam = self . _get_config_value ( self . config . train , 'lam' , default = 0.95 ) # Generation parameters response_length = self . _get_config_value ( self . config . train , 'response_length' , 'max_completion_length' , default = 128 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) stop_token = self . _get_config_value ( self . config . train , 'stop_token' , default = 'eos' ) missing_eos_penalty = self . _get_config_value ( self . config . train , 'missing_eos_penalty' , default = 1.0 ) # Evaluation and checkpointing max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = 1000 ) eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) # Logging logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Max steps: { max_steps } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"KL coefficient: { kl_coef } \" ) logger . info ( f \"Clip range: { cliprange } \" ) logger . info ( f \"Value function coef: { vf_coef } \" ) logger . info ( f \"Response length: { response_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup PPO trainer from trl import PPOTrainer , PPOConfig ppo_config = PPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , # PPO-specific kl_coef = kl_coef , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , gamma = gamma , lam = lam , # Generation response_length = response_length , temperature = temperature , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , # Evaluation max_steps = max_steps , eval_strategy = eval_strategy if self . eval_dataset else \"no\" , eval_steps = eval_steps if self . eval_dataset else None , # Checkpointing save_strategy = save_strategy , save_steps = save_steps , # Logging logging_steps = logging_steps , report_to = report_to if report_to else [], # Precision ** precision_args , # Other remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # Get PEFT config if using PEFT peft_config = None use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) if use_peft : from peft import LoraConfig lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) # Create PPO trainer self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = peft_config , ) logger . info ( \"PPO trainer setup completed successfully!\" ) def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL PPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) train_result = self . trainer . train () # Log samples after training try : raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step - handled by TRL internally.\"\"\" logger . debug ( \"train_step() called but TRL PPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader - handled by TRL internally.\"\"\" logger . debug ( \"create_data_loader() called but TRL PPO uses TRL's internal data loading\" ) return None # def evaluate(self) -> Dict[str, float]: # try: # logger.info(\"Running evaluation...\") # return self.trainer.evaluate() # except Exception as e: # logger.error(f\"Evaluation failed: {e}\") # return {} def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if self . train_dataset else 0 , 'val_size' : len ( self . eval_dataset ) if self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . policy_model is not None , 'device' : str ( next ( self . policy_model . parameters ()) . device ) if self . policy_model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . policy_model , 'peft_config' ) if self . policy_model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL PPO configuration saved to { path } \" ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained PPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/ppo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving PPO model to: { save_path } \" ) # Save using trainer self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"ppo_training_config.yaml\" self . save_config ( str ( config_path )) logger . info ( f \"PPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save PPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained PPO model.\"\"\" try : logger . info ( f \"Loading PPO model from: { path } \" ) from transformers import AutoModelForCausalLM , AutoTokenizer # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( path ) # Load model max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 2048 ) self . policy_model = AutoModelForCausalLM . from_pretrained ( path , device_map = \"auto\" , ) logger . info ( \"PPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load PPO model: { e } \" ) raise create_data_loader () \u00b6 Create data loader - handled by TRL internally. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1100 1101 1102 1103 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader - handled by TRL internally.\"\"\" logger . debug ( \"create_data_loader() called but TRL PPO uses TRL's internal data loading\" ) return None get_training_stats () \u00b6 Get training statistics. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if self . train_dataset else 0 , 'val_size' : len ( self . eval_dataset ) if self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . policy_model is not None , 'device' : str ( next ( self . policy_model . parameters ()) . device ) if self . policy_model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . policy_model , 'peft_config' ) if self . policy_model else False , }, 'training_history' : self . training_history , } return stats is_available () classmethod \u00b6 Check if TRL is available. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 58 59 60 61 62 63 64 65 66 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import PPOTrainer , PPOConfig from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer return True except ImportError : return False load_model ( path ) \u00b6 Load a trained PPO model. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 def load_model ( self , path : str ) -> None : \"\"\"Load a trained PPO model.\"\"\" try : logger . info ( f \"Loading PPO model from: { path } \" ) from transformers import AutoModelForCausalLM , AutoTokenizer # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( path ) # Load model max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 2048 ) self . policy_model = AutoModelForCausalLM . from_pretrained ( path , device_map = \"auto\" , ) logger . info ( \"PPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load PPO model: { e } \" ) raise save_config ( path ) \u00b6 Save configuration to YAML file. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL PPO configuration saved to { path } \" ) save_model ( path = None ) \u00b6 Save the trained PPO model. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained PPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/ppo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving PPO model to: { save_path } \" ) # Save using trainer self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"ppo_training_config.yaml\" self . save_config ( str ( config_path )) logger . info ( f \"PPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save PPO model: { e } \" ) raise setup_data () \u00b6 Setup datasets for PPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task (prompt-only format) from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict [ \"train\" ] self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # Apply sampling if specified if max_samples : logger . info ( f \"Limiting train dataset to { max_samples } samples\" ) self . train_dataset = self . train_dataset . select ( range ( min ( max_samples , len ( self . train_dataset )))) elif percent and percent < 100 : num_samples = int ( len ( self . train_dataset ) * percent / 100 ) logger . info ( f \"Using { percent } % of dataset ( { num_samples } samples)\" ) self . train_dataset = self . train_dataset . select ( range ( num_samples )) if self . eval_dataset and max_samples : eval_max = max_samples // 10 # Use 10% for eval self . eval_dataset = self . eval_dataset . select ( range ( min ( eval_max , len ( self . eval_dataset )))) logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO print ( \"Tokenizing dataset...\" ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" # Get prompt column (DataManager ensures consistent naming) prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) # Tokenize tokenized = self . tokenizer ( prompts , padding = False , # Keep False like TRL example truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] # Calculate lengths for each sequence in the batch lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length (like TRL example) logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure (like TRL example checks) assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) setup_model () \u00b6 Setup models (policy, reference, reward, value) using standard Transformers. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def setup_model ( self ) -> None : \"\"\"Setup models (policy, reference, reward, value) using standard Transformers.\"\"\" # Extract model config values safely policy_model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) # Check if integrated reward training is configured if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training is not None ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) else : # Extract from config as normal reward_model_name = self . _get_config_value ( self . config . model , 'reward_model_name' , 'reward_path' , default = None ) value_model_name = self . _get_config_value ( self . config . model , 'reward_value_model' , default = None ) sft_model_path = self . _get_config_value ( self . config . model , 'sft_path' , default = None ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL PPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Quantization settings load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # Auto-detect quantization from model name if not load_in_4bit and not load_in_8bit : model_name_lower = policy_model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { policy_model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { policy_model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL PPO models\" ) logger . info ( f \"Policy model: { policy_model_name } \" ) logger . info ( f \"SFT path: { sft_model_path or 'Same as policy' } \" ) logger . info ( f \"Reward model: { reward_model_name or 'Same as value model' } \" ) logger . info ( f \"Value model: { value_model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( policy_model_name , trust_remote_code = trust_remote_code , ) if self . tokenizer . padding_side != \"left\" : self . tokenizer . padding_side = \"left\" # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Common model kwargs base_model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified for policy model policy_model_kwargs = base_model_kwargs . copy () if load_in_4bit : logger . info ( \"Loading policy model with 4-bit quantization...\" ) policy_model_kwargs [ \"load_in_4bit\" ] = True policy_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 policy_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" policy_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading policy model with 8-bit quantization...\" ) policy_model_kwargs [ \"load_in_8bit\" ] = True # Load policy model (from SFT path if provided, else base model) logger . info ( \"Loading policy model...\" ) policy_path = sft_model_path or policy_model_name self . policy_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** policy_model_kwargs ) # Apply PEFT if specified if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration to policy model...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing policy model for k-bit training...\" ) self . policy_model = prepare_model_for_kbit_training ( self . policy_model ) # Extract PEFT config values lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . policy_model = get_peft_model ( self . policy_model , peft_config ) self . policy_model . print_trainable_parameters () logger . info ( \"PEFT adapters applied to policy model successfully\" ) # Reference model is None when using PEFT (TRL handles this) self . ref_model = None logger . info ( \"Reference model: None (PEFT mode - TRL will handle reference)\" ) else : # Load reference model (frozen copy of SFT model) logger . info ( \"Loading reference model (frozen copy)...\" ) self . ref_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** base_model_kwargs ) self . ref_model . eval () for param in self . ref_model . parameters (): param . requires_grad = False logger . info ( \"Reference model loaded and frozen\" ) # Load reward and value models logger . info ( \"Loading reward and value models...\" ) # Reward model quantization settings reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , default = {}) reward_load_4bit = reward_quant . get ( 'load_in_4bit' , False ) reward_load_8bit = reward_quant . get ( 'load_in_8bit' , False ) reward_model_kwargs = base_model_kwargs . copy () if reward_load_4bit : logger . info ( \"Loading reward model with 4-bit quantization...\" ) reward_model_kwargs [ \"load_in_4bit\" ] = True reward_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 reward_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" reward_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif reward_load_8bit : logger . info ( \"Loading reward model with 8-bit quantization...\" ) reward_model_kwargs [ \"load_in_8bit\" ] = True # Value model quantization settings value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , default = {}) value_load_4bit = value_quant . get ( 'load_in_4bit' , False ) value_load_8bit = value_quant . get ( 'load_in_8bit' , False ) value_model_kwargs = base_model_kwargs . copy () if value_load_4bit : logger . info ( \"Loading value model with 4-bit quantization...\" ) value_model_kwargs [ \"load_in_4bit\" ] = True value_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 value_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" value_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif value_load_8bit : logger . info ( \"Loading value model with 8-bit quantization...\" ) value_model_kwargs [ \"load_in_8bit\" ] = True # Load reward model (or use value model if not specified) try : reward_path = reward_model_name or value_model_name self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_path , num_labels = 1 , ** reward_model_kwargs ) logger . info ( f \"Reward model loaded from: { reward_path } \" ) except : self . reward_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** reward_model_kwargs ) # Load value model try : self . value_model = AutoModelForSequenceClassification . from_pretrained ( value_model_name , num_labels = 1 , ** value_model_kwargs ) logger . info ( f \"Value model loaded from: { value_model_name } \" ) except : self . value_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** value_model_kwargs ) setup_rewards () \u00b6 Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for PPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) setup_trainer () \u00b6 Set up the PPO trainer with all configurations. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 def setup_trainer ( self ) -> None : \"\"\"Set up the PPO trainer with all configurations.\"\"\" logger . info ( \"Setting up TRL PPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.01 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # PPO-specific parameters kl_coef = self . _get_config_value ( self . config . train , 'kl_coef' , default = 0.1 ) cliprange = self . _get_config_value ( self . config . train , 'cliprange' , default = 0.2 ) cliprange_value = self . _get_config_value ( self . config . train , 'cliprange_value' , default = 0.2 ) vf_coef = self . _get_config_value ( self . config . train , 'vf_coef' , default = 0.1 ) gamma = self . _get_config_value ( self . config . train , 'gamma' , default = 1.0 ) lam = self . _get_config_value ( self . config . train , 'lam' , default = 0.95 ) # Generation parameters response_length = self . _get_config_value ( self . config . train , 'response_length' , 'max_completion_length' , default = 128 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) stop_token = self . _get_config_value ( self . config . train , 'stop_token' , default = 'eos' ) missing_eos_penalty = self . _get_config_value ( self . config . train , 'missing_eos_penalty' , default = 1.0 ) # Evaluation and checkpointing max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = 1000 ) eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) # Logging logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Max steps: { max_steps } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"KL coefficient: { kl_coef } \" ) logger . info ( f \"Clip range: { cliprange } \" ) logger . info ( f \"Value function coef: { vf_coef } \" ) logger . info ( f \"Response length: { response_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup PPO trainer from trl import PPOTrainer , PPOConfig ppo_config = PPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , # PPO-specific kl_coef = kl_coef , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , gamma = gamma , lam = lam , # Generation response_length = response_length , temperature = temperature , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , # Evaluation max_steps = max_steps , eval_strategy = eval_strategy if self . eval_dataset else \"no\" , eval_steps = eval_steps if self . eval_dataset else None , # Checkpointing save_strategy = save_strategy , save_steps = save_steps , # Logging logging_steps = logging_steps , report_to = report_to if report_to else [], # Precision ** precision_args , # Other remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # Get PEFT config if using PEFT peft_config = None use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) if use_peft : from peft import LoraConfig lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) # Create PPO trainer self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = peft_config , ) logger . info ( \"PPO trainer setup completed successfully!\" ) train () \u00b6 Execute PPO training. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL PPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) train_result = self . trainer . train () # Log samples after training try : raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results train_step ( batch ) \u00b6 Execute a single training step - handled by TRL internally. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1095 1096 1097 1098 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step - handled by TRL internally.\"\"\" logger . debug ( \"train_step() called but TRL PPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.rl.ppo.ppo import TRLPPOTrainer from aligntune.core.rl.config import UnifiedConfig config = UnifiedConfig ( algo = AlgorithmType . PPO , ... ) trainer = TRLPPOTrainer ( config ) trainer . train () TRLGRPOTrainer \u00b6 TRL backend for Group Relative Policy Optimization. Bases: TrainerBase GRPO trainer using pure TRL GRPOTrainer with math, code, and enhanced rewards. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 class TRLGRPOTrainer ( TrainerBase ): \"\"\"GRPO trainer using pure TRL GRPOTrainer with math, code, and enhanced rewards.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . dataset = None self . eval_dataset = None # Add eval dataset support self . reward_functions = [] self . training_history = [] self . logging_manager = None # self.evaluator = None self . custom_evaluator = None # For BaseEvaluator/RLEvaluator self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import GRPOTrainer , GRPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers.\"\"\" # Extract model config values safely model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL GRPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Auto-detect quantization from model name if not explicitly set if not load_in_4bit and not load_in_8bit : model_name_lower = model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization (required for training # quantized models) if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL GRPO model: { model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( model_name , trust_remote_code = trust_remote_code , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model logger . info ( \"Loading model...\" ) model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified if load_in_4bit : logger . info ( \"Loading model with 4-bit quantization...\" ) model_kwargs [ \"load_in_4bit\" ] = True model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading model with 8-bit quantization...\" ) model_kwargs [ \"load_in_8bit\" ] = True self . model = AutoModelForCausalLM . from_pretrained ( model_name , ** model_kwargs ) # Apply PEFT if specified or if quantization is used if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing model for k-bit training...\" ) self . model = prepare_model_for_kbit_training ( self . model ) # Extract PEFT config values safely lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () logger . info ( \"PEFT adapters applied successfully\" ) logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) def setup_data ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) # Fallback: try to continue with other rewards rather than # failing continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments from TRL including: - prompts: List of prompts - test_list: List of test cases (for code datasets like MBPP) - answer/solution/reference: Reference answers (for math datasets) \"\"\" if not completions : return [] # Debug: log what TRL is passing (once) if not hasattr ( self , '_logged_kwargs' ): logger . info ( f \"[DEBUG] _combined_reward_function kwargs keys: { list ( kwargs . keys ()) } \" ) if 'test_list' in kwargs : sample = kwargs [ 'test_list' ][: 1 ] if kwargs [ 'test_list' ] else 'empty' logger . info ( f \"[DEBUG] test_list found, sample: { sample } \" ) else : logger . info ( \"[DEBUG] test_list NOT in kwargs!\" ) self . _logged_kwargs = True batch_rewards = [] # Extract batch data from kwargs (TRL passes these as lists) test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # CHECK MULTIPLE POSSIBLE REFERENCE COLUMN NAMES # Try different column names in order of preference references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'ground_truth' , 'response' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] # print(f\"[INFO] Found reference data in column: '{ref_key}'\") break # If no reference column found, use None for all completions if references is None : references = [ None ] * len ( completions ) print ( f \"[WARNING] No reference column found. Checked: answer, solution, reference, ground_truth, target\" ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): # Try different calling patterns try : # Pattern 1: text + test_cases (for code execution) reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : # Pattern 2: Just text reward = reward_func ( completion ) except TypeError : try : # Pattern 3: text + reference reward = reward_func ( completion , reference = reference ) except TypeError : try : # Pattern 4: text + reference + context reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics with completions summary if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step.\"\"\" logger . debug ( \"train_step() called but TRL GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader for training.\"\"\" logger . debug ( \"create_data_loader() called but TRL GRPO uses TRL's internal data loading\" ) return None def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_trainer ( self ) -> None : \"\"\" Set up the GRPO trainer with all configurations. \"\"\" logger . info ( \"Setting up TRL GRPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 32 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # GRPO-specific parameters (beta = KL coefficient, epsilon = clip # range) beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = 0.1 ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = 0.2 ) # Generation parameters num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = 4 ) max_completion_length = self . _get_config_value ( self . config . train , 'max_completion_length' , 'max_new_tokens' , default = 256 ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.95 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) # Save parameters save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"Beta (KL coefficient): { beta } \" ) logger . info ( f \"Epsilon (clip range): { epsilon } \" ) logger . info ( f \"Num generations per prompt: { num_generations } \" ) logger . info ( f \"Max completion length: { max_completion_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup GRPO trainer from trl import GRPOTrainer , GRPOConfig grpo_config = GRPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , per_device_eval_batch_size = per_device_eval_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , report_to = report_to if report_to else [], # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Training parameters weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , remove_unused_columns = False , # Generation parameters num_generations = num_generations , max_completion_length = max_completion_length , max_prompt_length = max_prompt_length , temperature = temperature , top_p = top_p , max_steps = max_steps , # GRPO-specific beta = beta , # Precision ** precision_args , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Create GRPO trainer import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) else : try : import unsloth logger . info ( \"Detected Unsloth environment, using reward_funcs parameter\" ) def unsloth_reward_wrapper ( prompts = None , completions = None , ** kwargs ): if completions is None : return [ 0.0 ] * ( len ( prompts ) if prompts else 1 ) return self . _combined_reward_function ( completions , ** kwargs ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ unsloth_reward_wrapper ], ) except ImportError : logger . info ( \"Using pure TRL, using reward_function parameter\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) logger . info ( \"GRPO trainer setup completed successfully!\" ) def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory for saving output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL GRPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Log samples after training try : # For qualitative samples we only need callable reward functions, # not the full metadata dicts stored in self.reward_functions. raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : # Fallback: disable reward logging rather than failing # samples reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : 1 , \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = True, # **kwargs # ) -> Dict[str, float]: # \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # # Auto-setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # logger.info(\"Auto-initializing evaluators for first evaluation...\") # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's unified evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'dataset_name' : self . config . dataset . name , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if hasattr ( self , 'dataset' ) and self . train_dataset else 0 , 'val_size' : 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'dataset_name' : self . config . dataset . name , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL GRPO configuration saved to { path } \" ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise create_data_loader () \u00b6 Create data loader for training. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 576 577 578 579 580 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader for training.\"\"\" logger . debug ( \"create_data_loader() called but TRL GRPO uses TRL's internal data loading\" ) return None get_training_stats () \u00b6 Get training statistics. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'dataset_name' : self . config . dataset . name , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if hasattr ( self , 'dataset' ) and self . train_dataset else 0 , 'val_size' : 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats is_available () classmethod \u00b6 Check if TRL is available. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 56 57 58 59 60 61 62 63 64 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import GRPOTrainer , GRPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False load_model ( path ) \u00b6 Load a trained GRPO model. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise save_config ( path ) \u00b6 Save configuration to YAML file. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'dataset_name' : self . config . dataset . name , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL GRPO configuration saved to { path } \" ) save_model ( path = None ) \u00b6 Save the trained GRPO model. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise setup_data () \u00b6 Setup datasets for GRPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def setup_data ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) setup_model () \u00b6 Setup model using standard Transformers. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers.\"\"\" # Extract model config values safely model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL GRPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Auto-detect quantization from model name if not explicitly set if not load_in_4bit and not load_in_8bit : model_name_lower = model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization (required for training # quantized models) if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL GRPO model: { model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( model_name , trust_remote_code = trust_remote_code , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model logger . info ( \"Loading model...\" ) model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified if load_in_4bit : logger . info ( \"Loading model with 4-bit quantization...\" ) model_kwargs [ \"load_in_4bit\" ] = True model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading model with 8-bit quantization...\" ) model_kwargs [ \"load_in_8bit\" ] = True self . model = AutoModelForCausalLM . from_pretrained ( model_name , ** model_kwargs ) # Apply PEFT if specified or if quantization is used if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing model for k-bit training...\" ) self . model = prepare_model_for_kbit_training ( self . model ) # Extract PEFT config values safely lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () logger . info ( \"PEFT adapters applied successfully\" ) logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) setup_rewards () \u00b6 Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) # Fallback: try to continue with other rewards rather than # failing continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) setup_trainer () \u00b6 Set up the GRPO trainer with all configurations. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 def setup_trainer ( self ) -> None : \"\"\" Set up the GRPO trainer with all configurations. \"\"\" logger . info ( \"Setting up TRL GRPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 32 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # GRPO-specific parameters (beta = KL coefficient, epsilon = clip # range) beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = 0.1 ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = 0.2 ) # Generation parameters num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = 4 ) max_completion_length = self . _get_config_value ( self . config . train , 'max_completion_length' , 'max_new_tokens' , default = 256 ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.95 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) # Save parameters save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"Beta (KL coefficient): { beta } \" ) logger . info ( f \"Epsilon (clip range): { epsilon } \" ) logger . info ( f \"Num generations per prompt: { num_generations } \" ) logger . info ( f \"Max completion length: { max_completion_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup GRPO trainer from trl import GRPOTrainer , GRPOConfig grpo_config = GRPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , per_device_eval_batch_size = per_device_eval_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , report_to = report_to if report_to else [], # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Training parameters weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , remove_unused_columns = False , # Generation parameters num_generations = num_generations , max_completion_length = max_completion_length , max_prompt_length = max_prompt_length , temperature = temperature , top_p = top_p , max_steps = max_steps , # GRPO-specific beta = beta , # Precision ** precision_args , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Create GRPO trainer import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) else : try : import unsloth logger . info ( \"Detected Unsloth environment, using reward_funcs parameter\" ) def unsloth_reward_wrapper ( prompts = None , completions = None , ** kwargs ): if completions is None : return [ 0.0 ] * ( len ( prompts ) if prompts else 1 ) return self . _combined_reward_function ( completions , ** kwargs ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ unsloth_reward_wrapper ], ) except ImportError : logger . info ( \"Using pure TRL, using reward_function parameter\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) logger . info ( \"GRPO trainer setup completed successfully!\" ) train () \u00b6 Execute GRPO training. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory for saving output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL GRPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Log samples after training try : # For qualitative samples we only need callable reward functions, # not the full metadata dicts stored in self.reward_functions. raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : # Fallback: disable reward logging rather than failing # samples reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : 1 , \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results train_step ( batch ) \u00b6 Execute a single training step. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 570 571 572 573 574 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step.\"\"\" logger . debug ( \"train_step() called but TRL GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Unsloth Backends \u00b6 UnslothSFTTrainer \u00b6 Unsloth backend for Supervised Fine-Tuning (faster). Bases: SFTTrainerBase Enhanced SFT trainer using Unsloth with task type support. Source code in src/aligntune/backends/unsloth/sft/sft.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 class UnslothSFTTrainer ( SFTTrainerBase ): \"\"\"Enhanced SFT trainer using Unsloth with task type support.\"\"\" # Supported task types for Unsloth SUPPORTED_TASKS = [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def __init__ ( self , config ): super () . __init__ ( config ) self . config = config self . task_type = self . _get_task_type () self . training_config = None self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . training_history = [] self . logging_manager = None self . evaluator = None self . unsloth_model = None self . train_dataset = None self . eval_dataset = None self . eval_dataset = None # Already exists - no need to add self . custom_evaluator = None # ADD THIS LINE (for BaseEvaluator) # Validate task type self . _validate_task_type () logger . info ( f \"Initialized UnslothSFTTrainer for task: { self . task_type . value } \" ) def _get_task_type ( self ) -> TaskType : \"\"\"Extract task type from config.\"\"\" if hasattr ( self . config , 'dataset' ) and hasattr ( self . config . dataset , 'task_type' ): task_type = self . config . dataset . task_type elif hasattr ( self . config , 'train' ) and hasattr ( self . config . train , 'task_type' ): task_type = self . config . train . task_type else : # Default to supervised fine-tuning task_type = TaskType . SUPERVISED_FINE_TUNING # Convert string to enum if needed if isinstance ( task_type , str ): task_type = TaskType ( task_type . lower ()) return task_type def _validate_task_type ( self ): \"\"\"Validate that the task type is supported by Unsloth.\"\"\" if self . task_type not in self . SUPPORTED_TASKS : raise ValueError ( f \"Task type { self . task_type . value } is not supported by Unsloth backend. \" f \"Supported tasks: { [ t . value for t in self . SUPPORTED_TASKS ] } . \" f \"Use TRL backend for classification tasks.\" ) @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth is available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTTrainer , SFTConfig , ModelConfig return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model with task-aware configuration.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth model for task: { self . task_type . value } \" ) logger . info ( f \"Model: { self . config . model . name_or_path } \" ) # Configure Unsloth model parameters # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"Unsloth SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Configure Unsloth model parameters # Unsloth only accepts load_in_4bit as boolean, not BitsAndBytesConfig parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : dtype , # Use dtype from PrecisionHandler instead of None \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , False ) if self . config . model . quantization else False , } # Unsloth handles quantization internally - don't pass BitsAndBytesConfig parameters # Parameters like bnb_4bit_compute_dtype, bnb_4bit_quant_type, etc. are not supported # Load model with Unsloth optimizations try : self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = self . config . model . name_or_path , ** model_kwargs ) except ( SyntaxError , RuntimeError , Exception ) as e : # Handle Unsloth compiled module syntax errors (may be wrapped in RuntimeError) error_msg = str ( e ) if \"unsloth_compiled_module\" in error_msg or \"unexpected indent\" in error_msg . lower (): logger . error ( f \"Unsloth compilation error: { e } \" ) logger . info ( \"This is a known Unsloth issue with compiled modules.\" ) logger . info ( \"Suggested fixes:\" ) logger . info ( \" 1. Clear Unsloth cache: rm -rf ~/.cache/unsloth/\" ) logger . info ( \" 2. Retry training after clearing cache\" ) logger . info ( \" 3. Use TRL backend instead: backend='trl'\" ) raise RuntimeError ( f \"Unsloth compiled module syntax error: { e } \\n \" f \"This is a known Unsloth library issue. \" f \"Please clear the Unsloth cache and retry, or use TRL backend instead.\" ) from e else : # Re-raise if it's a different error raise # Detect model architecture for appropriate target modules model_type = self . unsloth_model . config . model_type . lower () target_modules = self . _get_target_modules_for_architecture ( model_type ) # Validate target modules exist in the model available_modules = [ name for name , _ in self . unsloth_model . named_modules ()] valid_target_modules = [ tm for tm in target_modules if any ( tm in name for name in available_modules )] if not valid_target_modules : # Try to auto-detect target modules logger . warning ( f \"None of the target modules { target_modules } found in model. Attempting auto-detection...\" ) valid_target_modules = self . _auto_detect_target_modules ( available_modules , model_type ) if not valid_target_modules : raise ValueError ( f \"Target modules { target_modules } not found in the base model (type: { model_type } ). \" f \"Available modules include: { available_modules [: 10 ] } ... \" f \"Please check the target modules and try again, or use a different model architecture.\" ) logger . info ( f \"Using target modules for { model_type } : { valid_target_modules } \" ) # Configure model for training with LoRA self . unsloth_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , # LoRA rank target_modules = valid_target_modules , lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Setup tokenizer based on task type self . _setup_tokenizer_for_task () logger . info ( \"Unsloth model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth model: { e } \" ) raise def _get_target_modules_for_architecture ( self , model_type : str ) -> List [ str ]: \"\"\"Get appropriate target modules based on model architecture.\"\"\" if \"qwen\" in model_type or \"qwen2\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"llama\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"mistral\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"phi\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"dense\" ] elif \"gemma\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"gpt2\" in model_type or \"gpt\" in model_type or \"dialogpt\" in model_type : # GPT-2 and DialoGPT use different module names return [ \"c_attn\" , \"c_proj\" , \"c_fc\" ] else : # Try to auto-detect by checking model structure # Default to common attention modules, but will be validated return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] def _auto_detect_target_modules ( self , available_modules : List [ str ], model_type : str ) -> List [ str ]: \"\"\"Auto-detect target modules from available modules.\"\"\" # Common patterns for attention modules patterns = [ # GPT-2 style ( \"c_attn\" , \"c_proj\" ), # LLaMA/Qwen style ( \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ), # BERT style ( \"query\" , \"key\" , \"value\" ), # Generic attention ( \"attn\" , \"attention\" ), ] detected = [] for pattern_group in patterns : matches = [ mod for mod in available_modules if any ( p in mod . lower () for p in pattern_group )] if matches : # Get unique module names (remove duplicates) unique_matches = list ( set ([ m . split ( '.' )[ - 1 ] for m in matches if '.' in m ])) if unique_matches : detected . extend ( unique_matches [: 4 ]) # Limit to 4 modules break return detected if detected else [] def _setup_tokenizer_for_task ( self ): \"\"\"Setup tokenizer with task-specific configurations.\"\"\" # Ensure pad token is set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token # If a chat template is explicitly provided in config, try to apply it # so `tokenizer.apply_chat_template(...)` works across SFT task types. try : from unsloth import FastLanguageModel cfg_template = getattr ( self . config . dataset , \"chat_template\" , None ) has_template = hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None def _normalize_template_name ( name : Optional [ str ]) -> Optional [ str ]: if not name : return None v = str ( name ) . strip () . lower () if v in { \"auto\" , \"none\" }: return None # common aliases used in configs if v in { \"llama3\" , \"llama_3\" , \"llama-3\" }: return \"llama-3\" if v in { \"llama2\" , \"llama_2\" , \"llama-2\" }: return \"llama-2\" return name normalized = _normalize_template_name ( cfg_template ) if normalized and not has_template : mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = normalized , mapping = mapping , ) logger . info ( f \"Applied configured chat template for SFT: { normalized } \" ) except Exception as e : logger . debug ( f \"Skipping configured chat template application: { e } \" ) # Add chat template for chat completion tasks if self . task_type == TaskType . CHAT_COMPLETION : logger . info ( \"Applying Unsloth chat template...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = None if hasattr ( self . config . dataset , 'chat_template' ): chat_template = self . config . dataset . chat_template # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 if not specified mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to manual template.\" ) if not hasattr ( self . tokenizer , 'chat_template' ) or self . tokenizer . chat_template is None : self . tokenizer . chat_template = ( \"{ % f or message in messages %}\" \"{{ message['role'] }}: {{ message['content'] }} \\n \" \"{ % e ndfor %}Assistant:\" ) logger . info ( \"Added manual chat template to tokenizer\" ) # Add special tokens for instruction following if needed elif self . task_type == TaskType . INSTRUCTION_FOLLOWING : has_chat_template = ( hasattr ( self . tokenizer , \"apply_chat_template\" ) and hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None ) if not has_chat_template : special_tokens = [ \"<|im_start|>\" , \"<|im_end|>\" ] num_added = self . tokenizer . add_special_tokens ({ 'additional_special_tokens' : special_tokens }) if num_added > 0 : self . unsloth_model . resize_token_embeddings ( len ( self . tokenizer )) logger . info ( f \"Added { num_added } special tokens for instruction following\" ) def _apply_chat_template_safe ( self , messages : List [ Dict [ str , str ]], add_generation_prompt : bool = False ) -> Optional [ str ]: \"\"\"Apply tokenizer chat template with broad compatibility.\"\"\" if not self . tokenizer or not hasattr ( self . tokenizer , \"apply_chat_template\" ): return None try : return self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = add_generation_prompt , ) except TypeError : try : return self . tokenizer . apply_chat_template ( messages , tokenize = False ) except Exception : return None except Exception : return None def _format_with_chat_template ( self , examples : Dict [ str , Any ], ds_cfg ) -> Dict [ str , List [ str ]]: \"\"\" Build a `text` field using the model's chat template when available. Falls back to legacy string formatting if templating fails. \"\"\" system_prompt = getattr ( ds_cfg , \"system_prompt\" , None ) texts : List [ str ] = [] def _prepend_system_if_missing ( msgs : List [ Dict [ str , str ]]) -> List [ Dict [ str , str ]]: if system_prompt and ( not msgs or msgs [ 0 ] . get ( \"role\" ) != \"system\" ): return [{ \"role\" : \"system\" , \"content\" : system_prompt }] + msgs return msgs if self . task_type == TaskType . CHAT_COMPLETION : messages_field = ds_cfg . messages_column if hasattr ( ds_cfg , \"messages_column\" ) else \"messages\" for messages in examples . get ( messages_field , []): if isinstance ( messages , list ): msgs = _prepend_system_if_missing ( messages ) templated = self . _apply_chat_template_safe ( msgs , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) continue conversation = [ f \" { m . get ( 'role' , 'user' ) } : { m . get ( 'content' , '' ) } \" for m in msgs ] texts . append ( \" \\n \" . join ( conversation )) else : texts . append ( str ( messages ) if messages else \"\" ) return { \"text\" : texts } if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING ]: instr_col = getattr ( ds_cfg , \"instruction_column\" , \"instruction\" ) resp_col = getattr ( ds_cfg , \"response_column\" , \"response\" ) ctx_col = getattr ( ds_cfg , \"context_column\" , \"context\" ) input_col = getattr ( ds_cfg , \"input_column\" , \"input\" ) instructions = examples . get ( instr_col , examples . get ( \"instruction\" , [])) responses = examples . get ( resp_col , examples . get ( \"response\" , examples . get ( \"output\" , []))) n = min ( len ( instructions ), len ( responses )) for i in range ( n ): instruction = instructions [ i ] response = responses [ i ] context = \"\" if ctx_col in examples and i < len ( examples . get ( ctx_col , [])): context = examples . get ( ctx_col , [ \"\" ])[ i ] or \"\" input_text = \"\" if input_col in examples and i < len ( examples . get ( input_col , [])): input_text = examples . get ( input_col , [ \"\" ])[ i ] or \"\" sys_parts : List [ str ] = [] if system_prompt : sys_parts . append ( str ( system_prompt ) . strip ()) if context and str ( context ) . strip (): sys_parts . append ( f \"Context: { str ( context ) . strip () } \" ) messages : List [ Dict [ str , str ]] = [] if sys_parts : messages . append ({ \"role\" : \"system\" , \"content\" : \" \\n\\n \" . join ( sys_parts )}) user_content = str ( instruction ) if instruction is not None else \"\" if input_text and str ( input_text ) . strip (): user_content = f \" { user_content } \\n\\n { str ( input_text ) . strip () } \" messages . append ({ \"role\" : \"user\" , \"content\" : user_content }) messages . append ({ \"role\" : \"assistant\" , \"content\" : str ( response ) if response is not None else \"\" }) templated = self . _apply_chat_template_safe ( messages , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) else : # Fallback to legacy formatting (inline) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : if context and str ( context ) . strip (): texts . append ( f \"<|im_start|>system \\n Context: { str ( context ) . strip () } <|im_end|> \\n \" f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"### Instruction: \\n { user_content } \\n\\n ### Response: \\n { str ( response ) if response is not None else '' } \" ) if not texts : texts = [ \"[NO_VALID_TEXT]\" ] return { \"text\" : texts } return { \"text\" : examples . get ( \"text\" , [ \"[NO_VALID_TEXT]\" ])} def setup_data ( self ) -> None : \"\"\"Setup and prepare dataset for training (required by abstract base class).\"\"\" self . setup_dataset () def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step (required by abstract base class).\"\"\" if self . trainer is None : raise RuntimeError ( \"Trainer not initialized. Call setup_model() first.\" ) # The actual training step is handled by TRL's SFTTrainer return { \"loss\" : 0.0 , \"learning_rate\" : 0.0 } def setup_dataset ( self ) -> None : \"\"\"Setup and prepare dataset with task-aware formatting.\"\"\" try : from datasets import load_dataset ds_cfg = self . config . dataset logger . info ( f \"Loading dataset: { ds_cfg . name } for task: { self . task_type . value } \" ) # Prepare load_dataset arguments load_kwargs = {} # Add split parameter if hasattr ( ds_cfg , 'split' ) and ds_cfg . split : load_kwargs [ 'split' ] = ds_cfg . split else : load_kwargs [ 'split' ] = \"train\" # Handle dataset config/subset (for datasets like wikitext that require it) dataset_name = ds_cfg . name dataset_config = None if hasattr ( ds_cfg , 'subset' ) and ds_cfg . subset : dataset_config = ds_cfg . subset logger . info ( f \"Using dataset config/subset: { dataset_config } \" ) elif hasattr ( ds_cfg , 'config' ) and ds_cfg . config : dataset_config = ds_cfg . config logger . info ( f \"Using dataset config: { dataset_config } \" ) # Load dataset with or without config # Special handling for SQuAD (both v1.1 and v2.0) which use deprecated 'List' feature type if \"squad\" in dataset_name . lower (): logger . info ( f \"Detected SQuAD dataset. Using workaround for deprecated 'List' feature type...\" ) try : # Method 1: Try loading with builder to bypass feature validation from datasets import load_dataset_builder builder = load_dataset_builder ( dataset_name , dataset_config if dataset_config else None ) # Download and prepare builder . download_and_prepare () # Load from cache, bypassing feature type validation split_name = load_kwargs . get ( 'split' , 'train' ) dataset = builder . as_dataset ( split = split_name ) logger . info ( f \"Successfully loaded { dataset_name } using builder workaround\" ) except Exception as e_builder : logger . warning ( f \"Builder method failed: { e_builder } . Trying alternative approach...\" ) try : # Method 2: Try loading with ignore_verifications (if supported) load_kwargs_alt = load_kwargs . copy () load_kwargs_alt [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_alt ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_alt ) logger . info ( f \"Successfully loaded { dataset_name } with force_redownload\" ) except Exception as e2 : logger . warning ( f \"Force redownload failed: { e2 } . Trying feature patching...\" ) try : # Method 3: Patch Features.from_dict to handle List -> Sequence conversion import datasets import json original_from_dict = datasets . Features . from_dict def patched_from_dict ( features_dict ): \"\"\"Patch to convert 'List' to 'Sequence' in feature dict.\"\"\" # Convert to string, replace, convert back features_str = json . dumps ( features_dict ) features_str = features_str . replace ( '\"List\"' , '\"Sequence\"' ) features_str = features_str . replace ( \"'List'\" , \"'Sequence'\" ) features_dict = json . loads ( features_str ) return original_from_dict ( features_dict ) # Temporarily patch datasets . Features . from_dict = patched_from_dict try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) logger . info ( f \"Successfully loaded { dataset_name } with feature patching\" ) finally : # Restore original datasets . Features . from_dict = original_from_dict except Exception as e3 : logger . error ( f \"All SQuAD loading methods failed. Last error: { e3 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type that cannot be automatically fixed. \" f \"Please try: pip install --upgrade datasets>=2.14.0, \" f \"or consider using an alternative Q&A dataset like 'allenai/sciq' or 'deepmind/code_contests'. \" f \"Error: { str ( e3 )[: 200 ] } \" ) from e3 else : try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ( ValueError , TypeError ) as e : error_msg = str ( e ) # Handle deprecated 'List' feature type for other datasets if \"Feature type 'List' not found\" in error_msg or \"'List'\" in error_msg : logger . warning ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Trying to load with download_mode='force_redownload'...\" ) try : load_kwargs_retry = load_kwargs . copy () load_kwargs_retry [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_retry ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_retry ) logger . info ( f \"Successfully loaded dataset with updated features\" ) except Exception as e2 : logger . error ( f \"Failed to load dataset: { e2 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Please try: pip install --upgrade datasets>=2.14.0. \" f \"Original error: { error_msg [: 200 ] } \" ) from e elif \"Config name is missing\" in error_msg or \"config\" in error_msg . lower (): logger . error ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' parameter to your DatasetConfig. \" f \"Error: { error_msg } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Please add 'subset' or 'config' to your DatasetConfig. \" f \"Original error: { error_msg } \" ) else : raise logger . info ( f \"Dataset loaded: { len ( dataset ) } samples\" ) # Apply subset selection if hasattr ( ds_cfg , 'max_samples' ) and ds_cfg . max_samples : original_len = len ( dataset ) dataset = dataset . select ( range ( min ( ds_cfg . max_samples , len ( dataset )))) logger . info ( f \"Applied max_samples limit: { original_len } -> { len ( dataset ) } samples\" ) elif hasattr ( ds_cfg , 'percent' ) and ds_cfg . percent : original_len = len ( dataset ) max_samples = int ( len ( dataset ) * ds_cfg . percent / 100 ) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Applied percent limit ( { ds_cfg . percent } %): { original_len } -> { len ( dataset ) } samples\" ) # Auto-detect schema if enabled if hasattr ( ds_cfg , 'auto_detect_fields' ) and ds_cfg . auto_detect_fields : logger . info ( \"Auto-detecting dataset schema...\" ) detected_format = schema_detector . detect_format ( dataset ) if detected_format : logger . info ( f \"Auto-detected dataset format: { detected_format } \" ) if hasattr ( ds_cfg , 'format_type' ): ds_cfg . format_type = detected_format # Apply field mappings field_mappings = None if hasattr ( ds_cfg , 'field_mappings' ) and ds_cfg . field_mappings : field_mappings = ds_cfg . field_mappings logger . info ( f \"Applying field mappings: { field_mappings } \" ) elif hasattr ( ds_cfg , 'column_mapping' ) and ds_cfg . column_mapping : field_mappings = ds_cfg . column_mapping logger . info ( f \"Applying column mappings: { field_mappings } \" ) if field_mappings : # Only rename columns that exist in the dataset valid_mappings = { k : v for k , v in field_mappings . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) else : logger . warning ( f \"No valid column mappings found. Available columns: { dataset . column_names } \" ) # Log dataset structure logger . info ( f \"Dataset columns: { dataset . column_names } \" ) if len ( dataset ) > 0 : logger . info ( f \"Sample data (first item): { list ( dataset [ 0 ] . keys ()) } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , ds_cfg ) # Split train/test # Always create eval split if dataset has at least 2 samples (minimum for split) # For very small datasets, use a smaller test_size to ensure eval set has at least 1 sample if len ( dataset ) >= 2 : if len ( dataset ) >= 10 : test_size = 0.1 # 10% for larger datasets elif len ( dataset ) >= 5 : test_size = 0.2 # 20% for medium datasets (ensures at least 1 eval sample) else : test_size = 0.2 # 20% for very small datasets (2-4 samples -> at least 1 eval) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 ) self . train_dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Split dataset: { len ( self . train_dataset ) } train, { len ( self . eval_dataset ) } eval\" ) else : # Only skip split if dataset has less than 2 samples self . train_dataset = dataset self . eval_dataset = None logger . warning ( f \"Dataset too small for split (only { len ( dataset ) } samples), using all for training. No eval dataset created.\" ) logger . info ( f \"Dataset prepared: { len ( self . train_dataset ) } training samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def _format_dataset_for_task ( self , dataset , ds_cfg ): \"\"\"Format dataset based on task type.\"\"\" logger . info ( f \"Formatting dataset for task: { self . task_type . value } \" ) # Select appropriate formatter can_template = self . tokenizer is not None and hasattr ( self . tokenizer , \"apply_chat_template\" ) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_instruction_following elif self . task_type == TaskType . SUPERVISED_FINE_TUNING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_supervised_fine_tuning elif self . task_type == TaskType . TEXT_GENERATION : formatter = TaskFormatter . format_text_generation elif self . task_type == TaskType . CHAT_COMPLETION : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_chat_completion else : raise ValueError ( f \"Unsupported task type: { self . task_type } \" ) # Apply formatting try : formatted_dataset = dataset . map ( lambda examples : formatter ( examples , ds_cfg ), batched = True , remove_columns = dataset . column_names , desc = f \"Formatting for { self . task_type . value } \" ) # Verify formatting sample = formatted_dataset [ 0 ] logger . info ( f \"Sample formatted text (first 200 chars): { sample [ 'text' ][: 200 ] } \" ) return formatted_dataset except Exception as e : logger . error ( f \"Error formatting dataset: { e } \" ) # Fallback to simple formatting logger . warning ( \"Using fallback formatting\" ) return self . _fallback_format ( dataset , ds_cfg ) def _fallback_format ( self , dataset , ds_cfg ): \"\"\"Fallback formatting when task-specific formatting fails.\"\"\" def simple_format ( examples ): texts = [] # Try common column names for i in range ( len ( list ( examples . values ())[ 0 ])): text = \"\" if \"instruction\" in examples and \"response\" in examples : text = f \" { examples [ 'instruction' ][ i ] } \\n { examples [ 'response' ][ i ] } \" elif \"text\" in examples : text = examples [ \"text\" ][ i ] else : # Use first text-like column first_col = list ( examples . keys ())[ 0 ] text = str ( examples [ first_col ][ i ]) texts . append ( text ) return { \"text\" : texts } return dataset . map ( simple_format , batched = True , remove_columns = dataset . column_names ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL SFTTrainer with task-aware configuration.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig logger . info ( f \"Setting up TRL SFTTrainer for task: { self . task_type . value } \" ) # Get training parameters num_epochs = getattr ( self . config , 'num_epochs' , None ) or \\ getattr ( self . config . train , 'epochs' , 3 ) if hasattr ( self . config , 'train' ) else 3 batch_size = getattr ( self . config , 'batch_size' , None ) or \\ getattr ( self . config . train , 'per_device_batch_size' , 2 ) if hasattr ( self . config , 'train' ) else 2 grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) if hasattr ( self . config , 'train' ) else 1 lr = getattr ( self . config . train , 'learning_rate' , 2e-4 ) if hasattr ( self . config , 'train' ) else 2e-4 save_steps = getattr ( self . config . train , 'save_interval' , 500 ) if hasattr ( self . config , 'train' ) else 500 output_dir = getattr ( self . config . logging , 'output_dir' , './output' ) if hasattr ( self . config , 'logging' ) else './output' # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Create SFT configuration # Create SFT configuration self . training_config = SFTConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , gradient_accumulation_steps = grad_accum , learning_rate = lr , logging_steps = 10 , save_steps = save_steps , warmup_ratio = 0.1 , lr_scheduler_type = \"cosine\" , ** precision_args , dataloader_pin_memory = False , report_to = \"none\" if not hasattr ( self . config , 'logging' ) else \"tensorboard\" ) # Get max_seq_length max_seq_len = getattr ( self . config . model , 'max_seq_length' , 2048 ) if hasattr ( self . config , 'model' ) else 2048 # Task-specific trainer settings packing = False # Generally disable packing for better quality if self . task_type == TaskType . TEXT_GENERATION : packing = True # Can enable packing for simple text generation # Create trainer self . trainer = SFTTrainer ( model = self . unsloth_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = self . training_config , dataset_text_field = \"text\" , max_seq_length = max_seq_len , dataset_num_proc = 2 , packing = packing , ) logger . info ( f \"TRL SFTTrainer setup completed for { self . task_type . value } \" ) logger . info ( f \"Training config: { num_epochs } epochs, batch_size= { batch_size } , lr= { lr } \" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute training with Unsloth optimizations.\"\"\" try : logger . info ( f \"Starting Unsloth SFT training for task: { self . task_type . value } \" ) start_time = time . time () # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Start training training_result = self . trainer . train () # Save model output_dir = self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"task_type\" : self . task_type . value , \"training_time\" : training_time , \"final_loss\" : training_result . training_loss , \"total_steps\" : training_result . global_step , \"model_path\" : output_dir , \"training_history\" : self . training_history , } logger . info ( f \"Unsloth SFT training completed in { training_time : .2f } seconds\" ) logger . info ( f \"Task: { self . task_type . value } , Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise def evaluate ( self , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the trained model with task-specific metrics.\"\"\" try : if not self . eval_dataset : logger . warning ( \"No evaluation dataset available\" ) return {} if eval_dataset : self . eval_dataset = eval_dataset logger . info ( f \"Evaluating Unsloth SFT model for task: { self . task_type . value } \" ) # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation # It doesn't affect functionality, just makes gradient accumulation slightly less accurate with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Run basic evaluation from TRL trainer eval_results = self . trainer . evaluate () # Add task-specific metrics eval_results [ \"task_type\" ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator try : from aligntune.core.sft.evaluator import SFTEvaluator if self . unsloth_model and self . tokenizer and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . unsloth_model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) logger . info ( f \"Evaluation results: { len ( eval_results ) } metrics computed\" ) return eval_results except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate task-specific sample outputs.\"\"\" try : logger . info ( f \"Generating { num_samples } samples for task: { self . task_type . value } \" ) # Get task-specific prompts sample_prompts = self . _get_sample_prompts_for_task ( num_samples ) samples = [] for i , prompt in enumerate ( sample_prompts ): try : # Tokenize input inputs = self . tokenizer ( prompt , return_tensors = \"pt\" , truncation = True , max_length = 512 ) # Move to device if torch . cuda . is_available (): inputs = { k : v . cuda () for k , v in inputs . items ()} # Generate response with torch . no_grad (): outputs = self . unsloth_model . generate ( ** inputs , max_new_tokens = 100 , temperature = 0.7 , do_sample = True , pad_token_id = self . tokenizer . eos_token_id ) # Decode response response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : response , \"sample_id\" : i + 1 }) except Exception as e : logger . warning ( f \"Failed to generate sample { i + 1 } : { e } \" ) samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : f \"Generation failed: { e } \" , \"sample_id\" : i + 1 }) logger . info ( f \"Generated { len ( samples ) } samples\" ) return samples except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] def _get_sample_prompts_for_task ( self , num_samples : int ) -> List [ str ]: \"\"\"Get task-specific sample prompts for generation.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : prompts = [ \"<|im_start|>user \\n Explain machine learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Write a Python function.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Describe deep learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Explain quantum computing. \\n Assistant:\" , \"Human: Tell me a joke. \\n Assistant:\" , \"Human: What's the weather? \\n Assistant:\" ] elif self . task_type == TaskType . TEXT_GENERATION : prompts = [ \"The future of AI is\" , \"In a world where\" , \"The key to success is\" , \"Technology has changed\" , \"The most important skill is\" ] else : # SUPERVISED_FINE_TUNING prompts = [ \"### Instruction: \\n Explain machine learning. \\n\\n ### Response: \\n \" , \"### Instruction: \\n Write code. \\n\\n ### Response: \\n \" , \"### Instruction: \\n What is AI? \\n\\n ### Response: \\n \" , \"### Instruction: \\n Describe NLP. \\n\\n ### Response: \\n \" , \"### Instruction: \\n How to train models? \\n\\n ### Response: \\n \" ] return prompts [: num_samples ] def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained model with task metadata.\"\"\" try : save_path = path or ( self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' ) logger . info ( f \"Saving Unsloth model to: { save_path } \" ) # Save using Unsloth's optimized saving self . unsloth_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration with task type config_path = Path ( save_path ) / \"training_config.yaml\" config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else {} config_dict [ 'task_type' ] = self . task_type . value with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"Model saved successfully to: { save_path } \" ) logger . info ( f \"Task type: { self . task_type . value } \" ) # Store for push_to_hub self . _last_save_path = save_path return save_path except Exception as e : logger . error ( f \"Failed to save model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model with task type detection.\"\"\" try : logger . info ( f \"Loading Unsloth model from: { path } \" ) # Try to load task type from config config_path = Path ( path ) / \"training_config.yaml\" if config_path . exists (): with open ( config_path , 'r' ) as f : saved_config = yaml . safe_load ( f ) if 'task_type' in saved_config : self . task_type = TaskType ( saved_config [ 'task_type' ]) logger . info ( f \"Loaded task type: { self . task_type . value } \" ) import unsloth from unsloth import FastLanguageModel # Load model and tokenizer self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = self . config . model . max_seq_length if hasattr ( self . config , 'model' ) else 2048 , dtype = None , load_in_4bit = True , ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise evaluate ( eval_dataset = None ) \u00b6 Evaluate the trained model with task-specific metrics. Source code in src/aligntune/backends/unsloth/sft/sft.py 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 def evaluate ( self , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the trained model with task-specific metrics.\"\"\" try : if not self . eval_dataset : logger . warning ( \"No evaluation dataset available\" ) return {} if eval_dataset : self . eval_dataset = eval_dataset logger . info ( f \"Evaluating Unsloth SFT model for task: { self . task_type . value } \" ) # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation # It doesn't affect functionality, just makes gradient accumulation slightly less accurate with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Run basic evaluation from TRL trainer eval_results = self . trainer . evaluate () # Add task-specific metrics eval_results [ \"task_type\" ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator try : from aligntune.core.sft.evaluator import SFTEvaluator if self . unsloth_model and self . tokenizer and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . unsloth_model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) logger . info ( f \"Evaluation results: { len ( eval_results ) } metrics computed\" ) return eval_results except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise generate_samples ( num_samples = 5 ) \u00b6 Generate task-specific sample outputs. Source code in src/aligntune/backends/unsloth/sft/sft.py 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate task-specific sample outputs.\"\"\" try : logger . info ( f \"Generating { num_samples } samples for task: { self . task_type . value } \" ) # Get task-specific prompts sample_prompts = self . _get_sample_prompts_for_task ( num_samples ) samples = [] for i , prompt in enumerate ( sample_prompts ): try : # Tokenize input inputs = self . tokenizer ( prompt , return_tensors = \"pt\" , truncation = True , max_length = 512 ) # Move to device if torch . cuda . is_available (): inputs = { k : v . cuda () for k , v in inputs . items ()} # Generate response with torch . no_grad (): outputs = self . unsloth_model . generate ( ** inputs , max_new_tokens = 100 , temperature = 0.7 , do_sample = True , pad_token_id = self . tokenizer . eos_token_id ) # Decode response response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : response , \"sample_id\" : i + 1 }) except Exception as e : logger . warning ( f \"Failed to generate sample { i + 1 } : { e } \" ) samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : f \"Generation failed: { e } \" , \"sample_id\" : i + 1 }) logger . info ( f \"Generated { len ( samples ) } samples\" ) return samples except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] is_available () classmethod \u00b6 Check if Unsloth is available. Source code in src/aligntune/backends/unsloth/sft/sft.py 241 242 243 244 245 246 247 248 249 250 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth is available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTTrainer , SFTConfig , ModelConfig return True except ImportError : return False load_model ( path ) \u00b6 Load a trained model with task type detection. Source code in src/aligntune/backends/unsloth/sft/sft.py 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model with task type detection.\"\"\" try : logger . info ( f \"Loading Unsloth model from: { path } \" ) # Try to load task type from config config_path = Path ( path ) / \"training_config.yaml\" if config_path . exists (): with open ( config_path , 'r' ) as f : saved_config = yaml . safe_load ( f ) if 'task_type' in saved_config : self . task_type = TaskType ( saved_config [ 'task_type' ]) logger . info ( f \"Loaded task type: { self . task_type . value } \" ) import unsloth from unsloth import FastLanguageModel # Load model and tokenizer self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = self . config . model . max_seq_length if hasattr ( self . config , 'model' ) else 2048 , dtype = None , load_in_4bit = True , ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise save_model ( path = None ) \u00b6 Save the trained model with task metadata. Source code in src/aligntune/backends/unsloth/sft/sft.py 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained model with task metadata.\"\"\" try : save_path = path or ( self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' ) logger . info ( f \"Saving Unsloth model to: { save_path } \" ) # Save using Unsloth's optimized saving self . unsloth_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration with task type config_path = Path ( save_path ) / \"training_config.yaml\" config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else {} config_dict [ 'task_type' ] = self . task_type . value with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"Model saved successfully to: { save_path } \" ) logger . info ( f \"Task type: { self . task_type . value } \" ) # Store for push_to_hub self . _last_save_path = save_path return save_path except Exception as e : logger . error ( f \"Failed to save model: { e } \" ) raise setup_data () \u00b6 Setup and prepare dataset for training (required by abstract base class). Source code in src/aligntune/backends/unsloth/sft/sft.py 594 595 596 def setup_data ( self ) -> None : \"\"\"Setup and prepare dataset for training (required by abstract base class).\"\"\" self . setup_dataset () setup_dataset () \u00b6 Setup and prepare dataset with task-aware formatting. Source code in src/aligntune/backends/unsloth/sft/sft.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare dataset with task-aware formatting.\"\"\" try : from datasets import load_dataset ds_cfg = self . config . dataset logger . info ( f \"Loading dataset: { ds_cfg . name } for task: { self . task_type . value } \" ) # Prepare load_dataset arguments load_kwargs = {} # Add split parameter if hasattr ( ds_cfg , 'split' ) and ds_cfg . split : load_kwargs [ 'split' ] = ds_cfg . split else : load_kwargs [ 'split' ] = \"train\" # Handle dataset config/subset (for datasets like wikitext that require it) dataset_name = ds_cfg . name dataset_config = None if hasattr ( ds_cfg , 'subset' ) and ds_cfg . subset : dataset_config = ds_cfg . subset logger . info ( f \"Using dataset config/subset: { dataset_config } \" ) elif hasattr ( ds_cfg , 'config' ) and ds_cfg . config : dataset_config = ds_cfg . config logger . info ( f \"Using dataset config: { dataset_config } \" ) # Load dataset with or without config # Special handling for SQuAD (both v1.1 and v2.0) which use deprecated 'List' feature type if \"squad\" in dataset_name . lower (): logger . info ( f \"Detected SQuAD dataset. Using workaround for deprecated 'List' feature type...\" ) try : # Method 1: Try loading with builder to bypass feature validation from datasets import load_dataset_builder builder = load_dataset_builder ( dataset_name , dataset_config if dataset_config else None ) # Download and prepare builder . download_and_prepare () # Load from cache, bypassing feature type validation split_name = load_kwargs . get ( 'split' , 'train' ) dataset = builder . as_dataset ( split = split_name ) logger . info ( f \"Successfully loaded { dataset_name } using builder workaround\" ) except Exception as e_builder : logger . warning ( f \"Builder method failed: { e_builder } . Trying alternative approach...\" ) try : # Method 2: Try loading with ignore_verifications (if supported) load_kwargs_alt = load_kwargs . copy () load_kwargs_alt [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_alt ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_alt ) logger . info ( f \"Successfully loaded { dataset_name } with force_redownload\" ) except Exception as e2 : logger . warning ( f \"Force redownload failed: { e2 } . Trying feature patching...\" ) try : # Method 3: Patch Features.from_dict to handle List -> Sequence conversion import datasets import json original_from_dict = datasets . Features . from_dict def patched_from_dict ( features_dict ): \"\"\"Patch to convert 'List' to 'Sequence' in feature dict.\"\"\" # Convert to string, replace, convert back features_str = json . dumps ( features_dict ) features_str = features_str . replace ( '\"List\"' , '\"Sequence\"' ) features_str = features_str . replace ( \"'List'\" , \"'Sequence'\" ) features_dict = json . loads ( features_str ) return original_from_dict ( features_dict ) # Temporarily patch datasets . Features . from_dict = patched_from_dict try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) logger . info ( f \"Successfully loaded { dataset_name } with feature patching\" ) finally : # Restore original datasets . Features . from_dict = original_from_dict except Exception as e3 : logger . error ( f \"All SQuAD loading methods failed. Last error: { e3 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type that cannot be automatically fixed. \" f \"Please try: pip install --upgrade datasets>=2.14.0, \" f \"or consider using an alternative Q&A dataset like 'allenai/sciq' or 'deepmind/code_contests'. \" f \"Error: { str ( e3 )[: 200 ] } \" ) from e3 else : try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ( ValueError , TypeError ) as e : error_msg = str ( e ) # Handle deprecated 'List' feature type for other datasets if \"Feature type 'List' not found\" in error_msg or \"'List'\" in error_msg : logger . warning ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Trying to load with download_mode='force_redownload'...\" ) try : load_kwargs_retry = load_kwargs . copy () load_kwargs_retry [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_retry ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_retry ) logger . info ( f \"Successfully loaded dataset with updated features\" ) except Exception as e2 : logger . error ( f \"Failed to load dataset: { e2 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Please try: pip install --upgrade datasets>=2.14.0. \" f \"Original error: { error_msg [: 200 ] } \" ) from e elif \"Config name is missing\" in error_msg or \"config\" in error_msg . lower (): logger . error ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' parameter to your DatasetConfig. \" f \"Error: { error_msg } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Please add 'subset' or 'config' to your DatasetConfig. \" f \"Original error: { error_msg } \" ) else : raise logger . info ( f \"Dataset loaded: { len ( dataset ) } samples\" ) # Apply subset selection if hasattr ( ds_cfg , 'max_samples' ) and ds_cfg . max_samples : original_len = len ( dataset ) dataset = dataset . select ( range ( min ( ds_cfg . max_samples , len ( dataset )))) logger . info ( f \"Applied max_samples limit: { original_len } -> { len ( dataset ) } samples\" ) elif hasattr ( ds_cfg , 'percent' ) and ds_cfg . percent : original_len = len ( dataset ) max_samples = int ( len ( dataset ) * ds_cfg . percent / 100 ) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Applied percent limit ( { ds_cfg . percent } %): { original_len } -> { len ( dataset ) } samples\" ) # Auto-detect schema if enabled if hasattr ( ds_cfg , 'auto_detect_fields' ) and ds_cfg . auto_detect_fields : logger . info ( \"Auto-detecting dataset schema...\" ) detected_format = schema_detector . detect_format ( dataset ) if detected_format : logger . info ( f \"Auto-detected dataset format: { detected_format } \" ) if hasattr ( ds_cfg , 'format_type' ): ds_cfg . format_type = detected_format # Apply field mappings field_mappings = None if hasattr ( ds_cfg , 'field_mappings' ) and ds_cfg . field_mappings : field_mappings = ds_cfg . field_mappings logger . info ( f \"Applying field mappings: { field_mappings } \" ) elif hasattr ( ds_cfg , 'column_mapping' ) and ds_cfg . column_mapping : field_mappings = ds_cfg . column_mapping logger . info ( f \"Applying column mappings: { field_mappings } \" ) if field_mappings : # Only rename columns that exist in the dataset valid_mappings = { k : v for k , v in field_mappings . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) else : logger . warning ( f \"No valid column mappings found. Available columns: { dataset . column_names } \" ) # Log dataset structure logger . info ( f \"Dataset columns: { dataset . column_names } \" ) if len ( dataset ) > 0 : logger . info ( f \"Sample data (first item): { list ( dataset [ 0 ] . keys ()) } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , ds_cfg ) # Split train/test # Always create eval split if dataset has at least 2 samples (minimum for split) # For very small datasets, use a smaller test_size to ensure eval set has at least 1 sample if len ( dataset ) >= 2 : if len ( dataset ) >= 10 : test_size = 0.1 # 10% for larger datasets elif len ( dataset ) >= 5 : test_size = 0.2 # 20% for medium datasets (ensures at least 1 eval sample) else : test_size = 0.2 # 20% for very small datasets (2-4 samples -> at least 1 eval) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 ) self . train_dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Split dataset: { len ( self . train_dataset ) } train, { len ( self . eval_dataset ) } eval\" ) else : # Only skip split if dataset has less than 2 samples self . train_dataset = dataset self . eval_dataset = None logger . warning ( f \"Dataset too small for split (only { len ( dataset ) } samples), using all for training. No eval dataset created.\" ) logger . info ( f \"Dataset prepared: { len ( self . train_dataset ) } training samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise setup_model () \u00b6 Setup Unsloth-optimized model with task-aware configuration. Source code in src/aligntune/backends/unsloth/sft/sft.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model with task-aware configuration.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth model for task: { self . task_type . value } \" ) logger . info ( f \"Model: { self . config . model . name_or_path } \" ) # Configure Unsloth model parameters # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"Unsloth SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Configure Unsloth model parameters # Unsloth only accepts load_in_4bit as boolean, not BitsAndBytesConfig parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : dtype , # Use dtype from PrecisionHandler instead of None \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , False ) if self . config . model . quantization else False , } # Unsloth handles quantization internally - don't pass BitsAndBytesConfig parameters # Parameters like bnb_4bit_compute_dtype, bnb_4bit_quant_type, etc. are not supported # Load model with Unsloth optimizations try : self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = self . config . model . name_or_path , ** model_kwargs ) except ( SyntaxError , RuntimeError , Exception ) as e : # Handle Unsloth compiled module syntax errors (may be wrapped in RuntimeError) error_msg = str ( e ) if \"unsloth_compiled_module\" in error_msg or \"unexpected indent\" in error_msg . lower (): logger . error ( f \"Unsloth compilation error: { e } \" ) logger . info ( \"This is a known Unsloth issue with compiled modules.\" ) logger . info ( \"Suggested fixes:\" ) logger . info ( \" 1. Clear Unsloth cache: rm -rf ~/.cache/unsloth/\" ) logger . info ( \" 2. Retry training after clearing cache\" ) logger . info ( \" 3. Use TRL backend instead: backend='trl'\" ) raise RuntimeError ( f \"Unsloth compiled module syntax error: { e } \\n \" f \"This is a known Unsloth library issue. \" f \"Please clear the Unsloth cache and retry, or use TRL backend instead.\" ) from e else : # Re-raise if it's a different error raise # Detect model architecture for appropriate target modules model_type = self . unsloth_model . config . model_type . lower () target_modules = self . _get_target_modules_for_architecture ( model_type ) # Validate target modules exist in the model available_modules = [ name for name , _ in self . unsloth_model . named_modules ()] valid_target_modules = [ tm for tm in target_modules if any ( tm in name for name in available_modules )] if not valid_target_modules : # Try to auto-detect target modules logger . warning ( f \"None of the target modules { target_modules } found in model. Attempting auto-detection...\" ) valid_target_modules = self . _auto_detect_target_modules ( available_modules , model_type ) if not valid_target_modules : raise ValueError ( f \"Target modules { target_modules } not found in the base model (type: { model_type } ). \" f \"Available modules include: { available_modules [: 10 ] } ... \" f \"Please check the target modules and try again, or use a different model architecture.\" ) logger . info ( f \"Using target modules for { model_type } : { valid_target_modules } \" ) # Configure model for training with LoRA self . unsloth_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , # LoRA rank target_modules = valid_target_modules , lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Setup tokenizer based on task type self . _setup_tokenizer_for_task () logger . info ( \"Unsloth model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth model: { e } \" ) raise setup_trainer () \u00b6 Setup TRL SFTTrainer with task-aware configuration. Source code in src/aligntune/backends/unsloth/sft/sft.py 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 def setup_trainer ( self ) -> None : \"\"\"Setup TRL SFTTrainer with task-aware configuration.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig logger . info ( f \"Setting up TRL SFTTrainer for task: { self . task_type . value } \" ) # Get training parameters num_epochs = getattr ( self . config , 'num_epochs' , None ) or \\ getattr ( self . config . train , 'epochs' , 3 ) if hasattr ( self . config , 'train' ) else 3 batch_size = getattr ( self . config , 'batch_size' , None ) or \\ getattr ( self . config . train , 'per_device_batch_size' , 2 ) if hasattr ( self . config , 'train' ) else 2 grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) if hasattr ( self . config , 'train' ) else 1 lr = getattr ( self . config . train , 'learning_rate' , 2e-4 ) if hasattr ( self . config , 'train' ) else 2e-4 save_steps = getattr ( self . config . train , 'save_interval' , 500 ) if hasattr ( self . config , 'train' ) else 500 output_dir = getattr ( self . config . logging , 'output_dir' , './output' ) if hasattr ( self . config , 'logging' ) else './output' # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Create SFT configuration # Create SFT configuration self . training_config = SFTConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , gradient_accumulation_steps = grad_accum , learning_rate = lr , logging_steps = 10 , save_steps = save_steps , warmup_ratio = 0.1 , lr_scheduler_type = \"cosine\" , ** precision_args , dataloader_pin_memory = False , report_to = \"none\" if not hasattr ( self . config , 'logging' ) else \"tensorboard\" ) # Get max_seq_length max_seq_len = getattr ( self . config . model , 'max_seq_length' , 2048 ) if hasattr ( self . config , 'model' ) else 2048 # Task-specific trainer settings packing = False # Generally disable packing for better quality if self . task_type == TaskType . TEXT_GENERATION : packing = True # Can enable packing for simple text generation # Create trainer self . trainer = SFTTrainer ( model = self . unsloth_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = self . training_config , dataset_text_field = \"text\" , max_seq_length = max_seq_len , dataset_num_proc = 2 , packing = packing , ) logger . info ( f \"TRL SFTTrainer setup completed for { self . task_type . value } \" ) logger . info ( f \"Training config: { num_epochs } epochs, batch_size= { batch_size } , lr= { lr } \" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise train () \u00b6 Execute training with Unsloth optimizations. Source code in src/aligntune/backends/unsloth/sft/sft.py 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute training with Unsloth optimizations.\"\"\" try : logger . info ( f \"Starting Unsloth SFT training for task: { self . task_type . value } \" ) start_time = time . time () # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Start training training_result = self . trainer . train () # Save model output_dir = self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"task_type\" : self . task_type . value , \"training_time\" : training_time , \"final_loss\" : training_result . training_loss , \"total_steps\" : training_result . global_step , \"model_path\" : output_dir , \"training_history\" : self . training_history , } logger . info ( f \"Unsloth SFT training completed in { training_time : .2f } seconds\" ) logger . info ( f \"Task: { self . task_type . value } , Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise train_step ( batch ) \u00b6 Execute a single training step (required by abstract base class). Source code in src/aligntune/backends/unsloth/sft/sft.py 598 599 600 601 602 603 604 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step (required by abstract base class).\"\"\" if self . trainer is None : raise RuntimeError ( \"Trainer not initialized. Call setup_model() first.\" ) # The actual training step is handled by TRL's SFTTrainer return { \"loss\" : 0.0 , \"learning_rate\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.unsloth.sft.sft import UnslothSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = UnslothSFTTrainer ( config ) trainer . train () UnslothDPOTrainer \u00b6 Unsloth backend for Direct Preference Optimization. Bases: TrainerBase Unsloth-optimized DPO trainer using TRL's DPOTrainer. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 class UnslothDPOTrainer ( TrainerBase ): \"\"\"Unsloth-optimized DPO trainer using TRL's DPOTrainer.\"\"\" def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize Unsloth DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . custom_evaluator = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth DPO trainer is available.\"\"\" try : import unsloth from trl import DPOTrainer , DPOConfig , ModelConfig return True except ImportError : return False # Required abstract methods implementation def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO.\"\"\" pass def train_step ( self ) -> Dict [ str , Any ]: \"\"\"Single training step - handled by TRL internally.\"\"\" raise NotImplementedError ( \"Use train() method instead\" ) def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for DPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import DPOConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth DPO model: { self . config . model . name_or_path } \" ) device_map = self . config . model . device_map or 'auto' # Configure Unsloth model parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : None , # Auto-detect \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , True ), \"device_map\" : device_map } # Add quantization parameters if specified # FIXED: Only add parameters that are valid for # FastLanguageModel.from_pretrained if self . config . model . quantization : # These are the parameters that Unsloth's FastLanguageModel # accepts valid_quant_params = { 'load_in_4bit' , 'use_gradient_checkpointing' , 'rope_scaling' , 'fix_tokenizer' , 'trust_remote_code' , 'use_cache' , 'token' } for key , value in self . config . model . quantization . items (): if key in valid_quant_params and key != \"load_in_4bit\" : model_kwargs [ key ] = value # Silently skip unsupported parameters like bnb_4bit_compute_dtype # These are handled internally by Unsloth # Load model with Unsloth optimizations self . model , self . tokenizer = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Configure model for DPO training self . model = FastLanguageModel . get_peft_model ( self . model , r = 16 , # LoRA rank target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Create reference model for DPO self . reference_model , _ = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( \"Unsloth DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth DPO model: { e } \" ) raise def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : logger . info ( \"Setting up DPO dataset with DataManager...\" ) # Extract dataset configuration if not hasattr ( self . config , 'datasets' ) or len ( self . config . datasets ) == 0 : raise ValueError ( \"No dataset configuration found\" ) dataset_config = self . config . datasets [ 0 ] # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading DPO dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for DPO task from aligntune.data.manager import DataManager enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) logger . info ( f \"Dataset columns: { available_columns } \" ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column # structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \\ \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer with Unsloth model.\"\"\" try : from trl import DPOTrainer , DPOConfig , ModelConfig logger . info ( \"Setting up TRL DPOTrainer with Unsloth model\" ) from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , default = 1 ) if num_epochs is None : num_epochs = 1 per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , default = 4 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , default = 5e-5 ) max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default =- 1 ) if max_steps is None : max_steps = - 1 # Get output and logging parameters output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/unsloth_dpo' ) run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_dpo' ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None load_best_model_at_end = False metric_for_best_model = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = False ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) # DPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , default = 0.1 ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) label_smoothing = self . _get_config_value ( self . config . train , 'label_smoothing' , default = 0.0 ) truncation_mode = self . _get_config_value ( self . config . train , 'truncation_mode' , default = 'keep_end' ) # Sequence lengths max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ) max_length = self . _get_config_value ( self . config . train , 'max_length' , default = max_seq_length ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = max_seq_length // 2 ) # Adjust save strategy to save on epoch if self . eval_dataset : save_strategy = \"epoch\" # Create DPO configuration dpo_config = DPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_steps , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , max_steps = max_steps , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # DPO specific parameters beta = beta , loss_type = loss_type , label_smoothing = label_smoothing , max_length = max_length , max_prompt_length = max_prompt_length , truncation_mode = truncation_mode , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting Unsloth DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"Unsloth DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 )} except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained model.\"\"\" # try: # if not self.trainer or not self.eval_dataset: # logger.warning(\"No trainer or evaluation dataset available\") # return {} # logger.info(\"Running DPO evaluation\") # # Run evaluation # eval_result = self.trainer.evaluate() # logger.info(\"DPO evaluation completed\") # return { # \"eval_loss\": eval_result.get(\"eval_loss\", 0), # \"eval_metrics\": eval_result # } # except Exception as e: # logger.error(f\"DPO evaluation failed: {e}\") # return {} # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = True, # **kwargs # ) -> Dict[str, float]: # \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # # Auto-setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # logger.info(\"Auto-initializing evaluators for first evaluation...\") # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's unified evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def generate_preference_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate sample preference data for testing.\"\"\" try : if not self . tokenizer : logger . warning ( \"No tokenizer available for generation\" ) return [] # Sample prompts prompts = [ \"Explain the concept of machine learning\" , \"Write a short story about a robot\" , \"Describe the benefits of renewable energy\" , \"What are the key principles of good software design?\" , \"Explain quantum computing in simple terms\" ] samples = [] for i , prompt in enumerate ( prompts [: num_samples ]): samples . append ({ \"prompt\" : prompt , \"chosen\" : f \"Sample chosen response { i + 1 } \" , \"rejected\" : f \"Sample rejected response { i + 1 } \" }) return samples except Exception as e : logger . error ( f \"Failed to generate preference samples: { e } \" ) return [] def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise __init__ ( config ) \u00b6 Initialize Unsloth DPO trainer. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize Unsloth DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . custom_evaluator = None self . dataset_dict = None generate_preference_samples ( num_samples = 5 ) \u00b6 Generate sample preference data for testing. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def generate_preference_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate sample preference data for testing.\"\"\" try : if not self . tokenizer : logger . warning ( \"No tokenizer available for generation\" ) return [] # Sample prompts prompts = [ \"Explain the concept of machine learning\" , \"Write a short story about a robot\" , \"Describe the benefits of renewable energy\" , \"What are the key principles of good software design?\" , \"Explain quantum computing in simple terms\" ] samples = [] for i , prompt in enumerate ( prompts [: num_samples ]): samples . append ({ \"prompt\" : prompt , \"chosen\" : f \"Sample chosen response { i + 1 } \" , \"rejected\" : f \"Sample rejected response { i + 1 } \" }) return samples except Exception as e : logger . error ( f \"Failed to generate preference samples: { e } \" ) return [] is_available () classmethod \u00b6 Check if Unsloth DPO trainer is available. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 39 40 41 42 43 44 45 46 47 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth DPO trainer is available.\"\"\" try : import unsloth from trl import DPOTrainer , DPOConfig , ModelConfig return True except ImportError : return False load_model ( path ) \u00b6 Load a trained model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise save_model ( path = None ) \u00b6 Save model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise setup_data () \u00b6 Setup data - delegates to setup_dataset. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 62 63 64 def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () setup_dataset () \u00b6 Setup and prepare preference dataset for DPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : logger . info ( \"Setting up DPO dataset with DataManager...\" ) # Extract dataset configuration if not hasattr ( self . config , 'datasets' ) or len ( self . config . datasets ) == 0 : raise ValueError ( \"No dataset configuration found\" ) dataset_config = self . config . datasets [ 0 ] # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading DPO dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for DPO task from aligntune.data.manager import DataManager enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) logger . info ( f \"Dataset columns: { available_columns } \" ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column # structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \\ \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise setup_model () \u00b6 Setup Unsloth-optimized model and tokenizer for DPO. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for DPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import DPOConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth DPO model: { self . config . model . name_or_path } \" ) device_map = self . config . model . device_map or 'auto' # Configure Unsloth model parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : None , # Auto-detect \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , True ), \"device_map\" : device_map } # Add quantization parameters if specified # FIXED: Only add parameters that are valid for # FastLanguageModel.from_pretrained if self . config . model . quantization : # These are the parameters that Unsloth's FastLanguageModel # accepts valid_quant_params = { 'load_in_4bit' , 'use_gradient_checkpointing' , 'rope_scaling' , 'fix_tokenizer' , 'trust_remote_code' , 'use_cache' , 'token' } for key , value in self . config . model . quantization . items (): if key in valid_quant_params and key != \"load_in_4bit\" : model_kwargs [ key ] = value # Silently skip unsupported parameters like bnb_4bit_compute_dtype # These are handled internally by Unsloth # Load model with Unsloth optimizations self . model , self . tokenizer = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Configure model for DPO training self . model = FastLanguageModel . get_peft_model ( self . model , r = 16 , # LoRA rank target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Create reference model for DPO self . reference_model , _ = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( \"Unsloth DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth DPO model: { e } \" ) raise setup_rewards () \u00b6 Setup rewards - not used in DPO. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 66 67 68 def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO.\"\"\" pass setup_trainer () \u00b6 Setup TRL DPOTrainer with Unsloth model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer with Unsloth model.\"\"\" try : from trl import DPOTrainer , DPOConfig , ModelConfig logger . info ( \"Setting up TRL DPOTrainer with Unsloth model\" ) from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , default = 1 ) if num_epochs is None : num_epochs = 1 per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , default = 4 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , default = 5e-5 ) max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default =- 1 ) if max_steps is None : max_steps = - 1 # Get output and logging parameters output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/unsloth_dpo' ) run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_dpo' ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None load_best_model_at_end = False metric_for_best_model = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = False ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) # DPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , default = 0.1 ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) label_smoothing = self . _get_config_value ( self . config . train , 'label_smoothing' , default = 0.0 ) truncation_mode = self . _get_config_value ( self . config . train , 'truncation_mode' , default = 'keep_end' ) # Sequence lengths max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ) max_length = self . _get_config_value ( self . config . train , 'max_length' , default = max_seq_length ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = max_seq_length // 2 ) # Adjust save strategy to save on epoch if self . eval_dataset : save_strategy = \"epoch\" # Create DPO configuration dpo_config = DPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_steps , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , max_steps = max_steps , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # DPO specific parameters beta = beta , loss_type = loss_type , label_smoothing = label_smoothing , max_length = max_length , max_prompt_length = max_prompt_length , truncation_mode = truncation_mode , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise train () \u00b6 Run DPO training. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting Unsloth DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"Unsloth DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 )} except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise train_step () \u00b6 Single training step - handled by TRL internally. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 70 71 72 def train_step ( self ) -> Dict [ str , Any ]: \"\"\"Single training step - handled by TRL internally.\"\"\" raise NotImplementedError ( \"Use train() method instead\" ) options: show_source: true heading_level: 3 UnslothPPOTrainer \u00b6 Unsloth backend for Proximal Policy Optimization. Bases: TrainerBase PPO trainer with hybrid Unsloth architecture. Model Architecture: - Policy Model: Unsloth FastLanguageModel + LoRA (trained, optimized) - Reference Model: Unsloth FastLanguageModel (frozen, memory efficient) - Reward Model: Standard HuggingFace (frozen, any architecture) - Value Model: Standard HuggingFace (trained alongside policy) Key Insight: Unsloth models need manual patches for apply_qkv method. Standard models work naturally with TRL without patches. Reward Model Options: 1. Pretrained: Load from HuggingFace (DeBERTa, RoBERTa, etc.) 2. Custom trained: Train using TRL's RewardTrainer 3. Hybrid: Fine-tune pretrained with custom reward functions Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 class UnslothPPOTrainer ( TrainerBase ): \"\"\"PPO trainer with hybrid Unsloth architecture. Model Architecture: - Policy Model: Unsloth FastLanguageModel + LoRA (trained, optimized) - Reference Model: Unsloth FastLanguageModel (frozen, memory efficient) - Reward Model: Standard HuggingFace (frozen, any architecture) - Value Model: Standard HuggingFace (trained alongside policy) Key Insight: Unsloth models need manual patches for apply_qkv method. Standard models work naturally with TRL without patches. Reward Model Options: 1. Pretrained: Load from HuggingFace (DeBERTa, RoBERTa, etc.) 2. Custom trained: Train using TRL's RewardTrainer 3. Hybrid: Fine-tune pretrained with custom reward functions \"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) # Validate Unsloth is properly loaded import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : raise RuntimeError ( \"Unsloth PPO backend requires Unsloth to be imported first. \" \"This should be handled by the factory, but validation failed. \" \"Please report this as a bug.\" ) # Rest of existing initialization self . unsloth_model = None self . policy_model = None self . ref_model = None self . reward_model = None self . value_model = None self . trainer = None self . training_history = [] self . train_dataset = None self . eval_dataset = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available and properly configured.\"\"\" try : # Check Unsloth installation import unsloth from unsloth import FastLanguageModel # Check if Unsloth is properly configured import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : logger . warning ( \"Unsloth not properly initialized\" ) return False # Check TRL availability from trl import PPOTrainer , PPOConfig from transformers import AutoModelForSequenceClassification return True except ImportError as e : logger . warning ( f \"Unsloth PPO backend not available: { e } \" ) return False except Exception as e : logger . warning ( f \"Unsloth PPO backend configuration error: { e } \" ) return False def _get_config_value ( self , config_obj , key : str , default = None ): \"\"\"Helper to get config value from dict or object.\"\"\" if isinstance ( config_obj , dict ): return config_obj . get ( key , default ) else : return getattr ( config_obj , key , default ) def _detect_model_type ( self , model_name : str ) -> str : \"\"\"Detect if model should use Unsloth or standard loading. Returns: 'unsloth': Use Unsloth FastLanguageModel 'standard': Use standard transformers \"\"\" # Check config override first model_config = self . config . model if hasattr ( self . config , 'model' ) else {} if isinstance ( model_config , dict ): force_type = model_config . get ( 'reward_value_loading_type' ) else : force_type = getattr ( model_config , 'reward_value_loading_type' , None ) if force_type in [ 'unsloth' , 'standard' ]: logger . info ( f \"Using config-specified loading type: { force_type } \" ) return force_type # Models that CANNOT use Unsloth (encoder-only models) incompatible_models = [ 'bert' , 'roberta' , 'distilbert' , 'albert' , 'electra' , 'deberta' ] if any ( indicator in model_name . lower () for indicator in incompatible_models ): logger . info ( f \"Using standard loading for encoder model: { model_name } \" ) return 'standard' # Default to Unsloth for decoder models (Llama, GPT, Mistral, etc.) # This ensures they get proper rotary embedding patches logger . info ( f \"Using Unsloth loading for decoder model: { model_name } \" ) return 'unsloth' def _load_reward_value_models_unsloth ( self , model_name : str , max_seq_length : int , quantization : dict ): \"\"\"Load reward/value models using Unsloth for optimization.\"\"\" logger . info ( f \"Loading reward/value models with Unsloth: { model_name } \" ) # Check for integrated reward training first reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) # Check if integrated reward training is enabled if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) # Get quantization settings (configurable per model) reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , quantization ) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , quantization ) # Load reward model if reward_model_name : logger . info ( f \"Loading pre-trained reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification # Load pre-trained reward model self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch . bfloat16 , device_map = \"auto\" ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # DEBUG: Check wrapper logger . info ( f \"\ud83d\udd0d DEBUG: Wrapped reward model type: { type ( self . reward_model ) . __name__ } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Wrapped reward model has score: { hasattr ( self . reward_model , 'score' ) } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Base model type: { type ( self . reward_model . _model ) . __name__ } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Base model has classifier: { hasattr ( self . reward_model . _model , 'classifier' ) } \" ) # Verify score method exists for Unsloth PPO compatibility if not hasattr ( self . reward_model , 'score' ): logger . error ( \"Reward model wrapper missing score method!\" ) base_model = self . reward_model . _model if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to Unsloth reward model wrapper\" ) else : raise AttributeError ( \"Reward model must have score() or classifier() method\" ) else : logger . info ( \"Unsloth reward model wrapper has score method\" ) logger . info ( \"\u2705 Loaded pre-trained reward model with UniversalRewardModelWrapper\" ) else : # Create reward model from scratch (NO quantization for TRL compatibility) reward_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = False , # CRITICAL: Disable quantization for TRL compatibility ) if hasattr ( reward_base , \"config\" ): reward_base . config . output_hidden_states = True import torch.nn as nn hidden_size = reward_base . config . hidden_size base_dtype = next ( reward_base . parameters ()) . dtype self . reward_model = reward_base self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( base_dtype ) . to ( reward_base . device ) # Verify score layer was added correctly if not hasattr ( self . reward_model , 'score' ): raise RuntimeError ( \"Failed to add score layer to Unsloth reward model\" ) logger . info ( f \"Unsloth reward model score layer: { self . reward_model . score } \" ) # Load value model similarly (NO quantization to avoid .to() errors in PPOTrainer) value_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = False , # CRITICAL: Disable quantization for TRL compatibility ) if hasattr ( value_base , \"config\" ): value_base . config . output_hidden_states = True self . value_model = value_base value_dtype = next ( value_base . parameters ()) . dtype self . value_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( value_dtype ) . to ( value_base . device ) # Disable gradient checkpointing for model in [ self . reward_model , self . value_model ]: if hasattr ( model , 'gradient_checkpointing_disable' ): model . gradient_checkpointing_disable () if hasattr ( model , 'config' ): model . config . gradient_checkpointing = False model . config . use_cache = True logger . info ( \"\u2705 Reward/value models loaded with Unsloth optimizations\" ) def _load_reward_value_models_standard ( self , model_name : str ): \"\"\"Load reward/value models using standard transformers (no Unsloth patches).\"\"\" logger . info ( f \"Loading reward/value models with standard transformers: { model_name } \" ) from transformers import AutoModelForSequenceClassification # Get quantization settings (configurable per model) reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , {}) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , {}) # Get precision from config precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 # Load reward model self . reward_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = reward_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . reward_model , 'config' ): self . reward_model . config . output_hidden_states = True # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # Verify score method for standard transformers path if not hasattr ( self . reward_model , 'score' ): base_model = self . reward_model . _model if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to standard reward model wrapper\" ) else : logger . info ( \"Standard reward model wrapper has score method\" ) # Load value model self . value_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = value_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . value_model , 'config' ): self . value_model . config . output_hidden_states = True # Disable gradient checkpointing for model in [ self . reward_model , self . value_model ]: if hasattr ( model , 'gradient_checkpointing_disable' ): model . gradient_checkpointing_disable () if hasattr ( model , 'config' ): model . config . gradient_checkpointing = False model . config . use_cache = True # Standard HuggingFace models don't need Unsloth patches # They work naturally with TRL's PPOTrainer logger . info ( \"\u2705 Reward/value models loaded with standard transformers\" ) def _load_value_model_unsloth ( self , model_name : str , max_seq_length : int , quantization : dict ): \"\"\"Load only value model using Unsloth.\"\"\" logger . info ( f \"Loading value model with Unsloth: { model_name } \" ) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , quantization ) value_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = value_quant . get ( \"load_in_4bit\" , True ), ) if hasattr ( value_base , \"config\" ): value_base . config . output_hidden_states = True import torch.nn as nn hidden_size = value_base . config . hidden_size self . value_model = value_base base_dtype = next ( value_base . parameters ()) . dtype self . value_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( base_dtype ) . to ( value_base . device ) if hasattr ( self . value_model , 'gradient_checkpointing_disable' ): self . value_model . gradient_checkpointing_disable () if hasattr ( self . value_model , 'config' ): self . value_model . config . gradient_checkpointing = False self . value_model . config . use_cache = True logger . info ( \"\u2705 Value model loaded with Unsloth optimizations\" ) def _load_value_model_standard ( self , model_name : str ): \"\"\"Load only value model using standard transformers (NO Unsloth patches).\"\"\" logger . info ( f \"Loading value model with standard transformers: { model_name } \" ) # Warn if trying to load Unsloth model with standard transformers if \"unsloth\" in model_name . lower (): logger . warning ( f \"\u26a0\ufe0f Loading Unsloth model ' { model_name } ' with standard transformers\" ) logger . warning ( \"\u26a0\ufe0f This may cause compatibility issues with Unsloth's PPOTrainer\" ) logger . warning ( \"\u26a0\ufe0f Consider using a standard HuggingFace model or set reward_value_loading_type='unsloth'\" ) from transformers import AutoModelForSequenceClassification value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , {}) # Get precision from config precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 self . value_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = value_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . value_model , 'config' ): self . value_model . config . output_hidden_states = True # DEBUG: Check value model dtype after loading if DEBUG : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model loaded with dtype: { value_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) if hasattr ( self . value_model , 'gradient_checkpointing_disable' ): self . value_model . gradient_checkpointing_disable () if hasattr ( self . value_model , 'config' ): self . value_model . config . gradient_checkpointing = False self . value_model . config . use_cache = True # DO NOT apply Unsloth patches to standard models logger . info ( \"\u2705 Value model loaded with standard transformers (no Unsloth patches)\" ) def _get_config_hash ( self ): \"\"\"Generate hash of relevant configuration for cache invalidation.\"\"\" import hashlib import json # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) gradient_checkpointing = self . _get_config_value ( self . config . model , \"gradient_checkpointing\" , True ) reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , \"meta-llama/Llama-3.2-1B-Instruct\" ) # Get library versions try : import unsloth unsloth_version = getattr ( unsloth , \"__version__\" , \"unknown\" ) except : unsloth_version = \"not_installed\" try : import transformers transformers_version = getattr ( transformers , \"__version__\" , \"unknown\" ) except : transformers_version = \"not_installed\" try : import torch torch_version = getattr ( torch , \"__version__\" , \"unknown\" ) except : torch_version = \"not_installed\" config_dict = { 'model_name' : model_name , 'max_seq_length' : max_seq_length , 'quantization' : quantization , 'gradient_checkpointing' : gradient_checkpointing , 'reward_value_model' : reward_value_model , 'unsloth_version' : unsloth_version , 'transformers_version' : transformers_version , 'torch_version' : torch_version , } config_str = json . dumps ( config_dict , sort_keys = True ) return hashlib . md5 ( config_str . encode ()) . hexdigest () def _clear_unsloth_cache ( self , force : bool = False ): \"\"\"Clear Unsloth compiled cache to force recompilation. Args: force: If True, always clear cache. If False, only clear if config changed. \"\"\" import shutil from pathlib import Path # Clear local cache cache_dir = Path . cwd () / \"unsloth_compiled_cache\" if cache_dir . exists (): if force : logger . info ( \"\ud83d\uddd1\ufe0f Force clearing Unsloth compiled cache...\" ) shutil . rmtree ( cache_dir ) logger . info ( f \"\u2705 Cleared local cache: { cache_dir } \" ) else : logger . info ( f \"Cache exists at: { cache_dir } \" ) # Clear global Unsloth cache if set import os if 'UNSLOTH_COMPILED_CACHE' in os . environ : global_cache = Path ( os . environ [ 'UNSLOTH_COMPILED_CACHE' ]) if global_cache . exists (): if force : logger . info ( f \"\ud83d\uddd1\ufe0f Force clearing global Unsloth cache...\" ) shutil . rmtree ( global_cache ) logger . info ( f \"\u2705 Cleared global cache: { global_cache } \" ) else : logger . info ( f \"Global cache exists at: { global_cache } \" ) # Set environment variable to force recompilation if force : os . environ [ 'UNSLOTH_FORCE_RECOMPILE' ] = '1' logger . info ( \"\u2705 Set UNSLOTH_FORCE_RECOMPILE=1\" ) def _maybe_invalidate_unsloth_cache ( self ): \"\"\"Invalidate Unsloth cache if configuration has changed.\"\"\" from pathlib import Path import shutil cache_dir = Path . cwd () / \"unsloth_compiled_cache\" config_file = cache_dir / \".config_hash\" current_hash = self . _get_config_hash () logger . info ( f \"Current config hash: { current_hash [: 8 ] } ...\" ) if config_file . exists (): stored_hash = config_file . read_text () . strip () if stored_hash != current_hash : logger . info ( f \"Config changed ( { stored_hash [: 8 ] } ... -> { current_hash [: 8 ] } ...), invalidating cache\" ) if cache_dir . exists (): shutil . rmtree ( cache_dir ) logger . info ( f \"\u2713 Cleared Unsloth cache: { cache_dir } \" ) else : logger . info ( f \"Config unchanged ( { current_hash [: 8 ] } ...), reusing cache\" ) else : logger . info ( \"No previous cache found, will create new cache\" ) # Ensure cache dir exists and store new hash cache_dir . mkdir ( exist_ok = True ) config_file . write_text ( current_hash ) def _validate_model_compatibility ( self ): \"\"\"Validate that model types are compatible with loading strategy.\"\"\" # Check value model model_name = self . _get_config_value ( self . config . model , \"reward_value_model\" , \"\" ) loading_type = self . _get_config_value ( self . config . model , \"reward_value_loading_type\" , \"auto\" ) if loading_type == \"standard\" and \"unsloth\" in model_name . lower (): raise ValueError ( f \"Incompatible configuration: reward_value_model=' { model_name } ' \" f \"with reward_value_loading_type='standard'. \" f \"Either use a standard HuggingFace model or set loading_type='unsloth'\" ) # Check reward model reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) if reward_model_name and \"unsloth\" in reward_model_name . lower (): logger . warning ( f \"\u26a0\ufe0f Using Unsloth reward model ' { reward_model_name } ' with standard loading\" ) logger . warning ( \"\u26a0\ufe0f This may cause compatibility issues. Consider using a standard HuggingFace reward model\" ) def _disable_gradient_checkpointing_for_all ( self ): \"\"\"Disable gradient checkpointing on all models - UNSLOTH-SAFE VERSION.\"\"\" def _safe_replace_gc_attr ( obj , name = \"(unknown)\" , indent = \"\" ): \"\"\"Replace gradient checkpointing attributes safely.\"\"\" try : # DON'T replace _gradient_checkpointing_func - just disable the flag if hasattr ( obj , \"gradient_checkpointing\" ): obj . gradient_checkpointing = False # For config objects if hasattr ( obj , \"config\" ): cfg = getattr ( obj , \"config\" , None ) if cfg and hasattr ( cfg , \"gradient_checkpointing\" ): cfg . gradient_checkpointing = False if cfg and hasattr ( cfg , \"use_cache\" ): cfg . use_cache = True logger . info ( f \" { indent } \u2713 Disabled GC flags on { name } \" ) except Exception as e : logger . warning ( f \" { indent } \u2717 Could not disable GC on { name } : { e } \" ) def _recursive_disable ( model , model_name = \"model\" , depth = 0 ): if model is None or depth > 6 : return indent = \" \" * depth logger . info ( f \" { indent } Disabling GC on { model_name } (depth= { depth } )\" ) # 1. Use official API if available if hasattr ( model , \"gradient_checkpointing_disable\" ): try : model . gradient_checkpointing_disable () logger . info ( f \" { indent } \u2713 Called gradient_checkpointing_disable()\" ) except Exception as e : logger . warning ( f \" { indent } \u2717 gradient_checkpointing_disable failed: { e } \" ) # 2. Disable flags (but DON'T replace functions) _safe_replace_gc_attr ( model , model_name , indent ) # 3. Recurse into known wrappers for attr_name in [ \"model\" , \"base_model\" , \"transformer\" , \"bert\" , \"roberta\" ]: inner = getattr ( model , attr_name , None ) if inner is not None and inner is not model : _recursive_disable ( inner , f \" { model_name } . { attr_name } \" , depth + 1 ) logger . info ( \"=\" * 80 ) logger . info ( \"\ud83d\udd27 UNSLOTH-SAFE GRADIENT CHECKPOINTING DISABLE\" ) logger . info ( \"=\" * 80 ) for model_name , model in [ ( \"policy_model\" , getattr ( self , \"policy_model\" , None )), ( \"ref_model\" , getattr ( self , \"ref_model\" , None )), ( \"value_model\" , getattr ( self , \"value_model\" , None )), ( \"reward_model\" , getattr ( self , \"reward_model\" , None )), ]: if model is not None : logger . info ( f \" \\n >>> Processing { model_name } \" ) _recursive_disable ( model , model_name , 0 ) logger . info ( \"=\" * 80 ) logger . info ( \"\u2705 UNSLOTH-SAFE DISABLE COMPLETE\" ) logger . info ( \"=\" * 80 ) for model_name , model in [ ( \"policy_model\" , getattr ( self , \"policy_model\" , None )), ( \"ref_model\" , getattr ( self , \"ref_model\" , None )), ( \"value_model\" , getattr ( self , \"value_model\" , None )), ( \"reward_model\" , getattr ( self , \"reward_model\" , None )), ]: if model is not None : logger . info ( f \" \\n >>> Processing { model_name } \" ) _recursive_disable ( model , model_name , 0 ) logger . info ( \"=\" * 80 ) logger . info ( \"\u2705 NUCLEAR DISABLE COMPLETE (SAFE MODE)\" ) logger . info ( \"=\" * 80 ) # ========================================================================================= # FORCE DISABLE (used for specific model) # ========================================================================================= def _force_disable_gradient_checkpointing_on_model ( self , model , model_name = \"model\" ): \"\"\"Forcefully disable gradient checkpointing - UNSLOTH SAFE.\"\"\" if model is None : return try : # 1. Call disable() wherever possible for target in [ model , getattr ( model , \"base_model\" , None ), getattr ( model , \"model\" , None ), ]: if target and hasattr ( target , \"gradient_checkpointing_disable\" ): target . gradient_checkpointing_disable () logger . info ( f \"\u2713 { model_name } : called gradient_checkpointing_disable()\" ) # 2. Disable config flags for candidate in [ model , getattr ( model , \"base_model\" , None )]: cfg = getattr ( candidate , \"config\" , None ) if cfg : if hasattr ( cfg , \"gradient_checkpointing\" ): cfg . gradient_checkpointing = False if hasattr ( cfg , \"use_cache\" ): cfg . use_cache = True # 3. DON'T replace _gradient_checkpointing_func - Unsloth needs it! # Just set the flag to False on layers inner = getattr ( model , \"model\" , model ) layers = getattr ( inner , \"layers\" , None ) if layers : for layer in layers : if hasattr ( layer , \"gradient_checkpointing\" ): layer . gradient_checkpointing = False logger . info ( f \"\u2713 { model_name } : disabled GC flags on { len ( layers ) } layers\" ) logger . info ( f \"\u2705 Force-disabled GC on { model_name } (Unsloth-safe)\" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Error disabling GC on { model_name } : { e } \" ) def _patch_transformers_validation ( self ): \"\"\"Global patch for transformers validation to ignore num_logits_to_keep.\"\"\" try : from transformers.generation.utils import GenerationMixin if getattr ( GenerationMixin , \"_is_patched_validation\" , False ): return original_validate = GenerationMixin . _validate_model_kwargs def patched_validate ( self , model_kwargs ): # Strip num_logits_to_keep if present to avoid ValueError if 'num_logits_to_keep' in model_kwargs : # logger.info(\"Global Patch: Stripped num_logits_to_keep from validation\") model_kwargs . pop ( 'num_logits_to_keep' ) return original_validate ( self , model_kwargs ) GenerationMixin . _validate_model_kwargs = patched_validate GenerationMixin . _is_patched_validation = True logger . info ( \"\u2705 Globally patched transformers._validate_model_kwargs\" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Failed to patch transformers validation: { e } \" ) def _patch_model_forward_for_compatibility ( self , model , model_name = \"model\" ): \"\"\"Patch model's forward method to ignore incompatible kwargs (like position_ids).\"\"\" if model is None : return # Determine target model (handle wrappers) target_model = model if hasattr ( model , \"model\" ): target_model = model . model elif hasattr ( model , \"base_model\" ): target_model = model . base_model if not hasattr ( target_model , \"forward\" ): logger . warning ( f \"Cannot patch forward: { model_name } has no forward method\" ) return # Check if model is Decoder (Llama, GPT, etc.) or Encoder (BERT, RoBERTa, DistilBERT) # Only patch decoders conditionally; ALWAYS strip position_ids for encoders is_decoder = False is_encoder = False if hasattr ( model , 'config' ): model_type = getattr ( model . config , 'model_type' , '' ) . lower () decoder_models = [ 'llama' , 'gpt' , 'mistral' , 'gemma' , 'qwen' , 'opt' , 'bloom' ] encoder_models = [ 'bert' , 'roberta' , 'distilbert' , 'albert' , 'electra' , 'deberta' ] if any ( dec in model_type for dec in decoder_models ): is_decoder = True elif any ( enc in model_type for enc in encoder_models ): is_encoder = True if not is_decoder and not is_encoder : return # Patch the forward method of the model itself original_forward = target_model . forward def patched_forward ( self , * args , ** kwargs ): # For encoders: ALWAYS strip position_ids and use_cache (they don't support them) # For decoders: Only strip position_ids during training/eval (not inference) if is_encoder : if 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) if 'use_cache' in kwargs : kwargs . pop ( 'use_cache' ) else : # is_decoder is_inference = kwargs . get ( 'use_cache' , False ) or 'past_key_values' in kwargs if not is_inference and 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) return original_forward ( * args , ** kwargs ) import types target_model . forward = types . MethodType ( patched_forward , target_model ) logger . info ( f \"\u2705 Patched { model_name } forward to conditionally ignore position_ids\" ) # CRITICAL: Also patch the BACKBONE forward because get_reward calls it directly! # get_reward does: lm_backbone = getattr(model, model.base_model_prefix); lm_backbone(...) if hasattr ( model , \"base_model_prefix\" ): backbone_name = model . base_model_prefix if hasattr ( model , backbone_name ): backbone = getattr ( model , backbone_name ) if hasattr ( backbone , \"forward\" ) and backbone is not target_model : original_backbone_forward = backbone . forward def patched_backbone_forward ( self , * args , ** kwargs ): # For encoders: ALWAYS strip position_ids and use_cache # For decoders: Only strip position_ids during training/eval if is_encoder : if 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) if 'use_cache' in kwargs : kwargs . pop ( 'use_cache' ) else : # is_decoder is_inference = kwargs . get ( 'use_cache' , False ) or 'past_key_values' in kwargs if not is_inference and 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) return original_backbone_forward ( * args , ** kwargs ) backbone . forward = types . MethodType ( patched_backbone_forward , backbone ) logger . info ( f \"\u2705 Patched backbone ( { backbone_name } ) forward to conditionally ignore position_ids\" ) def _patch_model_generate_for_compatibility ( self , model , model_name = \"model\" ): \"\"\"Patch model's generate method to remove incompatible kwargs.\"\"\" if model is None : return # Recursive patching for wrappers (PeftModel, etc.) if hasattr ( model , \"base_model\" ) and model . base_model is not model : self . _patch_model_generate_for_compatibility ( model . base_model , f \" { model_name } .base_model\" ) if not hasattr ( model , \"generate\" ): return # Avoid double patching if getattr ( model . generate , \"_is_patched_compatibility\" , False ): return original_generate = model . generate def patched_generate ( self , * args , ** kwargs ): # Strip num_logits_to_keep if present (causes transformers validation error) if 'num_logits_to_keep' in kwargs : kwargs . pop ( 'num_logits_to_keep' ) # DEBUG: Force keys_to_ignore_at_inference to include num_logits_to_keep if hasattr ( self , 'config' ) and hasattr ( self . config , 'keys_to_ignore_at_inference' ): if 'num_logits_to_keep' not in self . config . keys_to_ignore_at_inference : # self.config.keys_to_ignore_at_inference.append('num_logits_to_keep') # Modify in place # Some configs use tuples? if isinstance ( self . config . keys_to_ignore_at_inference , tuple ): self . config . keys_to_ignore_at_inference = list ( self . config . keys_to_ignore_at_inference ) self . config . keys_to_ignore_at_inference . append ( 'num_logits_to_keep' ) return original_generate ( * args , ** kwargs ) patched_generate . _is_patched_compatibility = True import types model . generate = types . MethodType ( patched_generate , model ) logger . info ( f \"\u2705 Patched { model_name } generate to ignore num_logits_to_keep\" ) # CRITICAL: If model has _old_generate (Unsloth), patch it too! # Unsloth's generate might ADD num_logits_to_keep and then call _old_generate if hasattr ( model , \"_old_generate\" ): logger . info ( f \"Found _old_generate on { model_name } . Patching it.\" ) original_old_generate = model . _old_generate def patched_old_generate ( self , * args , ** kwargs ): if 'num_logits_to_keep' in kwargs : # logger.info(f\"Stripped num_logits_to_keep in _old_generate of {model_name}\") kwargs . pop ( 'num_logits_to_keep' ) return original_old_generate ( * args , ** kwargs ) import types model . _old_generate = types . MethodType ( patched_old_generate , model ) logger . info ( f \"\u2705 Patched { model_name } _old_generate to ignore num_logits_to_keep\" ) else : logger . info ( f \"\u274c _old_generate NOT found on { model_name } (Type: { type ( model ) . __name__ } )\" ) # ========================================================================================= # Integration inside setup_model (simplified relevant part) # ========================================================================================= def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for PPO.\"\"\" try : from unsloth import FastLanguageModel # CRITICAL: Clear cache and patch classes BEFORE any model loading from .unsloth_patches import clear_all_unsloth_caches , patch_attention_classes_globally logger . info ( \"Step 1: Clearing Unsloth caches...\" ) clear_all_unsloth_caches () logger . info ( \"Step 2: Patching attention classes at class level...\" ) patch_attention_classes_globally () # Validate model compatibility self . _validate_model_compatibility () # Check if user wants to clear cache (additional clearing if requested) clear_cache = self . _get_config_value ( self . config . model , 'clear_unsloth_cache' , False ) if clear_cache : logger . info ( \"\ud83d\uddd1\ufe0f Additional cache clearing requested (clear_unsloth_cache=True)\" ) self . _clear_unsloth_cache ( force = True ) else : # Smart cache invalidation based on config changes self . _maybe_invalidate_unsloth_cache () # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) # Get precision from config to ensure consistency across all models precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value logger . info ( f \"Using precision: { precision } for all models\" ) logger . info ( f \"Setting up Unsloth PPO model: { model_name } \" ) # Load tokenizer first (before any model loading) from transformers import AutoTokenizer from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE self . tokenizer = AutoTokenizer . from_pretrained ( model_name , padding_side = \"left\" , trust_remote_code = False ) # Tokenizer setup if self . tokenizer . pad_token is None : if self . tokenizer . eos_token : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : self . tokenizer . add_special_tokens ({ \"pad_token\" : \"[PAD]\" }) if self . tokenizer . pad_token_id is None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if self . tokenizer . chat_template is None : logger . info ( \"Applying Unsloth chat template for PPO...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = self . _get_config_value ( self . config . dataset , 'chat_template' , None ) # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to SIMPLE_CHAT_TEMPLATE.\" ) self . tokenizer . chat_template = SIMPLE_CHAT_TEMPLATE logger . info ( f \"Tokenizer ready (vocab= { len ( self . tokenizer ) } )\" ) # Check if separate reward model is specified reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) try : if self . config . model . reward_model_source . source_type == \"custom_trained\" : self . config . model . reward_model_source . training_config . base_model_name = self . _train_and_load_custom_reward_model ( self . config . model . reward_model_source . training_config ) except : pass if reward_model_name : # Check if reward model is already loaded and wrapped if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \"Reward model already loaded: { type ( self . reward_model ) . __name__ } \" ) return # Load specialized reward model (e.g., DeBERTa, RoBERTa, or Llama-based) logger . info ( f \"Loading specialized reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification , AutoTokenizer # Detect if reward model is a decoder (Llama, Mistral, etc.) or encoder (BERT, RoBERTa, etc.) reward_model_type = self . _detect_model_type ( reward_model_name ) # Load reward model with config precision torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 if reward_model_type == 'unsloth' : # Load decoder-based reward model via Unsloth logger . info ( f \"Loading reward model via Unsloth (decoder model): { reward_model_name } \" ) from unsloth import FastLanguageModel self . reward_model , _ = FastLanguageModel . from_pretrained ( model_name = reward_model_name , max_seq_length = max_seq_length , dtype = torch_dtype , load_in_4bit = False , # Disable quantization for reward model ) logger . info ( f \"\u2705 Reward model loaded via Unsloth\" ) # Unsloth loads as CausalLM. We need to add a score head if it's missing. import torch.nn as nn if not hasattr ( self . reward_model , 'score' ) and not hasattr ( self . reward_model , 'classifier' ): logger . info ( \"Adding ad-hoc score head to Unsloth model\" ) hidden_size = self . reward_model . config . hidden_size self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) self . reward_model . score . to ( self . reward_model . device ) . to ( torch_dtype ) logger . info ( \"\u2705 Ad-hoc score head added\" ) else : # Load encoder-based reward model via standard transformers logger . info ( f \"Loading reward model via standard transformers (encoder model): { reward_model_name } \" ) self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , ignore_mismatched_sizes = True , ) logger . info ( f \"\u2705 Reward model loaded via standard transformers\" ) # CRITICAL: Ensure ALL parameters are converted to the correct dtype if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Before conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Convert the entire model to the target dtype self . reward_model = self . reward_model . to ( torch_dtype ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: After .to() conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Also ensure all submodules are converted for name , module in self . reward_model . named_modules (): if hasattr ( module , 'weight' ) and module . weight is not None : old_dtype = module . weight . data . dtype module . weight . data = module . weight . data . to ( torch_dtype ) new_dtype = module . weight . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .weight from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .weight already { new_dtype } \" ) if hasattr ( module , 'bias' ) and module . bias is not None : old_dtype = module . bias . data . dtype module . bias . data = module . bias . data . to ( torch_dtype ) new_dtype = module . bias . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .bias from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .bias already { new_dtype } \" ) # CRITICAL: Check the classification head specifically if DEBUG : if hasattr ( self . reward_model , 'classifier' ): logger . info ( f \"\ud83d\udd0d DEBUG: Classifier weight dtype: { self . reward_model . classifier . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Classifier bias dtype: { self . reward_model . classifier . bias . dtype if self . reward_model . classifier . bias is not None else 'None' } \" ) if hasattr ( self . reward_model , 'score' ): logger . info ( f \"\ud83d\udd0d DEBUG: Score weight dtype: { self . reward_model . score . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Score bias dtype: { self . reward_model . score . bias . dtype if self . reward_model . score . bias is not None else 'None' } \" ) # Final verification final_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Final reward model dtype: { final_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) logger . info ( f \"\u2705 Reward model loaded with { precision } precision\" ) # CRITICAL: Check if classifier is actually in the right dtype # Handle different model architectures for head layer head_layer = None if hasattr ( self . reward_model , 'classifier' ): head_layer = self . reward_model . classifier if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using classifier - weight dtype: { head_layer . weight . dtype } \" ) elif hasattr ( self . reward_model , 'score' ): head_layer = self . reward_model . score if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using score - weight dtype: { head_layer . weight . dtype } \" ) else : raise AttributeError ( f \"Model { type ( self . reward_model ) } has no classifier or score layer\" ) # CRITICAL: Ensure head layer is in the right dtype try : # Use .to() which handles recursion for complex heads (like RoBERTa) head_layer . to ( torch_dtype ) logger . info ( f \"\u2705 Converted head layer to { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Could not convert head layer to { torch_dtype } : { e } \" ) # Load reward tokenizer self . reward_tokenizer = AutoTokenizer . from_pretrained ( reward_model_name ) # Load value model using same model as policy reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , model_name # Use policy model for value model ) if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_value_model_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_value_model_standard ( reward_value_model ) # DEBUG: Check value model dtype if DEBUG and hasattr ( self , 'value_model' ) and self . value_model is not None : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model dtype: { value_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) else : # Original behavior: use same model for reward/value reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , None # Default to None, we will set to model_name if missing ) # Fallback to policy model if not specified if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Reward/value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_reward_value_models_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_reward_value_models_standard ( reward_value_model ) # NOW build policy/ref models (Unsloth already imported at top) logger . info ( \"Building policy/ref models with Unsloth...\" ) # Load Unsloth policy model self . unsloth_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Explicitly set dtype to bfloat16 load_in_4bit = quantization . get ( \"load_in_4bit\" , True ), ) # CRITICAL FIX: Apply LoRA with use_gradient_checkpointing=False logger . info ( \"Applying LoRA WITHOUT gradient checkpointing...\" ) self . policy_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = False , # CHANGED: Set to False directly random_state = 3407 , use_rslora = False , ) # Using class-level patches applied before model loading from .unsloth_patches import verify_attention_patches logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . policy_model ) # FIX: Add num_logits_to_keep to ignored keys to prevent generation error # Apply to all reachable configs to be safe models_to_fix = [ self . policy_model , self . ref_model ] if hasattr ( self . policy_model , \"base_model\" ): models_to_fix . append ( self . policy_model . base_model ) if hasattr ( self . policy_model , \"model\" ): models_to_fix . append ( self . policy_model . model ) for m in models_to_fix : if hasattr ( m , \"config\" ): if not hasattr ( m . config , \"keys_to_ignore_at_inference\" ): m . config . keys_to_ignore_at_inference = [] if \"num_logits_to_keep\" not in m . config . keys_to_ignore_at_inference : m . config . keys_to_ignore_at_inference . append ( \"num_logits_to_keep\" ) logger . info ( f \"\u2705 Added num_logits_to_keep to keys_to_ignore_at_inference for { type ( m ) . __name__ } \" ) logger . info ( \"\u2705 Policy model ready\" ) # DEBUG: Check policy model dtype if DEBUG : try : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model dtype: { policy_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get policy model dtype: { e } \" ) # Reference model (no quantization, no GC) - USE SAME DTYPE AS POLICY logger . info ( \"Loading reference model...\" ) self . ref_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Use bfloat16 to match policy model load_in_4bit = False , ) for p in self . ref_model . parameters (): p . requires_grad = False # Using class-level patches applied before model loading logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . ref_model ) logger . info ( \"\u2705 Reference model ready\" ) # DEBUG: Check ref model dtype if DEBUG : try : ref_dtype = next ( self . ref_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reference model dtype: { ref_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get ref model dtype: { e } \" ) logger . info ( \"\u2705 All models loaded successfully\" ) # Log model architecture logger . info ( \"=\" * 60 ) logger . info ( \"Model Architecture:\" ) logger . info ( f \" Policy: { type ( self . policy_model ) . __name__ } (Unsloth + LoRA)\" ) logger . info ( f \" Reference: { type ( self . ref_model ) . __name__ } (Unsloth frozen)\" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \" Reward: { type ( self . reward_model ) . __name__ } (Standard HF)\" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : logger . info ( f \" Value: { type ( self . value_model ) . __name__ } (Standard HF)\" ) logger . info ( \"=\" * 60 ) # Load reward model if specified self . _load_reward_model_strict () # CRITICAL: Patch models to ignore position_ids (for compatibility with TRL get_reward) # Apply to ALL models including reward model (especially important for encoder models like BERT/RoBERTa/DistilBERT) self . _patch_model_forward_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_forward_for_compatibility ( self . ref_model , \"ref_model\" ) self . _patch_model_forward_for_compatibility ( self . value_model , \"value_model\" ) # CRITICAL FIX: Patch reward model attention layers with apply_qkv from policy model # This is necessary when loading multiple Unsloth models, as the second instance might miss the patch if hasattr ( self , 'reward_model' ) and self . reward_model is not None and hasattr ( self , 'policy_model' ) and self . policy_model is not None : try : policy_attn = None for module in self . policy_model . modules (): if 'Attention' in module . __class__ . __name__ : policy_attn = module break if policy_attn is not None and hasattr ( policy_attn , 'apply_qkv' ): logger . info ( \"Patching reward model attention layers with Unsloth methods from policy model...\" ) count = 0 for module in self . reward_model . modules (): if 'Attention' in module . __class__ . __name__ : patched_module = False # Copy all apply_* methods (apply_qkv, apply_o, etc.) for attr_name in dir ( policy_attn ): if attr_name . startswith ( 'apply_' ): if not hasattr ( module , attr_name ): setattr ( module , attr_name , getattr ( policy_attn , attr_name )) patched_module = True if patched_module : count += 1 if count > 0 : logger . info ( f \"\u2705 Patched { count } attention layers in reward model\" ) else : logger . debug ( \"Policy model has no apply_qkv to copy\" ) except Exception as e : logger . warning ( f \"Failed to patch apply_qkv on reward model: { e } \" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : # For wrapped reward models, patch the underlying model if hasattr ( self . reward_model , '_model' ): self . _patch_model_forward_for_compatibility ( self . reward_model . _model , \"reward_model._model\" ) else : self . _patch_model_forward_for_compatibility ( self . reward_model , \"reward_model\" ) # CRITICAL: Patch generate to ignore num_logits_to_keep self . _patch_model_generate_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_generate_for_compatibility ( self . ref_model , \"ref_model\" ) # Although ref_model usually doesn't generate # GLOBAL: Patch transformers validation as a last resort self . _patch_transformers_validation () except Exception as e : logger . error ( f \"\u274c Failed to setup model: { e } \" ) raise def _load_reward_model_strict ( self ): \"\"\"Load reward model with strict validation, NO fallbacks.\"\"\" if not hasattr ( self . config . model , 'reward_model_source' ): logger . info ( \"No reward_model_source in config, skipping custom reward model\" ) return if not self . config . model . reward_model_source : logger . info ( \"reward_model_source is None, skipping custom reward model\" ) return reward_source = self . config . model . reward_model_source # Validate source before loading from aligntune.rewards.training import RewardModelValidator RewardModelValidator . validate_reward_source ( reward_source ) logger . info ( f \"Loading reward model: source_type= { reward_source . source_type } \" ) try : if reward_source . source_type == \"pretrained_hf\" : self . reward_model = self . _load_pretrained_hf_reward_model ( reward_source . model_name ) elif reward_source . source_type == \"pretrained_local\" : self . reward_model = self . _load_local_reward_model ( reward_source . model_path ) elif reward_source . source_type == \"custom_trained\" : # self.reward_model = self._train_and_load_custom_reward_model(reward_source.training_config) self . reward_model = self . _load_local_reward_model ( reward_source . training_config . base_model_name ) else : raise ValueError ( f \"Invalid source_type: { reward_source . source_type } \" ) # CRITICAL: Wrap with UniversalRewardModelWrapper for TRL compatibility if not isinstance ( self . reward_model , UniversalRewardModelWrapper ): self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # CRITICAL: Add score method directly to the model for TRL compatibility # TRL's get_reward might bypass the wrapper and call model.score() directly base_model = self . reward_model . _model if not hasattr ( base_model , 'score' ): if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) base_model . score = score_method logger . info ( \"\u2705 Added score method to base reward model\" ) elif hasattr ( base_model , 'score' ): pass # Already has it else : logger . warning ( \"\u26a0\ufe0f Reward model has no classifier or score layer\" ) # Verify wrapper score method exists if not hasattr ( self . reward_model , 'score' ): logger . error ( \"Reward model wrapper missing score method!\" ) if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to reward model wrapper\" ) else : raise AttributeError ( \"Reward model must have score() or classifier() method\" ) else : logger . info ( \"\u2705 Reward model wrapper has score method\" ) logger . info ( \"\u2705 Reward model loaded successfully\" ) except Exception as e : logger . error ( f \"\u274c Failed to load reward model: { e } \" ) raise RuntimeError ( f \"Reward model loading failed: { e } \" ) from e def _load_pretrained_hf_reward_model ( self , model_name : str ): \"\"\"Load pre-trained reward model from HuggingFace Hub.\"\"\" from aligntune.rewards.training import RewardModelLoader logger . info ( f \"Loading HF reward model: { model_name } \" ) loader = RewardModelLoader () # Use ignore_mismatched_sizes to handle models trained with different num_labels return loader . load_from_huggingface ( model_name , ignore_mismatched_sizes = True ) def _load_local_reward_model ( self , model_path : str ): \"\"\"Load reward model from local path.\"\"\" from aligntune.rewards.training import RewardModelLoader logger . info ( f \"Loading local reward model: { model_path } \" ) loader = RewardModelLoader () return loader . load_from_local ( model_path ) def _train_and_load_custom_reward_model ( self , training_config ): \"\"\"Train custom reward model from reward functions.\"\"\" from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry logger . info ( \"Training custom reward model from reward functions\" ) logger . info ( f \"Reward functions: { training_config . reward_functions } \" ) logger . info ( f \"Training texts: { len ( training_config . training_texts ) } samples\" ) # Get reward functions from registry reward_funcs = [ RewardRegistry . get_reward_function ( name ) for name in training_config . reward_functions ] # Create trainer trainer = RewardModelTrainer ( base_model_name = training_config . base_model_name , reward_functions = reward_funcs , composite_weights = training_config . reward_weights ) # Generate training data training_data = trainer . generate_training_data ( texts = training_config . training_texts , references = training_config . reference_texts , batch_size = training_config . batch_size ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = training_config . output_dir , num_epochs = training_config . num_epochs , learning_rate = training_config . learning_rate , batch_size = training_config . batch_size , gradient_accumulation_steps = training_config . gradient_accumulation_steps ) logger . info ( f \"\u2705 Custom reward model trained: { model_path } \" ) # Load trained model # return self._load_local_reward_model(model_path) return model_path def setup_rewards ( self ) -> None : \"\"\"Setup reward functions based on configuration mode.\"\"\" logger . info ( \"Setting up Unsloth PPO reward functions...\" ) # Determine mode has_pretrained = hasattr ( self , 'reward_model' ) and self . reward_model is not None has_reward_funcs = self . config . rewards and len ( self . config . rewards ) > 0 fine_tune_mode = ( has_pretrained and has_reward_funcs and hasattr ( self . config . model , 'reward_model_source' ) and self . config . model . reward_model_source and getattr ( self . config . model . reward_model_source , 'fine_tune_with_rewards' , False ) ) # Check if reward training is enabled should_train_model = ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ) # Mode 1: Pretrained only if has_pretrained and not fine_tune_mode : logger . info ( \"\u2705 Mode: Pretrained reward model (using as-is)\" ) logger . info ( \" Reward functions will be ignored\" ) self . reward_functions = [] return # Mode 2: Custom trained (handled in _load_reward_model_strict) if not has_pretrained and has_reward_funcs : if should_train_model : logger . info ( \"\u2705 Mode: Custom reward model training\" ) logger . info ( \" Will train new model using reward functions\" ) # Setup reward functions for training self . _setup_reward_functions () return else : # NEW: Use reward functions directly with FunctionBasedRewardModel logger . info ( \"\u2705 Mode: Direct reward function usage\" ) logger . info ( \" Reward functions will be wrapped in FunctionBasedRewardModel\" ) self . _setup_reward_functions () return # Mode 3: Hybrid (fine-tune) if fine_tune_mode : logger . info ( \"\u2705 Mode: Hybrid (fine-tune pretrained with reward functions)\" ) self . _setup_reward_functions () self . _fine_tune_reward_model () return # No rewards configured logger . info ( \"No reward model or functions configured\" ) self . reward_functions = [] def _setup_reward_functions ( self ): \"\"\"Internal method to setup reward functions.\"\"\" self . reward_functions = [] for reward_config in self . config . rewards : reward_type = self . _get_config_value ( reward_config , 'type' ) reward_params = self . _get_config_value ( reward_config , 'params' , {}) reward_weight = self . _get_config_value ( reward_config , 'weight' , 1.0 ) reward_clip = self . _get_config_value ( reward_config , 'clip' , None ) # Get reward function from registry reward_func = RewardRegistry . get_reward_function ( reward_type ) # Apply weight if reward_weight != 1.0 : def weighted_reward ( text , weight = reward_weight , func = reward_func , ** kwargs ): return weight * func ( text , ** kwargs ) reward_func = weighted_reward # Apply clipping if reward_clip is not None : def clipped_reward ( text , clip = reward_clip , func = reward_func , ** kwargs ): return max ( - clip , min ( clip , func ( text , ** kwargs ))) reward_func = clipped_reward self . reward_functions . append ( reward_func ) logger . info ( f \"Configured { len ( self . reward_functions ) } reward functions\" ) def _fine_tune_reward_model ( self ): \"\"\"Fine-tune pretrained reward model with reward functions.\"\"\" logger . info ( \"Fine-tuning pretrained reward model...\" ) # Use reward functions to generate additional training data # Fine-tune the loaded model # This is optional advanced feature logger . warning ( \"\u26a0\ufe0f Fine-tuning not yet implemented, using pretrained as-is\" ) def _compute_rewards ( self , texts : List [ str ], references : Optional [ List [ str ]] = None ) -> List [ float ]: \"\"\"Compute rewards for given texts using configured reward functions.\"\"\" if not self . reward_functions : logger . warning ( \"No reward functions configured, returning zero rewards\" ) return [ 0.0 ] * len ( texts ) rewards = [] for text in texts : total_reward = 0.0 for reward_func in self . reward_functions : try : reward = reward_func ( text , reference = references [ texts . index ( text )] if references else None ) total_reward += reward except Exception as e : logger . warning ( f \"Error computing reward for text: { e } \" ) total_reward += 0.0 rewards . append ( total_reward ) return rewards def _detect_dataset_format ( self , sample ): \"\"\"Detect dataset format.\"\"\" if \"messages\" in sample : return \"chat_format\" elif \"chosen\" in sample and \"rejected\" in sample : return \"hh_rlhf_with_prompt\" if \"prompt\" in sample else \"hh_rlhf_no_prompt\" elif \"instruction\" in sample : return \"alpaca_format\" elif \"text\" in sample : return \"simple_text\" elif \"input\" in sample and \"output\" in sample : return \"input_output\" else : return \"unknown\" def _parse_element_by_format ( self , element , format_type , tokenizer ): \"\"\"Parse dataset element - EXACT SAME AS WORKING CODE.\"\"\" if format_type == \"chat_format\" : input_ids = tokenizer . apply_chat_template ( element [ \"messages\" ][: 1 ], padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"hh_rlhf_with_prompt\" : query_text = element [ \"prompt\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"hh_rlhf_no_prompt\" : chosen_text = element [ \"chosen\" ] . strip () if \" \\n\\n Human:\" in chosen_text and \" \\n\\n Assistant:\" in chosen_text : parts = chosen_text . split ( \" \\n\\n Assistant:\" ) human_part = parts [ 0 ] . replace ( \" \\n\\n Human:\" , \"\" ) . strip () if len ( parts ) > 1 else chosen_text . split ( \" \\n\\n Human:\" )[ - 1 ] . strip () elif \"Human:\" in chosen_text and \"Assistant:\" in chosen_text : parts = chosen_text . split ( \"Assistant:\" ) human_part = parts [ 0 ] . replace ( \"Human:\" , \"\" ) . strip () if len ( parts ) > 1 else chosen_text . split ( \"Human:\" )[ - 1 ] . strip () else : lines = chosen_text . split ( ' \\n ' ) human_part = lines [ 0 ] . strip () if lines else chosen_text [: 100 ] . strip () if len ( human_part ) < 10 : human_part = chosen_text [: 200 ] . strip () input_ids = tokenizer . encode ( human_part , add_special_tokens = True , return_tensors = None ) if len ( input_ids ) < 5 : padding_ids = tokenizer . encode ( \" What do you think about this?\" , add_special_tokens = False ) input_ids . extend ( padding_ids ) elif format_type == \"alpaca_format\" : instruction = element [ \"instruction\" ] . strip () if \"input\" in element and element [ \"input\" ] and element [ \"input\" ] . strip (): query_text = f \" { instruction } \\n\\n Input: { element [ 'input' ] . strip () } \" else : query_text = instruction messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"simple_text\" : query_text = element [ \"text\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"input_output\" : query_text = element [ \"input\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) else : raise ValueError ( f \"Unknown format: { format_type } \" ) # Remove trailing EOS try : eos_id = tokenizer . eos_token_id if eos_id is not None and input_ids and input_ids [ - 1 ] == eos_id : input_ids = input_ids [: - 1 ] except Exception : pass # Ensure minimum length if len ( input_ids ) < 5 : padding_ids = tokenizer . encode ( \" Please elaborate.\" , add_special_tokens = False ) input_ids . extend ( padding_ids ) return input_ids def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched ) # Load dataset - DataManager handles everything dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) tokenized = self . tokenizer ( prompts , padding = False , truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL PPOTrainer.\"\"\" # Set global reference for patched_get_reward global GLOBAL_UNSLOTH_TRAINER_REF GLOBAL_UNSLOTH_TRAINER_REF = self try : from trl import PPOTrainer , PPOConfig logger . info ( \"Setting up PPOTrainer\" ) # Logging configuration with defaults output_dir = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) run_name = getattr ( self . config . logging , 'run_name' , None ) or 'ppo_experiment' report_to = getattr ( self . config . logging , 'loggers' , 'none' ) # Training parameters from config with defaults num_epochs = getattr ( self . config . train , 'epochs' , None ) or 1 batch_size = getattr ( self . config . train , 'per_device_batch_size' , 1 ) grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 32 ) lr = getattr ( self . config . train , 'learning_rate' , 1e-6 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.05 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) cliprange_value = getattr ( self . config . train , 'cliprange_value' , 0.2 ) vf_coef = getattr ( self . config . train , 'vf_coef' , 0.1 ) gamma = getattr ( self . config . train , 'gamma' , 1.0 ) lam = getattr ( self . config . train , 'lam' , 0.95 ) max_grad_norm = getattr ( self . config . train , 'max_grad_norm' , 0.5 ) num_ppo_epochs = getattr ( self . config . train , 'num_ppo_epochs' , None ) or 4 whiten_rewards = getattr ( self . config . train , 'whiten_rewards' , False ) response_length = getattr ( self . config . train , 'response_length' , 53 ) stop_token = getattr ( self . config . train , 'stop_token' , 'eos' ) missing_eos_penalty = getattr ( self . config . train , 'missing_eos_penalty' , 1.0 ) save_steps = getattr ( self . config . train , 'save_steps' , 500 ) save_strategy = getattr ( self . config . train , 'save_strategy' , 'steps' ) save_total_limit = getattr ( self . config . train , 'save_total_limit' , 5 ) eval_strategy = getattr ( self . config . train , 'eval_strategy' , 'steps' ) eval_steps = getattr ( self . config . train , 'eval_steps' , None ) or 100 logging_steps = getattr ( self . config . train , 'logging_steps' , 10 ) gradient_checkpointing = getattr ( self . config . model , 'gradient_checkpointing' , False ) gradient_checkpointing_kwargs = getattr ( self . config . train , 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }) # Seed with default seed = getattr ( self . config . distributed , 'seed' , 42 ) # Precision handling - direct string comparison with default precision_str = getattr ( self . config . model , 'precision' , 'auto' ) if hasattr ( precision_str , 'value' ): # Handle enum precision_str = precision_str . value bf16 = precision_str in [ 'bf16' , 'auto' ] fp16 = precision_str == 'fp16' # Calculate total episodes total_episodes = len ( self . train_dataset ) * num_epochs # Create PPOConfig ppo_config = PPOConfig ( exp_name = run_name , learning_rate = lr , batch_size = batch_size , mini_batch_size = batch_size , gradient_accumulation_steps = grad_accum , total_episodes = total_episodes , num_ppo_epochs = num_ppo_epochs , max_grad_norm = max_grad_norm , seed = seed , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , kl_coef = kl_coef , whiten_rewards = whiten_rewards , gamma = gamma , lam = lam , response_length = response_length , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , local_rollout_forward_batch_size = batch_size , output_dir = output_dir , save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , eval_strategy = eval_strategy , eval_steps = eval_steps , logging_steps = logging_steps , bf16 = bf16 , fp16 = fp16 , remove_unused_columns = False , run_name = run_name , gradient_checkpointing = False , report_to = report_to , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # DEBUG: Final dtype check before creating trainer if DEBUG : logger . info ( \"\ud83d\udd0d DEBUG: Final dtype check before PPOTrainer creation:\" ) try : if hasattr ( self , 'reward_model' ) and self . reward_model is not None : reward_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reward model final dtype: { reward_dtype } \" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model final dtype: { value_dtype } \" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model final dtype: { policy_dtype } \" ) # CRITICAL: Test a forward pass to check hidden states dtype logger . info ( \"\ud83d\udd0d DEBUG: Testing forward pass dtypes...\" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : test_input = torch . tensor ([[ 1 , 2 , 3 , 4 , 5 ]], dtype = torch . long ) with torch . no_grad (): try : outputs = self . policy_model ( test_input ) if hasattr ( outputs , 'hidden_states' ) and outputs . hidden_states : hidden_dtype = outputs . hidden_states [ - 1 ] . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model hidden states dtype: { hidden_dtype } \" ) else : logger . info ( f \"\ud83d\udd0d DEBUG: Policy model outputs type: { type ( outputs ) } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not test policy model forward pass: { e } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get final dtypes: { e } \" ) # Check if we should use function-based reward model if not hasattr ( self , 'reward_model' ) or self . reward_model is None : if hasattr ( self , 'reward_functions' ) and self . reward_functions and len ( self . reward_functions ) > 0 : # Use function-based reward model from aligntune.core.rl.function_based_reward_model import FunctionBasedRewardModel logger . info ( \"=\" * 60 ) logger . info ( \"\u2705 Using function-based reward model with reward functions\" ) logger . info ( f \" { len ( self . reward_functions ) } reward function(s) configured\" ) logger . info ( \"=\" * 60 ) # Get device and dtype from policy model device = next ( self . policy_model . parameters ()) . device dtype = next ( self . policy_model . parameters ()) . dtype self . reward_model = FunctionBasedRewardModel ( reward_functions = self . reward_functions , tokenizer = self . tokenizer , device = str ( device ), dtype = dtype ) self . reward_model = self . reward_model . to ( device ) logger . info ( \"\u2705 FunctionBasedRewardModel created and moved to device\" ) else : raise ValueError ( \"TRL PPOTrainer requires either: \\n \" \" - reward_model: A neural network reward model (pretrained or custom trained) \\n \" \" Examples: 'OpenAssistant/reward-model-deberta-v3-large-v2', 'Skywork/Skywork-Reward-V2-Qwen3-0.6B' \\n \" \" - reward_functions: Rule-based reward functions (will be wrapped in FunctionBasedRewardModel) \\n \" \" Example: [{'type': 'length', 'weight': 1.0}, {'type': 'sentiment', 'weight': 0.5}] \\n \" \" - reward_training.enabled=True: Train a custom reward model from reward functions \\n \" \"Please provide one of these options.\" ) else : # Neural reward model provided logger . info ( \"\u2705 Using neural network reward model\" ) logger . info ( f \" Reward model type: { type ( self . reward_model ) . __name__ } \" ) # Create trainer with strict generation limits self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = None , ) # Patch PolicyAndValueWrapper (keep this) self . _patch_policy_wrapper () logger . info ( \"PPOTrainer created successfully\" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise def _patch_policy_wrapper ( self ): \"\"\"Patch PolicyAndValueWrapper to add missing methods for Unsloth compatibility.\"\"\" try : wrapper = getattr ( self . trainer , 'model' , None ) if wrapper is None : return # Add gradient_checkpointing_disable method if missing if not hasattr ( wrapper , 'gradient_checkpointing_disable' ): def _disable_gc (): \"\"\"Disable gradient checkpointing on underlying models.\"\"\" try : policy = getattr ( wrapper , 'policy_model' , None ) if policy and hasattr ( policy , 'gradient_checkpointing_disable' ): policy . gradient_checkpointing_disable () base = getattr ( wrapper , 'model' , None ) if base and hasattr ( base , 'gradient_checkpointing_disable' ): base . gradient_checkpointing_disable () except Exception : pass setattr ( wrapper , 'gradient_checkpointing_disable' , _disable_gc ) logger . info ( \"Added gradient_checkpointing_disable() to PolicyAndValueWrapper\" ) # Add gradient_checkpointing_enable method if missing if not hasattr ( wrapper , 'gradient_checkpointing_enable' ): def _enable_gc ( gradient_checkpointing_kwargs = None ): \"\"\"Enable gradient checkpointing on underlying models.\"\"\" try : policy = getattr ( wrapper , 'policy_model' , None ) if policy and hasattr ( policy , 'gradient_checkpointing_enable' ): if gradient_checkpointing_kwargs : policy . gradient_checkpointing_enable ( gradient_checkpointing_kwargs = gradient_checkpointing_kwargs ) else : policy . gradient_checkpointing_enable () base = getattr ( wrapper , 'model' , None ) if base and hasattr ( base , 'gradient_checkpointing_enable' ): if gradient_checkpointing_kwargs : base . gradient_checkpointing_enable ( gradient_checkpointing_kwargs = gradient_checkpointing_kwargs ) else : base . gradient_checkpointing_enable () except Exception : pass setattr ( wrapper , 'gradient_checkpointing_enable' , _enable_gc ) logger . info ( \"Added gradient_checkpointing_enable() to PolicyAndValueWrapper\" ) # Add generate method if missing (for Unsloth) if not hasattr ( wrapper , 'generate' ): policy = getattr ( wrapper , 'policy_model' , None ) base = getattr ( wrapper , 'model' , None ) if policy and hasattr ( policy , 'generate' ): target = policy . generate elif base and hasattr ( base , 'generate' ): target = base . generate else : return def _delegate_generate ( * args , ** kwargs ): return target ( * args , ** kwargs ) setattr ( wrapper , 'generate' , _delegate_generate ) logger . info ( \"Added generate() to PolicyAndValueWrapper\" ) except Exception as e : logger . warning ( f \"Could not patch PolicyAndValueWrapper: { e } \" ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Not used - TRL handles training loop.\"\"\" return { \"loss\" : 0.0 } def create_data_loader ( self ): \"\"\"Not used - TRL handles data loading.\"\"\" return None def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" try : logger . info ( \"Starting Unsloth PPO training\" ) start_time = time . time () # Setup everything self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Train logger . info ( \"Running PPO training...\" ) training_result = self . trainer . train () try : generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , getattr ( self , 'reward_functions' , None ), stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Save output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) logger . info ( f \"Saving to: { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # TRL PPOTrainer.train() may return None (or an object without global_step). # Use training_history length as a robust fallback so examples don't report 0 steps. fallback_steps = 0 try : if hasattr ( self , \"training_history\" ) and self . training_history : fallback_steps = len ( self . training_history ) except Exception : fallback_steps = 0 results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else fallback_steps , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : 0 , \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Training completed in { training_time : .2f } s\" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise def _train_custom_reward_model ( self ) -> str : \"\"\"Train a custom reward model using the reward_training config.\"\"\" from aligntune.rewards import RewardModelTrainer from aligntune.rewards.factory import create_reward_functions reward_config = self . config . reward_training # Create reward functions from config if reward_config . reward_functions : reward_functions = create_reward_functions ( reward_config . reward_functions ) else : # Default reward functions if none specified reward_functions = create_reward_functions ([ { 'type' : 'length' , 'weight' : 0.25 , 'params' : { 'min_length' : 10 , 'max_length' : 200 }}, { 'type' : 'sentiment' , 'weight' : 0.25 , 'params' : { 'positive_weight' : 1.0 }}, { 'type' : 'coherence' , 'weight' : 0.25 , 'params' : { 'threshold' : 0.7 }}, { 'type' : 'safety' , 'weight' : 0.25 , 'params' : { 'strict' : True }} ]) # Create composite weights composite_weights = [ 1.0 ] * len ( reward_functions ) # Determine base model for reward training base_model_name = reward_config . base_model or self . config . model . name_or_path # Create reward model trainer trainer = RewardModelTrainer ( base_model_name = base_model_name , reward_functions = reward_functions , composite_weights = composite_weights ) # Load training data training_texts = trainer . load_training_data ( texts = reward_config . training_texts , dataset_name = reward_config . dataset_name , dataset_path = reward_config . dataset_path , dataset_split = reward_config . dataset_split , text_column = reward_config . text_column , max_samples = reward_config . max_samples ) # Generate training dataset training_data = trainer . generate_training_data ( texts = training_texts , batch_size = reward_config . batch_size ) # Train the reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = reward_config . output_dir , num_epochs = reward_config . num_epochs , learning_rate = reward_config . learning_rate , batch_size = reward_config . batch_size , save_steps = reward_config . save_steps , logging_steps = reward_config . logging_steps ) logger . info ( f \"\u2705 Custom reward model training completed: { model_path } \" ) return model_path def evaluate ( self ) -> Dict [ str , Any ]: \"\"\"Evaluate model with fallback for missing memory tracker.\"\"\" try : if not self . eval_dataset : logger . info ( \"No evaluation dataset provided\" ) return {} logger . info ( \"Evaluating...\" ) # Initialize memory tracker if missing if not hasattr ( self . trainer , '_memory_tracker' ): from transformers.trainer_utils import TrainerMemoryTracker self . trainer . _memory_tracker = TrainerMemoryTracker ( skip_memory_metrics = True ) logger . info ( \"Initialized missing memory tracker\" ) eval_results = self . trainer . evaluate () return eval_results except AttributeError as e : if '_memory_tracker' in str ( e ): logger . warning ( f \"Memory tracker error during evaluation: { e } \" ) logger . warning ( \"Skipping evaluation - this is a known TRL/Transformers compatibility issue\" ) return { \"eval_skipped\" : True , \"reason\" : \"memory_tracker_missing\" } else : logger . error ( f \"Evaluation failed: { e } \" ) raise except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . policy_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load model.\"\"\" try : from unsloth import FastLanguageModel logger . info ( f \"Loading from: { path } \" ) max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , 2048 ) self . policy_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"Loaded successfully\" ) except Exception as e : logger . error ( f \"Load failed: { e } \" ) raise create_data_loader () \u00b6 Not used - TRL handles data loading. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2147 2148 2149 def create_data_loader ( self ): \"\"\"Not used - TRL handles data loading.\"\"\" return None evaluate () \u00b6 Evaluate model with fallback for missing memory tracker. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 def evaluate ( self ) -> Dict [ str , Any ]: \"\"\"Evaluate model with fallback for missing memory tracker.\"\"\" try : if not self . eval_dataset : logger . info ( \"No evaluation dataset provided\" ) return {} logger . info ( \"Evaluating...\" ) # Initialize memory tracker if missing if not hasattr ( self . trainer , '_memory_tracker' ): from transformers.trainer_utils import TrainerMemoryTracker self . trainer . _memory_tracker = TrainerMemoryTracker ( skip_memory_metrics = True ) logger . info ( \"Initialized missing memory tracker\" ) eval_results = self . trainer . evaluate () return eval_results except AttributeError as e : if '_memory_tracker' in str ( e ): logger . warning ( f \"Memory tracker error during evaluation: { e } \" ) logger . warning ( \"Skipping evaluation - this is a known TRL/Transformers compatibility issue\" ) return { \"eval_skipped\" : True , \"reason\" : \"memory_tracker_missing\" } else : logger . error ( f \"Evaluation failed: { e } \" ) raise except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise is_available () classmethod \u00b6 Check if Unsloth and TRL are available and properly configured. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available and properly configured.\"\"\" try : # Check Unsloth installation import unsloth from unsloth import FastLanguageModel # Check if Unsloth is properly configured import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : logger . warning ( \"Unsloth not properly initialized\" ) return False # Check TRL availability from trl import PPOTrainer , PPOConfig from transformers import AutoModelForSequenceClassification return True except ImportError as e : logger . warning ( f \"Unsloth PPO backend not available: { e } \" ) return False except Exception as e : logger . warning ( f \"Unsloth PPO backend configuration error: { e } \" ) return False load_model ( path ) \u00b6 Load model. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 def load_model ( self , path : str ) -> None : \"\"\"Load model.\"\"\" try : from unsloth import FastLanguageModel logger . info ( f \"Loading from: { path } \" ) max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , 2048 ) self . policy_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"Loaded successfully\" ) except Exception as e : logger . error ( f \"Load failed: { e } \" ) raise save_model ( path = None ) \u00b6 Save model. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . policy_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise setup_data () \u00b6 Setup datasets for PPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched ) # Load dataset - DataManager handles everything dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) tokenized = self . tokenizer ( prompts , padding = False , truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) setup_model () \u00b6 Setup Unsloth-optimized model and tokenizer for PPO. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for PPO.\"\"\" try : from unsloth import FastLanguageModel # CRITICAL: Clear cache and patch classes BEFORE any model loading from .unsloth_patches import clear_all_unsloth_caches , patch_attention_classes_globally logger . info ( \"Step 1: Clearing Unsloth caches...\" ) clear_all_unsloth_caches () logger . info ( \"Step 2: Patching attention classes at class level...\" ) patch_attention_classes_globally () # Validate model compatibility self . _validate_model_compatibility () # Check if user wants to clear cache (additional clearing if requested) clear_cache = self . _get_config_value ( self . config . model , 'clear_unsloth_cache' , False ) if clear_cache : logger . info ( \"\ud83d\uddd1\ufe0f Additional cache clearing requested (clear_unsloth_cache=True)\" ) self . _clear_unsloth_cache ( force = True ) else : # Smart cache invalidation based on config changes self . _maybe_invalidate_unsloth_cache () # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) # Get precision from config to ensure consistency across all models precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value logger . info ( f \"Using precision: { precision } for all models\" ) logger . info ( f \"Setting up Unsloth PPO model: { model_name } \" ) # Load tokenizer first (before any model loading) from transformers import AutoTokenizer from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE self . tokenizer = AutoTokenizer . from_pretrained ( model_name , padding_side = \"left\" , trust_remote_code = False ) # Tokenizer setup if self . tokenizer . pad_token is None : if self . tokenizer . eos_token : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : self . tokenizer . add_special_tokens ({ \"pad_token\" : \"[PAD]\" }) if self . tokenizer . pad_token_id is None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if self . tokenizer . chat_template is None : logger . info ( \"Applying Unsloth chat template for PPO...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = self . _get_config_value ( self . config . dataset , 'chat_template' , None ) # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to SIMPLE_CHAT_TEMPLATE.\" ) self . tokenizer . chat_template = SIMPLE_CHAT_TEMPLATE logger . info ( f \"Tokenizer ready (vocab= { len ( self . tokenizer ) } )\" ) # Check if separate reward model is specified reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) try : if self . config . model . reward_model_source . source_type == \"custom_trained\" : self . config . model . reward_model_source . training_config . base_model_name = self . _train_and_load_custom_reward_model ( self . config . model . reward_model_source . training_config ) except : pass if reward_model_name : # Check if reward model is already loaded and wrapped if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \"Reward model already loaded: { type ( self . reward_model ) . __name__ } \" ) return # Load specialized reward model (e.g., DeBERTa, RoBERTa, or Llama-based) logger . info ( f \"Loading specialized reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification , AutoTokenizer # Detect if reward model is a decoder (Llama, Mistral, etc.) or encoder (BERT, RoBERTa, etc.) reward_model_type = self . _detect_model_type ( reward_model_name ) # Load reward model with config precision torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 if reward_model_type == 'unsloth' : # Load decoder-based reward model via Unsloth logger . info ( f \"Loading reward model via Unsloth (decoder model): { reward_model_name } \" ) from unsloth import FastLanguageModel self . reward_model , _ = FastLanguageModel . from_pretrained ( model_name = reward_model_name , max_seq_length = max_seq_length , dtype = torch_dtype , load_in_4bit = False , # Disable quantization for reward model ) logger . info ( f \"\u2705 Reward model loaded via Unsloth\" ) # Unsloth loads as CausalLM. We need to add a score head if it's missing. import torch.nn as nn if not hasattr ( self . reward_model , 'score' ) and not hasattr ( self . reward_model , 'classifier' ): logger . info ( \"Adding ad-hoc score head to Unsloth model\" ) hidden_size = self . reward_model . config . hidden_size self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) self . reward_model . score . to ( self . reward_model . device ) . to ( torch_dtype ) logger . info ( \"\u2705 Ad-hoc score head added\" ) else : # Load encoder-based reward model via standard transformers logger . info ( f \"Loading reward model via standard transformers (encoder model): { reward_model_name } \" ) self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , ignore_mismatched_sizes = True , ) logger . info ( f \"\u2705 Reward model loaded via standard transformers\" ) # CRITICAL: Ensure ALL parameters are converted to the correct dtype if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Before conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Convert the entire model to the target dtype self . reward_model = self . reward_model . to ( torch_dtype ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: After .to() conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Also ensure all submodules are converted for name , module in self . reward_model . named_modules (): if hasattr ( module , 'weight' ) and module . weight is not None : old_dtype = module . weight . data . dtype module . weight . data = module . weight . data . to ( torch_dtype ) new_dtype = module . weight . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .weight from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .weight already { new_dtype } \" ) if hasattr ( module , 'bias' ) and module . bias is not None : old_dtype = module . bias . data . dtype module . bias . data = module . bias . data . to ( torch_dtype ) new_dtype = module . bias . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .bias from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .bias already { new_dtype } \" ) # CRITICAL: Check the classification head specifically if DEBUG : if hasattr ( self . reward_model , 'classifier' ): logger . info ( f \"\ud83d\udd0d DEBUG: Classifier weight dtype: { self . reward_model . classifier . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Classifier bias dtype: { self . reward_model . classifier . bias . dtype if self . reward_model . classifier . bias is not None else 'None' } \" ) if hasattr ( self . reward_model , 'score' ): logger . info ( f \"\ud83d\udd0d DEBUG: Score weight dtype: { self . reward_model . score . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Score bias dtype: { self . reward_model . score . bias . dtype if self . reward_model . score . bias is not None else 'None' } \" ) # Final verification final_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Final reward model dtype: { final_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) logger . info ( f \"\u2705 Reward model loaded with { precision } precision\" ) # CRITICAL: Check if classifier is actually in the right dtype # Handle different model architectures for head layer head_layer = None if hasattr ( self . reward_model , 'classifier' ): head_layer = self . reward_model . classifier if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using classifier - weight dtype: { head_layer . weight . dtype } \" ) elif hasattr ( self . reward_model , 'score' ): head_layer = self . reward_model . score if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using score - weight dtype: { head_layer . weight . dtype } \" ) else : raise AttributeError ( f \"Model { type ( self . reward_model ) } has no classifier or score layer\" ) # CRITICAL: Ensure head layer is in the right dtype try : # Use .to() which handles recursion for complex heads (like RoBERTa) head_layer . to ( torch_dtype ) logger . info ( f \"\u2705 Converted head layer to { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Could not convert head layer to { torch_dtype } : { e } \" ) # Load reward tokenizer self . reward_tokenizer = AutoTokenizer . from_pretrained ( reward_model_name ) # Load value model using same model as policy reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , model_name # Use policy model for value model ) if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_value_model_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_value_model_standard ( reward_value_model ) # DEBUG: Check value model dtype if DEBUG and hasattr ( self , 'value_model' ) and self . value_model is not None : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model dtype: { value_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) else : # Original behavior: use same model for reward/value reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , None # Default to None, we will set to model_name if missing ) # Fallback to policy model if not specified if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Reward/value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_reward_value_models_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_reward_value_models_standard ( reward_value_model ) # NOW build policy/ref models (Unsloth already imported at top) logger . info ( \"Building policy/ref models with Unsloth...\" ) # Load Unsloth policy model self . unsloth_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Explicitly set dtype to bfloat16 load_in_4bit = quantization . get ( \"load_in_4bit\" , True ), ) # CRITICAL FIX: Apply LoRA with use_gradient_checkpointing=False logger . info ( \"Applying LoRA WITHOUT gradient checkpointing...\" ) self . policy_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = False , # CHANGED: Set to False directly random_state = 3407 , use_rslora = False , ) # Using class-level patches applied before model loading from .unsloth_patches import verify_attention_patches logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . policy_model ) # FIX: Add num_logits_to_keep to ignored keys to prevent generation error # Apply to all reachable configs to be safe models_to_fix = [ self . policy_model , self . ref_model ] if hasattr ( self . policy_model , \"base_model\" ): models_to_fix . append ( self . policy_model . base_model ) if hasattr ( self . policy_model , \"model\" ): models_to_fix . append ( self . policy_model . model ) for m in models_to_fix : if hasattr ( m , \"config\" ): if not hasattr ( m . config , \"keys_to_ignore_at_inference\" ): m . config . keys_to_ignore_at_inference = [] if \"num_logits_to_keep\" not in m . config . keys_to_ignore_at_inference : m . config . keys_to_ignore_at_inference . append ( \"num_logits_to_keep\" ) logger . info ( f \"\u2705 Added num_logits_to_keep to keys_to_ignore_at_inference for { type ( m ) . __name__ } \" ) logger . info ( \"\u2705 Policy model ready\" ) # DEBUG: Check policy model dtype if DEBUG : try : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model dtype: { policy_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get policy model dtype: { e } \" ) # Reference model (no quantization, no GC) - USE SAME DTYPE AS POLICY logger . info ( \"Loading reference model...\" ) self . ref_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Use bfloat16 to match policy model load_in_4bit = False , ) for p in self . ref_model . parameters (): p . requires_grad = False # Using class-level patches applied before model loading logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . ref_model ) logger . info ( \"\u2705 Reference model ready\" ) # DEBUG: Check ref model dtype if DEBUG : try : ref_dtype = next ( self . ref_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reference model dtype: { ref_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get ref model dtype: { e } \" ) logger . info ( \"\u2705 All models loaded successfully\" ) # Log model architecture logger . info ( \"=\" * 60 ) logger . info ( \"Model Architecture:\" ) logger . info ( f \" Policy: { type ( self . policy_model ) . __name__ } (Unsloth + LoRA)\" ) logger . info ( f \" Reference: { type ( self . ref_model ) . __name__ } (Unsloth frozen)\" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \" Reward: { type ( self . reward_model ) . __name__ } (Standard HF)\" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : logger . info ( f \" Value: { type ( self . value_model ) . __name__ } (Standard HF)\" ) logger . info ( \"=\" * 60 ) # Load reward model if specified self . _load_reward_model_strict () # CRITICAL: Patch models to ignore position_ids (for compatibility with TRL get_reward) # Apply to ALL models including reward model (especially important for encoder models like BERT/RoBERTa/DistilBERT) self . _patch_model_forward_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_forward_for_compatibility ( self . ref_model , \"ref_model\" ) self . _patch_model_forward_for_compatibility ( self . value_model , \"value_model\" ) # CRITICAL FIX: Patch reward model attention layers with apply_qkv from policy model # This is necessary when loading multiple Unsloth models, as the second instance might miss the patch if hasattr ( self , 'reward_model' ) and self . reward_model is not None and hasattr ( self , 'policy_model' ) and self . policy_model is not None : try : policy_attn = None for module in self . policy_model . modules (): if 'Attention' in module . __class__ . __name__ : policy_attn = module break if policy_attn is not None and hasattr ( policy_attn , 'apply_qkv' ): logger . info ( \"Patching reward model attention layers with Unsloth methods from policy model...\" ) count = 0 for module in self . reward_model . modules (): if 'Attention' in module . __class__ . __name__ : patched_module = False # Copy all apply_* methods (apply_qkv, apply_o, etc.) for attr_name in dir ( policy_attn ): if attr_name . startswith ( 'apply_' ): if not hasattr ( module , attr_name ): setattr ( module , attr_name , getattr ( policy_attn , attr_name )) patched_module = True if patched_module : count += 1 if count > 0 : logger . info ( f \"\u2705 Patched { count } attention layers in reward model\" ) else : logger . debug ( \"Policy model has no apply_qkv to copy\" ) except Exception as e : logger . warning ( f \"Failed to patch apply_qkv on reward model: { e } \" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : # For wrapped reward models, patch the underlying model if hasattr ( self . reward_model , '_model' ): self . _patch_model_forward_for_compatibility ( self . reward_model . _model , \"reward_model._model\" ) else : self . _patch_model_forward_for_compatibility ( self . reward_model , \"reward_model\" ) # CRITICAL: Patch generate to ignore num_logits_to_keep self . _patch_model_generate_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_generate_for_compatibility ( self . ref_model , \"ref_model\" ) # Although ref_model usually doesn't generate # GLOBAL: Patch transformers validation as a last resort self . _patch_transformers_validation () except Exception as e : logger . error ( f \"\u274c Failed to setup model: { e } \" ) raise setup_rewards () \u00b6 Setup reward functions based on configuration mode. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions based on configuration mode.\"\"\" logger . info ( \"Setting up Unsloth PPO reward functions...\" ) # Determine mode has_pretrained = hasattr ( self , 'reward_model' ) and self . reward_model is not None has_reward_funcs = self . config . rewards and len ( self . config . rewards ) > 0 fine_tune_mode = ( has_pretrained and has_reward_funcs and hasattr ( self . config . model , 'reward_model_source' ) and self . config . model . reward_model_source and getattr ( self . config . model . reward_model_source , 'fine_tune_with_rewards' , False ) ) # Check if reward training is enabled should_train_model = ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ) # Mode 1: Pretrained only if has_pretrained and not fine_tune_mode : logger . info ( \"\u2705 Mode: Pretrained reward model (using as-is)\" ) logger . info ( \" Reward functions will be ignored\" ) self . reward_functions = [] return # Mode 2: Custom trained (handled in _load_reward_model_strict) if not has_pretrained and has_reward_funcs : if should_train_model : logger . info ( \"\u2705 Mode: Custom reward model training\" ) logger . info ( \" Will train new model using reward functions\" ) # Setup reward functions for training self . _setup_reward_functions () return else : # NEW: Use reward functions directly with FunctionBasedRewardModel logger . info ( \"\u2705 Mode: Direct reward function usage\" ) logger . info ( \" Reward functions will be wrapped in FunctionBasedRewardModel\" ) self . _setup_reward_functions () return # Mode 3: Hybrid (fine-tune) if fine_tune_mode : logger . info ( \"\u2705 Mode: Hybrid (fine-tune pretrained with reward functions)\" ) self . _setup_reward_functions () self . _fine_tune_reward_model () return # No rewards configured logger . info ( \"No reward model or functions configured\" ) self . reward_functions = [] setup_trainer () \u00b6 Setup TRL PPOTrainer. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 def setup_trainer ( self ) -> None : \"\"\"Setup TRL PPOTrainer.\"\"\" # Set global reference for patched_get_reward global GLOBAL_UNSLOTH_TRAINER_REF GLOBAL_UNSLOTH_TRAINER_REF = self try : from trl import PPOTrainer , PPOConfig logger . info ( \"Setting up PPOTrainer\" ) # Logging configuration with defaults output_dir = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) run_name = getattr ( self . config . logging , 'run_name' , None ) or 'ppo_experiment' report_to = getattr ( self . config . logging , 'loggers' , 'none' ) # Training parameters from config with defaults num_epochs = getattr ( self . config . train , 'epochs' , None ) or 1 batch_size = getattr ( self . config . train , 'per_device_batch_size' , 1 ) grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 32 ) lr = getattr ( self . config . train , 'learning_rate' , 1e-6 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.05 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) cliprange_value = getattr ( self . config . train , 'cliprange_value' , 0.2 ) vf_coef = getattr ( self . config . train , 'vf_coef' , 0.1 ) gamma = getattr ( self . config . train , 'gamma' , 1.0 ) lam = getattr ( self . config . train , 'lam' , 0.95 ) max_grad_norm = getattr ( self . config . train , 'max_grad_norm' , 0.5 ) num_ppo_epochs = getattr ( self . config . train , 'num_ppo_epochs' , None ) or 4 whiten_rewards = getattr ( self . config . train , 'whiten_rewards' , False ) response_length = getattr ( self . config . train , 'response_length' , 53 ) stop_token = getattr ( self . config . train , 'stop_token' , 'eos' ) missing_eos_penalty = getattr ( self . config . train , 'missing_eos_penalty' , 1.0 ) save_steps = getattr ( self . config . train , 'save_steps' , 500 ) save_strategy = getattr ( self . config . train , 'save_strategy' , 'steps' ) save_total_limit = getattr ( self . config . train , 'save_total_limit' , 5 ) eval_strategy = getattr ( self . config . train , 'eval_strategy' , 'steps' ) eval_steps = getattr ( self . config . train , 'eval_steps' , None ) or 100 logging_steps = getattr ( self . config . train , 'logging_steps' , 10 ) gradient_checkpointing = getattr ( self . config . model , 'gradient_checkpointing' , False ) gradient_checkpointing_kwargs = getattr ( self . config . train , 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }) # Seed with default seed = getattr ( self . config . distributed , 'seed' , 42 ) # Precision handling - direct string comparison with default precision_str = getattr ( self . config . model , 'precision' , 'auto' ) if hasattr ( precision_str , 'value' ): # Handle enum precision_str = precision_str . value bf16 = precision_str in [ 'bf16' , 'auto' ] fp16 = precision_str == 'fp16' # Calculate total episodes total_episodes = len ( self . train_dataset ) * num_epochs # Create PPOConfig ppo_config = PPOConfig ( exp_name = run_name , learning_rate = lr , batch_size = batch_size , mini_batch_size = batch_size , gradient_accumulation_steps = grad_accum , total_episodes = total_episodes , num_ppo_epochs = num_ppo_epochs , max_grad_norm = max_grad_norm , seed = seed , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , kl_coef = kl_coef , whiten_rewards = whiten_rewards , gamma = gamma , lam = lam , response_length = response_length , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , local_rollout_forward_batch_size = batch_size , output_dir = output_dir , save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , eval_strategy = eval_strategy , eval_steps = eval_steps , logging_steps = logging_steps , bf16 = bf16 , fp16 = fp16 , remove_unused_columns = False , run_name = run_name , gradient_checkpointing = False , report_to = report_to , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # DEBUG: Final dtype check before creating trainer if DEBUG : logger . info ( \"\ud83d\udd0d DEBUG: Final dtype check before PPOTrainer creation:\" ) try : if hasattr ( self , 'reward_model' ) and self . reward_model is not None : reward_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reward model final dtype: { reward_dtype } \" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model final dtype: { value_dtype } \" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model final dtype: { policy_dtype } \" ) # CRITICAL: Test a forward pass to check hidden states dtype logger . info ( \"\ud83d\udd0d DEBUG: Testing forward pass dtypes...\" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : test_input = torch . tensor ([[ 1 , 2 , 3 , 4 , 5 ]], dtype = torch . long ) with torch . no_grad (): try : outputs = self . policy_model ( test_input ) if hasattr ( outputs , 'hidden_states' ) and outputs . hidden_states : hidden_dtype = outputs . hidden_states [ - 1 ] . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model hidden states dtype: { hidden_dtype } \" ) else : logger . info ( f \"\ud83d\udd0d DEBUG: Policy model outputs type: { type ( outputs ) } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not test policy model forward pass: { e } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get final dtypes: { e } \" ) # Check if we should use function-based reward model if not hasattr ( self , 'reward_model' ) or self . reward_model is None : if hasattr ( self , 'reward_functions' ) and self . reward_functions and len ( self . reward_functions ) > 0 : # Use function-based reward model from aligntune.core.rl.function_based_reward_model import FunctionBasedRewardModel logger . info ( \"=\" * 60 ) logger . info ( \"\u2705 Using function-based reward model with reward functions\" ) logger . info ( f \" { len ( self . reward_functions ) } reward function(s) configured\" ) logger . info ( \"=\" * 60 ) # Get device and dtype from policy model device = next ( self . policy_model . parameters ()) . device dtype = next ( self . policy_model . parameters ()) . dtype self . reward_model = FunctionBasedRewardModel ( reward_functions = self . reward_functions , tokenizer = self . tokenizer , device = str ( device ), dtype = dtype ) self . reward_model = self . reward_model . to ( device ) logger . info ( \"\u2705 FunctionBasedRewardModel created and moved to device\" ) else : raise ValueError ( \"TRL PPOTrainer requires either: \\n \" \" - reward_model: A neural network reward model (pretrained or custom trained) \\n \" \" Examples: 'OpenAssistant/reward-model-deberta-v3-large-v2', 'Skywork/Skywork-Reward-V2-Qwen3-0.6B' \\n \" \" - reward_functions: Rule-based reward functions (will be wrapped in FunctionBasedRewardModel) \\n \" \" Example: [{'type': 'length', 'weight': 1.0}, {'type': 'sentiment', 'weight': 0.5}] \\n \" \" - reward_training.enabled=True: Train a custom reward model from reward functions \\n \" \"Please provide one of these options.\" ) else : # Neural reward model provided logger . info ( \"\u2705 Using neural network reward model\" ) logger . info ( f \" Reward model type: { type ( self . reward_model ) . __name__ } \" ) # Create trainer with strict generation limits self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = None , ) # Patch PolicyAndValueWrapper (keep this) self . _patch_policy_wrapper () logger . info ( \"PPOTrainer created successfully\" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise train () \u00b6 Execute PPO training. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" try : logger . info ( \"Starting Unsloth PPO training\" ) start_time = time . time () # Setup everything self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Train logger . info ( \"Running PPO training...\" ) training_result = self . trainer . train () try : generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , getattr ( self , 'reward_functions' , None ), stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Save output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) logger . info ( f \"Saving to: { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # TRL PPOTrainer.train() may return None (or an object without global_step). # Use training_history length as a robust fallback so examples don't report 0 steps. fallback_steps = 0 try : if hasattr ( self , \"training_history\" ) and self . training_history : fallback_steps = len ( self . training_history ) except Exception : fallback_steps = 0 results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else fallback_steps , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : 0 , \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Training completed in { training_time : .2f } s\" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise train_step ( batch ) \u00b6 Not used - TRL handles training loop. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2143 2144 2145 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Not used - TRL handles training loop.\"\"\" return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 UnslothGRPOTrainer \u00b6 Unsloth backend for Group Relative Policy Optimization. Bases: TrainerBase GRPO trainer using Unsloth's FastLanguageModel for optimized training. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 class UnslothGRPOTrainer ( TrainerBase ): \"\"\"GRPO trainer using Unsloth's FastLanguageModel for optimized training.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . training_history = [] self . logging_manager = None self . reward_functions = [] self . train_dataset = None self . eval_dataset = None self . reward_configs = [] self . custom_evaluator = None # self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import GRPOTrainer , GRPOConfig return True except ImportError : return False def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for GRPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): model_name = self . config . model . get ( 'name_or_path' ) max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) quantization = self . config . model . get ( 'quantization' , {}) else : model_name = self . config . model . name_or_path max_seq_length = self . config . model . max_seq_length quantization = getattr ( self . config . model , 'quantization' , {}) logger . info ( f \"Setting up Unsloth GRPO model: { model_name } \" ) # Configure Unsloth model parameters # Unsloth only accepts: max_seq_length, dtype, and load_in_4bit # Other quantization params are NOT passed to from_pretrained device_map = self . config . model . device_map or 'auto' model_kwargs = { \"max_seq_length\" : max_seq_length , # Auto-detect (Unsloth will use bfloat16 automatically) \"dtype\" : None , \"load_in_4bit\" : quantization . get ( \"load_in_4bit\" , True ) if isinstance ( quantization , dict ) else True , \"device_map\" : device_map , } logger . info ( f \"Loading model with kwargs: { model_kwargs } \" ) # Load model with Unsloth optimizations # NOTE: Unsloth handles all quantization internally - don't pass # bnb_4bit params self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = model_name , ** model_kwargs ) # Set pad token if not present if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Configure model for GRPO training with LoRA (use config values) # NOTE: AlignTune configs may be dataclasses or dicts depending on how the trainer was created. if isinstance ( self . config . model , dict ): use_peft = self . config . model . get ( \"use_peft\" , True ) lora_r = self . config . model . get ( \"lora_r\" , 16 ) lora_alpha = self . config . model . get ( \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = self . config . model . get ( \"lora_dropout\" , 0.0 ) target_modules = self . config . model . get ( \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = self . config . model . get ( \"gradient_checkpointing\" , True ) else : use_peft = getattr ( self . config . model , \"use_peft\" , True ) lora_r = getattr ( self . config . model , \"lora_r\" , 16 ) lora_alpha = getattr ( self . config . model , \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = getattr ( self . config . model , \"lora_dropout\" , 0.0 ) target_modules = getattr ( self . config . model , \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = getattr ( self . config . model , \"gradient_checkpointing\" , True ) if use_peft : self . model = FastLanguageModel . get_peft_model ( self . model , r = int ( lora_r ), target_modules = target_modules , lora_alpha = int ( lora_alpha ), lora_dropout = float ( lora_dropout ), bias = \"none\" , use_gradient_checkpointing = \"unsloth\" if gradient_checkpointing else False , random_state = 3407 , use_rslora = False , loftq_config = None , ) logger . info ( \"Unsloth GRPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth GRPO model: { e } \" ) raise def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) try : # \u2728 SPECIAL CASE: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"\u2713 Loaded custom reward function (weight: { weight } )\" ) # Store directly - no registry needed self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : \"custom\" }) continue # Skip registry lookup # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments from TRL including: - prompts: List of prompts - test_list: List of test cases (for code datasets like MBPP) - answer/solution/reference: Reference answers (for math datasets) \"\"\" if not completions : return [] # Debug: log what TRL is passing (once) if not hasattr ( self , '_logged_kwargs' ): logger . info ( f \"[DEBUG] _combined_reward_function kwargs keys: { list ( kwargs . keys ()) } \" ) if 'test_list' in kwargs : sample = kwargs [ 'test_list' ][: 1 ] if kwargs [ 'test_list' ] else 'empty' logger . info ( f \"[DEBUG] test_list found, sample: { sample } \" ) else : logger . info ( \"[DEBUG] test_list NOT in kwargs!\" ) self . _logged_kwargs = True batch_rewards = [] # Extract batch data from kwargs (TRL passes these as lists) test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # CHECK MULTIPLE POSSIBLE REFERENCE COLUMN NAMES # Try different column names in order of preference references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'response' , 'ground_truth' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] # print(f\"[INFO] Found reference data in column: '{ref_key}'\") break # If no reference column found, use None for all completions if references is None : references = [ None ] * len ( completions ) print ( f \"[WARNING] No reference column found. Checked: answer, solution, reference, ground_truth, target\" ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): # Try different calling patterns try : # Pattern 1: text + test_cases (for code execution) reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : # Pattern 2: Just text reward = reward_func ( completion ) except TypeError : try : # Pattern 3: text + reference reward = reward_func ( completion , reference = reference ) except TypeError : try : # Pattern 4: text + reference + context reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics with completions summary if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def setup_data ( self ) -> None : \"\"\"Setup datasets - compatible with base trainer interface.\"\"\" self . setup_dataset () def setup_dataset ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( self . config . train , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager # Columns to preserve for reward computation (test cases, solutions, # etc.) preserve_columns = { 'test_list' , 'test' , 'answer' , 'solution' , 'code' , 'canonical_solution' } manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset - DataManager handles all the complexity dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , max_samples = max_samples ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL GRPOTrainer with Unsloth model.\"\"\" try : from trl import GRPOTrainer , GRPOConfig logger . info ( \"Setting up TRL GRPOTrainer with Unsloth model\" ) # Handle both dict and object config if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' if isinstance ( self . config . train , dict ): num_epochs = self . config . train . get ( 'epochs' , 1 ) per_device_batch_size = self . config . train . get ( 'per_device_batch_size' , 4 ) gradient_accumulation_steps = self . config . train . get ( 'gradient_accumulation_steps' , 1 ) learning_rate = self . config . train . get ( 'learning_rate' , 2e-4 ) save_interval = self . config . train . get ( 'save_interval' , 500 ) eval_interval = self . config . train . get ( 'eval_interval' , 500 ) kl_coef = self . config . train . get ( 'kl_coef' , 0.1 ) cliprange = self . config . train . get ( 'cliprange' , 0.2 ) else : num_epochs = getattr ( self . config . train , 'epochs' , 1 ) per_device_batch_size = getattr ( self . config . train , 'per_device_batch_size' , 4 ) gradient_accumulation_steps = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) learning_rate = getattr ( self . config . train , 'learning_rate' , 2e-4 ) save_interval = getattr ( self . config . train , 'save_interval' , 500 ) eval_interval = getattr ( self . config . train , 'eval_interval' , 500 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.1 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) # Get max sequence length from model config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # CRITICAL: Calculate max_prompt_length and max_completion_length max_prompt_length = int ( max_seq_length * 0.6 ) # 60% for prompt max_completion_length = int ( max_seq_length * 0.4 ) # 40% for completion # Determine evaluation and save strategy based on eval_dataset has_eval = self . eval_dataset is not None and len ( self . eval_dataset ) > 0 # Set save strategy save_strategy = \"steps\" from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = None ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = False ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Run name run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_grpo' ) # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = True ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # GRPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = kl_coef ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = cliprange ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) scale_rewards = self . _get_config_value ( self . config . train , 'scale_rewards' , default = 'group' ) mask_truncated_completions = self . _get_config_value ( self . config . train , 'mask_truncated_completions' , default = True ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.9 ) num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = per_device_batch_size ) # TRL GRPOConfig supports GRPO variants: \"grpo\", \"dapo\", \"dr_grpo\" VALID_GRPO_LOSS_TYPES = { \"grpo\" , \"dapo\" , \"dr_grpo\" } if loss_type is None : loss_type = \"grpo\" else : loss_type = str ( loss_type ) . lower () . strip () # \"sigmoid\" is a DPO loss type, not valid for GRPO - raise error if loss_type == \"sigmoid\" : raise ValueError ( f \"loss_type='sigmoid' is a DPO/IPO loss type and is not valid for GRPO training. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } use grpo\" ) if loss_type not in VALID_GRPO_LOSS_TYPES : raise ValueError ( f \"Unknown loss_type=' { loss_type } ' for GRPO. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } \" ) # Adjust save strategy to save on epoch save_strategy = \"epoch\" if self . eval_dataset else \"steps\" # Create GRPO configuration grpo_config = GRPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , max_steps = max_steps , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_interval , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = 0.5 , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # GRPO specific parameters max_prompt_length = max_prompt_length , max_completion_length = max_completion_length , num_generations = num_generations , temperature = temperature , top_p = top_p , loss_type = loss_type , beta = beta , epsilon = epsilon , scale_rewards = scale_rewards , mask_truncated_completions = mask_truncated_completions , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , group_by_length = group_by_length , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Rest of your code stays the same... # Get max_seq_length for text truncation in reward functions # Use conservative limit: most reward models (e.g., toxic-bert) have max_length=512 # Truncate to min(512, model_max_seq_length) to prevent tensor # shape mismatches model_max_seq_length = 512 # Default if hasattr ( self . config , 'model' ): if isinstance ( self . config . model , dict ): model_max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : model_max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # Use conservative 512 limit for reward functions (most reward # models expect this) max_seq_length = min ( 512 , model_max_seq_length ) # # Create trainer with reward functions # self.trainer = GRPOTrainer( # model=self.model, # tokenizer=self.tokenizer, # train_dataset=self.train_dataset, # eval_dataset=self.eval_dataset, # args=grpo_config, # reward_funcs=self._combined_reward_function if self.reward_functions else None, # ) # # Create GRPO trainer use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if use_rewards_directly : # Use functions directly (no wrapper) reward_funcs = use_rewards_directly logger . info ( f \"\u2713 Using { len ( reward_funcs ) } reward functions DIRECTLY\" ) else : # Use combined wrapper (default) reward_funcs = self . _combined_reward_function logger . info ( \"\u2713 Using _combined_reward_function wrapper\" ) import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs if isinstance ( reward_funcs , list ) else [ reward_funcs ], ) else : try : import unsloth logger . info ( \"Detected Unsloth environment\" ) # Unsloth expects a list of functions if isinstance ( reward_funcs , list ): # Already a list (direct mode) logger . info ( f \"Passing { len ( reward_funcs ) } functions directly\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List of functions ) else : # Single function (wrapper mode) - wrap it for Unsloth # logger.info(\"Wrapping _combined_reward_function for Unsloth\") # def unsloth_reward_wrapper(prompts=None, completions=None, **kwargs): # if completions is None: # return [0.0] * (len(prompts) if prompts else 1) # return reward_funcs(completions, **kwargs) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List with wrapper ) except ImportError : logger . info ( \"Using pure TRL (no Unsloth)\" ) # Pure TRL can accept either format if isinstance ( reward_funcs , list ): # Direct mode: pass list as-is self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , ) else : # Wrapper mode: wrap in list self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ reward_funcs ], # Wrap single function in list ) logger . info ( \"GRPO trainer setup completed successfully!\" ) logger . info ( \"TRL GRPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup GRPO trainer: { e } \" ) raise def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\" Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. \"\"\" # This method is required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles the training loop internally logger . debug ( \"train_step() called but Unsloth GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\" Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. \"\"\" # This method might be required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles data loading internally logger . debug ( \"create_data_loader() called but Unsloth GRPO uses TRL's internal data loading\" ) return None def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training with Unsloth optimizations.\"\"\" try : logger . info ( \"Starting Unsloth GRPO training\" ) start_time = time . time () # Setup components self . setup_model () self . setup_data () use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if not use_rewards_directly : self . setup_rewards () else : logger . warning ( \"Skipping setup_rewards() - using reward functions directly from config\" ) self . setup_trainer () # Start training training_result = self . trainer . train () # Get output directory if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' # Save model self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Unsloth GRPO training completed in { training_time : .2f } seconds\" ) if hasattr ( training_result , 'training_loss' ): logger . info ( f \"Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"GRPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained GRPO model.\"\"\" # try: # if not self.eval_dataset: # logger.warning(\"No evaluation dataset available\") # return {} # logger.info(\"Evaluating Unsloth GRPO model\") # # Run evaluation # eval_results = self.trainer.evaluate() # logger.info(f\"GRPO evaluation results: {eval_results}\") # return eval_results # except Exception as e: # logger.error(f\"GRPO evaluation failed: {e}\") # raise def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = True , ** kwargs ) -> Dict [ str , float ]: \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # Auto-setup evaluators on first call if self . base_evaluator is None and self . rl_evaluator is None : logger . info ( \"Auto-initializing evaluators for first evaluation...\" ) self . setup_custom_evaluator ( evaluator_type = \"auto\" ) # Call parent's unified evaluate method return super () . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = metric_key_prefix , use_custom_evaluator = use_custom_evaluator , ** kwargs ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise create_data_loader () \u00b6 Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 924 925 926 927 928 929 930 931 932 933 934 935 936 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\" Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. \"\"\" # This method might be required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles data loading internally logger . debug ( \"create_data_loader() called but Unsloth GRPO uses TRL's internal data loading\" ) return None evaluate ( eval_dataset = None , metric_key_prefix = 'eval' , use_custom_evaluator = True , ** kwargs ) \u00b6 GRPO-specific evaluation - auto-setup evaluators and delegate to parent. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = True , ** kwargs ) -> Dict [ str , float ]: \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # Auto-setup evaluators on first call if self . base_evaluator is None and self . rl_evaluator is None : logger . info ( \"Auto-initializing evaluators for first evaluation...\" ) self . setup_custom_evaluator ( evaluator_type = \"auto\" ) # Call parent's unified evaluate method return super () . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = metric_key_prefix , use_custom_evaluator = use_custom_evaluator , ** kwargs ) is_available () classmethod \u00b6 Check if Unsloth and TRL are available. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 47 48 49 50 51 52 53 54 55 56 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import GRPOTrainer , GRPOConfig return True except ImportError : return False load_model ( path ) \u00b6 Load a trained GRPO model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise save_model ( path = None ) \u00b6 Save the trained GRPO model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise setup_data () \u00b6 Setup datasets - compatible with base trainer interface. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 443 444 445 def setup_data ( self ) -> None : \"\"\"Setup datasets - compatible with base trainer interface.\"\"\" self . setup_dataset () setup_dataset () \u00b6 Setup datasets for GRPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 def setup_dataset ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( self . config . train , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager # Columns to preserve for reward computation (test cases, solutions, # etc.) preserve_columns = { 'test_list' , 'test' , 'answer' , 'solution' , 'code' , 'canonical_solution' } manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset - DataManager handles all the complexity dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , max_samples = max_samples ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) setup_model () \u00b6 Setup Unsloth-optimized model and tokenizer for GRPO. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for GRPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): model_name = self . config . model . get ( 'name_or_path' ) max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) quantization = self . config . model . get ( 'quantization' , {}) else : model_name = self . config . model . name_or_path max_seq_length = self . config . model . max_seq_length quantization = getattr ( self . config . model , 'quantization' , {}) logger . info ( f \"Setting up Unsloth GRPO model: { model_name } \" ) # Configure Unsloth model parameters # Unsloth only accepts: max_seq_length, dtype, and load_in_4bit # Other quantization params are NOT passed to from_pretrained device_map = self . config . model . device_map or 'auto' model_kwargs = { \"max_seq_length\" : max_seq_length , # Auto-detect (Unsloth will use bfloat16 automatically) \"dtype\" : None , \"load_in_4bit\" : quantization . get ( \"load_in_4bit\" , True ) if isinstance ( quantization , dict ) else True , \"device_map\" : device_map , } logger . info ( f \"Loading model with kwargs: { model_kwargs } \" ) # Load model with Unsloth optimizations # NOTE: Unsloth handles all quantization internally - don't pass # bnb_4bit params self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = model_name , ** model_kwargs ) # Set pad token if not present if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Configure model for GRPO training with LoRA (use config values) # NOTE: AlignTune configs may be dataclasses or dicts depending on how the trainer was created. if isinstance ( self . config . model , dict ): use_peft = self . config . model . get ( \"use_peft\" , True ) lora_r = self . config . model . get ( \"lora_r\" , 16 ) lora_alpha = self . config . model . get ( \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = self . config . model . get ( \"lora_dropout\" , 0.0 ) target_modules = self . config . model . get ( \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = self . config . model . get ( \"gradient_checkpointing\" , True ) else : use_peft = getattr ( self . config . model , \"use_peft\" , True ) lora_r = getattr ( self . config . model , \"lora_r\" , 16 ) lora_alpha = getattr ( self . config . model , \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = getattr ( self . config . model , \"lora_dropout\" , 0.0 ) target_modules = getattr ( self . config . model , \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = getattr ( self . config . model , \"gradient_checkpointing\" , True ) if use_peft : self . model = FastLanguageModel . get_peft_model ( self . model , r = int ( lora_r ), target_modules = target_modules , lora_alpha = int ( lora_alpha ), lora_dropout = float ( lora_dropout ), bias = \"none\" , use_gradient_checkpointing = \"unsloth\" if gradient_checkpointing else False , random_state = 3407 , use_rslora = False , loftq_config = None , ) logger . info ( \"Unsloth GRPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth GRPO model: { e } \" ) raise setup_rewards () \u00b6 Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) try : # \u2728 SPECIAL CASE: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"\u2713 Loaded custom reward function (weight: { weight } )\" ) # Store directly - no registry needed self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : \"custom\" }) continue # Skip registry lookup # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) setup_trainer () \u00b6 Setup TRL GRPOTrainer with Unsloth model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 def setup_trainer ( self ) -> None : \"\"\"Setup TRL GRPOTrainer with Unsloth model.\"\"\" try : from trl import GRPOTrainer , GRPOConfig logger . info ( \"Setting up TRL GRPOTrainer with Unsloth model\" ) # Handle both dict and object config if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' if isinstance ( self . config . train , dict ): num_epochs = self . config . train . get ( 'epochs' , 1 ) per_device_batch_size = self . config . train . get ( 'per_device_batch_size' , 4 ) gradient_accumulation_steps = self . config . train . get ( 'gradient_accumulation_steps' , 1 ) learning_rate = self . config . train . get ( 'learning_rate' , 2e-4 ) save_interval = self . config . train . get ( 'save_interval' , 500 ) eval_interval = self . config . train . get ( 'eval_interval' , 500 ) kl_coef = self . config . train . get ( 'kl_coef' , 0.1 ) cliprange = self . config . train . get ( 'cliprange' , 0.2 ) else : num_epochs = getattr ( self . config . train , 'epochs' , 1 ) per_device_batch_size = getattr ( self . config . train , 'per_device_batch_size' , 4 ) gradient_accumulation_steps = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) learning_rate = getattr ( self . config . train , 'learning_rate' , 2e-4 ) save_interval = getattr ( self . config . train , 'save_interval' , 500 ) eval_interval = getattr ( self . config . train , 'eval_interval' , 500 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.1 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) # Get max sequence length from model config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # CRITICAL: Calculate max_prompt_length and max_completion_length max_prompt_length = int ( max_seq_length * 0.6 ) # 60% for prompt max_completion_length = int ( max_seq_length * 0.4 ) # 40% for completion # Determine evaluation and save strategy based on eval_dataset has_eval = self . eval_dataset is not None and len ( self . eval_dataset ) > 0 # Set save strategy save_strategy = \"steps\" from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = None ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = False ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Run name run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_grpo' ) # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = True ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # GRPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = kl_coef ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = cliprange ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) scale_rewards = self . _get_config_value ( self . config . train , 'scale_rewards' , default = 'group' ) mask_truncated_completions = self . _get_config_value ( self . config . train , 'mask_truncated_completions' , default = True ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.9 ) num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = per_device_batch_size ) # TRL GRPOConfig supports GRPO variants: \"grpo\", \"dapo\", \"dr_grpo\" VALID_GRPO_LOSS_TYPES = { \"grpo\" , \"dapo\" , \"dr_grpo\" } if loss_type is None : loss_type = \"grpo\" else : loss_type = str ( loss_type ) . lower () . strip () # \"sigmoid\" is a DPO loss type, not valid for GRPO - raise error if loss_type == \"sigmoid\" : raise ValueError ( f \"loss_type='sigmoid' is a DPO/IPO loss type and is not valid for GRPO training. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } use grpo\" ) if loss_type not in VALID_GRPO_LOSS_TYPES : raise ValueError ( f \"Unknown loss_type=' { loss_type } ' for GRPO. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } \" ) # Adjust save strategy to save on epoch save_strategy = \"epoch\" if self . eval_dataset else \"steps\" # Create GRPO configuration grpo_config = GRPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , max_steps = max_steps , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_interval , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = 0.5 , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # GRPO specific parameters max_prompt_length = max_prompt_length , max_completion_length = max_completion_length , num_generations = num_generations , temperature = temperature , top_p = top_p , loss_type = loss_type , beta = beta , epsilon = epsilon , scale_rewards = scale_rewards , mask_truncated_completions = mask_truncated_completions , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , group_by_length = group_by_length , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Rest of your code stays the same... # Get max_seq_length for text truncation in reward functions # Use conservative limit: most reward models (e.g., toxic-bert) have max_length=512 # Truncate to min(512, model_max_seq_length) to prevent tensor # shape mismatches model_max_seq_length = 512 # Default if hasattr ( self . config , 'model' ): if isinstance ( self . config . model , dict ): model_max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : model_max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # Use conservative 512 limit for reward functions (most reward # models expect this) max_seq_length = min ( 512 , model_max_seq_length ) # # Create trainer with reward functions # self.trainer = GRPOTrainer( # model=self.model, # tokenizer=self.tokenizer, # train_dataset=self.train_dataset, # eval_dataset=self.eval_dataset, # args=grpo_config, # reward_funcs=self._combined_reward_function if self.reward_functions else None, # ) # # Create GRPO trainer use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if use_rewards_directly : # Use functions directly (no wrapper) reward_funcs = use_rewards_directly logger . info ( f \"\u2713 Using { len ( reward_funcs ) } reward functions DIRECTLY\" ) else : # Use combined wrapper (default) reward_funcs = self . _combined_reward_function logger . info ( \"\u2713 Using _combined_reward_function wrapper\" ) import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs if isinstance ( reward_funcs , list ) else [ reward_funcs ], ) else : try : import unsloth logger . info ( \"Detected Unsloth environment\" ) # Unsloth expects a list of functions if isinstance ( reward_funcs , list ): # Already a list (direct mode) logger . info ( f \"Passing { len ( reward_funcs ) } functions directly\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List of functions ) else : # Single function (wrapper mode) - wrap it for Unsloth # logger.info(\"Wrapping _combined_reward_function for Unsloth\") # def unsloth_reward_wrapper(prompts=None, completions=None, **kwargs): # if completions is None: # return [0.0] * (len(prompts) if prompts else 1) # return reward_funcs(completions, **kwargs) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List with wrapper ) except ImportError : logger . info ( \"Using pure TRL (no Unsloth)\" ) # Pure TRL can accept either format if isinstance ( reward_funcs , list ): # Direct mode: pass list as-is self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , ) else : # Wrapper mode: wrap in list self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ reward_funcs ], # Wrap single function in list ) logger . info ( \"GRPO trainer setup completed successfully!\" ) logger . info ( \"TRL GRPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup GRPO trainer: { e } \" ) raise train () \u00b6 Execute GRPO training with Unsloth optimizations. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training with Unsloth optimizations.\"\"\" try : logger . info ( \"Starting Unsloth GRPO training\" ) start_time = time . time () # Setup components self . setup_model () self . setup_data () use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if not use_rewards_directly : self . setup_rewards () else : logger . warning ( \"Skipping setup_rewards() - using reward functions directly from config\" ) self . setup_trainer () # Start training training_result = self . trainer . train () # Get output directory if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' # Save model self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Unsloth GRPO training completed in { training_time : .2f } seconds\" ) if hasattr ( training_result , 'training_loss' ): logger . info ( f \"Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"GRPO training failed: { e } \" ) raise train_step ( batch ) \u00b6 Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 910 911 912 913 914 915 916 917 918 919 920 921 922 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\" Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. \"\"\" # This method is required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles the training loop internally logger . debug ( \"train_step() called but Unsloth GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Specialized Trainers \u00b6 ClassificationTrainer \u00b6 Specialized trainer for text classification tasks. Complete Classification Trainer for text and token classification. Supports: - Text Classification (sentiment, topic classification, etc.) - Token Classification (NER, POS tagging, etc.) Uses standard Transformers Trainer (NOT TRL) for proper classification. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 class ClassificationTrainer : \"\"\" Complete Classification Trainer for text and token classification. Supports: - Text Classification (sentiment, topic classification, etc.) - Token Classification (NER, POS tagging, etc.) Uses standard Transformers Trainer (NOT TRL) for proper classification. \"\"\" def __init__ ( self , config ): \"\"\"Initialize trainer with config.\"\"\" self . config = config self . model = None self . tokenizer = None self . train_dataset = None self . eval_dataset = None self . trainer = None self . task_type = config . dataset . task_type logger . info ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) print ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) def setup_model ( self ): \"\"\"Load model and tokenizer based on task type.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading model: { self . config . model . name_or_path } \" ) logger . info ( f \"Loading model: { self . config . model . name_or_path } \" ) # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , use_fast = True ) # Add padding token if missing if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Ensure pad_token_id is set (required for data collator batching) # For Llama and similar tokenizers, we need to set it on both tokenizer and config pad_token_id = None if hasattr ( self . tokenizer , 'pad_token_id' ): pad_token_id = self . tokenizer . pad_token_id if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : # If pad_token was added, get its ID from the tokenizer try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id from tokenizer: { e } \" ) # Set pad_token_id on tokenizer (multiple ways to ensure it sticks) if pad_token_id is not None : # Set directly on tokenizer if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id # Also set on tokenizer's special_tokens_map if it exists if hasattr ( self . tokenizer , 'special_tokens_map' ): if 'pad_token' not in self . tokenizer . special_tokens_map : self . tokenizer . special_tokens_map [ 'pad_token' ] = self . tokenizer . pad_token # Set on tokenizer config if it exists if hasattr ( self . tokenizer , 'init_kwargs' ) and 'pad_token_id' in self . tokenizer . init_kwargs : self . tokenizer . init_kwargs [ 'pad_token_id' ] = pad_token_id # Also try setting via __setattr__ to ensure it persists try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : logger . error ( \"pad_token_id is still None after setup! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Determine precision and quantization settings use_8bit = False torch_dtype = None device_map = \"cpu\" # Keep on CPU initially # Check if precision is set and configure accordingly if hasattr ( self . config . model , 'precision' ) and self . config . model . precision : precision_value = self . config . model . precision . value if hasattr ( self . config . model . precision , 'value' ) else str ( self . config . model . precision ) if precision_value == \"bf16\" and torch . cuda . is_available () and torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 elif precision_value == \"fp16\" : torch_dtype = torch . float16 # For 4B+ models, use lower precision dtype to reduce memory # NOTE: 8-bit quantization (BitsAndBytes) requires PEFT adapters for fine-tuning # Since we're not using PEFT for classification, we skip 8-bit quantization # and rely on BF16/FP16 precision instead model_size_estimate = \"4B\" if \"4B\" in self . config . model . name_or_path or \"4b\" in self . config . model . name_or_path . lower () else \"unknown\" use_8bit = False # Disable 8-bit quantization for classification (requires PEFT) quantization_config = None # Ensure we use lower precision dtype for 4B models if torch_dtype is None and torch . cuda . is_available (): # Default to BF16 if supported, else FP16 if torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 logger . info ( \"Using BF16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) else : torch_dtype = torch . float16 logger . info ( \"Using FP16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) # Load appropriate model based on task # Load on CPU first with appropriate precision/quantization to avoid OOM model_kwargs = { \"num_labels\" : self . config . model . num_labels , \"ignore_mismatched_sizes\" : True , \"device_map\" : device_map , # Keep on CPU \"torch_dtype\" : torch_dtype if torch_dtype else None } # Add quantization config if using 8-bit if use_8bit and quantization_config : model_kwargs [ \"quantization_config\" ] = quantization_config # Remove None values model_kwargs = { k : v for k , v in model_kwargs . items () if v is not None } if self . task_type == TaskType . TEXT_CLASSIFICATION : self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForSequenceClassification with { self . config . model . num_labels } labels\" ) else : # TOKEN_CLASSIFICATION self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForTokenClassification with { self . config . model . num_labels } labels\" ) # Ensure model stays on CPU (device_map should handle this, but double-check) if self . model is not None and not use_8bit : # 8-bit models handle device_map automatically try : # Only move to CPU if not already there and not using device_map current_device = next ( self . model . parameters ()) . device if current_device . type != \"cpu\" : self . model = self . model . to ( \"cpu\" ) logger . info ( \"Model moved to CPU to avoid OOM during setup\" ) else : logger . info ( \"Model already on CPU (via device_map)\" ) except Exception as move_error : logger . warning ( f \"Could not verify/move model to CPU: { move_error } \" ) # Resize token embeddings if we added tokens if len ( self . tokenizer ) != self . model . config . vocab_size : self . model . resize_token_embeddings ( len ( self . tokenizer )) # CRITICAL: Set pad_token_id on model config (required for DataCollatorWithPadding) # This is often the missing piece that causes \"Cannot handle batch sizes > 1\" errors pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is not None and hasattr ( self . model , 'config' ): if hasattr ( self . model . config , 'pad_token_id' ): self . model . config . pad_token_id = pad_token_id # Also try setting on model.config if it's a dict-like object try : setattr ( self . model . config , 'pad_token_id' , pad_token_id ) except : pass logger . info ( f \"Set model.config.pad_token_id = { pad_token_id } \" ) # Enable gradient checkpointing to reduce memory usage (helps with OOM) if hasattr ( self . model , 'gradient_checkpointing_enable' ): try : self . model . gradient_checkpointing_enable () logger . info ( \"Gradient checkpointing enabled to reduce memory usage\" ) except Exception as e : logger . warning ( f \"Could not enable gradient checkpointing: { e } \" ) # Apply PEFT (LoRA) if enabled if self . config . model . peft_enabled : from peft import LoraConfig , get_peft_model # Get target modules from config or auto-detect target_modules = self . config . model . target_modules if target_modules is None : # Auto-detect based on model architecture if \"distilbert\" in self . config . model . name_or_path . lower (): target_modules = [ \"q_lin\" , \"k_lin\" , \"v_lin\" , \"out_lin\" ] elif \"bert\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] elif \"roberta\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] else : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] logger . info ( f \"Auto-detected target modules: { target_modules } \" ) # Create PEFT config peft_config = LoraConfig ( r = self . config . model . lora_rank , lora_alpha = self . config . model . lora_alpha , lora_dropout = self . config . model . lora_dropout , bias = self . config . model . bias , task_type = \"SEQ_CLS\" if self . task_type == TaskType . TEXT_CLASSIFICATION else \"TOKEN_CLS\" , target_modules = target_modules ) # Apply PEFT self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () print ( f \"\u2713 Model loaded successfully\" ) def setup_data ( self ): \"\"\"Load and prepare dataset.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading dataset: { self . config . dataset . name } \" ) logger . info ( f \"Loading dataset: { self . config . dataset . name } \" ) # Load dataset with trust_remote_code for compatibility try : if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split , trust_remote_code = True ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split , trust_remote_code = True ) except Exception as e : logger . error ( f \"Failed to load dataset with trust_remote_code: { e } \" ) # Fallback without trust_remote_code if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split ) # Limit samples if specified if self . config . dataset . max_samples : max_samples = min ( self . config . dataset . max_samples , len ( dataset )) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Limited dataset to { max_samples } samples\" ) # Apply column mapping if provided (must be done before splitting) if self . config . dataset . column_mapping : logger . info ( f \"Applying column mapping: { self . config . dataset . column_mapping } \" ) # Rename columns: old_name -> new_name dataset = dataset . rename_columns ( self . config . dataset . column_mapping ) logger . info ( f \"Renamed columns: { self . config . dataset . column_mapping } \" ) # Split into train/eval (90/10) split = dataset . train_test_split ( test_size = 0.1 , seed = 42 ) train_dataset = split [ 'train' ] eval_dataset = split [ 'test' ] # Tokenize based on task type if self . task_type == TaskType . TEXT_CLASSIFICATION : def tokenize_function ( examples ): \"\"\"Tokenize text classification data.\"\"\" result = self . tokenizer ( examples [ self . config . dataset . text_column ], truncation = True , padding = False , max_length = self . config . model . max_seq_length ) # Get labels and convert to 0-indexed if needed (PyTorch expects 0-indexed) labels = examples [ self . config . dataset . label_column ] if isinstance ( labels , list ) and len ( labels ) > 0 : min_label = min ( labels ) if min_label > 0 : # Labels are 1-indexed, convert to 0-indexed labels = [ l - 1 for l in labels ] result [ 'labels' ] = labels return result else : # TOKEN_CLASSIFICATION def tokenize_function ( examples ): \"\"\"Tokenize token classification data with label alignment.\"\"\" tokenized = self . tokenizer ( examples [ self . config . dataset . tokens_column ], truncation = True , is_split_into_words = True , padding = False , max_length = self . config . model . max_seq_length ) # Align labels with tokens labels = [] for i , label in enumerate ( examples [ self . config . dataset . tags_column ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : # Special tokens get -100 label_ids . append ( - 100 ) elif word_idx != previous_word_idx : # First token of a word gets the label label_ids . append ( label [ word_idx ]) else : # Other tokens of the same word get -100 label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ 'labels' ] = labels return tokenized # Apply tokenization self . train_dataset = train_dataset . map ( tokenize_function , batched = True , remove_columns = train_dataset . column_names , desc = \"Tokenizing train dataset\" ) self . eval_dataset = eval_dataset . map ( tokenize_function , batched = True , remove_columns = eval_dataset . column_names , desc = \"Tokenizing eval dataset\" ) print ( f \"\u2713 Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) logger . info ( f \"Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) def setup_training ( self ): \"\"\"Setup trainer with appropriate settings.\"\"\" from aligntune.core.sft.config import TaskType print ( \" \\n Configuring trainer...\" ) logger . info ( \"Configuring trainer...\" ) # Clear GPU cache before creating trainer to avoid OOM errors try : if torch . cuda . is_available (): torch . cuda . empty_cache () except Exception as cache_error : logger . warning ( f \"Could not clear GPU cache: { cache_error } \" ) # Training arguments # For quick testing: use max_steps=10, then revert to full training max_steps_for_testing = None # Set to None for full training, or 10 for quick testing training_args_dict = { \"output_dir\" : self . config . logging . output_dir , \"run_name\" : self . config . logging . run_name , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size , \"per_device_eval_batch_size\" : self . config . train . per_device_batch_size , \"learning_rate\" : self . config . train . learning_rate , \"weight_decay\" : self . config . train . weight_decay , \"warmup_ratio\" : self . config . train . warmup_ratio , \"eval_strategy\" : \"steps\" , \"eval_steps\" : self . config . train . eval_interval , \"save_strategy\" : \"steps\" , \"save_steps\" : self . config . train . save_interval , \"load_best_model_at_end\" : self . config . train . load_best_model_at_end , \"metric_for_best_model\" : self . config . train . metric_for_best_model , \"greater_is_better\" : self . config . train . greater_is_better , \"logging_steps\" : 10 , \"logging_dir\" : f \" { self . config . logging . output_dir } /logs\" , # Use precision from model config, default to BF16 if CUDA supports it (for memory efficiency) \"fp16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"fp16\" ) if hasattr ( self . config . model , 'precision' ) else False , \"bf16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"bf16\" ) if hasattr ( self . config . model , 'precision' ) else ( torch . cuda . is_available () and torch . cuda . is_bf16_supported ()), # Auto-enable BF16 if supported \"report_to\" : \"none\" , # Disable wandb/tensorboard \"save_total_limit\" : 2 , # Keep only 2 checkpoints \"push_to_hub\" : False } # Add gradient_accumulation_steps if specified in config (helps with OOM) if hasattr ( self . config . train , 'gradient_accumulation_steps' ) and self . config . train . gradient_accumulation_steps is not None : training_args_dict [ \"gradient_accumulation_steps\" ] = self . config . train . gradient_accumulation_steps # Set either max_steps (for quick testing) or num_train_epochs (for full training) # Don't pass num_train_epochs at all when max_steps is set (TrainingArguments doesn't like both) if max_steps_for_testing is not None : training_args_dict [ \"max_steps\" ] = max_steps_for_testing # Don't include num_train_epochs when max_steps is set else : training_args_dict [ \"num_train_epochs\" ] = self . config . train . epochs # Don't include max_steps when num_train_epochs is set training_args = TrainingArguments ( ** training_args_dict ) # Ensure pad_token_id is set before creating data collator (required for batching) if self . tokenizer is not None : if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id != self . tokenizer . unk_token_id : # Make sure it's not the unk token self . tokenizer . pad_token_id = pad_token_id else : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not set pad_token_id: { e } \" ) # Final check: ensure pad_token_id is set (critical for batching) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : logger . error ( \"pad_token_id is still None before creating data collator! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Choose data collator based on task # Explicitly pass pad_token_id to ensure it's used pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : raise ValueError ( \"pad_token_id must be set before creating data collator\" ) # Create a wrapper class to ensure pad_token_id is always set before batching class SafeDataCollatorWithPadding ( DataCollatorWithPadding ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : # Try to set it from eos_token_id if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # If still None, try to get it from pad_token if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) class SafeDataCollatorForTokenClassification ( DataCollatorForTokenClassification ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) if self . task_type == TaskType . TEXT_CLASSIFICATION : data_collator = SafeDataCollatorWithPadding ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 # Optimize for tensor cores ) else : # TOKEN_CLASSIFICATION data_collator = SafeDataCollatorForTokenClassification ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 ) # Compute metrics function def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics.\"\"\" predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) # For token classification, flatten and filter if self . task_type == TaskType . TOKEN_CLASSIFICATION : predictions = predictions . flatten () labels = labels . flatten () # Remove ignored indices (-100) mask = labels != - 100 predictions = predictions [ mask ] labels = labels [ mask ] # Calculate metrics return { 'accuracy' : accuracy_score ( labels , predictions ), 'f1' : f1_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'precision' : precision_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'recall' : recall_score ( labels , predictions , average = 'weighted' , zero_division = 0 ) } # Clear CUDA cache before creating trainer if torch . cuda . is_available (): torch . cuda . empty_cache () # Create trainer with error handling for OOM try : self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , data_collator = data_collator , compute_metrics = compute_metrics , tokenizer = self . tokenizer ) except ( torch . cuda . OutOfMemoryError , RuntimeError ) as oom_error : # Check if it's actually an OOM error is_oom = isinstance ( oom_error , torch . cuda . OutOfMemoryError ) or \"out of memory\" in str ( oom_error ) . lower () if is_oom : # Clear cache and try to provide helpful error message if torch . cuda . is_available (): torch . cuda . empty_cache () error_msg = ( f \"CUDA out of memory when creating trainer. \" f \"This usually means other processes are using GPU memory. \" f \"Try: (1) Free GPU memory by stopping other processes, \" f \"(2) Reduce batch_size in config, (3) Use a smaller model. \" f \"Original error: { str ( oom_error )[: 200 ] } \" ) logger . error ( error_msg ) raise RuntimeError ( error_msg ) from oom_error else : # Re-raise if it's not an OOM error raise print ( \"\u2713 Trainer configured\" ) logger . info ( \"Trainer configured successfully\" ) def train ( self ): \"\"\"Train the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"TRAINING\" ) print ( \"=\" * 80 ) self . setup_model () self . setup_data () self . setup_training () # Clear CUDA cache before training to avoid OOM if torch . cuda . is_available (): torch . cuda . empty_cache () logger . info ( \"Cleared CUDA cache before training\" ) logger . info ( \"Starting training...\" ) try : train_result = self . trainer . train () except RuntimeError as e : if \"out of memory\" in str ( e ) . lower () or \"OOM\" in str ( e ): if torch . cuda . is_available (): torch . cuda . empty_cache () logger . error ( f \"CUDA OOM during training: { e } \" ) print ( f \" \\n \u274c CUDA Out of Memory error during training.\" ) print ( \" Suggestions:\" ) print ( \" - Reduce batch_size in your config\" ) print ( \" - Increase gradient_accumulation_steps to maintain effective batch size\" ) print ( \" - Reduce max_seq_length\" ) print ( \" - Use a smaller model\" ) raise else : raise print ( \" \\n \u2713 Training completed\" ) logger . info ( \"Training completed\" ) return train_result def evaluate ( self ): \"\"\"Evaluate the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"EVALUATION\" ) print ( \"=\" * 80 ) # Ensure pad_token is set before evaluation (required for batching) if self . tokenizer is not None and self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Also ensure pad_token_id is set (critical for batching) if self . tokenizer is not None : pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id in evaluate(): { e } \" ) # Set pad_token_id if we found one if pad_token_id is not None : if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : error_msg = \"pad_token_id is still None in evaluate()! Cannot handle batch sizes > 1 without padding token.\" logger . error ( error_msg ) raise ValueError ( error_msg ) logger . info ( \"Starting evaluation...\" ) eval_result = self . trainer . evaluate () # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_result : try : eval_result [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_result [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) print ( \" \\n Evaluation Results:\" ) for key , value in eval_result . items (): if isinstance ( value , float ): print ( f \" { key } : { value : .4f } \" ) else : print ( f \" { key } : { value } \" ) logger . info ( f \"Evaluation completed: { len ( eval_result ) } metrics computed\" ) return eval_result def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : save_path = f \" { self . config . logging . output_dir } / { self . config . logging . run_name } /final_model\" else : save_path = output_dir print ( f \" \\n Saving model to { save_path } ...\" ) logger . info ( f \"Saving model to { save_path } \" ) self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) print ( f \"\u2713 Model saved\" ) logger . info ( \"Model saved successfully\" ) # Store for push_to_hub self . _last_save_path = save_path return save_path def run_experiment ( self ): \"\"\" Run complete training experiment. This is the main method that backend_factory calls! \"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( f \"CLASSIFICATION TRAINING - { self . task_type . value . upper () } \" ) print ( \"=\" * 80 ) logger . info ( f \"Starting classification experiment: { self . task_type . value } \" ) try : # Setup self . setup_model () self . setup_data () self . setup_training () # Train train_result = self . train () # Evaluate eval_result = self . evaluate () # Save model_path = self . save_model () # Return complete results results = { 'task_type' : self . task_type . value , 'train_loss' : train_result . training_loss , 'eval_results' : eval_result , 'model_path' : model_path , 'steps' : train_result . global_step } print ( \" \\n \" + \"=\" * 80 ) print ( \"\u2705 EXPERIMENT COMPLETE\" ) print ( \"=\" * 80 ) print ( f \" Task: { results [ 'task_type' ] } \" ) print ( f \" Train Loss: { results [ 'train_loss' ] : .4f } \" ) print ( f \" Eval Accuracy: { eval_result . get ( 'eval_accuracy' , 0 ) : .4f } \" ) print ( f \" Eval F1: { eval_result . get ( 'eval_f1' , 0 ) : .4f } \" ) print ( f \" Model: { model_path } \" ) print ( \"=\" * 80 ) logger . info ( f \"Experiment completed successfully: { results } \" ) return results except Exception as e : logger . error ( f \"Experiment failed: { e } \" , exc_info = True ) raise __init__ ( config ) \u00b6 Initialize trainer with config. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , config ): \"\"\"Initialize trainer with config.\"\"\" self . config = config self . model = None self . tokenizer = None self . train_dataset = None self . eval_dataset = None self . trainer = None self . task_type = config . dataset . task_type logger . info ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) print ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) evaluate () \u00b6 Evaluate the model. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def evaluate ( self ): \"\"\"Evaluate the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"EVALUATION\" ) print ( \"=\" * 80 ) # Ensure pad_token is set before evaluation (required for batching) if self . tokenizer is not None and self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Also ensure pad_token_id is set (critical for batching) if self . tokenizer is not None : pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id in evaluate(): { e } \" ) # Set pad_token_id if we found one if pad_token_id is not None : if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : error_msg = \"pad_token_id is still None in evaluate()! Cannot handle batch sizes > 1 without padding token.\" logger . error ( error_msg ) raise ValueError ( error_msg ) logger . info ( \"Starting evaluation...\" ) eval_result = self . trainer . evaluate () # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_result : try : eval_result [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_result [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) print ( \" \\n Evaluation Results:\" ) for key , value in eval_result . items (): if isinstance ( value , float ): print ( f \" { key } : { value : .4f } \" ) else : print ( f \" { key } : { value } \" ) logger . info ( f \"Evaluation completed: { len ( eval_result ) } metrics computed\" ) return eval_result run_experiment () \u00b6 Run complete training experiment. This is the main method that backend_factory calls! Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 def run_experiment ( self ): \"\"\" Run complete training experiment. This is the main method that backend_factory calls! \"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( f \"CLASSIFICATION TRAINING - { self . task_type . value . upper () } \" ) print ( \"=\" * 80 ) logger . info ( f \"Starting classification experiment: { self . task_type . value } \" ) try : # Setup self . setup_model () self . setup_data () self . setup_training () # Train train_result = self . train () # Evaluate eval_result = self . evaluate () # Save model_path = self . save_model () # Return complete results results = { 'task_type' : self . task_type . value , 'train_loss' : train_result . training_loss , 'eval_results' : eval_result , 'model_path' : model_path , 'steps' : train_result . global_step } print ( \" \\n \" + \"=\" * 80 ) print ( \"\u2705 EXPERIMENT COMPLETE\" ) print ( \"=\" * 80 ) print ( f \" Task: { results [ 'task_type' ] } \" ) print ( f \" Train Loss: { results [ 'train_loss' ] : .4f } \" ) print ( f \" Eval Accuracy: { eval_result . get ( 'eval_accuracy' , 0 ) : .4f } \" ) print ( f \" Eval F1: { eval_result . get ( 'eval_f1' , 0 ) : .4f } \" ) print ( f \" Model: { model_path } \" ) print ( \"=\" * 80 ) logger . info ( f \"Experiment completed successfully: { results } \" ) return results except Exception as e : logger . error ( f \"Experiment failed: { e } \" , exc_info = True ) raise save_model ( output_dir = None ) \u00b6 Save the trained model. Parameters: Name Type Description Default output_dir Optional [ str ] Directory to save to (defaults to config output_dir) None Returns: Type Description str Path where model was saved Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : save_path = f \" { self . config . logging . output_dir } / { self . config . logging . run_name } /final_model\" else : save_path = output_dir print ( f \" \\n Saving model to { save_path } ...\" ) logger . info ( f \"Saving model to { save_path } \" ) self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) print ( f \"\u2713 Model saved\" ) logger . info ( \"Model saved successfully\" ) # Store for push_to_hub self . _last_save_path = save_path return save_path setup_data () \u00b6 Load and prepare dataset. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def setup_data ( self ): \"\"\"Load and prepare dataset.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading dataset: { self . config . dataset . name } \" ) logger . info ( f \"Loading dataset: { self . config . dataset . name } \" ) # Load dataset with trust_remote_code for compatibility try : if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split , trust_remote_code = True ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split , trust_remote_code = True ) except Exception as e : logger . error ( f \"Failed to load dataset with trust_remote_code: { e } \" ) # Fallback without trust_remote_code if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split ) # Limit samples if specified if self . config . dataset . max_samples : max_samples = min ( self . config . dataset . max_samples , len ( dataset )) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Limited dataset to { max_samples } samples\" ) # Apply column mapping if provided (must be done before splitting) if self . config . dataset . column_mapping : logger . info ( f \"Applying column mapping: { self . config . dataset . column_mapping } \" ) # Rename columns: old_name -> new_name dataset = dataset . rename_columns ( self . config . dataset . column_mapping ) logger . info ( f \"Renamed columns: { self . config . dataset . column_mapping } \" ) # Split into train/eval (90/10) split = dataset . train_test_split ( test_size = 0.1 , seed = 42 ) train_dataset = split [ 'train' ] eval_dataset = split [ 'test' ] # Tokenize based on task type if self . task_type == TaskType . TEXT_CLASSIFICATION : def tokenize_function ( examples ): \"\"\"Tokenize text classification data.\"\"\" result = self . tokenizer ( examples [ self . config . dataset . text_column ], truncation = True , padding = False , max_length = self . config . model . max_seq_length ) # Get labels and convert to 0-indexed if needed (PyTorch expects 0-indexed) labels = examples [ self . config . dataset . label_column ] if isinstance ( labels , list ) and len ( labels ) > 0 : min_label = min ( labels ) if min_label > 0 : # Labels are 1-indexed, convert to 0-indexed labels = [ l - 1 for l in labels ] result [ 'labels' ] = labels return result else : # TOKEN_CLASSIFICATION def tokenize_function ( examples ): \"\"\"Tokenize token classification data with label alignment.\"\"\" tokenized = self . tokenizer ( examples [ self . config . dataset . tokens_column ], truncation = True , is_split_into_words = True , padding = False , max_length = self . config . model . max_seq_length ) # Align labels with tokens labels = [] for i , label in enumerate ( examples [ self . config . dataset . tags_column ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : # Special tokens get -100 label_ids . append ( - 100 ) elif word_idx != previous_word_idx : # First token of a word gets the label label_ids . append ( label [ word_idx ]) else : # Other tokens of the same word get -100 label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ 'labels' ] = labels return tokenized # Apply tokenization self . train_dataset = train_dataset . map ( tokenize_function , batched = True , remove_columns = train_dataset . column_names , desc = \"Tokenizing train dataset\" ) self . eval_dataset = eval_dataset . map ( tokenize_function , batched = True , remove_columns = eval_dataset . column_names , desc = \"Tokenizing eval dataset\" ) print ( f \"\u2713 Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) logger . info ( f \"Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) setup_model () \u00b6 Load model and tokenizer based on task type. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def setup_model ( self ): \"\"\"Load model and tokenizer based on task type.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading model: { self . config . model . name_or_path } \" ) logger . info ( f \"Loading model: { self . config . model . name_or_path } \" ) # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , use_fast = True ) # Add padding token if missing if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Ensure pad_token_id is set (required for data collator batching) # For Llama and similar tokenizers, we need to set it on both tokenizer and config pad_token_id = None if hasattr ( self . tokenizer , 'pad_token_id' ): pad_token_id = self . tokenizer . pad_token_id if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : # If pad_token was added, get its ID from the tokenizer try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id from tokenizer: { e } \" ) # Set pad_token_id on tokenizer (multiple ways to ensure it sticks) if pad_token_id is not None : # Set directly on tokenizer if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id # Also set on tokenizer's special_tokens_map if it exists if hasattr ( self . tokenizer , 'special_tokens_map' ): if 'pad_token' not in self . tokenizer . special_tokens_map : self . tokenizer . special_tokens_map [ 'pad_token' ] = self . tokenizer . pad_token # Set on tokenizer config if it exists if hasattr ( self . tokenizer , 'init_kwargs' ) and 'pad_token_id' in self . tokenizer . init_kwargs : self . tokenizer . init_kwargs [ 'pad_token_id' ] = pad_token_id # Also try setting via __setattr__ to ensure it persists try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : logger . error ( \"pad_token_id is still None after setup! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Determine precision and quantization settings use_8bit = False torch_dtype = None device_map = \"cpu\" # Keep on CPU initially # Check if precision is set and configure accordingly if hasattr ( self . config . model , 'precision' ) and self . config . model . precision : precision_value = self . config . model . precision . value if hasattr ( self . config . model . precision , 'value' ) else str ( self . config . model . precision ) if precision_value == \"bf16\" and torch . cuda . is_available () and torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 elif precision_value == \"fp16\" : torch_dtype = torch . float16 # For 4B+ models, use lower precision dtype to reduce memory # NOTE: 8-bit quantization (BitsAndBytes) requires PEFT adapters for fine-tuning # Since we're not using PEFT for classification, we skip 8-bit quantization # and rely on BF16/FP16 precision instead model_size_estimate = \"4B\" if \"4B\" in self . config . model . name_or_path or \"4b\" in self . config . model . name_or_path . lower () else \"unknown\" use_8bit = False # Disable 8-bit quantization for classification (requires PEFT) quantization_config = None # Ensure we use lower precision dtype for 4B models if torch_dtype is None and torch . cuda . is_available (): # Default to BF16 if supported, else FP16 if torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 logger . info ( \"Using BF16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) else : torch_dtype = torch . float16 logger . info ( \"Using FP16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) # Load appropriate model based on task # Load on CPU first with appropriate precision/quantization to avoid OOM model_kwargs = { \"num_labels\" : self . config . model . num_labels , \"ignore_mismatched_sizes\" : True , \"device_map\" : device_map , # Keep on CPU \"torch_dtype\" : torch_dtype if torch_dtype else None } # Add quantization config if using 8-bit if use_8bit and quantization_config : model_kwargs [ \"quantization_config\" ] = quantization_config # Remove None values model_kwargs = { k : v for k , v in model_kwargs . items () if v is not None } if self . task_type == TaskType . TEXT_CLASSIFICATION : self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForSequenceClassification with { self . config . model . num_labels } labels\" ) else : # TOKEN_CLASSIFICATION self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForTokenClassification with { self . config . model . num_labels } labels\" ) # Ensure model stays on CPU (device_map should handle this, but double-check) if self . model is not None and not use_8bit : # 8-bit models handle device_map automatically try : # Only move to CPU if not already there and not using device_map current_device = next ( self . model . parameters ()) . device if current_device . type != \"cpu\" : self . model = self . model . to ( \"cpu\" ) logger . info ( \"Model moved to CPU to avoid OOM during setup\" ) else : logger . info ( \"Model already on CPU (via device_map)\" ) except Exception as move_error : logger . warning ( f \"Could not verify/move model to CPU: { move_error } \" ) # Resize token embeddings if we added tokens if len ( self . tokenizer ) != self . model . config . vocab_size : self . model . resize_token_embeddings ( len ( self . tokenizer )) # CRITICAL: Set pad_token_id on model config (required for DataCollatorWithPadding) # This is often the missing piece that causes \"Cannot handle batch sizes > 1\" errors pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is not None and hasattr ( self . model , 'config' ): if hasattr ( self . model . config , 'pad_token_id' ): self . model . config . pad_token_id = pad_token_id # Also try setting on model.config if it's a dict-like object try : setattr ( self . model . config , 'pad_token_id' , pad_token_id ) except : pass logger . info ( f \"Set model.config.pad_token_id = { pad_token_id } \" ) # Enable gradient checkpointing to reduce memory usage (helps with OOM) if hasattr ( self . model , 'gradient_checkpointing_enable' ): try : self . model . gradient_checkpointing_enable () logger . info ( \"Gradient checkpointing enabled to reduce memory usage\" ) except Exception as e : logger . warning ( f \"Could not enable gradient checkpointing: { e } \" ) # Apply PEFT (LoRA) if enabled if self . config . model . peft_enabled : from peft import LoraConfig , get_peft_model # Get target modules from config or auto-detect target_modules = self . config . model . target_modules if target_modules is None : # Auto-detect based on model architecture if \"distilbert\" in self . config . model . name_or_path . lower (): target_modules = [ \"q_lin\" , \"k_lin\" , \"v_lin\" , \"out_lin\" ] elif \"bert\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] elif \"roberta\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] else : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] logger . info ( f \"Auto-detected target modules: { target_modules } \" ) # Create PEFT config peft_config = LoraConfig ( r = self . config . model . lora_rank , lora_alpha = self . config . model . lora_alpha , lora_dropout = self . config . model . lora_dropout , bias = self . config . model . bias , task_type = \"SEQ_CLS\" if self . task_type == TaskType . TEXT_CLASSIFICATION else \"TOKEN_CLS\" , target_modules = target_modules ) # Apply PEFT self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () print ( f \"\u2713 Model loaded successfully\" ) setup_training () \u00b6 Setup trainer with appropriate settings. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def setup_training ( self ): \"\"\"Setup trainer with appropriate settings.\"\"\" from aligntune.core.sft.config import TaskType print ( \" \\n Configuring trainer...\" ) logger . info ( \"Configuring trainer...\" ) # Clear GPU cache before creating trainer to avoid OOM errors try : if torch . cuda . is_available (): torch . cuda . empty_cache () except Exception as cache_error : logger . warning ( f \"Could not clear GPU cache: { cache_error } \" ) # Training arguments # For quick testing: use max_steps=10, then revert to full training max_steps_for_testing = None # Set to None for full training, or 10 for quick testing training_args_dict = { \"output_dir\" : self . config . logging . output_dir , \"run_name\" : self . config . logging . run_name , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size , \"per_device_eval_batch_size\" : self . config . train . per_device_batch_size , \"learning_rate\" : self . config . train . learning_rate , \"weight_decay\" : self . config . train . weight_decay , \"warmup_ratio\" : self . config . train . warmup_ratio , \"eval_strategy\" : \"steps\" , \"eval_steps\" : self . config . train . eval_interval , \"save_strategy\" : \"steps\" , \"save_steps\" : self . config . train . save_interval , \"load_best_model_at_end\" : self . config . train . load_best_model_at_end , \"metric_for_best_model\" : self . config . train . metric_for_best_model , \"greater_is_better\" : self . config . train . greater_is_better , \"logging_steps\" : 10 , \"logging_dir\" : f \" { self . config . logging . output_dir } /logs\" , # Use precision from model config, default to BF16 if CUDA supports it (for memory efficiency) \"fp16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"fp16\" ) if hasattr ( self . config . model , 'precision' ) else False , \"bf16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"bf16\" ) if hasattr ( self . config . model , 'precision' ) else ( torch . cuda . is_available () and torch . cuda . is_bf16_supported ()), # Auto-enable BF16 if supported \"report_to\" : \"none\" , # Disable wandb/tensorboard \"save_total_limit\" : 2 , # Keep only 2 checkpoints \"push_to_hub\" : False } # Add gradient_accumulation_steps if specified in config (helps with OOM) if hasattr ( self . config . train , 'gradient_accumulation_steps' ) and self . config . train . gradient_accumulation_steps is not None : training_args_dict [ \"gradient_accumulation_steps\" ] = self . config . train . gradient_accumulation_steps # Set either max_steps (for quick testing) or num_train_epochs (for full training) # Don't pass num_train_epochs at all when max_steps is set (TrainingArguments doesn't like both) if max_steps_for_testing is not None : training_args_dict [ \"max_steps\" ] = max_steps_for_testing # Don't include num_train_epochs when max_steps is set else : training_args_dict [ \"num_train_epochs\" ] = self . config . train . epochs # Don't include max_steps when num_train_epochs is set training_args = TrainingArguments ( ** training_args_dict ) # Ensure pad_token_id is set before creating data collator (required for batching) if self . tokenizer is not None : if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id != self . tokenizer . unk_token_id : # Make sure it's not the unk token self . tokenizer . pad_token_id = pad_token_id else : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not set pad_token_id: { e } \" ) # Final check: ensure pad_token_id is set (critical for batching) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : logger . error ( \"pad_token_id is still None before creating data collator! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Choose data collator based on task # Explicitly pass pad_token_id to ensure it's used pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : raise ValueError ( \"pad_token_id must be set before creating data collator\" ) # Create a wrapper class to ensure pad_token_id is always set before batching class SafeDataCollatorWithPadding ( DataCollatorWithPadding ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : # Try to set it from eos_token_id if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # If still None, try to get it from pad_token if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) class SafeDataCollatorForTokenClassification ( DataCollatorForTokenClassification ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) if self . task_type == TaskType . TEXT_CLASSIFICATION : data_collator = SafeDataCollatorWithPadding ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 # Optimize for tensor cores ) else : # TOKEN_CLASSIFICATION data_collator = SafeDataCollatorForTokenClassification ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 ) # Compute metrics function def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics.\"\"\" predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) # For token classification, flatten and filter if self . task_type == TaskType . TOKEN_CLASSIFICATION : predictions = predictions . flatten () labels = labels . flatten () # Remove ignored indices (-100) mask = labels != - 100 predictions = predictions [ mask ] labels = labels [ mask ] # Calculate metrics return { 'accuracy' : accuracy_score ( labels , predictions ), 'f1' : f1_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'precision' : precision_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'recall' : recall_score ( labels , predictions , average = 'weighted' , zero_division = 0 ) } # Clear CUDA cache before creating trainer if torch . cuda . is_available (): torch . cuda . empty_cache () # Create trainer with error handling for OOM try : self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , data_collator = data_collator , compute_metrics = compute_metrics , tokenizer = self . tokenizer ) except ( torch . cuda . OutOfMemoryError , RuntimeError ) as oom_error : # Check if it's actually an OOM error is_oom = isinstance ( oom_error , torch . cuda . OutOfMemoryError ) or \"out of memory\" in str ( oom_error ) . lower () if is_oom : # Clear cache and try to provide helpful error message if torch . cuda . is_available (): torch . cuda . empty_cache () error_msg = ( f \"CUDA out of memory when creating trainer. \" f \"This usually means other processes are using GPU memory. \" f \"Try: (1) Free GPU memory by stopping other processes, \" f \"(2) Reduce batch_size in config, (3) Use a smaller model. \" f \"Original error: { str ( oom_error )[: 200 ] } \" ) logger . error ( error_msg ) raise RuntimeError ( error_msg ) from oom_error else : # Re-raise if it's not an OOM error raise print ( \"\u2713 Trainer configured\" ) logger . info ( \"Trainer configured successfully\" ) train () \u00b6 Train the model. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 def train ( self ): \"\"\"Train the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"TRAINING\" ) print ( \"=\" * 80 ) self . setup_model () self . setup_data () self . setup_training () # Clear CUDA cache before training to avoid OOM if torch . cuda . is_available (): torch . cuda . empty_cache () logger . info ( \"Cleared CUDA cache before training\" ) logger . info ( \"Starting training...\" ) try : train_result = self . trainer . train () except RuntimeError as e : if \"out of memory\" in str ( e ) . lower () or \"OOM\" in str ( e ): if torch . cuda . is_available (): torch . cuda . empty_cache () logger . error ( f \"CUDA OOM during training: { e } \" ) print ( f \" \\n \u274c CUDA Out of Memory error during training.\" ) print ( \" Suggestions:\" ) print ( \" - Reduce batch_size in your config\" ) print ( \" - Increase gradient_accumulation_steps to maintain effective batch size\" ) print ( \" - Reduce max_seq_length\" ) print ( \" - Use a smaller model\" ) raise else : raise print ( \" \\n \u2713 Training completed\" ) logger . info ( \"Training completed\" ) return train_result options: show_source: true heading_level: 3 Note: Only available for TRL backend. Usage Examples \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Predict result = trainer . predict ( \"What is AI?\" ) RL Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Push to Hub url = trainer . push_to_hub ( \"username/my-model\" ) Next Steps \u00b6 Backend Factory - Trainer creation functions Configuration Classes - Configuration options User Guide - Usage guide","title":"Trainers"},{"location":"api-reference/trainers/#trainers-api-reference","text":"Complete API reference for AlignTune trainer classes.","title":"Trainers API Reference"},{"location":"api-reference/trainers/#base-trainers","text":"","title":"Base Trainers"},{"location":"api-reference/trainers/#trainerbase-rl","text":"Abstract base trainer for RL training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. Source code in src/aligntune/core/rl/trainer_base.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 class TrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All RLHF trainers should inherit from this class and implement the abstract methods for model, data, and reward setup. \"\"\" def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = False, # metrics: Optional[List] = None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) # metric_key_prefix: Prefix for metric keys # use_custom_evaluator: If True, use BaseEvaluator/RLEvaluator. If False, use native. # metrics: Metrics to compute (only for custom evaluator, overrides setup) # **kwargs: Additional evaluation arguments (e.g., reference_model, reward_model for RL) # Returns: # Dictionary of evaluation metrics # \"\"\" # if not self.backend.is_rank_0(): # return {} # logger.info(\"Running evaluation...\") # # Route to appropriate evaluator # if use_custom_evaluator: # eval_metrics = self._evaluate_with_custom(eval_dataset, metrics, **kwargs) # else: # eval_metrics = self._evaluate_native(eval_dataset, **kwargs) # # Apply metric key prefix # if metric_key_prefix and eval_metrics: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # if eval_metrics: # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics # def _evaluate_with_custom( # self, # eval_dataset, # metrics: Optional[List], # **kwargs # ) -> Dict[str, float]: # \"\"\"Evaluate using BaseEvaluator or RLEvaluator.\"\"\" # # Auto-setup if not configured # if self.custom_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\", metrics=metrics) # # Use provided dataset or fall back to configured dataset # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # # RL Evaluation # if isinstance(self.custom_evaluator, RLEvaluator): # # Get reference model (defaults to policy model if not provided) # reference_model = kwargs.pop('reference_model', self.model) # reward_model = kwargs.pop('reward_model', None) # return self.custom_evaluator.evaluate_rl( # policy_model=self.model, # reference_model=reference_model, # tokenizer=self.tokenizer, # dataset=dataset, # reward_model=reward_model, # **kwargs # ) # # Base Evaluation (SFT) # else: # task_name = kwargs.pop('task_name', 'text_generation') # return self.custom_evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # task_name=task_name, # **kwargs # ) # def _evaluate_native( # self, # eval_dataset=None, # **kwargs # ) -> Dict[str, float]: # \"\"\"Use UnifiedEvaluator (native evaluation). # Subclasses can override this to use their trainer-specific evaluation. # \"\"\" # dataset = eval_dataset or self.eval_dataset # if dataset is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None def _evaluate_with_custom_evaluator ( self , eval_dataset = None , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Custom evaluation using unified framework (BaseEvaluator + RLEvaluator).\"\"\" dataset = eval_dataset or self . eval_dataset if not dataset : logger . warning ( \"No eval dataset available for custom evaluation\" ) return {} eval_results = {} max_samples = kwargs . get ( 'max_samples' , None ) # 1. BaseEvaluator: Text generation metrics (ROUGE, BLEU, Perplexity) if self . base_evaluator : try : logger . info ( \"Running BaseEvaluator for text generation metrics...\" ) base_metrics = self . base_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = kwargs . get ( 'task_name' , 'text_generation' ), max_samples = max_samples ) eval_results . update ( base_metrics ) logger . info ( f \"BaseEvaluator completed: { list ( base_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"BaseEvaluator failed: { e } \" ) logger . debug ( \"BaseEvaluator error details:\" , exc_info = True ) # 2. RLEvaluator: RL-specific metrics (KL Divergence, Reward Accuracy) if self . rl_evaluator : try : logger . info ( \"Running RLEvaluator for RL-specific metrics...\" ) # Get reference model and reward model from kwargs or use defaults reference_model = kwargs . get ( 'reference_model' , self . model ) reward_model = kwargs . get ( 'reward_model' , getattr ( self , '_combined_reward_function' , None )) # Run RL evaluation rl_metrics = self . rl_evaluator . evaluate_rl ( policy_model = self . model , reference_model = reference_model , tokenizer = self . tokenizer , dataset = dataset , reward_model = reward_model , max_samples = max_samples ) eval_results . update ( rl_metrics ) logger . info ( f \"RLEvaluator completed: { list ( rl_metrics . keys ()) } \" ) except Exception as e : logger . warning ( f \"RLEvaluator failed: { e } \" ) logger . debug ( \"RLEvaluator error details:\" , exc_info = True ) if not eval_results : logger . warning ( \"No evaluation metrics were computed successfully\" ) return eval_results def _evaluate_native ( self , eval_dataset = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Native evaluation - uses UnifiedEvaluator or trainer's built-in evaluate. Subclasses can override this to use their trainer-specific evaluation. \"\"\" dataset = eval_dataset or self . eval_dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Try trainer's built-in evaluate first (if exists) if hasattr ( self , 'trainer' ) and self . trainer and hasattr ( self . trainer , 'evaluate' ): try : return self . trainer . evaluate () except Exception as e : logger . warning ( f \"Trainer evaluation failed: { e } \" ) # Fallback to UnifiedEvaluator # return self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset, # config=self.config, # **kwargs # ) return {} def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" ) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step ) def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step () def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return [] def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" ) def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False","title":"TrainerBase (RL)"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__enter__","text":"Context manager entry. Source code in src/aligntune/core/rl/trainer_base.py 911 912 913 def __enter__ ( self ): \"\"\"Context manager entry.\"\"\" return self","title":"__enter__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__exit__","text":"Context manager exit. Source code in src/aligntune/core/rl/trainer_base.py 915 916 917 918 919 920 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Context manager exit.\"\"\" self . cleanup () if exc_type is not None : logger . error ( f \"Training failed with { exc_type . __name__ } : { exc_val } \" ) return False","title":"__exit__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.__init__","text":"Initialize trainer with configuration. Source code in src/aligntune/core/rl/trainer_base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def __init__ ( self , config : UnifiedConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize distributed backend self . backend = BackendFactory . create ( config . distributed ) # Initialize logging (rank-0 only) self . logger = UnifiedLogger ( config . logging ) # Initialize evaluator (rank-0 only) self . evaluator = UnifiedEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . reward_functions = [] # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup # Evaluation components self . base_evaluator = None self . rl_evaluator = None self . eval_dataset = None # To be set by subclasses logger . info ( f \"Initialized { self . __class__ . __name__ } with { config . algo . value } algorithm\" )","title":"__init__"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.cleanup","text":"Cleanup resources. Source code in src/aligntune/core/rl/trainer_base.py 904 905 906 907 908 909 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if hasattr ( self , 'backend' ): self . backend . cleanup () logger . info ( \"Trainer cleanup completed\" )","title":"cleanup"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.create_data_loader","text":"Create data loader (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 699 700 701 def create_data_loader ( self ): \"\"\"Create data loader (to be implemented by subclasses).\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" )","title":"create_data_loader"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.evaluate","text":"Evaluate the trained model with flexible evaluation strategies. Parameters: Name Type Description Default eval_dataset Dataset to evaluate on (uses self.eval_dataset if None) None metric_key_prefix str Prefix for metric names in results 'eval' use_custom_evaluator bool If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation False metrics Optional [ List ] Optional list of additional metrics to compute None **kwargs Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics with the specified prefix Source code in src/aligntune/core/rl/trainer_base.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\" Evaluate the trained model with flexible evaluation strategies. Args: eval_dataset: Dataset to evaluate on (uses self.eval_dataset if None) metric_key_prefix: Prefix for metric names in results use_custom_evaluator: If True, use unified evaluation framework (BaseEvaluator + RLEvaluator) If False, use native TRL trainer evaluation metrics: Optional list of additional metrics to compute **kwargs: Additional arguments: - max_samples: Maximum number of samples to evaluate (default: 50) - compute_text_metrics: Force text generation metrics (ROUGE, BLEU) - compute_rl_metrics: Force RL metrics (KL, Reward Accuracy) Returns: Dictionary of evaluation metrics with the specified prefix \"\"\" if not self . backend . is_rank_0 (): return {} if self . callback_handler is None : from aligntune.core.callbacks import CallbackHandler self . callback_handler = CallbackHandler ( callbacks = self . callbacks , model = self . model , tokenizer = self . tokenizer ) logger . debug ( \"Initialized minimal callback handler for standalone evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"Starting Evaluation\" ) logger . info ( f \"Use custom evaluator: { use_custom_evaluator } \" ) logger . info ( f \"Metric prefix: { metric_key_prefix } \" ) logger . info ( \"=\" * 80 ) eval_results = {} # Strategy 1: Use unified evaluation framework (BaseEvaluator + RLEvaluator) if use_custom_evaluator : if not self . base_evaluator and not self . rl_evaluator : logger . warning ( \"Custom evaluators not available, falling back to native evaluation\" ) use_custom_evaluator = False else : custom_results = self . _evaluate_with_custom_evaluator ( eval_dataset = eval_dataset , metrics = metrics , ** kwargs ) eval_results . update ( custom_results ) # Strategy 2: Use native evaluation (fallback or if custom disabled) if not use_custom_evaluator : native_results = self . _evaluate_native ( eval_dataset = eval_dataset , ** kwargs ) eval_results . update ( native_results ) # Apply metric prefix to all keys if metric_key_prefix and eval_results : prefixed_results = { f \" { metric_key_prefix } / { k } \" if \"/\" not in k else k : v for k , v in eval_results . items () } eval_results = prefixed_results # Log evaluation metrics if eval_results : self . logger . log_metrics ( eval_results , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_results ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_results : improved = self . state . update_best_metric ( eval_results [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_results [ accuracy_key ] : .4f } \" ) # Log summary logger . info ( \"=\" * 80 ) logger . info ( \"Evaluation Results Summary\" ) logger . info ( \"-\" * 80 ) for metric_name , metric_value in sorted ( eval_results . items ()): if isinstance ( metric_value , ( int , float )): logger . info ( f \" { metric_name } : { metric_value : .4f } \" ) else : logger . info ( f \" { metric_name } : { metric_value } \" ) logger . info ( \"=\" * 80 ) return eval_results","title":"evaluate"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.get_next_batch","text":"Get next batch from data loader. Source code in src/aligntune/core/rl/trainer_base.py 687 688 689 690 691 692 693 694 695 696 697 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader if exhausted or not an iterator self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader )","title":"get_next_batch"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.get_sample_outputs","text":"Get sample outputs for logging (to be implemented by subclasses). Source code in src/aligntune/core/rl/trainer_base.py 715 716 717 def get_sample_outputs ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Get sample outputs for logging (to be implemented by subclasses).\"\"\" return []","title":"get_sample_outputs"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.load_checkpoint","text":"Load checkpoint and broadcast to all ranks. Source code in src/aligntune/core/rl/trainer_base.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 def load_checkpoint ( self , checkpoint_path : Union [ str , Path ]) -> None : \"\"\"Load checkpoint and broadcast to all ranks.\"\"\" checkpoint_path = Path ( checkpoint_path ) if not checkpoint_path . exists (): raise FileNotFoundError ( f \"Checkpoint not found: { checkpoint_path } \" ) logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model . load_state_dict ( torch . load ( checkpoint_path / \"pytorch_model.bin\" , map_location = \"cpu\" ) ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = checkpoint_path / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) # Broadcast checkpoint path to all ranks self . backend . broadcast_checkpoint_path ( str ( checkpoint_path )) logger . info ( f \"Checkpoint loaded: { checkpoint_path } \" )","title":"load_checkpoint"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.on_eval","text":"Hook for evaluation operations. Source code in src/aligntune/core/rl/trainer_base.py 709 710 711 712 713 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for evaluation operations.\"\"\" # Update learning rate scheduler if needed if hasattr ( self , 'scheduler' ): self . scheduler . step ()","title":"on_eval"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.on_step","text":"Hook for step-level operations. Source code in src/aligntune/core/rl/trainer_base.py 703 704 705 706 707 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook for step-level operations.\"\"\" # Log sample outputs periodically if step % ( self . config . train . eval_interval * 2 ) == 0 : self . logger . log_samples ( self . get_sample_outputs (), step )","title":"on_step"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.predict","text":"Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/rl/trainer_base.py 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"predict should only be called on rank 0\" ) return \"\" if isinstance ( inputs , str ) else [] if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions","title":"predict"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.push_to_hub","text":"Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/rl/trainer_base.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if not self . backend . is_rank_0 (): logger . warning ( \"push_to_hub should only be called on rank 0\" ) return \"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url","title":"push_to_hub"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.save_checkpoint","text":"Save checkpoint (rank-0 only). Source code in src/aligntune/core/rl/trainer_base.py 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint (rank-0 only).\"\"\" if not self . backend . is_rank_0 (): return checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"elapsed_time\" : self . state . get_elapsed_time () }, f , indent = 2 ) # Save resolved configuration from .config_loader import ConfigLoader ConfigLoader . save_resolved_config ( self . config , checkpoint_dir ) self . state . checkpoint_path = str ( checkpoint_dir ) logger . info ( f \"Checkpoint saved: { checkpoint_dir } \" ) self . callback_handler . on_save ( self . config , self . state , self . control )","title":"save_checkpoint"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_custom_evaluator","text":"Setup custom evaluators (BaseEvaluator/RLEvaluator). Parameters: Name Type Description Default evaluator_type str \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) 'auto' metrics Optional [ List ] Optional list of metrics to use (overrides defaults) None Source code in src/aligntune/core/rl/trainer_base.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 def setup_custom_evaluator ( self , evaluator_type : str = \"auto\" , metrics : Optional [ List ] = None ) -> None : \"\"\"Setup custom evaluators (BaseEvaluator/RLEvaluator). Args: evaluator_type: \"base\", \"rl\", or \"auto\" (auto-detects based on algorithm) metrics: Optional list of metrics to use (overrides defaults) \"\"\" try : eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , 'per_device_batch_size' , default = 4 ) # Setup BaseEvaluator for text generation metrics if evaluator_type in [ \"base\" , \"auto\" ]: self . base_evaluator = BaseEvaluator ( metrics = metrics or [ RougeMetric (), BleuMetric (), PerplexityMetric ()], batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"BaseEvaluator initialized (batch_size= { eval_batch_size } )\" ) # Setup RLEvaluator for RL-specific metrics if evaluator_type in [ \"rl\" , \"auto\" ]: self . rl_evaluator = RLEvaluator ( batch_size = eval_batch_size , use_cache = True ) logger . info ( f \"RLEvaluator initialized (batch_size= { eval_batch_size } )\" ) except Exception as e : logger . warning ( f \"Could not initialize custom evaluators: { e } \" ) self . base_evaluator = None self . rl_evaluator = None","title":"setup_custom_evaluator"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_data","text":"Setup datasets and data loaders. Source code in src/aligntune/core/rl/trainer_base.py 111 112 113 114 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass","title":"setup_data"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_model","text":"Setup model, tokenizer, and optimization. Source code in src/aligntune/core/rl/trainer_base.py 106 107 108 109 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass","title":"setup_model"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.setup_rewards","text":"Setup reward functions and evaluators. Source code in src/aligntune/core/rl/trainer_base.py 116 117 118 119 @abstractmethod def setup_rewards ( self ) -> None : \"\"\"Setup reward functions and evaluators.\"\"\" pass","title":"setup_rewards"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.train","text":"Main training loop with hooks. Source code in src/aligntune/core/rl/trainer_base.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting training...\" ) # Setup phase self . setup_model () self . setup_data () self . setup_rewards () # CRITICAL FIX: Create data loader AFTER setup_data() if self . data_loader is None : self . data_loader = self . create_data_loader () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , # RL trainers usually wrap optimizer internally scheduler = None ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs if self . config . train . epochs is not None : max_steps = self . config . train . epochs * len ( self . data_loader ) else : raise ValueError ( \"Either max_steps or epochs must be specified in training config\" ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"Training completed\" )","title":"train"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainerBase.train_step","text":"Execute single training step. Source code in src/aligntune/core/rl/trainer_base.py 121 122 123 124 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.rl.trainer_base import TrainerBase from aligntune.core.rl.config import UnifiedConfig # TrainerBase is abstract - use concrete implementations # See Backend Trainers below","title":"train_step"},{"location":"api-reference/trainers/#sfttrainerbase","text":"Abstract base trainer for SFT training with lifecycle management. Bases: ABC Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. Source code in src/aligntune/core/sft/trainer_base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class SFTTrainerBase ( ABC ): \"\"\" Abstract base trainer with lifecycle management. All SFT trainers should inherit from this class and implement the abstract methods for model, data, and training setup. \"\"\" def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" ) @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" ) # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # **kwargs # ) -> Dict[str, float]: # \"\"\"Run evaluation and return metrics. # Args: # eval_dataset: Dataset to evaluate on (defaults to validation set) # metric_key_prefix: Prefix for metric keys # **kwargs: Additional evaluation arguments # Returns: # Dictionary of evaluation metrics # \"\"\" # logger.info(\"Running evaluation...\") # # Use provided dataset or fall back to configured dataset # dataset_to_use = eval_dataset if eval_dataset is not None else self.dataset # if dataset_to_use is None: # logger.warning(\"No evaluation dataset provided or configured\") # return {} # eval_metrics = self.evaluator.evaluate( # model=self.model, # tokenizer=self.tokenizer, # dataset=dataset_to_use, # config=self.config, # **kwargs # ) # # Apply metric key prefix # if metric_key_prefix: # prefixed_metrics = {f\"{metric_key_prefix}/{k}\": v for k, v in eval_metrics.items()} # eval_metrics = prefixed_metrics # # Log evaluation metrics # self.logger.log_metrics(eval_metrics, self.state.step, prefix=\"eval/\") # self.callback_handler.on_evaluate(self.config, self.state, self.control, metrics=eval_metrics) # # Update best metric # accuracy_key = f\"{metric_key_prefix}/accuracy\" if metric_key_prefix else \"accuracy\" # if accuracy_key in eval_metrics: # improved = self.state.update_best_metric(eval_metrics[accuracy_key]) # if improved: # logger.info(f\"New best metric: {eval_metrics[accuracy_key]:.4f}\") # return eval_metrics def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics def _evaluate_with_custom ( self , eval_dataset , metrics : Optional [ List ], ** kwargs ) -> Dict [ str , float ]: \"\"\"Evaluate using BaseEvaluator (SFT only - no RL metrics).\"\"\" # Auto-setup if not configured if self . custom_evaluator is None : self . setup_custom_evaluator ( evaluator_type = \"base\" , metrics = metrics ) # Use provided dataset or fall back to configured dataset dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} # Base Evaluation (SFT) task_name = kwargs . pop ( 'task_name' , 'text_generation' ) return self . custom_evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , task_name = task_name , ** kwargs ) def _evaluate_native ( self , eval_dataset = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Use SFTEvaluator (native evaluation).\"\"\" dataset = eval_dataset or self . eval_dataset or self . dataset if dataset is None : logger . warning ( \"No evaluation dataset provided or configured\" ) return {} return self . evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = dataset , config = self . config , ** kwargs ) def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control ) def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ()) def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader ) def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" ) def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions # TrainerCallback methods (can be overridden by subclasses if needed) def on_init_end ( self , args , state , control , ** kwargs ): pass def on_train_begin ( self , args , state , control , ** kwargs ): pass def on_train_end ( self , args , state , control , ** kwargs ): pass def on_epoch_begin ( self , args , state , control , ** kwargs ): pass def on_epoch_end ( self , args , state , control , ** kwargs ): pass def on_step_begin ( self , args , state , control , ** kwargs ): pass def on_step_end ( self , args , state , control , ** kwargs ): # Original hook call pass def on_evaluate ( self , args , state , control , ** kwargs ): # Original hook call pass def on_save ( self , args , state , control , ** kwargs ): pass def on_log ( self , args , state , control , ** kwargs ): pass def on_prediction_step ( self , args , state , control , ** kwargs ): pass def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader","title":"SFTTrainerBase"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.__init__","text":"Initialize trainer with configuration. Source code in src/aligntune/core/sft/trainer_base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , config : SFTConfig , callbacks : Optional [ List [ TrainerCallback ]] = None ): \"\"\"Initialize trainer with configuration.\"\"\" self . config = config self . state = TrainingState () self . control = TrainerControl () # Initialize logging self . logger = SFTLogger ( config . logging ) # Initialize evaluator self . evaluator = SFTEvaluator ( config ) # Training components (to be set by subclasses) self . model = None self . tokenizer = None self . dataset = None self . data_loader = None self . trainer = None # Setup callbacks self . callbacks = callbacks or [] self . callback_handler = None # Will be initialized after model/tokenizer setup self . eval_dataset = None # ADD THIS LINE self . custom_evaluator = None # ADD THIS LINE logger . info ( f \"Initialized { self . __class__ . __name__ } for { config . dataset . task_type . value } task\" )","title":"__init__"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.cleanup","text":"Cleanup resources. Source code in src/aligntune/core/sft/trainer_base.py 592 593 594 595 596 597 598 599 600 601 def cleanup ( self ) -> None : \"\"\"Cleanup resources.\"\"\" if self . model is not None : del self . model if self . tokenizer is not None : del self . tokenizer if self . dataset is not None : del self . dataset if self . data_loader is not None : del self . data_loader","title":"cleanup"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.create_data_loader","text":"Create data loader - to be implemented by subclasses. Source code in src/aligntune/core/sft/trainer_base.py 395 396 397 def create_data_loader ( self ): \"\"\"Create data loader - to be implemented by subclasses.\"\"\" raise NotImplementedError ( \"Subclasses must implement create_data_loader\" )","title":"create_data_loader"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.evaluate","text":"Run evaluation and return metrics. Parameters: Name Type Description Default eval_dataset Dataset to evaluate on (defaults to self.eval_dataset) None metric_key_prefix str Prefix for metric keys 'eval' use_custom_evaluator bool If True, use BaseEvaluator. If False, use native SFTEvaluator. False metrics Optional [ List ] Metrics to compute (only for custom evaluator, overrides setup) None **kwargs Additional evaluation arguments {} Returns: Type Description Dict [ str , float ] Dictionary of evaluation metrics Source code in src/aligntune/core/sft/trainer_base.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = False , metrics : Optional [ List ] = None , ** kwargs ) -> Dict [ str , float ]: \"\"\"Run evaluation and return metrics. Args: eval_dataset: Dataset to evaluate on (defaults to self.eval_dataset) metric_key_prefix: Prefix for metric keys use_custom_evaluator: If True, use BaseEvaluator. If False, use native SFTEvaluator. metrics: Metrics to compute (only for custom evaluator, overrides setup) **kwargs: Additional evaluation arguments Returns: Dictionary of evaluation metrics \"\"\" logger . info ( \"Running evaluation...\" ) # Route to appropriate evaluator if use_custom_evaluator : eval_metrics = self . _evaluate_with_custom ( eval_dataset , metrics , ** kwargs ) else : eval_metrics = self . _evaluate_native ( eval_dataset , ** kwargs ) # Apply metric key prefix if metric_key_prefix and eval_metrics : prefixed_metrics = { f \" { metric_key_prefix } / { k } \" : v for k , v in eval_metrics . items ()} eval_metrics = prefixed_metrics # Log evaluation metrics if eval_metrics : self . logger . log_metrics ( eval_metrics , self . state . step , prefix = \"eval/\" ) self . callback_handler . on_evaluate ( self . config , self . state , self . control , metrics = eval_metrics ) # Update best metric accuracy_key = f \" { metric_key_prefix } /accuracy\" if metric_key_prefix else \"accuracy\" if accuracy_key in eval_metrics : improved = self . state . update_best_metric ( eval_metrics [ accuracy_key ]) if improved : logger . info ( f \"New best metric: { eval_metrics [ accuracy_key ] : .4f } \" ) return eval_metrics","title":"evaluate"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.get_next_batch","text":"Get next batch from data loader. Source code in src/aligntune/core/sft/trainer_base.py 383 384 385 386 387 388 389 390 391 392 393 def get_next_batch ( self ) -> Dict [ str , Any ]: \"\"\"Get next batch from data loader.\"\"\" if self . data_loader is None : raise RuntimeError ( \"Data loader not initialized. Call setup_data() first.\" ) try : return next ( self . data_loader ) except ( StopIteration , TypeError ): # Restart data loader self . data_loader = iter ( self . create_data_loader ()) return next ( self . data_loader )","title":"get_next_batch"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.load_checkpoint","text":"Load checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def load_checkpoint ( self , checkpoint_path : str ) -> None : \"\"\"Load checkpoint.\"\"\" logger . info ( f \"Loading checkpoint from { checkpoint_path } \" ) # Load model and tokenizer if self . model is not None : self . model = self . model . from_pretrained ( checkpoint_path ) if self . tokenizer is not None : self . tokenizer = self . tokenizer . from_pretrained ( checkpoint_path ) # Load training state state_path = Path ( checkpoint_path ) / \"training_state.json\" if state_path . exists (): import json with open ( state_path , 'r' ) as f : state_data = json . load ( f ) self . state . step = state_data . get ( \"step\" , 0 ) self . state . epoch = state_data . get ( \"epoch\" , 0 ) self . state . best_metric = state_data . get ( \"best_metric\" , 0.0 ) self . state . start_time = state_data . get ( \"start_time\" , time . time ())","title":"load_checkpoint"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.on_eval","text":"Hook called after each evaluation. Source code in src/aligntune/core/sft/trainer_base.py 403 404 405 def on_eval ( self , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each evaluation.\"\"\" pass","title":"on_eval"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.on_step","text":"Hook called after each training step. Source code in src/aligntune/core/sft/trainer_base.py 399 400 401 def on_step ( self , step : int , metrics : Dict [ str , float ]) -> None : \"\"\"Hook called after each training step.\"\"\" pass","title":"on_step"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.predict","text":"Generate predictions from trained model. Parameters: Name Type Description Default inputs Union [ str , List [ str ]] Input text(s) to generate from required max_new_tokens int Maximum number of tokens to generate 100 temperature float Sampling temperature (higher = more random) 1.0 top_p float Nucleus sampling parameter 0.9 do_sample bool Whether to use sampling True **kwargs Additional generation arguments {} Returns: Type Description Union [ str , List [ str ]] Generated text(s) - single string if input was string, list if input was list Raises: Type Description RuntimeError If model or tokenizer not loaded Source code in src/aligntune/core/sft/trainer_base.py 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def predict ( self , inputs : Union [ str , List [ str ]], max_new_tokens : int = 100 , temperature : float = 1.0 , top_p : float = 0.9 , do_sample : bool = True , ** kwargs ) -> Union [ str , List [ str ]]: \"\"\"Generate predictions from trained model. Args: inputs: Input text(s) to generate from max_new_tokens: Maximum number of tokens to generate temperature: Sampling temperature (higher = more random) top_p: Nucleus sampling parameter do_sample: Whether to use sampling **kwargs: Additional generation arguments Returns: Generated text(s) - single string if input was string, list if input was list Raises: RuntimeError: If model or tokenizer not loaded \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) if self . callback_handler is None : self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = None , scheduler = None ) self . model . eval () is_single = isinstance ( inputs , str ) if is_single : inputs = [ inputs ] # Tokenize tokenized = self . tokenizer ( inputs , return_tensors = \"pt\" , padding = True , truncation = True , max_length = getattr ( self . config . model , 'max_seq_length' , 512 ), ) # Move to device device = next ( self . model . parameters ()) . device tokenized = { k : v . to ( device ) for k , v in tokenized . items ()} # Generate with torch . no_grad (): outputs = self . model . generate ( ** tokenized , max_new_tokens = max_new_tokens , temperature = temperature , top_p = top_p , do_sample = do_sample , pad_token_id = self . tokenizer . pad_token_id or self . tokenizer . eos_token_id , eos_token_id = self . tokenizer . eos_token_id , ** kwargs ) # Decode predictions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) # Remove input prefix from predictions for i , ( input_text , prediction ) in enumerate ( zip ( inputs , predictions )): if prediction . startswith ( input_text ): predictions [ i ] = prediction [ len ( input_text ):] . strip () else : # If input wasn't at start, just return the full prediction predictions [ i ] = prediction . strip () self . callback_handler . on_prediction_step ( self . config , self . state , self . control ) return predictions [ 0 ] if is_single else predictions","title":"predict"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.push_to_hub","text":"Push model to HuggingFace Hub. Parameters: Name Type Description Default repo_id str Repository ID on HuggingFace Hub (e.g., 'username/model-name') required private bool Whether the repository should be private False token Optional [ str ] HuggingFace token (if not provided, uses logged-in token) None commit_message str Commit message for the upload 'Upload fine-tuned model' **kwargs Additional arguments for upload_folder {} Returns: Type Description str URL of the uploaded repository Raises: Type Description RuntimeError If model or tokenizer not loaded ImportError If huggingface_hub not installed Source code in src/aligntune/core/sft/trainer_base.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def push_to_hub ( self , repo_id : str , private : bool = False , token : Optional [ str ] = None , commit_message : str = \"Upload fine-tuned model\" , ** kwargs ) -> str : \"\"\"Push model to HuggingFace Hub. Args: repo_id: Repository ID on HuggingFace Hub (e.g., 'username/model-name') private: Whether the repository should be private token: HuggingFace token (if not provided, uses logged-in token) commit_message: Commit message for the upload **kwargs: Additional arguments for upload_folder Returns: URL of the uploaded repository Raises: RuntimeError: If model or tokenizer not loaded ImportError: If huggingface_hub not installed \"\"\" if self . model is None or self . tokenizer is None : raise RuntimeError ( \"Model not loaded. Call train() first or load a model.\" ) try : from huggingface_hub import HfApi , login except ImportError : raise ImportError ( \"huggingface_hub is required for push_to_hub. \" \"Install with: pip install huggingface_hub\" ) # Login if token provided if token : login ( token = token ) # Save model first (if not already saved) if not hasattr ( self , '_last_save_path' ) or self . _last_save_path is None : save_path = self . save_model () if hasattr ( self , 'save_model' ) else None if save_path is None : # Fallback: save to temp directory import tempfile save_path = tempfile . mkdtemp () self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) else : save_path = self . _last_save_path # Push to hub api = HfApi () api . upload_folder ( folder_path = save_path , repo_id = repo_id , repo_type = \"model\" , private = private , commit_message = commit_message , ** kwargs ) repo_url = f \"https://huggingface.co/ { repo_id } \" logger . info ( f \"\u2705 Model pushed to { repo_url } \" ) return repo_url","title":"push_to_hub"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.save_checkpoint","text":"Save checkpoint. Source code in src/aligntune/core/sft/trainer_base.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 def save_checkpoint ( self ) -> None : \"\"\"Save checkpoint.\"\"\" checkpoint_dir = Path ( self . config . logging . output_dir ) / f \"checkpoint- { self . state . step } \" checkpoint_dir . mkdir ( parents = True , exist_ok = True ) logger . info ( f \"Saving checkpoint to { checkpoint_dir } \" ) # Save model and tokenizer if self . model is not None : self . model . save_pretrained ( checkpoint_dir ) if self . tokenizer is not None : self . tokenizer . save_pretrained ( checkpoint_dir ) # Save training state state_path = checkpoint_dir / \"training_state.json\" import json with open ( state_path , 'w' ) as f : json . dump ({ \"step\" : self . state . step , \"epoch\" : self . state . epoch , \"best_metric\" : self . state . best_metric , \"start_time\" : self . state . start_time }, f , indent = 2 ) self . state . checkpoint_path = str ( checkpoint_dir ) self . state . last_save_time = time . time () self . callback_handler . on_save ( self . config , self . state , self . control )","title":"save_checkpoint"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.setup_data","text":"Setup datasets and data loaders. Source code in src/aligntune/core/sft/trainer_base.py 99 100 101 102 @abstractmethod def setup_data ( self ) -> None : \"\"\"Setup datasets and data loaders.\"\"\" pass","title":"setup_data"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.setup_model","text":"Setup model, tokenizer, and optimization. Source code in src/aligntune/core/sft/trainer_base.py 94 95 96 97 @abstractmethod def setup_model ( self ) -> None : \"\"\"Setup model, tokenizer, and optimization.\"\"\" pass","title":"setup_model"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.train","text":"Main training loop with hooks. Source code in src/aligntune/core/sft/trainer_base.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def train ( self ) -> None : \"\"\"Main training loop with hooks.\"\"\" logger . info ( \"Starting SFT training...\" ) # Setup phase self . setup_model () self . setup_data () # Initialize callback handler self . callback_handler = CallbackHandler ( self . callbacks , self . model , self . tokenizer , optimizer = getattr ( self . trainer , \"optimizer\" , None ), scheduler = getattr ( self . trainer , \"lr_scheduler\" , None ) ) self . callback_handler . add_callback ( self ) # Add self as callback for simple hooks # Call on_init_end self . callback_handler . on_init_end ( self . config , self . state , self . control ) # Log initial configuration self . logger . log_config ( self . config ) # Training loop max_steps = self . config . train . max_steps if max_steps is None : # Calculate steps from epochs max_steps = self . config . train . epochs * len ( self . data_loader ) logger . info ( f \"Training for { max_steps } steps\" ) self . callback_handler . on_train_begin ( self . config , self . state , self . control ) for step in range ( max_steps ): self . control . should_training_stop = False self . callback_handler . on_step_begin ( self . config , self . state , self . control ) # Get next batch batch = self . get_next_batch () # Execute training step metrics = self . train_step ( batch ) # Update state self . state . update_step ( step ) # Log metrics self . logger . log_metrics ( metrics , step ) self . callback_handler . on_log ( self . config , self . state , self . control , logs = metrics ) # Hook for step operations # self.on_step(step, metrics) # Handled by callback_handler now self . callback_handler . on_step_end ( self . config , self . state , self . control ) # Evaluation if step % self . config . train . eval_interval == 0 and step > 0 : eval_metrics = self . evaluate () # self.on_eval(eval_metrics) # Handled by callback_handler # Checkpointing if step % self . config . train . save_interval == 0 and step > 0 : self . save_checkpoint () if self . control . should_training_stop : logger . info ( \"Training stopped by callback\" ) break self . callback_handler . on_train_end ( self . config , self . state , self . control ) # Final evaluation and checkpoint final_eval_metrics = self . evaluate () # self.on_eval(final_eval_metrics) self . save_checkpoint () logger . info ( \"SFT training completed\" )","title":"train"},{"location":"api-reference/trainers/#core.sft.trainer_base.SFTTrainerBase.train_step","text":"Execute single training step. Source code in src/aligntune/core/sft/trainer_base.py 104 105 106 107 @abstractmethod def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute single training step.\"\"\" pass options: show_source: true heading_level: 3 Example : from aligntune.core.sft.trainer_base import SFTTrainerBase from aligntune.core.sft.config import SFTConfig # SFTTrainerBase is abstract - use concrete implementations # See Backend Trainers below","title":"train_step"},{"location":"api-reference/trainers/#trainingstate","text":"Training state tracking dataclass. Training state tracking. Source code in src/aligntune/core/rl/trainer_base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TrainingState : \"\"\"Training state tracking.\"\"\" step : int = 0 epoch : int = 0 best_metric : float = 0.0 checkpoint_path : Optional [ str ] = None start_time : float = field ( default_factory = time . time ) last_eval_time : float = field ( default_factory = time . time ) last_save_time : float = field ( default_factory = time . time ) def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time","title":"TrainingState"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.get_elapsed_time","text":"Get elapsed training time in seconds. Source code in src/aligntune/core/rl/trainer_base.py 60 61 62 def get_elapsed_time ( self ) -> float : \"\"\"Get elapsed training time in seconds.\"\"\" return time . time () - self . start_time","title":"get_elapsed_time"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_best_metric","text":"Update best metric if improved. Source code in src/aligntune/core/rl/trainer_base.py 53 54 55 56 57 58 def update_best_metric ( self , metric : float ): \"\"\"Update best metric if improved.\"\"\" if metric > self . best_metric : self . best_metric = metric return True return False","title":"update_best_metric"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_epoch","text":"Update current epoch. Source code in src/aligntune/core/rl/trainer_base.py 49 50 51 def update_epoch ( self , epoch : int ): \"\"\"Update current epoch.\"\"\" self . epoch = epoch","title":"update_epoch"},{"location":"api-reference/trainers/#core.rl.trainer_base.TrainingState.update_step","text":"Update current step. Source code in src/aligntune/core/rl/trainer_base.py 45 46 47 def update_step ( self , step : int ): \"\"\"Update current step.\"\"\" self . step = step options: show_source: true heading_level: 3","title":"update_step"},{"location":"api-reference/trainers/#backend-trainers","text":"","title":"Backend Trainers"},{"location":"api-reference/trainers/#trl-backends","text":"","title":"TRL Backends"},{"location":"api-reference/trainers/#trlsfttrainer","text":"TRL backend for Supervised Fine-Tuning. Bases: SFTTrainerBase SFT trainer using pure TRL SFTTrainer with comprehensive task type support. Source code in src/aligntune/backends/trl/sft/sft.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 class TRLSFTTrainer ( SFTTrainerBase ): \"\"\"SFT trainer using pure TRL SFTTrainer with comprehensive task type support.\"\"\" # All supported task types SUPPORTED_TASKS = [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION , TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ] def __init__ ( self , config ): super () . __init__ ( config ) self . config = config self . task_type = self . _get_task_type () self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . dataset = None self . training_history = [] self . logging_manager = None self . evaluator = None self . eval_dataset = None self . eval_dataset = None # Already exists - no need to add self . custom_evaluator = None # ADD THIS LINE (for BaseEvaluator) # Validate task type self . _validate_task_type () logger . info ( f \"Initialized TRLSFTTrainer for task: { self . task_type . value } \" ) def _get_task_type ( self ) -> TaskType : \"\"\"Extract task type from config.\"\"\" if hasattr ( self . config , 'dataset' ) and hasattr ( self . config . dataset , 'task_type' ): task_type = self . config . dataset . task_type elif hasattr ( self . config , 'train' ) and hasattr ( self . config . train , 'task_type' ): task_type = self . config . train . task_type else : task_type = TaskType . SUPERVISED_FINE_TUNING if isinstance ( task_type , str ): task_type = TaskType ( task_type . lower ()) return task_type def _validate_task_type ( self ): \"\"\"Validate that the task type is supported.\"\"\" if self . task_type not in self . SUPPORTED_TASKS : raise ValueError ( f \"Task type { self . task_type . value } is not supported by TRL backend. \" f \"Supported tasks: { [ t . value for t in self . SUPPORTED_TASKS ] } \" ) @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) return True except ImportError : return False # Replace lines 465-476 in setup_model: def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers with task type awareness.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT model: { self . config . model . name_or_path } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) try : from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) except ImportError as e : raise ImportError ( \"Transformers not available.\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model based on task type logger . info ( f \"Loading model for task: { self . task_type . value } \" ) if self . task_type == TaskType . TEXT_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 2 ) logger . info ( f \"Loading classification model with { num_labels } labels\" ) self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 9 ) logger . info ( f \"Loading token classification model with { num_labels } labels\" ) self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) else : # Causal LM for generation tasks logger . info ( \"Loading causal language model\" ) # Handle quantization configuration quantization_config = None quantization_dict = getattr ( self . config . model , 'quantization' , {}) if quantization_dict . get ( 'load_in_4bit' , False ) or quantization_dict . get ( 'load_in_8bit' , False ): try : from transformers import BitsAndBytesConfig load_in_4bit = quantization_dict . get ( 'load_in_4bit' , False ) load_in_8bit = quantization_dict . get ( 'load_in_8bit' , False ) if load_in_4bit : compute_dtype_str = quantization_dict . get ( 'bnb_4bit_compute_dtype' , 'bfloat16' ) compute_dtype = torch . bfloat16 if compute_dtype_str == 'bfloat16' else torch . float16 quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_compute_dtype = compute_dtype , bnb_4bit_quant_type = quantization_dict . get ( 'bnb_4bit_quant_type' , 'nf4' ), bnb_4bit_use_double_quant = quantization_dict . get ( 'bnb_4bit_use_double_quant' , False ), ) logger . info ( \"\u2705 4-bit quantization enabled for memory optimization\" ) elif load_in_8bit : quantization_config = BitsAndBytesConfig ( load_in_8bit = True ) logger . info ( \"\u2705 8-bit quantization enabled for memory optimization\" ) except ImportError : logger . warning ( \"\u26a0\ufe0f BitsAndBytes not available. Install with: pip install bitsandbytes\" ) quantization_config = None # Get max_memory from config if specified max_memory = getattr ( self . config . model , 'max_memory' , None ) model_kwargs = { 'torch_dtype' : dtype , 'trust_remote_code' : True , 'device_map' : 'auto' if torch . cuda . is_available () else None , 'low_cpu_mem_usage' : True , } # Add max_memory if specified if max_memory : model_kwargs [ 'max_memory' ] = max_memory if quantization_config : model_kwargs [ 'quantization_config' ] = quantization_config logger . info ( f \"Quantization config: { quantization_config } \" ) else : # Only set these if not using quantization model_kwargs [ 'load_in_4bit' ] = False model_kwargs [ 'load_in_8bit' ] = False logger . info ( \"No quantization configured - loading full precision model\" ) self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Move to GPU only if not using device_map='auto' (which handles it automatically) if not quantization_config and torch . cuda . is_available () and model_kwargs . get ( 'device_map' ) != 'auto' : self . model = self . model . cuda () logger . info ( \"Model moved to GPU\" ) elif quantization_config : logger . info ( \"Model loaded with quantization (device_map='auto')\" ) # CRITICAL: When using quantization, PEFT adapters MUST be enabled # TRL SFTTrainer will handle this, but we need to ensure PEFT is enabled in config if not getattr ( self . config . model , 'peft_enabled' , False ): logger . warning ( \"\u26a0\ufe0f Quantization requires PEFT adapters. Enabling PEFT automatically.\" ) self . config . model . peft_enabled = True # Setup tokenizer for specific tasks self . _setup_tokenizer_for_task () logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) def _setup_tokenizer_for_task ( self ): \"\"\"Setup tokenizer with task-specific configurations.\"\"\" # If a custom chat template (jinja) is provided in config, set it so # `tokenizer.apply_chat_template(...)` can be used during SFT formatting. cfg_template = getattr ( self . config . dataset , \"chat_template\" , None ) if cfg_template and isinstance ( cfg_template , str ): if ( \"{%\" in cfg_template or \"{{\" in cfg_template ) and ( not hasattr ( self . tokenizer , \"chat_template\" ) or self . tokenizer . chat_template is None ): try : self . tokenizer . chat_template = cfg_template logger . info ( \"Applied custom chat_template from config to tokenizer\" ) except Exception as e : logger . debug ( f \"Failed to set tokenizer.chat_template from config: { e } \" ) if self . task_type == TaskType . CHAT_COMPLETION : if not hasattr ( self . tokenizer , 'chat_template' ) or self . tokenizer . chat_template is None : self . tokenizer . chat_template = ( \"{ % f or message in messages %}\" \"{{ message['role'] }}: {{ message['content'] }} \\n \" \"{ % e ndfor %}Assistant:\" ) logger . info ( \"Added chat template to tokenizer\" ) elif self . task_type == TaskType . INSTRUCTION_FOLLOWING : # If the tokenizer already supports chat templating, prefer using that # instead of introducing ChatML-style tokens. has_chat_template = ( hasattr ( self . tokenizer , \"apply_chat_template\" ) and hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None ) if not has_chat_template : special_tokens = [ \"<|im_start|>\" , \"<|im_end|>\" ] num_added = self . tokenizer . add_special_tokens ({ 'additional_special_tokens' : special_tokens }) if num_added > 0 : self . model . resize_token_embeddings ( len ( self . tokenizer )) logger . info ( f \"Added { num_added } special tokens for instruction following\" ) def _apply_chat_template_safe ( self , messages : List [ Dict [ str , str ]], add_generation_prompt : bool = False ) -> Optional [ str ]: \"\"\"Apply tokenizer chat template with broad compatibility.\"\"\" if not self . tokenizer or not hasattr ( self . tokenizer , \"apply_chat_template\" ): return None try : return self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = add_generation_prompt , ) except TypeError : # Some tokenizers don't support add_generation_prompt or other kwargs. try : return self . tokenizer . apply_chat_template ( messages , tokenize = False ) except Exception : return None except Exception : return None def _format_with_chat_template ( self , examples : Dict [ str , Any ], dataset_cfg ) -> Dict [ str , List [ str ]]: \"\"\" Build a `text` field using the model's chat template when available. Falls back to legacy string formatting if templating fails. \"\"\" system_prompt = getattr ( dataset_cfg , \"system_prompt\" , None ) texts : List [ str ] = [] def _prepend_system_if_missing ( msgs : List [ Dict [ str , str ]]) -> List [ Dict [ str , str ]]: if system_prompt and ( not msgs or msgs [ 0 ] . get ( \"role\" ) != \"system\" ): return [{ \"role\" : \"system\" , \"content\" : system_prompt }] + msgs return msgs if self . task_type == TaskType . CHAT_COMPLETION : messages_field = dataset_cfg . messages_column if hasattr ( dataset_cfg , \"messages_column\" ) else \"messages\" for messages in examples . get ( messages_field , []): if isinstance ( messages , list ): msgs = _prepend_system_if_missing ( messages ) templated = self . _apply_chat_template_safe ( msgs , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) continue # Fallback: plain \"role: content\" formatting conversation = [ f \" { m . get ( 'role' , 'user' ) } : { m . get ( 'content' , '' ) } \" for m in msgs ] texts . append ( \" \\n \" . join ( conversation )) else : texts . append ( str ( messages ) if messages else \"\" ) return { \"text\" : texts } if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING ]: # Determine columns (prefer config, fall back to common names) instr_col = getattr ( dataset_cfg , \"instruction_column\" , \"instruction\" ) resp_col = getattr ( dataset_cfg , \"response_column\" , \"response\" ) ctx_col = getattr ( dataset_cfg , \"context_column\" , \"context\" ) input_col = getattr ( dataset_cfg , \"input_column\" , \"input\" ) instructions = examples . get ( instr_col , examples . get ( \"instruction\" , [])) # response could be in response/output/completion depending on dataset responses = examples . get ( resp_col , examples . get ( \"response\" , examples . get ( \"output\" , []))) n = min ( len ( instructions ), len ( responses )) for i in range ( n ): instruction = instructions [ i ] response = responses [ i ] context = \"\" if ctx_col in examples and i < len ( examples . get ( ctx_col , [])): context = examples . get ( ctx_col , [ \"\" ])[ i ] or \"\" input_text = \"\" if input_col in examples and i < len ( examples . get ( input_col , [])): input_text = examples . get ( input_col , [ \"\" ])[ i ] or \"\" sys_parts : List [ str ] = [] if system_prompt : sys_parts . append ( str ( system_prompt ) . strip ()) if context and str ( context ) . strip (): sys_parts . append ( f \"Context: { str ( context ) . strip () } \" ) messages : List [ Dict [ str , str ]] = [] if sys_parts : messages . append ({ \"role\" : \"system\" , \"content\" : \" \\n\\n \" . join ( sys_parts )}) user_content = str ( instruction ) if instruction is not None else \"\" if input_text and str ( input_text ) . strip (): user_content = f \" { user_content } \\n\\n { str ( input_text ) . strip () } \" messages . append ({ \"role\" : \"user\" , \"content\" : user_content }) messages . append ({ \"role\" : \"assistant\" , \"content\" : str ( response ) if response is not None else \"\" }) templated = self . _apply_chat_template_safe ( messages , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) else : # Fallback to legacy formatting (inline; avoid recomputing full batch) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : if context and str ( context ) . strip (): texts . append ( f \"<|im_start|>system \\n Context: { str ( context ) . strip () } <|im_end|> \\n \" f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : if user_content and response is not None : texts . append ( f \"### Instruction: \\n { user_content } \\n\\n ### Response: \\n { str ( response ) } \" ) else : texts . append ( str ( examples . get ( \"text\" , [ \"\" ])[ i ]) if i < len ( examples . get ( \"text\" , [ \"\" ])) else \"\" ) if not texts : texts = [ \"[NO_VALID_TEXT]\" ] return { \"text\" : texts } # Default fallback (shouldn't be hit for tasks we call this for) return { \"text\" : examples . get ( \"text\" , [ \"[NO_VALID_TEXT]\" ])} def setup_data ( self ) -> None : \"\"\"Setup datasets for SFT training with task-aware formatting.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT datasets: { self . config . dataset . name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Initialize dataset cache cache_root = getattr ( self . config , 'caching' , {}) . get ( \"root\" , \"./cache\" ) if hasattr ( self . config , 'caching' ) else \"./cache\" self . dataset_cache = DatasetCache ( cache_root = cache_root ) logger . info ( f \"Dataset cache initialized at: { self . dataset_cache . cache_root } \" ) # Load dataset logger . info ( \"Loading dataset...\" ) dataset_config = self . config . dataset from datasets import load_dataset # Handle dataset config/subset load_kwargs = { 'split' : dataset_config . split or \"train\" } dataset_name = dataset_config . name dataset_subset = None if hasattr ( dataset_config , 'subset' ) and dataset_config . subset : dataset_subset = dataset_config . subset elif hasattr ( dataset_config , 'config' ) and dataset_config . config : dataset_subset = dataset_config . config # Check if dataset has a registered custom loader if dataset_name in DatasetRegistry . list_loaders (): logger . info ( f \"Using registered dataset loader: { dataset_name } \" ) dataset = DatasetRegistry . load_dataset ( name = dataset_name , split = dataset_config . split or \"train\" , max_samples = dataset_config . max_samples , ** { k : v for k , v in dataset_config . __dict__ . items () if k not in [ 'name' , 'split' , 'max_samples' ] and v is not None } ) else : # Load dataset try : if dataset_subset : dataset = load_dataset ( dataset_name , dataset_subset , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ValueError as e : if \"Config name is missing\" in str ( e ): raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' to your DatasetConfig.\" ) raise logger . info ( f \"Original dataset size: { len ( dataset ) } samples\" ) # \ud83d\udd39 CRITICAL FIX: Shuffle dataset BEFORE selecting samples # This ensures balanced class distribution for classification tasks if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . info ( \"Shuffling dataset to ensure balanced sampling...\" ) dataset = dataset . shuffle ( seed = 42 ) # Apply dataset size limits if dataset_config . percent and dataset_config . percent < 1.0 : dataset_size = int ( len ( dataset ) * dataset_config . percent ) dataset = dataset . select ( range ( dataset_size )) logger . info ( f \"Using { dataset_config . percent * 100 } % of dataset: { dataset_size } samples\" ) elif dataset_config . max_samples : dataset = dataset . select ( range ( min ( dataset_config . max_samples , len ( dataset )))) logger . info ( f \"Using max_samples limit: { len ( dataset ) } samples\" ) # Apply column mappings if needed if hasattr ( dataset_config , 'column_mapping' ) and dataset_config . column_mapping : valid_mappings = { k : v for k , v in dataset_config . column_mapping . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , dataset_config ) # Create evaluation split try : test_size = 0.1 if len ( dataset ) >= 1000 : test_size = 0.1 elif len ( dataset ) >= 200 : test_size = 0.1 else : test_size = min ( 0.2 , max ( 0.1 , 50 / max ( 1 , len ( dataset )))) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 , shuffle = True ) self . dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Created eval split. Train: { len ( self . dataset ) } | Eval: { len ( self . eval_dataset ) } \" ) except Exception as e : self . dataset = dataset self . eval_dataset = None logger . warning ( f \"Could not create eval split: { str ( e ) } \" ) logger . info ( \"=\" * 80 ) logger . info ( f \"TRL SFT dataset setup completed: { len ( self . dataset ) } samples\" ) logger . info ( \"=\" * 80 ) def _format_dataset_for_task ( self , dataset , dataset_config ): \"\"\"Format dataset based on task type.\"\"\" logger . info ( f \"Formatting dataset for task: { self . task_type . value } \" ) # Select appropriate formatter can_template = self . tokenizer is not None and hasattr ( self . tokenizer , \"apply_chat_template\" ) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_instruction_following elif self . task_type == TaskType . SUPERVISED_FINE_TUNING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_supervised_fine_tuning elif self . task_type == TaskType . TEXT_GENERATION : formatter = TaskFormatter . format_text_generation elif self . task_type == TaskType . CHAT_COMPLETION : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_chat_completion elif self . task_type == TaskType . TEXT_CLASSIFICATION : formatter = TaskFormatter . format_classification elif self . task_type == TaskType . TOKEN_CLASSIFICATION : formatter = TaskFormatter . format_token_classification else : raise ValueError ( f \"Unsupported task type: { self . task_type } \" ) # Apply formatting try : # For classification, we need to keep the columns AND rename properly if self . task_type == TaskType . TEXT_CLASSIFICATION : # Don't remove columns, just map them formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = [ col for col in dataset . column_names if col not in [ 'text' , 'label' , 'labels' ]], desc = f \"Formatting for { self . task_type . value } \" ) # NOW remove the old 'label' column if it exists (keep only 'labels') if 'label' in formatted_dataset . column_names and 'labels' in formatted_dataset . column_names : formatted_dataset = formatted_dataset . remove_columns ([ 'label' ]) logger . info ( \"Removed duplicate 'label' column, keeping 'labels'\" ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = [], desc = f \"Formatting for { self . task_type . value } \" ) else : # For generation tasks, remove all columns formatted_dataset = dataset . map ( lambda examples : formatter ( examples , dataset_config ), batched = True , remove_columns = dataset . column_names , desc = f \"Formatting for { self . task_type . value } \" ) # Verify sample = formatted_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) if 'text' in sample : logger . info ( f \"Sample text (first 200 chars): { sample [ 'text' ][: 200 ] } \" ) if 'labels' in sample : logger . info ( f \"Sample label: { sample [ 'labels' ] } \" ) # CRITICAL: For classification, check label distribution if self . task_type == TaskType . TEXT_CLASSIFICATION : labels_sample = [ formatted_dataset [ i ][ 'labels' ] for i in range ( min ( 100 , len ( formatted_dataset )))] unique_labels = set ( labels_sample ) label_counts = { label : labels_sample . count ( label ) for label in unique_labels } logger . info ( f \"\u2713 Label distribution (first 100): { label_counts } \" ) logger . info ( f \"\u2713 Unique labels: { unique_labels } \" ) if len ( unique_labels ) == 1 : logger . error ( f \"\u274c ERROR: All labels are { list ( unique_labels )[ 0 ] } ! Dataset is corrupted!\" ) raise ValueError ( \"Dataset has only one label class!\" ) return formatted_dataset except Exception as e : logger . error ( f \"Error formatting dataset: { e } \" ) raise def setup_rewards ( self ) -> None : \"\"\"SFT doesn't use reward functions.\"\"\" logger . info ( \"SFT training doesn't use reward functions\" ) self . reward_functions = [] def train ( self ) -> Dict [ str , Any ]: \"\"\"Train using TRL SFTTrainer with task-aware setup.\"\"\" logger . info ( f \"Starting TRL SFT training for task: { self . task_type . value } \" ) # Setup model and data if not already done if self . model is None : self . setup_model () if self . dataset is None : self . setup_data () try : from trl import SFTTrainer from transformers import TrainingArguments , Trainer from transformers import DataCollatorWithPadding , DataCollatorForTokenClassification import evaluate except ImportError as e : raise ImportError ( \"TRL required for SFT training. Install with: pip install trl\" ) from e # Create training arguments # Get optimizer and scheduler configurations from ....core.optimization import get_optimizer_for_config , get_scheduler_for_config # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate or 2e-4 , self . config . train . weight_decay or 0.01 ) # Calculate max_steps - use config value or calculate from epochs max_steps = self . config . train . max_steps if max_steps is None : # Calculate from epochs if max_steps not specified epochs = getattr ( self . config . train , 'epochs' , 3 ) or 3 dataset_size = len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 1000 batch_size = self . config . train . per_device_batch_size or 2 grad_accum = self . config . train . gradient_accumulation_steps or 4 max_steps = ( dataset_size * epochs ) // ( batch_size * grad_accum ) max_steps = max ( max_steps , 10 ) # Minimum 10 steps logger . info ( f \"Calculated max_steps= { max_steps } from { epochs } epochs, { dataset_size } samples\" ) # Calculate warmup steps warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" # Exclude lr and weight_decay since TrainingArguments already sets them filtered = { k : v for k , v in kwargs_dict . items () if k not in [ 'lr' , 'learning_rate' , 'weight_decay' ]} if not filtered : return None return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in filtered . items ()) optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) # print(lr_scheduler_kwargs_str) # print(scheduler_config) filtered_scheduler_kwargs = { k : v for k , v in scheduler_config [ 'lr_scheduler_kwargs' ] . items () if k not in [ 'num_training_steps' , 'num_warmup_steps' ] } # Build TrainingArguments kwargs, conditionally including optim_args training_kwargs = { \"output_dir\" : self . config . logging . output_dir , \"max_steps\" : max_steps , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size or 2 , \"gradient_accumulation_steps\" : self . config . train . gradient_accumulation_steps or 4 , \"learning_rate\" : self . config . train . learning_rate or 2e-4 , \"weight_decay\" : self . config . train . weight_decay or 0.01 , \"warmup_steps\" : warmup_steps , \"logging_steps\" : getattr ( self . config . logging , 'log_interval' , 10 ), \"save_steps\" : getattr ( self . config . train , 'save_interval' , 500 ), \"eval_steps\" : getattr ( self . config . train , 'eval_interval' , 500 ), \"eval_strategy\" : \"no\" , \"save_strategy\" : \"steps\" , \"load_best_model_at_end\" : False , \"report_to\" : [], \"run_name\" : self . config . logging . run_name , \"seed\" : 42 , ** precision_args , \"dataloader_num_workers\" : 0 , \"remove_unused_columns\" : False , \"optim\" : optimizer_name , \"lr_scheduler_type\" : scheduler_name , \"lr_scheduler_kwargs\" : filtered_scheduler_kwargs , } # Only include optim_args if it's not None/empty and valid. # For 8-bit optimizers (adamw_8bit, paged_adamw_8bit, etc.), bitsandbytes handles kwargs internally, # so we skip optim_args to avoid Transformers parsing issues. # Also skip if the string doesn't contain \"=\" (would cause parsing errors). should_skip_optim_args = ( not optim_args_str or # None or empty string not optim_args_str . strip () or # Whitespace only \"=\" not in optim_args_str or # Invalid format (no key=value pairs) any ( x in optimizer_name . lower () for x in [ \"8bit\" , \"8_bit\" , \"bnb\" , \"paged\" ]) # 8-bit optimizers ) if not should_skip_optim_args : training_kwargs [ \"optim_args\" ] = optim_args_str training_args = TrainingArguments ( ** training_kwargs ) # Create trainer based on task type if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: self . _setup_classification_trainer ( training_args ) else : # Generation tasks use SFTTrainer # Setup PEFT config if enabled (required for quantized models) peft_config = None if getattr ( self . config . model , 'peft_enabled' , False ): try : from peft import LoraConfig # Get LoRA parameters from config lora_r = getattr ( self . config . model , 'lora_rank' , 16 ) lora_alpha = getattr ( self . config . model , 'lora_alpha' , 32 ) lora_dropout = getattr ( self . config . model , 'lora_dropout' , 0.1 ) target_modules = getattr ( self . config . model , 'target_modules' , None ) # If target_modules not specified, use default for Llama models if target_modules is None : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) logger . info ( f \"\u2705 PEFT LoRA config created: r= { lora_r } , alpha= { lora_alpha } , modules= { target_modules } \" ) except ImportError : logger . warning ( \"\u26a0\ufe0f PEFT not available. Install with: pip install peft\" ) peft_config = None # Generation tasks use SFTTrainer self . trainer = SFTTrainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , peft_config = peft_config , # Pass PEFT config to SFTTrainer ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( f \"Starting TRL SFT Training - Task: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Log training metrics if self . logging_manager and hasattr ( train_result , 'metrics' ): self . logging_manager . log_metrics ( train_result . metrics ) # Save model self . trainer . save_model () # Add to training history self . training_history . append ({ 'timestamp' : time . time (), 'duration' : training_duration , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'task_type' : self . task_type . value , 'model_path' : self . config . logging . output_dir , }) logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT training completed successfully!\" ) logger . info ( \"=\" * 80 ) # Return training results return { 'training_time' : training_duration , 'final_loss' : getattr ( train_result , 'train_loss' , 0.0 ), 'model_path' : self . config . logging . output_dir , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'epochs' : getattr ( train_result , 'epoch' , 0 ), 'metrics' : getattr ( train_result , 'metrics' , {}), 'task_type' : self . task_type . value } def _setup_classification_trainer ( self , training_args ): \"\"\"Setup trainer for classification tasks - FIXED VERSION.\"\"\" from transformers import Trainer , DataCollatorWithPadding import evaluate if self . task_type == TaskType . TEXT_CLASSIFICATION : # CRITICAL: Verify dataset has both text and labels BEFORE tokenization logger . info ( f \"Dataset columns before tokenization: { self . dataset . column_names } \" ) logger . info ( f \"Sample before tokenization: { self . dataset [ 0 ] } \" ) # Ensure we have the required columns if 'text' not in self . dataset . column_names or 'labels' not in self . dataset . column_names : raise ValueError ( f \"Dataset must have 'text' and 'labels' columns. \" f \"Found: { self . dataset . column_names } \" ) def tokenize_function ( examples ): \"\"\"Tokenize text and preserve labels - FIXED\"\"\" # Get texts - handle both list and single values texts = examples [ 'text' ] if not isinstance ( texts , list ): texts = [ texts ] # Clean texts texts = [ str ( t ) if t is not None else \"\" for t in texts ] # Tokenize WITHOUT max_length padding here tokenized = self . tokenizer ( texts , truncation = True , padding = False , # FIX: Don't pad here, let collator handle it max_length = self . config . model . max_seq_length , return_tensors = None ) # Get labels - handle both list and single values labels = examples [ 'labels' ] if not isinstance ( labels , list ): labels = [ labels ] # FIX: Ensure labels are integers and valid try : tokenized [ 'labels' ] = [ int ( label ) for label in labels ] except ( ValueError , TypeError ) as e : logger . error ( f \"Label conversion error: { e } \" ) logger . error ( f \"Problematic labels: { labels } \" ) raise return tokenized # Store original column names original_train_cols = self . dataset . column_names . copy () # Tokenize train dataset - REMOVE original columns logger . info ( \"Tokenizing train dataset...\" ) self . dataset = self . dataset . map ( tokenize_function , batched = True , remove_columns = original_train_cols , desc = \"Tokenizing train dataset\" ) # Tokenize eval dataset if it exists if self . eval_dataset : original_eval_cols = self . eval_dataset . column_names . copy () logger . info ( \"Tokenizing eval dataset...\" ) self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = original_eval_cols , desc = \"Tokenizing eval dataset\" ) # Verify tokenization logger . info ( f \"\u2713 Train dataset columns after tokenization: { self . dataset . column_names } \" ) sample = self . dataset [ 0 ] logger . info ( f \"\u2713 Sample keys: { list ( sample . keys ()) } \" ) logger . info ( f \"\u2713 Sample input_ids length: { len ( sample [ 'input_ids' ]) } \" ) logger . info ( f \"\u2713 Sample label: { sample [ 'labels' ] } \" ) logger . info ( f \"\u2713 Sample label type: { type ( sample [ 'labels' ]) } \" ) # FIX: Verify label distribution and validity sample_size = min ( 100 , len ( self . dataset )) all_labels = [ self . dataset [ i ][ 'labels' ] for i in range ( sample_size )] unique_labels = set ( all_labels ) label_counts = { label : all_labels . count ( label ) for label in unique_labels } logger . info ( f \"\u2713 First { sample_size } labels distribution: { label_counts } \" ) logger . info ( f \"\u2713 Unique labels: { unique_labels } \" ) logger . info ( f \"\u2713 Expected labels: { set ( range ( self . config . model . num_labels )) } \" ) # Validate labels expected_labels = set ( range ( self . config . model . num_labels )) if not unique_labels . issubset ( expected_labels ): raise ValueError ( f \"Found unexpected labels: { unique_labels - expected_labels } . \" f \"Expected labels in range [0, { self . config . model . num_labels } )\" ) if len ( unique_labels ) == 1 : logger . error ( f \"\u274c ERROR: All labels are { list ( unique_labels )[ 0 ] } !\" ) raise ValueError ( \"Dataset has only one label class after tokenization!\" ) # FIX: Use DataCollatorWithPadding that handles padding dynamically data_collator = DataCollatorWithPadding ( tokenizer = self . tokenizer , padding = True , # Pad dynamically to longest in batch max_length = self . config . model . max_seq_length , pad_to_multiple_of = 8 # Optimize for GPU ) logger . info ( \"\u2713 Data collator created with dynamic padding\" ) def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics\"\"\" predictions , labels = eval_pred # Handle logits if len ( predictions . shape ) > 1 : predictions = np . argmax ( predictions , axis = 1 ) # Log predictions distribution unique_preds = np . unique ( predictions ) unique_labels = np . unique ( labels ) logger . info ( f \"Predictions distribution: { np . bincount ( predictions ) } \" ) logger . info ( f \"Labels distribution: { np . bincount ( labels . astype ( int )) } \" ) metrics = {} # Accuracy accuracy_metric = evaluate . load ( \"accuracy\" ) metrics . update ( accuracy_metric . compute ( predictions = predictions , references = labels )) # F1 score try : f1_metric = evaluate . load ( \"f1\" ) metrics . update ( f1_metric . compute ( predictions = predictions , references = labels , average = 'weighted' )) except Exception as e : logger . warning ( f \"Could not compute F1: { e } \" ) return metrics # Create trainer self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , data_collator = data_collator , compute_metrics = compute_metrics ) logger . info ( \"\u2713 Classification trainer created successfully\" ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : # Tokenize for token classification def tokenize_and_align_labels ( examples ): tokenized = self . tokenizer ( examples [ 'tokens' ], truncation = True , is_split_into_words = True , max_length = self . config . model . max_seq_length ) labels = [] for i , label in enumerate ( examples [ 'labels' ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : label_ids . append ( - 100 ) elif word_idx != previous_word_idx : label_ids . append ( label [ word_idx ]) else : label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ \"labels\" ] = labels return tokenized self . dataset = self . dataset . map ( tokenize_and_align_labels , batched = True ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_and_align_labels , batched = True ) data_collator = DataCollatorForTokenClassification ( tokenizer = self . tokenizer ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred predictions = np . argmax ( predictions , axis = 2 ) true_predictions = [ [ p for ( p , l ) in zip ( pred , label ) if l != - 100 ] for pred , label in zip ( predictions , labels ) ] true_labels = [ [ l for ( p , l ) in zip ( pred , label ) if l != - 100 ] for pred , label in zip ( predictions , labels ) ] flat_preds = [ item for sublist in true_predictions for item in sublist ] flat_labels = [ item for sublist in true_labels for item in sublist ] accuracy = evaluate . load ( \"accuracy\" ) return accuracy . compute ( predictions = flat_preds , references = flat_labels ) self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , data_collator = data_collator , compute_metrics = compute_metrics ) def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model and tokenizer. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : output_dir = Path ( self . config . logging . output_dir ) / \"sft_model\" else : output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) if hasattr ( self , 'trainer' ) and self . trainer is not None : self . trainer . save_model ( output_dir ) logger . info ( f \"Model saved to { output_dir } \" ) else : logger . warning ( \"No trainer available to save model\" ) if hasattr ( self , 'tokenizer' ) and self . tokenizer is not None : self . tokenizer . save_pretrained ( output_dir ) logger . info ( f \"Tokenizer saved to { output_dir } \" ) else : logger . warning ( \"No tokenizer available to save\" ) # Save task type info config_path = output_dir / \"training_config.yaml\" config_dict = { 'task_type' : self . task_type . value , 'model_name' : self . config . model . name_or_path } with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f ) logger . info ( f \"Training config saved to { config_path } \" ) # Store for push_to_hub self . _last_save_path = str ( output_dir ) return str ( output_dir ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute training step (handled by TRL trainer).\"\"\" return { \"loss\" : 0.0 } def create_data_loader ( self ) -> DataLoader : \"\"\"Create data loader for training.\"\"\" if self . dataset is None : raise RuntimeError ( \"Dataset not loaded. Call setup_data() first.\" ) from torch.utils.data import DataLoader return DataLoader ( self . dataset , batch_size = self . config . train . per_device_batch_size , shuffle = True , collate_fn = self . _collate_fn ) def _collate_fn ( self , batch ): \"\"\"Collate function for data loader.\"\"\" if not batch : return {} # Extract text from batch texts = [] for item in batch : if isinstance ( item , dict ): if \"text\" in item : texts . append ( item [ \"text\" ]) elif \"input_ids\" in item : return item else : for key , value in item . items (): if isinstance ( value , str ) and len ( value . strip ()) > 0 : texts . append ( value ) break elif isinstance ( item , str ): texts . append ( item ) if not texts : raise ValueError ( \"No valid text found in batch\" ) return self . tokenizer ( texts , padding = True , truncation = True , max_length = self . config . model . max_seq_length , return_tensors = \"pt\" ) def get_sample_outputs ( self ) -> list : \"\"\"Get sample outputs for logging.\"\"\" if not hasattr ( self , 'dataset' ) or self . dataset is None : return [] # Classification tasks don't generate samples if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: return [] sample_outputs = [] for i in range ( min ( 3 , len ( self . dataset ))): sample = self . dataset [ i ] if \"text\" in sample : text = sample [ \"text\" ] elif \"messages\" in sample : messages = sample [ \"messages\" ] text = self . tokenizer . apply_chat_template ( messages , tokenize = False ) else : text = str ( sample . get ( \"input\" , \"\" )) inputs = self . tokenizer ( text , return_tensors = \"pt\" , truncation = True , max_length = 512 ) with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 50 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), pad_token_id = self . tokenizer . eos_token_id ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) sample_outputs . append ({ \"input\" : text [: 100 ] + \"...\" if len ( text ) > 100 else text , \"output\" : response , \"model\" : \"trl_sft\" , \"task_type\" : self . task_type . value }) return sample_outputs def cleanup ( self ) -> None : \"\"\"Cleanup TRL resources.\"\"\" super () . cleanup () if self . model is not None : del self . model torch . cuda . empty_cache () logger . info ( \"TRL SFT trainer cleanup completed\" ) def evaluate ( self , eval_config : Optional [ Dict ] = None , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the model performance with enhanced metrics.\"\"\" logger . info ( f \"Running TRL SFT evaluation for task: { self . task_type . value } \" ) if not self . trainer : raise ValueError ( \"Training not setup. Call train() first.\" ) if eval_dataset : self . eval_dataset = eval_dataset # Run evaluation eval_results = {} if hasattr ( self . trainer , 'evaluate' ): if getattr ( self , 'eval_dataset' , None ) is not None : eval_results = self . trainer . evaluate () else : logger . warning ( \"No eval_dataset available; skipping evaluation.\" ) if self . logging_manager : self . logging_manager . log_metrics ( eval_results ) # Add task type to results eval_results [ 'task_type' ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator for generation tasks if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]: try : from aligntune.core.sft.evaluator import SFTEvaluator if self . model and self . tokenizer and hasattr ( self , 'eval_dataset' ) and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) # Generate qualitative samples for generation tasks if self . task_type not in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : samples = self . _generate_samples () eval_results [ 'qualitative_samples' ] = samples except Exception as e : logger . warning ( f \"Could not generate qualitative samples: { str ( e ) } \" ) eval_results [ 'qualitative_samples' ] = [] logger . info ( f \"TRL SFT evaluation completed: { len ( eval_results ) } metrics computed\" ) return eval_results def _generate_samples ( self ) -> List [ Dict [ str , str ]]: \"\"\"Generate qualitative samples for manual inspection.\"\"\" samples = [] self . model . eval () # Task-specific prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : sample_prompts = [ \"<|im_start|>user \\n What is artificial intelligence?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain machine learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Write a Python function.<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : sample_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Explain quantum computing. \\n Assistant:\" ] else : sample_prompts = [ \"The future of artificial intelligence is\" , \"In a world where technology advances rapidly,\" , \"The most important skill for the 21st century is\" ] for i , prompt in enumerate ( sample_prompts [: 3 ]): try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): generated = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), top_p = 0.9 , repetition_penalty = 1.1 , pad_token_id = self . tokenizer . eos_token_id , ) generated_text = self . tokenizer . decode ( generated [ 0 ], skip_special_tokens = True ) response = generated_text [ len ( prompt ):] . strip () samples . append ({ 'prompt' : prompt , 'generated_response' : response , 'task_type' : self . task_type . value }) logger . info ( f \"Generated sample { i + 1 } /3\" ) except Exception as e : logger . warning ( f \"Error generating sample { i } : { str ( e ) } \" ) samples . append ({ 'prompt' : prompt , 'generated_response' : f \"Error: { str ( e ) } \" , 'task_type' : self . task_type . value }) return samples def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Public method to generate samples.\"\"\" if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Sample generation not supported for classification tasks\" ) return [] try : samples = self . _generate_samples () # Return limited number and ensure all have 'response' key return [ s for s in samples [: num_samples ] if 'generated_response' in s or 'response' in s ] except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] def run_zero_shot_evaluation ( self , test_prompts = None ) -> Dict [ str , Any ]: \"\"\"Run zero-shot evaluation before training with enhanced metrics.\"\"\" logger . info ( f \"Running zero-shot evaluation for task: { self . task_type . value } \" ) # Classification tasks don't support zero-shot evaluation if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Zero-shot evaluation not supported for classification tasks\" ) return {} if test_prompts is None : # Task-specific default prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : test_prompts = [ \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain ML.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : test_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Tell me about AI. \\n Assistant:\" ] else : test_prompts = [ \"What is artificial intelligence?\" , \"Explain machine learning briefly.\" , \"How does a computer work?\" ] results = [] self . model . eval () for prompt in test_prompts : try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = False , pad_token_id = self . tokenizer . eos_token_id , temperature = 0.7 , top_p = 0.9 ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () results . append ({ 'prompt' : prompt , 'response' : response , 'response_length' : len ( response . split ()), 'coherence_score' : self . _calculate_coherence_score ( response ) }) except Exception as e : logger . warning ( f \"Zero-shot evaluation failed for prompt: { e } \" ) results . append ({ 'prompt' : prompt , 'response' : f \"Error: { str ( e ) } \" , 'response_length' : 0 , 'coherence_score' : 0.0 }) # Calculate aggregate metrics avg_length = sum ( r [ 'response_length' ] for r in results ) / len ( results ) if results else 0 avg_coherence = sum ( r [ 'coherence_score' ] for r in results ) / len ( results ) if results else 0 successful_responses = len ([ r for r in results if not r [ 'response' ] . startswith ( 'Error' )]) zero_shot_metrics = { 'task_type' : self . task_type . value , 'zero_shot_results' : results , 'avg_response_length' : avg_length , 'avg_coherence_score' : avg_coherence , 'successful_responses' : successful_responses , 'success_rate' : successful_responses / len ( results ) if results else 0 } if self . logging_manager : self . logging_manager . log_metrics ({ 'zero_shot_avg_length' : avg_length , 'zero_shot_coherence' : avg_coherence , 'zero_shot_success_rate' : successful_responses / len ( results ) if results else 0 }) logger . info ( f \"Zero-shot evaluation completed. Success rate: { successful_responses } / { len ( results ) } \" ) return zero_shot_metrics def _calculate_coherence_score ( self , text : str ) -> float : \"\"\"Calculate a simple coherence score for generated text.\"\"\" if not text or len ( text . strip ()) == 0 : return 0.0 words = text . split () if len ( words ) == 0 : return 0.0 # Check for repetition unique_words = len ( set ( words )) repetition_penalty = unique_words / len ( words ) # Check for reasonable length length_score = min ( 1.0 , len ( words ) / 20.0 ) if len ( words ) < 20 else max ( 0.5 , 40.0 / len ( words )) # Simple readability check sentences = text . split ( '.' ) avg_sentence_length = sum ( len ( s . split ()) for s in sentences ) / len ( sentences ) if sentences else 0 readability_score = 1.0 if 5 <= avg_sentence_length <= 25 else 0.7 # Combine scores coherence_score = ( repetition_penalty + length_score + readability_score ) / 3.0 return min ( 1.0 , max ( 0.0 , coherence_score )) def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get enhanced training statistics and information.\"\"\" stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'dataset_name' : self . config . dataset . name , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'learning_rate' : self . config . train . learning_rate , 'batch_size' : self . config . train . per_device_batch_size , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : self . config . model . precision if hasattr ( self . config . model , 'precision' ) else 'auto' , }, 'dataset_info' : { 'train_size' : len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 0 , 'val_size' : len ( self . eval_dataset ) if hasattr ( self , 'eval_dataset' ) and self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save enhanced configuration to YAML file.\"\"\" config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'max_seq_length' : self . config . model . max_seq_length , 'learning_rate' : self . config . train . learning_rate , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'batch_size' : self . config . train . per_device_batch_size , 'dataset_name' : self . config . dataset . name , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : str ( self . config . model . precision ) if hasattr ( self . config . model , 'precision' ) else 'auto' , } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL SFT configuration saved to { path } \" ) def run_experiment ( self ): \"\"\"Run a complete experiment with enhanced features.\"\"\" logger . info ( f \"Starting TRL SFT experiment: { self . config . logging . run_name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) try : # Setup model and data self . setup_model () self . setup_data () # Run zero-shot evaluation if configured zero_shot_results = None if hasattr ( self . config , 'evaluation' ) and getattr ( self . config . evaluation , 'run_zero_shot' , False ): logger . info ( \"Running pre-training zero-shot evaluation...\" ) zero_shot_results = self . run_zero_shot_evaluation () # Start training logger . info ( \"Starting training...\" ) start_time = time . time () train_results = self . train () end_time = time . time () training_duration = end_time - start_time # Evaluate after training logger . info ( \"Running post-training evaluation...\" ) eval_results = self . evaluate () # Combine results results = { 'mode' : 'train_and_eval' , 'task_type' : self . task_type . value , 'train_results' : train_results , 'eval_results' : eval_results , 'training_stats' : self . get_training_stats (), 'config' : { 'model' : self . config . model . __dict__ if hasattr ( self . config . model , '__dict__' ) else str ( self . config . model ), 'dataset' : self . config . dataset . __dict__ if hasattr ( self . config . dataset , '__dict__' ) else str ( self . config . dataset ), 'train' : self . config . train . __dict__ if hasattr ( self . config . train , '__dict__' ) else str ( self . config . train ), } } # Add zero-shot results if available if zero_shot_results : results [ 'zero_shot_evaluation' ] = zero_shot_results # Save results output_dir = Path ( self . config . logging . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) results_path = output_dir / \"experiment_results.yaml\" with open ( results_path , 'w' ) as f : yaml . dump ( results , f , default_flow_style = False ) logger . info ( f \"TRL SFT experiment completed. Results saved to { results_path } \" ) return results except Exception as e : logger . error ( f \"TRL SFT experiment failed: { e } \" ) import traceback logger . error ( traceback . format_exc ()) raise finally : if self . logging_manager : self . logging_manager . finish ()","title":"TRLSFTTrainer"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.cleanup","text":"Cleanup TRL resources. Source code in src/aligntune/backends/trl/sft/sft.py 1276 1277 1278 1279 1280 1281 1282 1283 1284 def cleanup ( self ) -> None : \"\"\"Cleanup TRL resources.\"\"\" super () . cleanup () if self . model is not None : del self . model torch . cuda . empty_cache () logger . info ( \"TRL SFT trainer cleanup completed\" )","title":"cleanup"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.create_data_loader","text":"Create data loader for training. Source code in src/aligntune/backends/trl/sft/sft.py 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 def create_data_loader ( self ) -> DataLoader : \"\"\"Create data loader for training.\"\"\" if self . dataset is None : raise RuntimeError ( \"Dataset not loaded. Call setup_data() first.\" ) from torch.utils.data import DataLoader return DataLoader ( self . dataset , batch_size = self . config . train . per_device_batch_size , shuffle = True , collate_fn = self . _collate_fn )","title":"create_data_loader"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.evaluate","text":"Evaluate the model performance with enhanced metrics. Source code in src/aligntune/backends/trl/sft/sft.py 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 def evaluate ( self , eval_config : Optional [ Dict ] = None , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the model performance with enhanced metrics.\"\"\" logger . info ( f \"Running TRL SFT evaluation for task: { self . task_type . value } \" ) if not self . trainer : raise ValueError ( \"Training not setup. Call train() first.\" ) if eval_dataset : self . eval_dataset = eval_dataset # Run evaluation eval_results = {} if hasattr ( self . trainer , 'evaluate' ): if getattr ( self , 'eval_dataset' , None ) is not None : eval_results = self . trainer . evaluate () else : logger . warning ( \"No eval_dataset available; skipping evaluation.\" ) if self . logging_manager : self . logging_manager . log_metrics ( eval_results ) # Add task type to results eval_results [ 'task_type' ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator for generation tasks if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ]: try : from aligntune.core.sft.evaluator import SFTEvaluator if self . model and self . tokenizer and hasattr ( self , 'eval_dataset' ) and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) # Generate qualitative samples for generation tasks if self . task_type not in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: try : samples = self . _generate_samples () eval_results [ 'qualitative_samples' ] = samples except Exception as e : logger . warning ( f \"Could not generate qualitative samples: { str ( e ) } \" ) eval_results [ 'qualitative_samples' ] = [] logger . info ( f \"TRL SFT evaluation completed: { len ( eval_results ) } metrics computed\" ) return eval_results","title":"evaluate"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.generate_samples","text":"Public method to generate samples. Source code in src/aligntune/backends/trl/sft/sft.py 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Public method to generate samples.\"\"\" if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Sample generation not supported for classification tasks\" ) return [] try : samples = self . _generate_samples () # Return limited number and ensure all have 'response' key return [ s for s in samples [: num_samples ] if 'generated_response' in s or 'response' in s ] except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return []","title":"generate_samples"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.get_sample_outputs","text":"Get sample outputs for logging. Source code in src/aligntune/backends/trl/sft/sft.py 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 def get_sample_outputs ( self ) -> list : \"\"\"Get sample outputs for logging.\"\"\" if not hasattr ( self , 'dataset' ) or self . dataset is None : return [] # Classification tasks don't generate samples if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: return [] sample_outputs = [] for i in range ( min ( 3 , len ( self . dataset ))): sample = self . dataset [ i ] if \"text\" in sample : text = sample [ \"text\" ] elif \"messages\" in sample : messages = sample [ \"messages\" ] text = self . tokenizer . apply_chat_template ( messages , tokenize = False ) else : text = str ( sample . get ( \"input\" , \"\" )) inputs = self . tokenizer ( text , return_tensors = \"pt\" , truncation = True , max_length = 512 ) with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 50 , do_sample = True , temperature = getattr ( self . config . train , 'temperature' , 0.7 ), pad_token_id = self . tokenizer . eos_token_id ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) sample_outputs . append ({ \"input\" : text [: 100 ] + \"...\" if len ( text ) > 100 else text , \"output\" : response , \"model\" : \"trl_sft\" , \"task_type\" : self . task_type . value }) return sample_outputs","title":"get_sample_outputs"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.get_training_stats","text":"Get enhanced training statistics and information. Source code in src/aligntune/backends/trl/sft/sft.py 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get enhanced training statistics and information.\"\"\" stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'dataset_name' : self . config . dataset . name , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'learning_rate' : self . config . train . learning_rate , 'batch_size' : self . config . train . per_device_batch_size , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : self . config . model . precision if hasattr ( self . config . model , 'precision' ) else 'auto' , }, 'dataset_info' : { 'train_size' : len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 0 , 'val_size' : len ( self . eval_dataset ) if hasattr ( self , 'eval_dataset' ) and self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats","title":"get_training_stats"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.is_available","text":"Check if TRL is available. Source code in src/aligntune/backends/trl/sft/sft.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.run_experiment","text":"Run a complete experiment with enhanced features. Source code in src/aligntune/backends/trl/sft/sft.py 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 def run_experiment ( self ): \"\"\"Run a complete experiment with enhanced features.\"\"\" logger . info ( f \"Starting TRL SFT experiment: { self . config . logging . run_name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) try : # Setup model and data self . setup_model () self . setup_data () # Run zero-shot evaluation if configured zero_shot_results = None if hasattr ( self . config , 'evaluation' ) and getattr ( self . config . evaluation , 'run_zero_shot' , False ): logger . info ( \"Running pre-training zero-shot evaluation...\" ) zero_shot_results = self . run_zero_shot_evaluation () # Start training logger . info ( \"Starting training...\" ) start_time = time . time () train_results = self . train () end_time = time . time () training_duration = end_time - start_time # Evaluate after training logger . info ( \"Running post-training evaluation...\" ) eval_results = self . evaluate () # Combine results results = { 'mode' : 'train_and_eval' , 'task_type' : self . task_type . value , 'train_results' : train_results , 'eval_results' : eval_results , 'training_stats' : self . get_training_stats (), 'config' : { 'model' : self . config . model . __dict__ if hasattr ( self . config . model , '__dict__' ) else str ( self . config . model ), 'dataset' : self . config . dataset . __dict__ if hasattr ( self . config . dataset , '__dict__' ) else str ( self . config . dataset ), 'train' : self . config . train . __dict__ if hasattr ( self . config . train , '__dict__' ) else str ( self . config . train ), } } # Add zero-shot results if available if zero_shot_results : results [ 'zero_shot_evaluation' ] = zero_shot_results # Save results output_dir = Path ( self . config . logging . output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) results_path = output_dir / \"experiment_results.yaml\" with open ( results_path , 'w' ) as f : yaml . dump ( results , f , default_flow_style = False ) logger . info ( f \"TRL SFT experiment completed. Results saved to { results_path } \" ) return results except Exception as e : logger . error ( f \"TRL SFT experiment failed: { e } \" ) import traceback logger . error ( traceback . format_exc ()) raise finally : if self . logging_manager : self . logging_manager . finish ()","title":"run_experiment"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.run_zero_shot_evaluation","text":"Run zero-shot evaluation before training with enhanced metrics. Source code in src/aligntune/backends/trl/sft/sft.py 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 def run_zero_shot_evaluation ( self , test_prompts = None ) -> Dict [ str , Any ]: \"\"\"Run zero-shot evaluation before training with enhanced metrics.\"\"\" logger . info ( f \"Running zero-shot evaluation for task: { self . task_type . value } \" ) # Classification tasks don't support zero-shot evaluation if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . warning ( \"Zero-shot evaluation not supported for classification tasks\" ) return {} if test_prompts is None : # Task-specific default prompts if self . task_type == TaskType . INSTRUCTION_FOLLOWING : test_prompts = [ \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Explain ML.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : test_prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Tell me about AI. \\n Assistant:\" ] else : test_prompts = [ \"What is artificial intelligence?\" , \"Explain machine learning briefly.\" , \"How does a computer work?\" ] results = [] self . model . eval () for prompt in test_prompts : try : inputs = self . tokenizer ( prompt , return_tensors = \"pt\" ) if torch . cuda . is_available (): inputs = { k : v . to ( self . model . device ) for k , v in inputs . items ()} with torch . no_grad (): outputs = self . model . generate ( ** inputs , max_new_tokens = 100 , do_sample = False , pad_token_id = self . tokenizer . eos_token_id , temperature = 0.7 , top_p = 0.9 ) response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () results . append ({ 'prompt' : prompt , 'response' : response , 'response_length' : len ( response . split ()), 'coherence_score' : self . _calculate_coherence_score ( response ) }) except Exception as e : logger . warning ( f \"Zero-shot evaluation failed for prompt: { e } \" ) results . append ({ 'prompt' : prompt , 'response' : f \"Error: { str ( e ) } \" , 'response_length' : 0 , 'coherence_score' : 0.0 }) # Calculate aggregate metrics avg_length = sum ( r [ 'response_length' ] for r in results ) / len ( results ) if results else 0 avg_coherence = sum ( r [ 'coherence_score' ] for r in results ) / len ( results ) if results else 0 successful_responses = len ([ r for r in results if not r [ 'response' ] . startswith ( 'Error' )]) zero_shot_metrics = { 'task_type' : self . task_type . value , 'zero_shot_results' : results , 'avg_response_length' : avg_length , 'avg_coherence_score' : avg_coherence , 'successful_responses' : successful_responses , 'success_rate' : successful_responses / len ( results ) if results else 0 } if self . logging_manager : self . logging_manager . log_metrics ({ 'zero_shot_avg_length' : avg_length , 'zero_shot_coherence' : avg_coherence , 'zero_shot_success_rate' : successful_responses / len ( results ) if results else 0 }) logger . info ( f \"Zero-shot evaluation completed. Success rate: { successful_responses } / { len ( results ) } \" ) return zero_shot_metrics","title":"run_zero_shot_evaluation"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.save_config","text":"Save enhanced configuration to YAML file. Source code in src/aligntune/backends/trl/sft/sft.py 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 def save_config ( self , path : str ): \"\"\"Save enhanced configuration to YAML file.\"\"\" config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : self . task_type . value , 'max_seq_length' : self . config . model . max_seq_length , 'learning_rate' : self . config . train . learning_rate , 'epochs' : self . config . train . num_epochs if hasattr ( self . config . train , 'num_epochs' ) else None , 'max_steps' : self . config . train . max_steps , 'batch_size' : self . config . train . per_device_batch_size , 'dataset_name' : self . config . dataset . name , 'use_peft' : self . config . model . use_peft if hasattr ( self . config . model , 'use_peft' ) else False , 'precision' : str ( self . config . model . precision ) if hasattr ( self . config . model , 'precision' ) else 'auto' , } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL SFT configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.save_model","text":"Save the trained model and tokenizer. Parameters: Name Type Description Default output_dir Optional [ str ] Directory to save to (defaults to config output_dir) None Returns: Type Description str Path where model was saved Source code in src/aligntune/backends/trl/sft/sft.py 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model and tokenizer. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : output_dir = Path ( self . config . logging . output_dir ) / \"sft_model\" else : output_dir = Path ( output_dir ) output_dir . mkdir ( parents = True , exist_ok = True ) if hasattr ( self , 'trainer' ) and self . trainer is not None : self . trainer . save_model ( output_dir ) logger . info ( f \"Model saved to { output_dir } \" ) else : logger . warning ( \"No trainer available to save model\" ) if hasattr ( self , 'tokenizer' ) and self . tokenizer is not None : self . tokenizer . save_pretrained ( output_dir ) logger . info ( f \"Tokenizer saved to { output_dir } \" ) else : logger . warning ( \"No tokenizer available to save\" ) # Save task type info config_path = output_dir / \"training_config.yaml\" config_dict = { 'task_type' : self . task_type . value , 'model_name' : self . config . model . name_or_path } with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f ) logger . info ( f \"Training config saved to { config_path } \" ) # Store for push_to_hub self . _last_save_path = str ( output_dir ) return str ( output_dir )","title":"save_model"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.setup_data","text":"Setup datasets for SFT training with task-aware formatting. Source code in src/aligntune/backends/trl/sft/sft.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 def setup_data ( self ) -> None : \"\"\"Setup datasets for SFT training with task-aware formatting.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT datasets: { self . config . dataset . name } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Initialize dataset cache cache_root = getattr ( self . config , 'caching' , {}) . get ( \"root\" , \"./cache\" ) if hasattr ( self . config , 'caching' ) else \"./cache\" self . dataset_cache = DatasetCache ( cache_root = cache_root ) logger . info ( f \"Dataset cache initialized at: { self . dataset_cache . cache_root } \" ) # Load dataset logger . info ( \"Loading dataset...\" ) dataset_config = self . config . dataset from datasets import load_dataset # Handle dataset config/subset load_kwargs = { 'split' : dataset_config . split or \"train\" } dataset_name = dataset_config . name dataset_subset = None if hasattr ( dataset_config , 'subset' ) and dataset_config . subset : dataset_subset = dataset_config . subset elif hasattr ( dataset_config , 'config' ) and dataset_config . config : dataset_subset = dataset_config . config # Check if dataset has a registered custom loader if dataset_name in DatasetRegistry . list_loaders (): logger . info ( f \"Using registered dataset loader: { dataset_name } \" ) dataset = DatasetRegistry . load_dataset ( name = dataset_name , split = dataset_config . split or \"train\" , max_samples = dataset_config . max_samples , ** { k : v for k , v in dataset_config . __dict__ . items () if k not in [ 'name' , 'split' , 'max_samples' ] and v is not None } ) else : # Load dataset try : if dataset_subset : dataset = load_dataset ( dataset_name , dataset_subset , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ValueError as e : if \"Config name is missing\" in str ( e ): raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' to your DatasetConfig.\" ) raise logger . info ( f \"Original dataset size: { len ( dataset ) } samples\" ) # \ud83d\udd39 CRITICAL FIX: Shuffle dataset BEFORE selecting samples # This ensures balanced class distribution for classification tasks if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: logger . info ( \"Shuffling dataset to ensure balanced sampling...\" ) dataset = dataset . shuffle ( seed = 42 ) # Apply dataset size limits if dataset_config . percent and dataset_config . percent < 1.0 : dataset_size = int ( len ( dataset ) * dataset_config . percent ) dataset = dataset . select ( range ( dataset_size )) logger . info ( f \"Using { dataset_config . percent * 100 } % of dataset: { dataset_size } samples\" ) elif dataset_config . max_samples : dataset = dataset . select ( range ( min ( dataset_config . max_samples , len ( dataset )))) logger . info ( f \"Using max_samples limit: { len ( dataset ) } samples\" ) # Apply column mappings if needed if hasattr ( dataset_config , 'column_mapping' ) and dataset_config . column_mapping : valid_mappings = { k : v for k , v in dataset_config . column_mapping . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , dataset_config ) # Create evaluation split try : test_size = 0.1 if len ( dataset ) >= 1000 : test_size = 0.1 elif len ( dataset ) >= 200 : test_size = 0.1 else : test_size = min ( 0.2 , max ( 0.1 , 50 / max ( 1 , len ( dataset )))) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 , shuffle = True ) self . dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Created eval split. Train: { len ( self . dataset ) } | Eval: { len ( self . eval_dataset ) } \" ) except Exception as e : self . dataset = dataset self . eval_dataset = None logger . warning ( f \"Could not create eval split: { str ( e ) } \" ) logger . info ( \"=\" * 80 ) logger . info ( f \"TRL SFT dataset setup completed: { len ( self . dataset ) } samples\" ) logger . info ( \"=\" * 80 )","title":"setup_data"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.setup_model","text":"Setup model using standard Transformers with task type awareness. Source code in src/aligntune/backends/trl/sft/sft.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers with task type awareness.\"\"\" logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL SFT model: { self . config . model . name_or_path } \" ) logger . info ( f \"Task Type: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) try : from transformers import ( AutoModelForCausalLM , AutoModelForSequenceClassification , AutoModelForTokenClassification , AutoTokenizer ) except ImportError as e : raise ImportError ( \"Transformers not available.\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model based on task type logger . info ( f \"Loading model for task: { self . task_type . value } \" ) if self . task_type == TaskType . TEXT_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 2 ) logger . info ( f \"Loading classification model with { num_labels } labels\" ) self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) elif self . task_type == TaskType . TOKEN_CLASSIFICATION : num_labels = getattr ( self . config . model , 'num_labels' , 9 ) logger . info ( f \"Loading token classification model with { num_labels } labels\" ) self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , num_labels = num_labels , torch_dtype = dtype , trust_remote_code = True , ) else : # Causal LM for generation tasks logger . info ( \"Loading causal language model\" ) # Handle quantization configuration quantization_config = None quantization_dict = getattr ( self . config . model , 'quantization' , {}) if quantization_dict . get ( 'load_in_4bit' , False ) or quantization_dict . get ( 'load_in_8bit' , False ): try : from transformers import BitsAndBytesConfig load_in_4bit = quantization_dict . get ( 'load_in_4bit' , False ) load_in_8bit = quantization_dict . get ( 'load_in_8bit' , False ) if load_in_4bit : compute_dtype_str = quantization_dict . get ( 'bnb_4bit_compute_dtype' , 'bfloat16' ) compute_dtype = torch . bfloat16 if compute_dtype_str == 'bfloat16' else torch . float16 quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_compute_dtype = compute_dtype , bnb_4bit_quant_type = quantization_dict . get ( 'bnb_4bit_quant_type' , 'nf4' ), bnb_4bit_use_double_quant = quantization_dict . get ( 'bnb_4bit_use_double_quant' , False ), ) logger . info ( \"\u2705 4-bit quantization enabled for memory optimization\" ) elif load_in_8bit : quantization_config = BitsAndBytesConfig ( load_in_8bit = True ) logger . info ( \"\u2705 8-bit quantization enabled for memory optimization\" ) except ImportError : logger . warning ( \"\u26a0\ufe0f BitsAndBytes not available. Install with: pip install bitsandbytes\" ) quantization_config = None # Get max_memory from config if specified max_memory = getattr ( self . config . model , 'max_memory' , None ) model_kwargs = { 'torch_dtype' : dtype , 'trust_remote_code' : True , 'device_map' : 'auto' if torch . cuda . is_available () else None , 'low_cpu_mem_usage' : True , } # Add max_memory if specified if max_memory : model_kwargs [ 'max_memory' ] = max_memory if quantization_config : model_kwargs [ 'quantization_config' ] = quantization_config logger . info ( f \"Quantization config: { quantization_config } \" ) else : # Only set these if not using quantization model_kwargs [ 'load_in_4bit' ] = False model_kwargs [ 'load_in_8bit' ] = False logger . info ( \"No quantization configured - loading full precision model\" ) self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Move to GPU only if not using device_map='auto' (which handles it automatically) if not quantization_config and torch . cuda . is_available () and model_kwargs . get ( 'device_map' ) != 'auto' : self . model = self . model . cuda () logger . info ( \"Model moved to GPU\" ) elif quantization_config : logger . info ( \"Model loaded with quantization (device_map='auto')\" ) # CRITICAL: When using quantization, PEFT adapters MUST be enabled # TRL SFTTrainer will handle this, but we need to ensure PEFT is enabled in config if not getattr ( self . config . model , 'peft_enabled' , False ): logger . warning ( \"\u26a0\ufe0f Quantization requires PEFT adapters. Enabling PEFT automatically.\" ) self . config . model . peft_enabled = True # Setup tokenizer for specific tasks self . _setup_tokenizer_for_task () logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 )","title":"setup_model"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.setup_rewards","text":"SFT doesn't use reward functions. Source code in src/aligntune/backends/trl/sft/sft.py 699 700 701 702 def setup_rewards ( self ) -> None : \"\"\"SFT doesn't use reward functions.\"\"\" logger . info ( \"SFT training doesn't use reward functions\" ) self . reward_functions = []","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.train","text":"Train using TRL SFTTrainer with task-aware setup. Source code in src/aligntune/backends/trl/sft/sft.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def train ( self ) -> Dict [ str , Any ]: \"\"\"Train using TRL SFTTrainer with task-aware setup.\"\"\" logger . info ( f \"Starting TRL SFT training for task: { self . task_type . value } \" ) # Setup model and data if not already done if self . model is None : self . setup_model () if self . dataset is None : self . setup_data () try : from trl import SFTTrainer from transformers import TrainingArguments , Trainer from transformers import DataCollatorWithPadding , DataCollatorForTokenClassification import evaluate except ImportError as e : raise ImportError ( \"TRL required for SFT training. Install with: pip install trl\" ) from e # Create training arguments # Get optimizer and scheduler configurations from ....core.optimization import get_optimizer_for_config , get_scheduler_for_config # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate or 2e-4 , self . config . train . weight_decay or 0.01 ) # Calculate max_steps - use config value or calculate from epochs max_steps = self . config . train . max_steps if max_steps is None : # Calculate from epochs if max_steps not specified epochs = getattr ( self . config . train , 'epochs' , 3 ) or 3 dataset_size = len ( self . dataset ) if hasattr ( self , 'dataset' ) and self . dataset else 1000 batch_size = self . config . train . per_device_batch_size or 2 grad_accum = self . config . train . gradient_accumulation_steps or 4 max_steps = ( dataset_size * epochs ) // ( batch_size * grad_accum ) max_steps = max ( max_steps , 10 ) # Minimum 10 steps logger . info ( f \"Calculated max_steps= { max_steps } from { epochs } epochs, { dataset_size } samples\" ) # Calculate warmup steps warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" # Exclude lr and weight_decay since TrainingArguments already sets them filtered = { k : v for k , v in kwargs_dict . items () if k not in [ 'lr' , 'learning_rate' , 'weight_decay' ]} if not filtered : return None return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in filtered . items ()) optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) # print(lr_scheduler_kwargs_str) # print(scheduler_config) filtered_scheduler_kwargs = { k : v for k , v in scheduler_config [ 'lr_scheduler_kwargs' ] . items () if k not in [ 'num_training_steps' , 'num_warmup_steps' ] } # Build TrainingArguments kwargs, conditionally including optim_args training_kwargs = { \"output_dir\" : self . config . logging . output_dir , \"max_steps\" : max_steps , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size or 2 , \"gradient_accumulation_steps\" : self . config . train . gradient_accumulation_steps or 4 , \"learning_rate\" : self . config . train . learning_rate or 2e-4 , \"weight_decay\" : self . config . train . weight_decay or 0.01 , \"warmup_steps\" : warmup_steps , \"logging_steps\" : getattr ( self . config . logging , 'log_interval' , 10 ), \"save_steps\" : getattr ( self . config . train , 'save_interval' , 500 ), \"eval_steps\" : getattr ( self . config . train , 'eval_interval' , 500 ), \"eval_strategy\" : \"no\" , \"save_strategy\" : \"steps\" , \"load_best_model_at_end\" : False , \"report_to\" : [], \"run_name\" : self . config . logging . run_name , \"seed\" : 42 , ** precision_args , \"dataloader_num_workers\" : 0 , \"remove_unused_columns\" : False , \"optim\" : optimizer_name , \"lr_scheduler_type\" : scheduler_name , \"lr_scheduler_kwargs\" : filtered_scheduler_kwargs , } # Only include optim_args if it's not None/empty and valid. # For 8-bit optimizers (adamw_8bit, paged_adamw_8bit, etc.), bitsandbytes handles kwargs internally, # so we skip optim_args to avoid Transformers parsing issues. # Also skip if the string doesn't contain \"=\" (would cause parsing errors). should_skip_optim_args = ( not optim_args_str or # None or empty string not optim_args_str . strip () or # Whitespace only \"=\" not in optim_args_str or # Invalid format (no key=value pairs) any ( x in optimizer_name . lower () for x in [ \"8bit\" , \"8_bit\" , \"bnb\" , \"paged\" ]) # 8-bit optimizers ) if not should_skip_optim_args : training_kwargs [ \"optim_args\" ] = optim_args_str training_args = TrainingArguments ( ** training_kwargs ) # Create trainer based on task type if self . task_type in [ TaskType . TEXT_CLASSIFICATION , TaskType . TOKEN_CLASSIFICATION ]: self . _setup_classification_trainer ( training_args ) else : # Generation tasks use SFTTrainer # Setup PEFT config if enabled (required for quantized models) peft_config = None if getattr ( self . config . model , 'peft_enabled' , False ): try : from peft import LoraConfig # Get LoRA parameters from config lora_r = getattr ( self . config . model , 'lora_rank' , 16 ) lora_alpha = getattr ( self . config . model , 'lora_alpha' , 32 ) lora_dropout = getattr ( self . config . model , 'lora_dropout' , 0.1 ) target_modules = getattr ( self . config . model , 'target_modules' , None ) # If target_modules not specified, use default for Llama models if target_modules is None : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) logger . info ( f \"\u2705 PEFT LoRA config created: r= { lora_r } , alpha= { lora_alpha } , modules= { target_modules } \" ) except ImportError : logger . warning ( \"\u26a0\ufe0f PEFT not available. Install with: pip install peft\" ) peft_config = None # Generation tasks use SFTTrainer self . trainer = SFTTrainer ( model = self . model , args = training_args , train_dataset = self . dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , peft_config = peft_config , # Pass PEFT config to SFTTrainer ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( f \"Starting TRL SFT Training - Task: { self . task_type . value } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Log training metrics if self . logging_manager and hasattr ( train_result , 'metrics' ): self . logging_manager . log_metrics ( train_result . metrics ) # Save model self . trainer . save_model () # Add to training history self . training_history . append ({ 'timestamp' : time . time (), 'duration' : training_duration , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'task_type' : self . task_type . value , 'model_path' : self . config . logging . output_dir , }) logger . info ( \"=\" * 80 ) logger . info ( \"TRL SFT training completed successfully!\" ) logger . info ( \"=\" * 80 ) # Return training results return { 'training_time' : training_duration , 'final_loss' : getattr ( train_result , 'train_loss' , 0.0 ), 'model_path' : self . config . logging . output_dir , 'steps' : getattr ( train_result , 'global_step' , 0 ), 'epochs' : getattr ( train_result , 'epoch' , 0 ), 'metrics' : getattr ( train_result , 'metrics' , {}), 'task_type' : self . task_type . value }","title":"train"},{"location":"api-reference/trainers/#backends.trl.sft.sft.TRLSFTTrainer.train_step","text":"Execute training step (handled by TRL trainer). Source code in src/aligntune/backends/trl/sft/sft.py 1183 1184 1185 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute training step (handled by TRL trainer).\"\"\" return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.sft.sft import TRLSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = TRLSFTTrainer ( config ) trainer . train ()","title":"train_step"},{"location":"api-reference/trainers/#trldpotrainer","text":"TRL backend for Direct Preference Optimization. Bases: TrainerBase TRL-based DPO trainer using TRL's DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 class TRLDPOTrainer ( TrainerBase ): \"\"\"TRL-based DPO trainer using TRL's DPOTrainer.\"\"\" def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize TRL DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . dataset_cache = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL DPO trainer is available.\"\"\" try : from trl import DPOTrainer , DPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default # Required abstract methods def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO (uses preferences).\"\"\" logger . info ( \"DPO uses preference pairs instead of explicit rewards\" ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Single training step - handled internally by TRL DPOTrainer.\"\"\" if not self . trainer : raise RuntimeError ( \"Trainer not initialized. Call train() first.\" ) # The actual training loop is handled by self.trainer.train() return {} def setup_model ( self ) -> None : \"\"\"Setup model and tokenizer for DPO.\"\"\" try : from transformers import AutoModelForCausalLM , AutoTokenizer , BitsAndBytesConfig logger . info ( f \"Setting up TRL DPO model: { self . config . model . name_or_path } \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL DPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Determine dtype dtype = None if self . config . model . precision . value == \"bf16\" : dtype = torch . bfloat16 elif self . config . model . precision . value == \"fp16\" : dtype = torch . float16 elif self . config . model . precision . value == \"fp32\" : dtype = torch . float32 # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Setup quantization config quantization_config = None use_quantization = False if self . config . model . quantization : if self . config . model . quantization . get ( \"load_in_4bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_quant_type = \"nf4\" , bnb_4bit_compute_dtype = dtype or torch . bfloat16 , bnb_4bit_use_double_quant = True , ) use_quantization = True logger . info ( \"Using 4-bit quantization with BitsAndBytesConfig\" ) elif self . config . model . quantization . get ( \"load_in_8bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_8bit = True , ) use_quantization = True logger . info ( \"Using 8-bit quantization with BitsAndBytesConfig\" ) # Load main model model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : model_kwargs [ \"quantization_config\" ] = quantization_config self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # ---- Extract LoRA config safely ---- lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) # ---- Apply LoRA ---- from peft import LoraConfig , get_peft_model if use_quantization : from peft import prepare_model_for_kbit_training self . model = prepare_model_for_kbit_training ( self . model ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to quantized model\" ) elif getattr ( self . config . model , \"use_peft\" , False ): peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to model\" ) # Create reference model (frozen copy) # For quantized models, reference model uses same quantization but no LoRA ref_model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : ref_model_kwargs [ \"quantization_config\" ] = quantization_config self . reference_model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** ref_model_kwargs ) logger . info ( \"TRL DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup TRL DPO model: { e } \" ) raise def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer.\"\"\" try : from trl import DPOTrainer , DPOConfig logger . info ( \"Setting up TRL DPOTrainer\" ) # Create DPO configuration # Get optimizer and scheduler configurations from aligntune.core.optimization import get_optimizer_for_config , get_scheduler_for_config # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate , self . config . train . weight_decay or 0.01 ) # Calculate warmup steps num_train_epochs = self . config . train . epochs or 1 # Estimate max_steps for scheduler if not provided max_steps = getattr ( self . config . train , 'max_steps' , None ) if max_steps is None and hasattr ( self . train_dataset , '__len__' ): # Rough estimation: steps = epochs * (dataset_size / batch_size / accumulation) dataset_size = len ( self . train_dataset ) batch_size = self . config . train . per_device_batch_size or 1 accumulation = self . config . train . gradient_accumulation_steps or 1 effective_batch_size = batch_size * accumulation max_steps = int ( num_train_epochs * dataset_size / effective_batch_size ) else : max_steps = 1000 # fallback warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) # print( scheduler_config) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in kwargs_dict . items ()) if kwargs_dict else None optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = None ) # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Get max_steps from config (if set, it overrides num_train_epochs) config_max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = None ) dpo_config = DPOConfig ( output_dir = self . config . logging . output_dir , num_train_epochs = num_train_epochs , max_steps = config_max_steps if config_max_steps else - 1 , # -1 means use num_train_epochs per_device_train_batch_size = self . config . train . per_device_batch_size , per_device_eval_batch_size = self . config . train . per_device_batch_size , # ADD THIS gradient_accumulation_steps = self . config . train . gradient_accumulation_steps , learning_rate = self . config . train . learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Optimizer and scheduler lr_scheduler_type = scheduler_name , optim = optimizer_name , weight_decay = optimizer_config [ 'optimizer_kwargs' ] . get ( 'weight_decay' , 0.01 ), optim_args = optim_args_str , # Precision ** precision_args , # Data handling dataloader_pin_memory = False , remove_unused_columns = False , # DPO-specific parameters beta = self . config . train . beta , loss_type = getattr ( self . config . train , 'loss_type' , 'sigmoid' ), max_length = self . config . model . max_seq_length , max_prompt_length = getattr ( self . config . train , 'max_prompt_length' , self . config . model . max_seq_length // 2 ), truncation_mode = getattr ( self . config . train , 'truncation_mode' , 'keep_end' ), precompute_ref_log_probs = getattr ( self . config . train , 'precompute_ref_log_probs' , False ), # Logging configuration report_to = report_to if report_to else ( self . config . logging . loggers if self . config . logging . loggers else []), ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , processing_class = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting TRL DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"TRL DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 ) } except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained model.\"\"\" # try: # if not self.trainer or not self.eval_dataset: # logger.warning(\"No trainer or evaluation dataset available\") # return {} # logger.info(\"Running DPO evaluation\") # # Run evaluation # eval_result = self.trainer.evaluate() # logger.info(\"DPO evaluation completed\") # return { # \"eval_loss\": eval_result.get(\"eval_loss\", 0), # \"eval_metrics\": eval_result # } # except Exception as e: # logger.error(f\"DPO evaluation failed: {e}\") # return {} # def evaluate(self, eval_dataset=None, metric_key_prefix: str = \"eval\",use_custom_evaluator=False, **kwargs) -> Dict[str, float]: # # Setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"TRLDPOTrainer"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.__init__","text":"Initialize TRL DPO trainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize TRL DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . dataset_cache = None self . dataset_dict = None","title":"__init__"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.is_available","text":"Check if TRL DPO trainer is available. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 39 40 41 42 43 44 45 46 47 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL DPO trainer is available.\"\"\" try : from trl import DPOTrainer , DPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.load_model","text":"Load a trained model. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.save_model","text":"Save model. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.setup_data","text":"Setup data - delegates to setup_dataset. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 62 63 64 def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset ()","title":"setup_data"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.setup_dataset","text":"Setup and prepare preference dataset for DPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise","title":"setup_dataset"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.setup_model","text":"Setup model and tokenizer for DPO. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def setup_model ( self ) -> None : \"\"\"Setup model and tokenizer for DPO.\"\"\" try : from transformers import AutoModelForCausalLM , AutoTokenizer , BitsAndBytesConfig logger . info ( f \"Setting up TRL DPO model: { self . config . model . name_or_path } \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL DPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Determine dtype dtype = None if self . config . model . precision . value == \"bf16\" : dtype = torch . bfloat16 elif self . config . model . precision . value == \"fp16\" : dtype = torch . float16 elif self . config . model . precision . value == \"fp32\" : dtype = torch . float32 # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , trust_remote_code = True ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Setup quantization config quantization_config = None use_quantization = False if self . config . model . quantization : if self . config . model . quantization . get ( \"load_in_4bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_4bit = True , bnb_4bit_quant_type = \"nf4\" , bnb_4bit_compute_dtype = dtype or torch . bfloat16 , bnb_4bit_use_double_quant = True , ) use_quantization = True logger . info ( \"Using 4-bit quantization with BitsAndBytesConfig\" ) elif self . config . model . quantization . get ( \"load_in_8bit\" , False ): quantization_config = BitsAndBytesConfig ( load_in_8bit = True , ) use_quantization = True logger . info ( \"Using 8-bit quantization with BitsAndBytesConfig\" ) # Load main model model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : model_kwargs [ \"quantization_config\" ] = quantization_config self . model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # ---- Extract LoRA config safely ---- lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) # ---- Apply LoRA ---- from peft import LoraConfig , get_peft_model if use_quantization : from peft import prepare_model_for_kbit_training self . model = prepare_model_for_kbit_training ( self . model ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to quantized model\" ) elif getattr ( self . config . model , \"use_peft\" , False ): peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) logger . info ( \"Applied LoRA adapters to model\" ) # Create reference model (frozen copy) # For quantized models, reference model uses same quantization but no LoRA ref_model_kwargs = { \"torch_dtype\" : dtype , \"device_map\" : self . config . model . device_map or \"auto\" , \"trust_remote_code\" : True , } if quantization_config is not None : ref_model_kwargs [ \"quantization_config\" ] = quantization_config self . reference_model = AutoModelForCausalLM . from_pretrained ( self . config . model . name_or_path , ** ref_model_kwargs ) logger . info ( \"TRL DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup TRL DPO model: { e } \" ) raise","title":"setup_model"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.setup_rewards","text":"Setup rewards - not used in DPO (uses preferences). Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 66 67 68 def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO (uses preferences).\"\"\" logger . info ( \"DPO uses preference pairs instead of explicit rewards\" )","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.setup_trainer","text":"Setup TRL DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer.\"\"\" try : from trl import DPOTrainer , DPOConfig logger . info ( \"Setting up TRL DPOTrainer\" ) # Create DPO configuration # Get optimizer and scheduler configurations from aligntune.core.optimization import get_optimizer_for_config , get_scheduler_for_config # Use config-specified optimizer or default to adamw_torch optimizer_name = getattr ( self . config . train , 'optimizer' , 'adamw_torch' ) scheduler_name = getattr ( self . config . train , 'lr_scheduler' , 'cosine' ) # Create optimizer configuration optimizer_config = get_optimizer_for_config ( optimizer_name , self . config . train . learning_rate , self . config . train . weight_decay or 0.01 ) # Calculate warmup steps num_train_epochs = self . config . train . epochs or 1 # Estimate max_steps for scheduler if not provided max_steps = getattr ( self . config . train , 'max_steps' , None ) if max_steps is None and hasattr ( self . train_dataset , '__len__' ): # Rough estimation: steps = epochs * (dataset_size / batch_size / accumulation) dataset_size = len ( self . train_dataset ) batch_size = self . config . train . per_device_batch_size or 1 accumulation = self . config . train . gradient_accumulation_steps or 1 effective_batch_size = batch_size * accumulation max_steps = int ( num_train_epochs * dataset_size / effective_batch_size ) else : max_steps = 1000 # fallback warmup_steps = getattr ( self . config . train , 'warmup_steps' , 0 ) if hasattr ( self . config . train , 'warmup_ratio' ) and self . config . train . warmup_ratio : warmup_steps = int ( max_steps * self . config . train . warmup_ratio ) # Create scheduler configuration scheduler_config = get_scheduler_for_config ( scheduler_name , max_steps , warmup_steps ) # print( scheduler_config) def kwargs_to_str ( kwargs_dict ): \"\"\"Convert optimizer/scheduler kwargs dict to string format.\"\"\" return \",\" . join ( f \" { k } = { f '( { ',' . join ( map ( str , v )) } )' if isinstance ( v , tuple ) else v } \" for k , v in kwargs_dict . items ()) if kwargs_dict else None optim_args_str = kwargs_to_str ( optimizer_config [ 'optimizer_kwargs' ]) precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = None ) # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Get max_steps from config (if set, it overrides num_train_epochs) config_max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = None ) dpo_config = DPOConfig ( output_dir = self . config . logging . output_dir , num_train_epochs = num_train_epochs , max_steps = config_max_steps if config_max_steps else - 1 , # -1 means use num_train_epochs per_device_train_batch_size = self . config . train . per_device_batch_size , per_device_eval_batch_size = self . config . train . per_device_batch_size , # ADD THIS gradient_accumulation_steps = self . config . train . gradient_accumulation_steps , learning_rate = self . config . train . learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Optimizer and scheduler lr_scheduler_type = scheduler_name , optim = optimizer_name , weight_decay = optimizer_config [ 'optimizer_kwargs' ] . get ( 'weight_decay' , 0.01 ), optim_args = optim_args_str , # Precision ** precision_args , # Data handling dataloader_pin_memory = False , remove_unused_columns = False , # DPO-specific parameters beta = self . config . train . beta , loss_type = getattr ( self . config . train , 'loss_type' , 'sigmoid' ), max_length = self . config . model . max_seq_length , max_prompt_length = getattr ( self . config . train , 'max_prompt_length' , self . config . model . max_seq_length // 2 ), truncation_mode = getattr ( self . config . train , 'truncation_mode' , 'keep_end' ), precompute_ref_log_probs = getattr ( self . config . train , 'precompute_ref_log_probs' , False ), # Logging configuration report_to = report_to if report_to else ( self . config . logging . loggers if self . config . logging . loggers else []), ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , processing_class = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.train","text":"Run DPO training. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting TRL DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"TRL DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 ) } except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise","title":"train"},{"location":"api-reference/trainers/#backends.trl.rl.dpo.dpo.TRLDPOTrainer.train_step","text":"Single training step - handled internally by TRL DPOTrainer. Source code in src/aligntune/backends/trl/rl/dpo/dpo.py 70 71 72 73 74 75 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Single training step - handled internally by TRL DPOTrainer.\"\"\" if not self . trainer : raise RuntimeError ( \"Trainer not initialized. Call train() first.\" ) # The actual training loop is handled by self.trainer.train() return {} options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.rl.dpo.dpo import TRLDPOTrainer from aligntune.core.rl.config import UnifiedConfig , AlgorithmType config = UnifiedConfig ( algo = AlgorithmType . DPO , ... ) trainer = TRLDPOTrainer ( config ) trainer . train ()","title":"train_step"},{"location":"api-reference/trainers/#trlppotrainer","text":"TRL backend for Proximal Policy Optimization. Bases: TrainerBase PPO trainer using pure TRL PPOTrainer with math, code, and enhanced rewards. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 class TRLPPOTrainer ( TrainerBase ): \"\"\"PPO trainer using pure TRL PPOTrainer with math, code, and enhanced rewards.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . policy_model = None self . ref_model = None self . reward_model = None self . value_model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . train_dataset = None self . eval_dataset = None self . reward_functions = [] self . training_history = [] self . logging_manager = None self . custom_evaluator = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import PPOTrainer , PPOConfig from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup models (policy, reference, reward, value) using standard Transformers.\"\"\" # Extract model config values safely policy_model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) # Check if integrated reward training is configured if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training is not None ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) else : # Extract from config as normal reward_model_name = self . _get_config_value ( self . config . model , 'reward_model_name' , 'reward_path' , default = None ) value_model_name = self . _get_config_value ( self . config . model , 'reward_value_model' , default = None ) sft_model_path = self . _get_config_value ( self . config . model , 'sft_path' , default = None ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL PPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Quantization settings load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # Auto-detect quantization from model name if not load_in_4bit and not load_in_8bit : model_name_lower = policy_model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { policy_model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { policy_model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL PPO models\" ) logger . info ( f \"Policy model: { policy_model_name } \" ) logger . info ( f \"SFT path: { sft_model_path or 'Same as policy' } \" ) logger . info ( f \"Reward model: { reward_model_name or 'Same as value model' } \" ) logger . info ( f \"Value model: { value_model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( policy_model_name , trust_remote_code = trust_remote_code , ) if self . tokenizer . padding_side != \"left\" : self . tokenizer . padding_side = \"left\" # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Common model kwargs base_model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified for policy model policy_model_kwargs = base_model_kwargs . copy () if load_in_4bit : logger . info ( \"Loading policy model with 4-bit quantization...\" ) policy_model_kwargs [ \"load_in_4bit\" ] = True policy_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 policy_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" policy_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading policy model with 8-bit quantization...\" ) policy_model_kwargs [ \"load_in_8bit\" ] = True # Load policy model (from SFT path if provided, else base model) logger . info ( \"Loading policy model...\" ) policy_path = sft_model_path or policy_model_name self . policy_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** policy_model_kwargs ) # Apply PEFT if specified if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration to policy model...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing policy model for k-bit training...\" ) self . policy_model = prepare_model_for_kbit_training ( self . policy_model ) # Extract PEFT config values lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . policy_model = get_peft_model ( self . policy_model , peft_config ) self . policy_model . print_trainable_parameters () logger . info ( \"PEFT adapters applied to policy model successfully\" ) # Reference model is None when using PEFT (TRL handles this) self . ref_model = None logger . info ( \"Reference model: None (PEFT mode - TRL will handle reference)\" ) else : # Load reference model (frozen copy of SFT model) logger . info ( \"Loading reference model (frozen copy)...\" ) self . ref_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** base_model_kwargs ) self . ref_model . eval () for param in self . ref_model . parameters (): param . requires_grad = False logger . info ( \"Reference model loaded and frozen\" ) # Load reward and value models logger . info ( \"Loading reward and value models...\" ) # Reward model quantization settings reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , default = {}) reward_load_4bit = reward_quant . get ( 'load_in_4bit' , False ) reward_load_8bit = reward_quant . get ( 'load_in_8bit' , False ) reward_model_kwargs = base_model_kwargs . copy () if reward_load_4bit : logger . info ( \"Loading reward model with 4-bit quantization...\" ) reward_model_kwargs [ \"load_in_4bit\" ] = True reward_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 reward_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" reward_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif reward_load_8bit : logger . info ( \"Loading reward model with 8-bit quantization...\" ) reward_model_kwargs [ \"load_in_8bit\" ] = True # Value model quantization settings value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , default = {}) value_load_4bit = value_quant . get ( 'load_in_4bit' , False ) value_load_8bit = value_quant . get ( 'load_in_8bit' , False ) value_model_kwargs = base_model_kwargs . copy () if value_load_4bit : logger . info ( \"Loading value model with 4-bit quantization...\" ) value_model_kwargs [ \"load_in_4bit\" ] = True value_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 value_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" value_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif value_load_8bit : logger . info ( \"Loading value model with 8-bit quantization...\" ) value_model_kwargs [ \"load_in_8bit\" ] = True # Load reward model (or use value model if not specified) try : reward_path = reward_model_name or value_model_name self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_path , num_labels = 1 , ** reward_model_kwargs ) logger . info ( f \"Reward model loaded from: { reward_path } \" ) except : self . reward_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** reward_model_kwargs ) # Load value model try : self . value_model = AutoModelForSequenceClassification . from_pretrained ( value_model_name , num_labels = 1 , ** value_model_kwargs ) logger . info ( f \"Value model loaded from: { value_model_name } \" ) except : self . value_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** value_model_kwargs ) def _train_custom_reward_model ( self ) -> str : \"\"\"Train a custom reward model using the reward_training config.\"\"\" from aligntune.rewards import RewardModelTrainer from aligntune.rewards.factory import create_reward_functions reward_config = self . config . reward_training # Validate required fields if not hasattr ( reward_config , 'base_model_name' ) or not reward_config . base_model_name : raise ValueError ( \"reward_training.base_model_name is required\" ) if not hasattr ( reward_config , 'output_dir' ) or not reward_config . output_dir : raise ValueError ( \"reward_training.output_dir is required\" ) if not hasattr ( reward_config , 'reward_functions' ) or not reward_config . reward_functions : raise ValueError ( \"reward_training.reward_functions is required\" ) if not hasattr ( reward_config , 'training_texts' ) or not reward_config . training_texts : raise ValueError ( \"reward_training.training_texts is required (minimum 10 samples)\" ) logger . info ( f \"Training reward model with base: { reward_config . base_model_name } \" ) logger . info ( f \"Output directory: { reward_config . output_dir } \" ) logger . info ( f \"Training samples: { len ( reward_config . training_texts ) } \" ) logger . info ( f \"Reward functions: { reward_config . reward_functions } \" ) # Create reward functions from config # reward_functions is a List[str] according to config from aligntune.rewards.registry import RewardRegistry reward_func_list = [] for rf_name in reward_config . reward_functions : try : rf = RewardRegistry . get_reward_function ( rf_name ) reward_func_list . append ( rf ) except Exception as e : logger . warning ( f \"Could not load reward function ' { rf_name } ': { e } \" ) if not reward_func_list : raise ValueError ( \"No valid reward functions could be loaded\" ) # Get composite weights if provided composite_weights = None if hasattr ( reward_config , 'reward_weights' ) and reward_config . reward_weights : composite_weights = reward_config . reward_weights else : composite_weights = [ 1.0 ] * len ( reward_func_list ) # Create reward model trainer trainer = RewardModelTrainer ( base_model_name = reward_config . base_model_name , reward_functions = reward_func_list , composite_weights = composite_weights ) # Generate training dataset from training_texts logger . info ( \"Generating training data from reward functions...\" ) training_data = trainer . generate_training_data ( texts = reward_config . training_texts , references = reward_config . reference_texts if hasattr ( reward_config , 'reference_texts' ) else None , batch_size = reward_config . batch_size ) logger . info ( f \"Generated { len ( training_data ) } training examples\" ) # Train the reward model logger . info ( \"Starting reward model training...\" ) model_path = trainer . train_reward_model ( training_data = training_data , output_dir = reward_config . output_dir , num_epochs = reward_config . num_epochs , learning_rate = reward_config . learning_rate , batch_size = reward_config . batch_size , gradient_accumulation_steps = reward_config . gradient_accumulation_steps ) logger . info ( f \"\u2705 Custom reward model training completed: { model_path } \" ) return model_path def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task (prompt-only format) from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict [ \"train\" ] self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # Apply sampling if specified if max_samples : logger . info ( f \"Limiting train dataset to { max_samples } samples\" ) self . train_dataset = self . train_dataset . select ( range ( min ( max_samples , len ( self . train_dataset )))) elif percent and percent < 100 : num_samples = int ( len ( self . train_dataset ) * percent / 100 ) logger . info ( f \"Using { percent } % of dataset ( { num_samples } samples)\" ) self . train_dataset = self . train_dataset . select ( range ( num_samples )) if self . eval_dataset and max_samples : eval_max = max_samples // 10 # Use 10% for eval self . eval_dataset = self . eval_dataset . select ( range ( min ( eval_max , len ( self . eval_dataset )))) logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO print ( \"Tokenizing dataset...\" ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" # Get prompt column (DataManager ensures consistent naming) prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) # Tokenize tokenized = self . tokenizer ( prompts , padding = False , # Keep False like TRL example truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] # Calculate lengths for each sequence in the batch lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length (like TRL example) logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure (like TRL example checks) assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for PPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments including prompts, references, etc. \"\"\" if not completions : return [] batch_rewards = [] # Extract batch data from kwargs test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # Check multiple possible reference column names references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'ground_truth' , 'response' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] break if references is None : references = [ None ] * len ( completions ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): try : reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : reward = reward_func ( completion ) except TypeError : try : reward = reward_func ( completion , reference = reference ) except TypeError : try : reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_trainer ( self ) -> None : \"\"\"Set up the PPO trainer with all configurations.\"\"\" logger . info ( \"Setting up TRL PPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.01 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # PPO-specific parameters kl_coef = self . _get_config_value ( self . config . train , 'kl_coef' , default = 0.1 ) cliprange = self . _get_config_value ( self . config . train , 'cliprange' , default = 0.2 ) cliprange_value = self . _get_config_value ( self . config . train , 'cliprange_value' , default = 0.2 ) vf_coef = self . _get_config_value ( self . config . train , 'vf_coef' , default = 0.1 ) gamma = self . _get_config_value ( self . config . train , 'gamma' , default = 1.0 ) lam = self . _get_config_value ( self . config . train , 'lam' , default = 0.95 ) # Generation parameters response_length = self . _get_config_value ( self . config . train , 'response_length' , 'max_completion_length' , default = 128 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) stop_token = self . _get_config_value ( self . config . train , 'stop_token' , default = 'eos' ) missing_eos_penalty = self . _get_config_value ( self . config . train , 'missing_eos_penalty' , default = 1.0 ) # Evaluation and checkpointing max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = 1000 ) eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) # Logging logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Max steps: { max_steps } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"KL coefficient: { kl_coef } \" ) logger . info ( f \"Clip range: { cliprange } \" ) logger . info ( f \"Value function coef: { vf_coef } \" ) logger . info ( f \"Response length: { response_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup PPO trainer from trl import PPOTrainer , PPOConfig ppo_config = PPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , # PPO-specific kl_coef = kl_coef , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , gamma = gamma , lam = lam , # Generation response_length = response_length , temperature = temperature , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , # Evaluation max_steps = max_steps , eval_strategy = eval_strategy if self . eval_dataset else \"no\" , eval_steps = eval_steps if self . eval_dataset else None , # Checkpointing save_strategy = save_strategy , save_steps = save_steps , # Logging logging_steps = logging_steps , report_to = report_to if report_to else [], # Precision ** precision_args , # Other remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # Get PEFT config if using PEFT peft_config = None use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) if use_peft : from peft import LoraConfig lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) # Create PPO trainer self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = peft_config , ) logger . info ( \"PPO trainer setup completed successfully!\" ) def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL PPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) train_result = self . trainer . train () # Log samples after training try : raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step - handled by TRL internally.\"\"\" logger . debug ( \"train_step() called but TRL PPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader - handled by TRL internally.\"\"\" logger . debug ( \"create_data_loader() called but TRL PPO uses TRL's internal data loading\" ) return None # def evaluate(self) -> Dict[str, float]: # try: # logger.info(\"Running evaluation...\") # return self.trainer.evaluate() # except Exception as e: # logger.error(f\"Evaluation failed: {e}\") # return {} def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if self . train_dataset else 0 , 'val_size' : len ( self . eval_dataset ) if self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . policy_model is not None , 'device' : str ( next ( self . policy_model . parameters ()) . device ) if self . policy_model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . policy_model , 'peft_config' ) if self . policy_model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL PPO configuration saved to { path } \" ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained PPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/ppo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving PPO model to: { save_path } \" ) # Save using trainer self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"ppo_training_config.yaml\" self . save_config ( str ( config_path )) logger . info ( f \"PPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save PPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained PPO model.\"\"\" try : logger . info ( f \"Loading PPO model from: { path } \" ) from transformers import AutoModelForCausalLM , AutoTokenizer # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( path ) # Load model max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 2048 ) self . policy_model = AutoModelForCausalLM . from_pretrained ( path , device_map = \"auto\" , ) logger . info ( \"PPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load PPO model: { e } \" ) raise","title":"TRLPPOTrainer"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.create_data_loader","text":"Create data loader - handled by TRL internally. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1100 1101 1102 1103 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader - handled by TRL internally.\"\"\" logger . debug ( \"create_data_loader() called but TRL PPO uses TRL's internal data loading\" ) return None","title":"create_data_loader"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.get_training_stats","text":"Get training statistics. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if self . train_dataset else 0 , 'val_size' : len ( self . eval_dataset ) if self . eval_dataset else 0 , }, 'model_info' : { 'loaded' : self . policy_model is not None , 'device' : str ( next ( self . policy_model . parameters ()) . device ) if self . policy_model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . policy_model , 'peft_config' ) if self . policy_model else False , }, 'training_history' : self . training_history , } return stats","title":"get_training_stats"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.is_available","text":"Check if TRL is available. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 58 59 60 61 62 63 64 65 66 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import PPOTrainer , PPOConfig from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.load_model","text":"Load a trained PPO model. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 def load_model ( self , path : str ) -> None : \"\"\"Load a trained PPO model.\"\"\" try : logger . info ( f \"Loading PPO model from: { path } \" ) from transformers import AutoModelForCausalLM , AutoTokenizer # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( path ) # Load model max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 2048 ) self . policy_model = AutoModelForCausalLM . from_pretrained ( path , device_map = \"auto\" , ) logger . info ( \"PPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load PPO model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'proximal_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ), 'dataset_name' : self . config . datasets [ 0 ] . name if self . config . datasets else 'unknown' , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL PPO configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.save_model","text":"Save the trained PPO model. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained PPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/ppo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving PPO model to: { save_path } \" ) # Save using trainer self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"ppo_training_config.yaml\" self . save_config ( str ( config_path )) logger . info ( f \"PPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save PPO model: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.setup_data","text":"Setup datasets for PPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task (prompt-only format) from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict [ \"train\" ] self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # Apply sampling if specified if max_samples : logger . info ( f \"Limiting train dataset to { max_samples } samples\" ) self . train_dataset = self . train_dataset . select ( range ( min ( max_samples , len ( self . train_dataset )))) elif percent and percent < 100 : num_samples = int ( len ( self . train_dataset ) * percent / 100 ) logger . info ( f \"Using { percent } % of dataset ( { num_samples } samples)\" ) self . train_dataset = self . train_dataset . select ( range ( num_samples )) if self . eval_dataset and max_samples : eval_max = max_samples // 10 # Use 10% for eval self . eval_dataset = self . eval_dataset . select ( range ( min ( eval_max , len ( self . eval_dataset )))) logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO print ( \"Tokenizing dataset...\" ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" # Get prompt column (DataManager ensures consistent naming) prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) # Tokenize tokenized = self . tokenizer ( prompts , padding = False , # Keep False like TRL example truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] # Calculate lengths for each sequence in the batch lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length (like TRL example) logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure (like TRL example checks) assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" )","title":"setup_data"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.setup_model","text":"Setup models (policy, reference, reward, value) using standard Transformers. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def setup_model ( self ) -> None : \"\"\"Setup models (policy, reference, reward, value) using standard Transformers.\"\"\" # Extract model config values safely policy_model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) # Check if integrated reward training is configured if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training is not None ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) else : # Extract from config as normal reward_model_name = self . _get_config_value ( self . config . model , 'reward_model_name' , 'reward_path' , default = None ) value_model_name = self . _get_config_value ( self . config . model , 'reward_value_model' , default = None ) sft_model_path = self . _get_config_value ( self . config . model , 'sft_path' , default = None ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL PPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Quantization settings load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # Auto-detect quantization from model name if not load_in_4bit and not load_in_8bit : model_name_lower = policy_model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { policy_model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { policy_model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL PPO models\" ) logger . info ( f \"Policy model: { policy_model_name } \" ) logger . info ( f \"SFT path: { sft_model_path or 'Same as policy' } \" ) logger . info ( f \"Reward model: { reward_model_name or 'Same as value model' } \" ) logger . info ( f \"Value model: { value_model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoModelForSequenceClassification , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( policy_model_name , trust_remote_code = trust_remote_code , ) if self . tokenizer . padding_side != \"left\" : self . tokenizer . padding_side = \"left\" # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Common model kwargs base_model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified for policy model policy_model_kwargs = base_model_kwargs . copy () if load_in_4bit : logger . info ( \"Loading policy model with 4-bit quantization...\" ) policy_model_kwargs [ \"load_in_4bit\" ] = True policy_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 policy_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" policy_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading policy model with 8-bit quantization...\" ) policy_model_kwargs [ \"load_in_8bit\" ] = True # Load policy model (from SFT path if provided, else base model) logger . info ( \"Loading policy model...\" ) policy_path = sft_model_path or policy_model_name self . policy_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** policy_model_kwargs ) # Apply PEFT if specified if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration to policy model...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing policy model for k-bit training...\" ) self . policy_model = prepare_model_for_kbit_training ( self . policy_model ) # Extract PEFT config values lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . policy_model = get_peft_model ( self . policy_model , peft_config ) self . policy_model . print_trainable_parameters () logger . info ( \"PEFT adapters applied to policy model successfully\" ) # Reference model is None when using PEFT (TRL handles this) self . ref_model = None logger . info ( \"Reference model: None (PEFT mode - TRL will handle reference)\" ) else : # Load reference model (frozen copy of SFT model) logger . info ( \"Loading reference model (frozen copy)...\" ) self . ref_model = AutoModelForCausalLM . from_pretrained ( policy_path , ** base_model_kwargs ) self . ref_model . eval () for param in self . ref_model . parameters (): param . requires_grad = False logger . info ( \"Reference model loaded and frozen\" ) # Load reward and value models logger . info ( \"Loading reward and value models...\" ) # Reward model quantization settings reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , default = {}) reward_load_4bit = reward_quant . get ( 'load_in_4bit' , False ) reward_load_8bit = reward_quant . get ( 'load_in_8bit' , False ) reward_model_kwargs = base_model_kwargs . copy () if reward_load_4bit : logger . info ( \"Loading reward model with 4-bit quantization...\" ) reward_model_kwargs [ \"load_in_4bit\" ] = True reward_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 reward_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" reward_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif reward_load_8bit : logger . info ( \"Loading reward model with 8-bit quantization...\" ) reward_model_kwargs [ \"load_in_8bit\" ] = True # Value model quantization settings value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , default = {}) value_load_4bit = value_quant . get ( 'load_in_4bit' , False ) value_load_8bit = value_quant . get ( 'load_in_8bit' , False ) value_model_kwargs = base_model_kwargs . copy () if value_load_4bit : logger . info ( \"Loading value model with 4-bit quantization...\" ) value_model_kwargs [ \"load_in_4bit\" ] = True value_model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 value_model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" value_model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif value_load_8bit : logger . info ( \"Loading value model with 8-bit quantization...\" ) value_model_kwargs [ \"load_in_8bit\" ] = True # Load reward model (or use value model if not specified) try : reward_path = reward_model_name or value_model_name self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_path , num_labels = 1 , ** reward_model_kwargs ) logger . info ( f \"Reward model loaded from: { reward_path } \" ) except : self . reward_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** reward_model_kwargs ) # Load value model try : self . value_model = AutoModelForSequenceClassification . from_pretrained ( value_model_name , num_labels = 1 , ** value_model_kwargs ) logger . info ( f \"Value model loaded from: { value_model_name } \" ) except : self . value_model = AutoModelForSequenceClassification . from_pretrained ( policy_model_name , num_labels = 1 , ** value_model_kwargs )","title":"setup_model"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.setup_rewards","text":"Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for PPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" )","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.setup_trainer","text":"Set up the PPO trainer with all configurations. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 def setup_trainer ( self ) -> None : \"\"\"Set up the PPO trainer with all configurations.\"\"\" logger . info ( \"Setting up TRL PPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.01 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # PPO-specific parameters kl_coef = self . _get_config_value ( self . config . train , 'kl_coef' , default = 0.1 ) cliprange = self . _get_config_value ( self . config . train , 'cliprange' , default = 0.2 ) cliprange_value = self . _get_config_value ( self . config . train , 'cliprange_value' , default = 0.2 ) vf_coef = self . _get_config_value ( self . config . train , 'vf_coef' , default = 0.1 ) gamma = self . _get_config_value ( self . config . train , 'gamma' , default = 1.0 ) lam = self . _get_config_value ( self . config . train , 'lam' , default = 0.95 ) # Generation parameters response_length = self . _get_config_value ( self . config . train , 'response_length' , 'max_completion_length' , default = 128 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) stop_token = self . _get_config_value ( self . config . train , 'stop_token' , default = 'eos' ) missing_eos_penalty = self . _get_config_value ( self . config . train , 'missing_eos_penalty' , default = 1.0 ) # Evaluation and checkpointing max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default = 1000 ) eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) # Logging logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Max steps: { max_steps } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"KL coefficient: { kl_coef } \" ) logger . info ( f \"Clip range: { cliprange } \" ) logger . info ( f \"Value function coef: { vf_coef } \" ) logger . info ( f \"Response length: { response_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup PPO trainer from trl import PPOTrainer , PPOConfig ppo_config = PPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , # PPO-specific kl_coef = kl_coef , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , gamma = gamma , lam = lam , # Generation response_length = response_length , temperature = temperature , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , # Evaluation max_steps = max_steps , eval_strategy = eval_strategy if self . eval_dataset else \"no\" , eval_steps = eval_steps if self . eval_dataset else None , # Checkpointing save_strategy = save_strategy , save_steps = save_steps , # Logging logging_steps = logging_steps , report_to = report_to if report_to else [], # Precision ** precision_args , # Other remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # Get PEFT config if using PEFT peft_config = None use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) if use_peft : from peft import LoraConfig lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) # Create PPO trainer self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = peft_config , ) logger . info ( \"PPO trainer setup completed successfully!\" )","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.train","text":"Execute PPO training. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/ppo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL PPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) train_result = self . trainer . train () # Log samples after training try : raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL PPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results","title":"train"},{"location":"api-reference/trainers/#backends.trl.rl.ppo.ppo.TRLPPOTrainer.train_step","text":"Execute a single training step - handled by TRL internally. Source code in src/aligntune/backends/trl/rl/ppo/ppo.py 1095 1096 1097 1098 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step - handled by TRL internally.\"\"\" logger . debug ( \"train_step() called but TRL PPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.trl.rl.ppo.ppo import TRLPPOTrainer from aligntune.core.rl.config import UnifiedConfig config = UnifiedConfig ( algo = AlgorithmType . PPO , ... ) trainer = TRLPPOTrainer ( config ) trainer . train ()","title":"train_step"},{"location":"api-reference/trainers/#trlgrpotrainer","text":"TRL backend for Group Relative Policy Optimization. Bases: TrainerBase GRPO trainer using pure TRL GRPOTrainer with math, code, and enhanced rewards. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 class TRLGRPOTrainer ( TrainerBase ): \"\"\"GRPO trainer using pure TRL GRPOTrainer with math, code, and enhanced rewards.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . dataset = None self . eval_dataset = None # Add eval dataset support self . reward_functions = [] self . training_history = [] self . logging_manager = None # self.evaluator = None self . custom_evaluator = None # For BaseEvaluator/RLEvaluator self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import GRPOTrainer , GRPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers.\"\"\" # Extract model config values safely model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL GRPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Auto-detect quantization from model name if not explicitly set if not load_in_4bit and not load_in_8bit : model_name_lower = model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization (required for training # quantized models) if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL GRPO model: { model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( model_name , trust_remote_code = trust_remote_code , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model logger . info ( \"Loading model...\" ) model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified if load_in_4bit : logger . info ( \"Loading model with 4-bit quantization...\" ) model_kwargs [ \"load_in_4bit\" ] = True model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading model with 8-bit quantization...\" ) model_kwargs [ \"load_in_8bit\" ] = True self . model = AutoModelForCausalLM . from_pretrained ( model_name , ** model_kwargs ) # Apply PEFT if specified or if quantization is used if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing model for k-bit training...\" ) self . model = prepare_model_for_kbit_training ( self . model ) # Extract PEFT config values safely lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () logger . info ( \"PEFT adapters applied successfully\" ) logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 ) def setup_data ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) # Fallback: try to continue with other rewards rather than # failing continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments from TRL including: - prompts: List of prompts - test_list: List of test cases (for code datasets like MBPP) - answer/solution/reference: Reference answers (for math datasets) \"\"\" if not completions : return [] # Debug: log what TRL is passing (once) if not hasattr ( self , '_logged_kwargs' ): logger . info ( f \"[DEBUG] _combined_reward_function kwargs keys: { list ( kwargs . keys ()) } \" ) if 'test_list' in kwargs : sample = kwargs [ 'test_list' ][: 1 ] if kwargs [ 'test_list' ] else 'empty' logger . info ( f \"[DEBUG] test_list found, sample: { sample } \" ) else : logger . info ( \"[DEBUG] test_list NOT in kwargs!\" ) self . _logged_kwargs = True batch_rewards = [] # Extract batch data from kwargs (TRL passes these as lists) test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # CHECK MULTIPLE POSSIBLE REFERENCE COLUMN NAMES # Try different column names in order of preference references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'ground_truth' , 'response' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] # print(f\"[INFO] Found reference data in column: '{ref_key}'\") break # If no reference column found, use None for all completions if references is None : references = [ None ] * len ( completions ) print ( f \"[WARNING] No reference column found. Checked: answer, solution, reference, ground_truth, target\" ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): # Try different calling patterns try : # Pattern 1: text + test_cases (for code execution) reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : # Pattern 2: Just text reward = reward_func ( completion ) except TypeError : try : # Pattern 3: text + reference reward = reward_func ( completion , reference = reference ) except TypeError : try : # Pattern 4: text + reference + context reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics with completions summary if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step.\"\"\" logger . debug ( \"train_step() called but TRL GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader for training.\"\"\" logger . debug ( \"create_data_loader() called but TRL GRPO uses TRL's internal data loading\" ) return None def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_trainer ( self ) -> None : \"\"\" Set up the GRPO trainer with all configurations. \"\"\" logger . info ( \"Setting up TRL GRPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 32 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # GRPO-specific parameters (beta = KL coefficient, epsilon = clip # range) beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = 0.1 ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = 0.2 ) # Generation parameters num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = 4 ) max_completion_length = self . _get_config_value ( self . config . train , 'max_completion_length' , 'max_new_tokens' , default = 256 ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.95 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) # Save parameters save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"Beta (KL coefficient): { beta } \" ) logger . info ( f \"Epsilon (clip range): { epsilon } \" ) logger . info ( f \"Num generations per prompt: { num_generations } \" ) logger . info ( f \"Max completion length: { max_completion_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup GRPO trainer from trl import GRPOTrainer , GRPOConfig grpo_config = GRPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , per_device_eval_batch_size = per_device_eval_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , report_to = report_to if report_to else [], # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Training parameters weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , remove_unused_columns = False , # Generation parameters num_generations = num_generations , max_completion_length = max_completion_length , max_prompt_length = max_prompt_length , temperature = temperature , top_p = top_p , max_steps = max_steps , # GRPO-specific beta = beta , # Precision ** precision_args , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Create GRPO trainer import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) else : try : import unsloth logger . info ( \"Detected Unsloth environment, using reward_funcs parameter\" ) def unsloth_reward_wrapper ( prompts = None , completions = None , ** kwargs ): if completions is None : return [ 0.0 ] * ( len ( prompts ) if prompts else 1 ) return self . _combined_reward_function ( completions , ** kwargs ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ unsloth_reward_wrapper ], ) except ImportError : logger . info ( \"Using pure TRL, using reward_function parameter\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) logger . info ( \"GRPO trainer setup completed successfully!\" ) def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory for saving output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL GRPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Log samples after training try : # For qualitative samples we only need callable reward functions, # not the full metadata dicts stored in self.reward_functions. raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : # Fallback: disable reward logging rather than failing # samples reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : 1 , \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = True, # **kwargs # ) -> Dict[str, float]: # \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # # Auto-setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # logger.info(\"Auto-initializing evaluators for first evaluation...\") # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's unified evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'dataset_name' : self . config . dataset . name , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if hasattr ( self , 'dataset' ) and self . train_dataset else 0 , 'val_size' : 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'dataset_name' : self . config . dataset . name , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL GRPO configuration saved to { path } \" ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise","title":"TRLGRPOTrainer"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.create_data_loader","text":"Create data loader for training. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 576 577 578 579 580 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\"Create data loader for training.\"\"\" logger . debug ( \"create_data_loader() called but TRL GRPO uses TRL's internal data loading\" ) return None","title":"create_data_loader"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.get_training_stats","text":"Get training statistics. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 def get_training_stats ( self ) -> Dict [ str , Any ]: \"\"\"Get training statistics.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) stats = { 'config' : { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'dataset_name' : self . config . dataset . name , 'epochs' : num_epochs , 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), }, 'dataset_info' : { 'train_size' : len ( self . train_dataset ) if hasattr ( self , 'dataset' ) and self . train_dataset else 0 , 'val_size' : 0 , }, 'model_info' : { 'loaded' : self . model is not None , 'device' : str ( next ( self . model . parameters ()) . device ) if self . model else 'unknown' , 'vocab_size' : len ( self . tokenizer ) if self . tokenizer else 0 , 'has_peft' : hasattr ( self . model , 'peft_config' ) if self . model else False , }, 'training_history' : self . training_history , } return stats","title":"get_training_stats"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.is_available","text":"Check if TRL is available. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 56 57 58 59 60 61 62 63 64 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if TRL is available.\"\"\" try : from trl import GRPOTrainer , GRPOConfig from transformers import AutoModelForCausalLM , AutoTokenizer return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.load_model","text":"Load a trained GRPO model. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.save_config","text":"Save configuration to YAML file. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 def save_config ( self , path : str ): \"\"\"Save configuration to YAML file.\"\"\" num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) config_dict = { 'model_name' : self . config . model . name_or_path , 'task_type' : 'group_relative_policy_optimization' , 'max_seq_length' : self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ), 'learning_rate' : self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 2e-4 ), 'epochs' : num_epochs , 'batch_size' : self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 4 ), 'dataset_name' : self . config . dataset . name , 'use_peft' : self . _get_config_value ( self . config . model , 'use_peft' , default = False ), 'precision' : self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ), 'num_reward_functions' : len ( self . reward_functions ), } with open ( path , 'w' ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"TRL GRPO configuration saved to { path } \" )","title":"save_config"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.save_model","text":"Save the trained GRPO model. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.setup_data","text":"Setup datasets for GRPO training using unified DataManager. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def setup_data ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , # System prompt tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , # \u2705 ADD THIS - Enable thinking mode column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , ) # Load dataset - DataManager handles everything including chat template dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" )","title":"setup_data"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.setup_model","text":"Setup model using standard Transformers. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def setup_model ( self ) -> None : \"\"\"Setup model using standard Transformers.\"\"\" # Extract model config values safely model_name = self . _get_config_value ( self . config . model , 'name_or_path' , 'model_name' , default = 'gpt2' ) trust_remote_code = self . _get_config_value ( self . config . model , 'trust_remote_code' , default = False ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) device_map = self . _get_config_value ( self . config . model , 'device_map' , default = 'auto' ) is_ddp = ( device_map in [ \"DDP\" , \"ddp\" ] or os . environ . get ( 'ACCELERATE_USE_DDP' ) == 'true' ) if is_ddp : from accelerate import PartialState device_string = PartialState () . process_index device_map = { '' : device_string } logger . info ( f \"DDP mode detected: device_map= {{ '': { device_string } }} \" ) load_in_4bit = self . _get_config_value ( self . config . model , 'load_in_4bit' , default = False ) load_in_8bit = self . _get_config_value ( self . config . model , 'load_in_8bit' , default = False ) use_peft = self . _get_config_value ( self . config . model , 'use_peft' , default = False ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"TRL GRPO\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Auto-detect quantization from model name if not explicitly set if not load_in_4bit and not load_in_8bit : model_name_lower = model_name . lower () if 'bnb-4bit' in model_name_lower or '4bit' in model_name_lower or 'awq' in model_name_lower : logger . info ( f \"Auto-detected 4-bit quantization from model name: { model_name } \" ) load_in_4bit = True elif 'bnb-8bit' in model_name_lower or '8bit' in model_name_lower : logger . info ( f \"Auto-detected 8-bit quantization from model name: { model_name } \" ) load_in_8bit = True # Auto-enable PEFT if using quantization (required for training # quantized models) if ( load_in_4bit or load_in_8bit ) and not use_peft : logger . info ( \"Quantization detected - auto-enabling PEFT/LoRA adapters (required for training)\" ) use_peft = True logger . info ( \"=\" * 80 ) logger . info ( f \"Setting up TRL GRPO model: { model_name } \" ) logger . info ( \"=\" * 80 ) try : from transformers import AutoModelForCausalLM , AutoTokenizer except ImportError as e : raise ImportError ( \"Transformers not available. Install with: pip install transformers\" ) from e # Load tokenizer first logger . info ( \"Loading tokenizer...\" ) self . tokenizer = AutoTokenizer . from_pretrained ( model_name , trust_remote_code = trust_remote_code , ) # Set pad token if not set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token logger . info ( \"Set pad token to eos token\" ) # Load model logger . info ( \"Loading model...\" ) model_kwargs = { \"dtype\" : dtype , \"device_map\" : device_map , \"trust_remote_code\" : trust_remote_code , } # Add quantization if specified if load_in_4bit : logger . info ( \"Loading model with 4-bit quantization...\" ) model_kwargs [ \"load_in_4bit\" ] = True model_kwargs [ \"bnb_4bit_compute_dtype\" ] = torch . float16 model_kwargs [ \"bnb_4bit_quant_type\" ] = \"nf4\" model_kwargs [ \"bnb_4bit_use_double_quant\" ] = True elif load_in_8bit : logger . info ( \"Loading model with 8-bit quantization...\" ) model_kwargs [ \"load_in_8bit\" ] = True self . model = AutoModelForCausalLM . from_pretrained ( model_name , ** model_kwargs ) # Apply PEFT if specified or if quantization is used if use_peft : logger . info ( \"Applying PEFT (LoRA) configuration...\" ) from peft import LoraConfig , get_peft_model , prepare_model_for_kbit_training # Prepare model for k-bit training if using quantization if load_in_4bit or load_in_8bit : logger . info ( \"Preparing model for k-bit training...\" ) self . model = prepare_model_for_kbit_training ( self . model ) # Extract PEFT config values safely lora_r = self . _get_config_value ( self . config . model , 'lora_r' , 'r' , default = 16 ) lora_alpha = self . _get_config_value ( self . config . model , 'lora_alpha' , 'alpha' , default = 32 ) lora_target_modules = self . _get_config_value ( self . config . model , 'lora_target_modules' , 'target_modules' , default = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] ) lora_dropout = self . _get_config_value ( self . config . model , 'lora_dropout' , 'dropout' , default = 0.05 ) logger . info ( f \"LoRA config: r= { lora_r } , alpha= { lora_alpha } , dropout= { lora_dropout } \" ) logger . info ( f \"Target modules: { lora_target_modules } \" ) peft_config = LoraConfig ( r = lora_r , lora_alpha = lora_alpha , target_modules = lora_target_modules , lora_dropout = lora_dropout , bias = \"none\" , task_type = \"CAUSAL_LM\" , ) self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () logger . info ( \"PEFT adapters applied successfully\" ) logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO model setup completed successfully\" ) logger . info ( f \"Tokenizer vocab size: { len ( self . tokenizer ) } \" ) logger . info ( f \"Model device: { next ( self . model . parameters ()) . device } \" ) logger . info ( \"=\" * 80 )","title":"setup_model"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.setup_rewards","text":"Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : # Handle both dict and RewardConfig object if isinstance ( reward_config , dict ): reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) else : # RewardConfig object reward_type = getattr ( reward_config , 'type' , 'length' ) weight = getattr ( reward_config , 'weight' , 1.0 ) params = getattr ( reward_config , 'params' , {}) try : # Special case: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"Loaded custom reward function (weight: { weight } )\" ) else : # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) # Fallback: try to continue with other rewards rather than # failing continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" )","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.setup_trainer","text":"Set up the GRPO trainer with all configurations. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 def setup_trainer ( self ) -> None : \"\"\" Set up the GRPO trainer with all configurations. \"\"\" logger . info ( \"Setting up TRL GRPO trainer...\" ) # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , 'num_train_epochs' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , 'lr' , default = 1e-6 ) per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , 'batch_size' , default = 1 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 32 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 10 ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) precision = self . _get_config_value ( self . config . model , 'precision' , default = 'fp32' ) output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # GRPO-specific parameters (beta = KL coefficient, epsilon = clip # range) beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = 0.1 ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = 0.2 ) # Generation parameters num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = 4 ) max_completion_length = self . _get_config_value ( self . config . train , 'max_completion_length' , 'max_new_tokens' , default = 256 ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.95 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'epoch' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) # Save parameters save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True if self . eval_dataset else False ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' if self . eval_dataset else None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) report_to = self . config . logging . loggers if self . config . logging . loggers else [] # Use eval_dataset-aware defaults if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Configuration\" ) logger . info ( f \"Epochs: { num_epochs } \" ) logger . info ( f \"Learning rate: { learning_rate } \" ) logger . info ( f \"Batch size: { per_device_batch_size } \" ) logger . info ( f \"Gradient accumulation: { gradient_accumulation_steps } \" ) logger . info ( f \"Beta (KL coefficient): { beta } \" ) logger . info ( f \"Epsilon (clip range): { epsilon } \" ) logger . info ( f \"Num generations per prompt: { num_generations } \" ) logger . info ( f \"Max completion length: { max_completion_length } \" ) logger . info ( f \"Temperature: { temperature } \" ) logger . info ( f \"Output directory: { output_dir } \" ) logger . info ( \"=\" * 80 ) # Create output directory Path ( output_dir ) . mkdir ( parents = True , exist_ok = True ) # Setup GRPO trainer from trl import GRPOTrainer , GRPOConfig grpo_config = GRPOConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , per_device_eval_batch_size = per_device_eval_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_steps = warmup_steps , # Evaluation parameters eval_strategy = eval_strategy , eval_steps = eval_steps , # Logging parameters logging_strategy = logging_strategy , logging_steps = logging_steps , report_to = report_to if report_to else [], # Save parameters save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , load_best_model_at_end = load_best_model_at_end , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , # Training parameters weight_decay = weight_decay , max_grad_norm = max_grad_norm , seed = seed , remove_unused_columns = False , # Generation parameters num_generations = num_generations , max_completion_length = max_completion_length , max_prompt_length = max_prompt_length , temperature = temperature , top_p = top_p , max_steps = max_steps , # GRPO-specific beta = beta , # Precision ** precision_args , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Create GRPO trainer import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) else : try : import unsloth logger . info ( \"Detected Unsloth environment, using reward_funcs parameter\" ) def unsloth_reward_wrapper ( prompts = None , completions = None , ** kwargs ): if completions is None : return [ 0.0 ] * ( len ( prompts ) if prompts else 1 ) return self . _combined_reward_function ( completions , ** kwargs ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ unsloth_reward_wrapper ], ) except ImportError : logger . info ( \"Using pure TRL, using reward_function parameter\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = self . _combined_reward_function , ) logger . info ( \"GRPO trainer setup completed successfully!\" )","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.train","text":"Execute GRPO training. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training.\"\"\" # Setup components self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Get output directory for saving output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/grpo_trl' ) # Record training start start_time = time . time () logger . info ( \"=\" * 80 ) logger . info ( \"Starting TRL GRPO Training\" ) logger . info ( f \"Dataset size: { len ( self . train_dataset ) } \" ) logger . info ( f \"Num reward functions: { len ( self . reward_functions ) } \" ) logger . info ( \"=\" * 80 ) # Start training train_result = self . trainer . train () # Log samples after training try : # For qualitative samples we only need callable reward functions, # not the full metadata dicts stored in self.reward_functions. raw_reward_funcs = getattr ( self , \"reward_functions\" , None ) reward_callables = None if isinstance ( raw_reward_funcs , list ): try : reward_callables = [ rf [ \"function\" ] for rf in raw_reward_funcs if isinstance ( rf , dict ) and callable ( rf . get ( \"function\" )) ] except Exception : # Fallback: disable reward logging rather than failing # samples reward_callables = None generate_and_log_samples ( self . config . logging . sample_logging , self . model , self . tokenizer , reward_callables , stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Record training end end_time = time . time () training_duration = end_time - start_time logger . info ( f \"Training completed in { training_duration : .2f } seconds\" ) # Extract metrics metrics = {} if hasattr ( train_result , 'metrics' ): metrics = train_result . metrics # Save model logger . info ( f \"Saving model to { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) # Compile results results = { \"training_time\" : training_duration , \"final_loss\" : train_result . training_loss if hasattr ( train_result , 'training_loss' ) else metrics . get ( 'train_loss' , 0.0 ), \"total_steps\" : train_result . global_step if hasattr ( train_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : 1 , \"metrics\" : metrics , } logger . info ( \"=\" * 80 ) logger . info ( \"TRL GRPO Training Completed Successfully!\" ) logger . info ( f \"Final loss: { results [ 'final_loss' ] : .4f } \" ) logger . info ( f \"Total steps: { results [ 'total_steps' ] } \" ) logger . info ( f \"Model saved to: { results [ 'model_path' ] } \" ) logger . info ( \"=\" * 80 ) return results","title":"train"},{"location":"api-reference/trainers/#backends.trl.rl.grpo.grpo.TRLGRPOTrainer.train_step","text":"Execute a single training step. Source code in src/aligntune/backends/trl/rl/grpo/grpo.py 570 571 572 573 574 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step.\"\"\" logger . debug ( \"train_step() called but TRL GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3","title":"train_step"},{"location":"api-reference/trainers/#unsloth-backends","text":"","title":"Unsloth Backends"},{"location":"api-reference/trainers/#unslothsfttrainer","text":"Unsloth backend for Supervised Fine-Tuning (faster). Bases: SFTTrainerBase Enhanced SFT trainer using Unsloth with task type support. Source code in src/aligntune/backends/unsloth/sft/sft.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 class UnslothSFTTrainer ( SFTTrainerBase ): \"\"\"Enhanced SFT trainer using Unsloth with task type support.\"\"\" # Supported task types for Unsloth SUPPORTED_TASKS = [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING , TaskType . TEXT_GENERATION , TaskType . CHAT_COMPLETION ] def __init__ ( self , config ): super () . __init__ ( config ) self . config = config self . task_type = self . _get_task_type () self . training_config = None self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . training_history = [] self . logging_manager = None self . evaluator = None self . unsloth_model = None self . train_dataset = None self . eval_dataset = None self . eval_dataset = None # Already exists - no need to add self . custom_evaluator = None # ADD THIS LINE (for BaseEvaluator) # Validate task type self . _validate_task_type () logger . info ( f \"Initialized UnslothSFTTrainer for task: { self . task_type . value } \" ) def _get_task_type ( self ) -> TaskType : \"\"\"Extract task type from config.\"\"\" if hasattr ( self . config , 'dataset' ) and hasattr ( self . config . dataset , 'task_type' ): task_type = self . config . dataset . task_type elif hasattr ( self . config , 'train' ) and hasattr ( self . config . train , 'task_type' ): task_type = self . config . train . task_type else : # Default to supervised fine-tuning task_type = TaskType . SUPERVISED_FINE_TUNING # Convert string to enum if needed if isinstance ( task_type , str ): task_type = TaskType ( task_type . lower ()) return task_type def _validate_task_type ( self ): \"\"\"Validate that the task type is supported by Unsloth.\"\"\" if self . task_type not in self . SUPPORTED_TASKS : raise ValueError ( f \"Task type { self . task_type . value } is not supported by Unsloth backend. \" f \"Supported tasks: { [ t . value for t in self . SUPPORTED_TASKS ] } . \" f \"Use TRL backend for classification tasks.\" ) @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth is available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTTrainer , SFTConfig , ModelConfig return True except ImportError : return False def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model with task-aware configuration.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth model for task: { self . task_type . value } \" ) logger . info ( f \"Model: { self . config . model . name_or_path } \" ) # Configure Unsloth model parameters # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"Unsloth SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Configure Unsloth model parameters # Unsloth only accepts load_in_4bit as boolean, not BitsAndBytesConfig parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : dtype , # Use dtype from PrecisionHandler instead of None \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , False ) if self . config . model . quantization else False , } # Unsloth handles quantization internally - don't pass BitsAndBytesConfig parameters # Parameters like bnb_4bit_compute_dtype, bnb_4bit_quant_type, etc. are not supported # Load model with Unsloth optimizations try : self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = self . config . model . name_or_path , ** model_kwargs ) except ( SyntaxError , RuntimeError , Exception ) as e : # Handle Unsloth compiled module syntax errors (may be wrapped in RuntimeError) error_msg = str ( e ) if \"unsloth_compiled_module\" in error_msg or \"unexpected indent\" in error_msg . lower (): logger . error ( f \"Unsloth compilation error: { e } \" ) logger . info ( \"This is a known Unsloth issue with compiled modules.\" ) logger . info ( \"Suggested fixes:\" ) logger . info ( \" 1. Clear Unsloth cache: rm -rf ~/.cache/unsloth/\" ) logger . info ( \" 2. Retry training after clearing cache\" ) logger . info ( \" 3. Use TRL backend instead: backend='trl'\" ) raise RuntimeError ( f \"Unsloth compiled module syntax error: { e } \\n \" f \"This is a known Unsloth library issue. \" f \"Please clear the Unsloth cache and retry, or use TRL backend instead.\" ) from e else : # Re-raise if it's a different error raise # Detect model architecture for appropriate target modules model_type = self . unsloth_model . config . model_type . lower () target_modules = self . _get_target_modules_for_architecture ( model_type ) # Validate target modules exist in the model available_modules = [ name for name , _ in self . unsloth_model . named_modules ()] valid_target_modules = [ tm for tm in target_modules if any ( tm in name for name in available_modules )] if not valid_target_modules : # Try to auto-detect target modules logger . warning ( f \"None of the target modules { target_modules } found in model. Attempting auto-detection...\" ) valid_target_modules = self . _auto_detect_target_modules ( available_modules , model_type ) if not valid_target_modules : raise ValueError ( f \"Target modules { target_modules } not found in the base model (type: { model_type } ). \" f \"Available modules include: { available_modules [: 10 ] } ... \" f \"Please check the target modules and try again, or use a different model architecture.\" ) logger . info ( f \"Using target modules for { model_type } : { valid_target_modules } \" ) # Configure model for training with LoRA self . unsloth_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , # LoRA rank target_modules = valid_target_modules , lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Setup tokenizer based on task type self . _setup_tokenizer_for_task () logger . info ( \"Unsloth model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth model: { e } \" ) raise def _get_target_modules_for_architecture ( self , model_type : str ) -> List [ str ]: \"\"\"Get appropriate target modules based on model architecture.\"\"\" if \"qwen\" in model_type or \"qwen2\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"llama\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"mistral\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"phi\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"dense\" ] elif \"gemma\" in model_type : return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ] elif \"gpt2\" in model_type or \"gpt\" in model_type or \"dialogpt\" in model_type : # GPT-2 and DialoGPT use different module names return [ \"c_attn\" , \"c_proj\" , \"c_fc\" ] else : # Try to auto-detect by checking model structure # Default to common attention modules, but will be validated return [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] def _auto_detect_target_modules ( self , available_modules : List [ str ], model_type : str ) -> List [ str ]: \"\"\"Auto-detect target modules from available modules.\"\"\" # Common patterns for attention modules patterns = [ # GPT-2 style ( \"c_attn\" , \"c_proj\" ), # LLaMA/Qwen style ( \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ), # BERT style ( \"query\" , \"key\" , \"value\" ), # Generic attention ( \"attn\" , \"attention\" ), ] detected = [] for pattern_group in patterns : matches = [ mod for mod in available_modules if any ( p in mod . lower () for p in pattern_group )] if matches : # Get unique module names (remove duplicates) unique_matches = list ( set ([ m . split ( '.' )[ - 1 ] for m in matches if '.' in m ])) if unique_matches : detected . extend ( unique_matches [: 4 ]) # Limit to 4 modules break return detected if detected else [] def _setup_tokenizer_for_task ( self ): \"\"\"Setup tokenizer with task-specific configurations.\"\"\" # Ensure pad token is set if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token # If a chat template is explicitly provided in config, try to apply it # so `tokenizer.apply_chat_template(...)` works across SFT task types. try : from unsloth import FastLanguageModel cfg_template = getattr ( self . config . dataset , \"chat_template\" , None ) has_template = hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None def _normalize_template_name ( name : Optional [ str ]) -> Optional [ str ]: if not name : return None v = str ( name ) . strip () . lower () if v in { \"auto\" , \"none\" }: return None # common aliases used in configs if v in { \"llama3\" , \"llama_3\" , \"llama-3\" }: return \"llama-3\" if v in { \"llama2\" , \"llama_2\" , \"llama-2\" }: return \"llama-2\" return name normalized = _normalize_template_name ( cfg_template ) if normalized and not has_template : mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = normalized , mapping = mapping , ) logger . info ( f \"Applied configured chat template for SFT: { normalized } \" ) except Exception as e : logger . debug ( f \"Skipping configured chat template application: { e } \" ) # Add chat template for chat completion tasks if self . task_type == TaskType . CHAT_COMPLETION : logger . info ( \"Applying Unsloth chat template...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = None if hasattr ( self . config . dataset , 'chat_template' ): chat_template = self . config . dataset . chat_template # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 if not specified mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to manual template.\" ) if not hasattr ( self . tokenizer , 'chat_template' ) or self . tokenizer . chat_template is None : self . tokenizer . chat_template = ( \"{ % f or message in messages %}\" \"{{ message['role'] }}: {{ message['content'] }} \\n \" \"{ % e ndfor %}Assistant:\" ) logger . info ( \"Added manual chat template to tokenizer\" ) # Add special tokens for instruction following if needed elif self . task_type == TaskType . INSTRUCTION_FOLLOWING : has_chat_template = ( hasattr ( self . tokenizer , \"apply_chat_template\" ) and hasattr ( self . tokenizer , \"chat_template\" ) and self . tokenizer . chat_template is not None ) if not has_chat_template : special_tokens = [ \"<|im_start|>\" , \"<|im_end|>\" ] num_added = self . tokenizer . add_special_tokens ({ 'additional_special_tokens' : special_tokens }) if num_added > 0 : self . unsloth_model . resize_token_embeddings ( len ( self . tokenizer )) logger . info ( f \"Added { num_added } special tokens for instruction following\" ) def _apply_chat_template_safe ( self , messages : List [ Dict [ str , str ]], add_generation_prompt : bool = False ) -> Optional [ str ]: \"\"\"Apply tokenizer chat template with broad compatibility.\"\"\" if not self . tokenizer or not hasattr ( self . tokenizer , \"apply_chat_template\" ): return None try : return self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = add_generation_prompt , ) except TypeError : try : return self . tokenizer . apply_chat_template ( messages , tokenize = False ) except Exception : return None except Exception : return None def _format_with_chat_template ( self , examples : Dict [ str , Any ], ds_cfg ) -> Dict [ str , List [ str ]]: \"\"\" Build a `text` field using the model's chat template when available. Falls back to legacy string formatting if templating fails. \"\"\" system_prompt = getattr ( ds_cfg , \"system_prompt\" , None ) texts : List [ str ] = [] def _prepend_system_if_missing ( msgs : List [ Dict [ str , str ]]) -> List [ Dict [ str , str ]]: if system_prompt and ( not msgs or msgs [ 0 ] . get ( \"role\" ) != \"system\" ): return [{ \"role\" : \"system\" , \"content\" : system_prompt }] + msgs return msgs if self . task_type == TaskType . CHAT_COMPLETION : messages_field = ds_cfg . messages_column if hasattr ( ds_cfg , \"messages_column\" ) else \"messages\" for messages in examples . get ( messages_field , []): if isinstance ( messages , list ): msgs = _prepend_system_if_missing ( messages ) templated = self . _apply_chat_template_safe ( msgs , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) continue conversation = [ f \" { m . get ( 'role' , 'user' ) } : { m . get ( 'content' , '' ) } \" for m in msgs ] texts . append ( \" \\n \" . join ( conversation )) else : texts . append ( str ( messages ) if messages else \"\" ) return { \"text\" : texts } if self . task_type in [ TaskType . INSTRUCTION_FOLLOWING , TaskType . SUPERVISED_FINE_TUNING ]: instr_col = getattr ( ds_cfg , \"instruction_column\" , \"instruction\" ) resp_col = getattr ( ds_cfg , \"response_column\" , \"response\" ) ctx_col = getattr ( ds_cfg , \"context_column\" , \"context\" ) input_col = getattr ( ds_cfg , \"input_column\" , \"input\" ) instructions = examples . get ( instr_col , examples . get ( \"instruction\" , [])) responses = examples . get ( resp_col , examples . get ( \"response\" , examples . get ( \"output\" , []))) n = min ( len ( instructions ), len ( responses )) for i in range ( n ): instruction = instructions [ i ] response = responses [ i ] context = \"\" if ctx_col in examples and i < len ( examples . get ( ctx_col , [])): context = examples . get ( ctx_col , [ \"\" ])[ i ] or \"\" input_text = \"\" if input_col in examples and i < len ( examples . get ( input_col , [])): input_text = examples . get ( input_col , [ \"\" ])[ i ] or \"\" sys_parts : List [ str ] = [] if system_prompt : sys_parts . append ( str ( system_prompt ) . strip ()) if context and str ( context ) . strip (): sys_parts . append ( f \"Context: { str ( context ) . strip () } \" ) messages : List [ Dict [ str , str ]] = [] if sys_parts : messages . append ({ \"role\" : \"system\" , \"content\" : \" \\n\\n \" . join ( sys_parts )}) user_content = str ( instruction ) if instruction is not None else \"\" if input_text and str ( input_text ) . strip (): user_content = f \" { user_content } \\n\\n { str ( input_text ) . strip () } \" messages . append ({ \"role\" : \"user\" , \"content\" : user_content }) messages . append ({ \"role\" : \"assistant\" , \"content\" : str ( response ) if response is not None else \"\" }) templated = self . _apply_chat_template_safe ( messages , add_generation_prompt = False ) if templated is not None : texts . append ( templated ) else : # Fallback to legacy formatting (inline) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : if context and str ( context ) . strip (): texts . append ( f \"<|im_start|>system \\n Context: { str ( context ) . strip () } <|im_end|> \\n \" f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"<|im_start|>user \\n { user_content } <|im_end|> \\n \" f \"<|im_start|>assistant \\n { str ( response ) if response is not None else '' } <|im_end|>\" ) else : texts . append ( f \"### Instruction: \\n { user_content } \\n\\n ### Response: \\n { str ( response ) if response is not None else '' } \" ) if not texts : texts = [ \"[NO_VALID_TEXT]\" ] return { \"text\" : texts } return { \"text\" : examples . get ( \"text\" , [ \"[NO_VALID_TEXT]\" ])} def setup_data ( self ) -> None : \"\"\"Setup and prepare dataset for training (required by abstract base class).\"\"\" self . setup_dataset () def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step (required by abstract base class).\"\"\" if self . trainer is None : raise RuntimeError ( \"Trainer not initialized. Call setup_model() first.\" ) # The actual training step is handled by TRL's SFTTrainer return { \"loss\" : 0.0 , \"learning_rate\" : 0.0 } def setup_dataset ( self ) -> None : \"\"\"Setup and prepare dataset with task-aware formatting.\"\"\" try : from datasets import load_dataset ds_cfg = self . config . dataset logger . info ( f \"Loading dataset: { ds_cfg . name } for task: { self . task_type . value } \" ) # Prepare load_dataset arguments load_kwargs = {} # Add split parameter if hasattr ( ds_cfg , 'split' ) and ds_cfg . split : load_kwargs [ 'split' ] = ds_cfg . split else : load_kwargs [ 'split' ] = \"train\" # Handle dataset config/subset (for datasets like wikitext that require it) dataset_name = ds_cfg . name dataset_config = None if hasattr ( ds_cfg , 'subset' ) and ds_cfg . subset : dataset_config = ds_cfg . subset logger . info ( f \"Using dataset config/subset: { dataset_config } \" ) elif hasattr ( ds_cfg , 'config' ) and ds_cfg . config : dataset_config = ds_cfg . config logger . info ( f \"Using dataset config: { dataset_config } \" ) # Load dataset with or without config # Special handling for SQuAD (both v1.1 and v2.0) which use deprecated 'List' feature type if \"squad\" in dataset_name . lower (): logger . info ( f \"Detected SQuAD dataset. Using workaround for deprecated 'List' feature type...\" ) try : # Method 1: Try loading with builder to bypass feature validation from datasets import load_dataset_builder builder = load_dataset_builder ( dataset_name , dataset_config if dataset_config else None ) # Download and prepare builder . download_and_prepare () # Load from cache, bypassing feature type validation split_name = load_kwargs . get ( 'split' , 'train' ) dataset = builder . as_dataset ( split = split_name ) logger . info ( f \"Successfully loaded { dataset_name } using builder workaround\" ) except Exception as e_builder : logger . warning ( f \"Builder method failed: { e_builder } . Trying alternative approach...\" ) try : # Method 2: Try loading with ignore_verifications (if supported) load_kwargs_alt = load_kwargs . copy () load_kwargs_alt [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_alt ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_alt ) logger . info ( f \"Successfully loaded { dataset_name } with force_redownload\" ) except Exception as e2 : logger . warning ( f \"Force redownload failed: { e2 } . Trying feature patching...\" ) try : # Method 3: Patch Features.from_dict to handle List -> Sequence conversion import datasets import json original_from_dict = datasets . Features . from_dict def patched_from_dict ( features_dict ): \"\"\"Patch to convert 'List' to 'Sequence' in feature dict.\"\"\" # Convert to string, replace, convert back features_str = json . dumps ( features_dict ) features_str = features_str . replace ( '\"List\"' , '\"Sequence\"' ) features_str = features_str . replace ( \"'List'\" , \"'Sequence'\" ) features_dict = json . loads ( features_str ) return original_from_dict ( features_dict ) # Temporarily patch datasets . Features . from_dict = patched_from_dict try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) logger . info ( f \"Successfully loaded { dataset_name } with feature patching\" ) finally : # Restore original datasets . Features . from_dict = original_from_dict except Exception as e3 : logger . error ( f \"All SQuAD loading methods failed. Last error: { e3 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type that cannot be automatically fixed. \" f \"Please try: pip install --upgrade datasets>=2.14.0, \" f \"or consider using an alternative Q&A dataset like 'allenai/sciq' or 'deepmind/code_contests'. \" f \"Error: { str ( e3 )[: 200 ] } \" ) from e3 else : try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ( ValueError , TypeError ) as e : error_msg = str ( e ) # Handle deprecated 'List' feature type for other datasets if \"Feature type 'List' not found\" in error_msg or \"'List'\" in error_msg : logger . warning ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Trying to load with download_mode='force_redownload'...\" ) try : load_kwargs_retry = load_kwargs . copy () load_kwargs_retry [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_retry ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_retry ) logger . info ( f \"Successfully loaded dataset with updated features\" ) except Exception as e2 : logger . error ( f \"Failed to load dataset: { e2 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Please try: pip install --upgrade datasets>=2.14.0. \" f \"Original error: { error_msg [: 200 ] } \" ) from e elif \"Config name is missing\" in error_msg or \"config\" in error_msg . lower (): logger . error ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' parameter to your DatasetConfig. \" f \"Error: { error_msg } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Please add 'subset' or 'config' to your DatasetConfig. \" f \"Original error: { error_msg } \" ) else : raise logger . info ( f \"Dataset loaded: { len ( dataset ) } samples\" ) # Apply subset selection if hasattr ( ds_cfg , 'max_samples' ) and ds_cfg . max_samples : original_len = len ( dataset ) dataset = dataset . select ( range ( min ( ds_cfg . max_samples , len ( dataset )))) logger . info ( f \"Applied max_samples limit: { original_len } -> { len ( dataset ) } samples\" ) elif hasattr ( ds_cfg , 'percent' ) and ds_cfg . percent : original_len = len ( dataset ) max_samples = int ( len ( dataset ) * ds_cfg . percent / 100 ) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Applied percent limit ( { ds_cfg . percent } %): { original_len } -> { len ( dataset ) } samples\" ) # Auto-detect schema if enabled if hasattr ( ds_cfg , 'auto_detect_fields' ) and ds_cfg . auto_detect_fields : logger . info ( \"Auto-detecting dataset schema...\" ) detected_format = schema_detector . detect_format ( dataset ) if detected_format : logger . info ( f \"Auto-detected dataset format: { detected_format } \" ) if hasattr ( ds_cfg , 'format_type' ): ds_cfg . format_type = detected_format # Apply field mappings field_mappings = None if hasattr ( ds_cfg , 'field_mappings' ) and ds_cfg . field_mappings : field_mappings = ds_cfg . field_mappings logger . info ( f \"Applying field mappings: { field_mappings } \" ) elif hasattr ( ds_cfg , 'column_mapping' ) and ds_cfg . column_mapping : field_mappings = ds_cfg . column_mapping logger . info ( f \"Applying column mappings: { field_mappings } \" ) if field_mappings : # Only rename columns that exist in the dataset valid_mappings = { k : v for k , v in field_mappings . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) else : logger . warning ( f \"No valid column mappings found. Available columns: { dataset . column_names } \" ) # Log dataset structure logger . info ( f \"Dataset columns: { dataset . column_names } \" ) if len ( dataset ) > 0 : logger . info ( f \"Sample data (first item): { list ( dataset [ 0 ] . keys ()) } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , ds_cfg ) # Split train/test # Always create eval split if dataset has at least 2 samples (minimum for split) # For very small datasets, use a smaller test_size to ensure eval set has at least 1 sample if len ( dataset ) >= 2 : if len ( dataset ) >= 10 : test_size = 0.1 # 10% for larger datasets elif len ( dataset ) >= 5 : test_size = 0.2 # 20% for medium datasets (ensures at least 1 eval sample) else : test_size = 0.2 # 20% for very small datasets (2-4 samples -> at least 1 eval) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 ) self . train_dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Split dataset: { len ( self . train_dataset ) } train, { len ( self . eval_dataset ) } eval\" ) else : # Only skip split if dataset has less than 2 samples self . train_dataset = dataset self . eval_dataset = None logger . warning ( f \"Dataset too small for split (only { len ( dataset ) } samples), using all for training. No eval dataset created.\" ) logger . info ( f \"Dataset prepared: { len ( self . train_dataset ) } training samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def _format_dataset_for_task ( self , dataset , ds_cfg ): \"\"\"Format dataset based on task type.\"\"\" logger . info ( f \"Formatting dataset for task: { self . task_type . value } \" ) # Select appropriate formatter can_template = self . tokenizer is not None and hasattr ( self . tokenizer , \"apply_chat_template\" ) if self . task_type == TaskType . INSTRUCTION_FOLLOWING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_instruction_following elif self . task_type == TaskType . SUPERVISED_FINE_TUNING : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_supervised_fine_tuning elif self . task_type == TaskType . TEXT_GENERATION : formatter = TaskFormatter . format_text_generation elif self . task_type == TaskType . CHAT_COMPLETION : formatter = self . _format_with_chat_template if can_template else TaskFormatter . format_chat_completion else : raise ValueError ( f \"Unsupported task type: { self . task_type } \" ) # Apply formatting try : formatted_dataset = dataset . map ( lambda examples : formatter ( examples , ds_cfg ), batched = True , remove_columns = dataset . column_names , desc = f \"Formatting for { self . task_type . value } \" ) # Verify formatting sample = formatted_dataset [ 0 ] logger . info ( f \"Sample formatted text (first 200 chars): { sample [ 'text' ][: 200 ] } \" ) return formatted_dataset except Exception as e : logger . error ( f \"Error formatting dataset: { e } \" ) # Fallback to simple formatting logger . warning ( \"Using fallback formatting\" ) return self . _fallback_format ( dataset , ds_cfg ) def _fallback_format ( self , dataset , ds_cfg ): \"\"\"Fallback formatting when task-specific formatting fails.\"\"\" def simple_format ( examples ): texts = [] # Try common column names for i in range ( len ( list ( examples . values ())[ 0 ])): text = \"\" if \"instruction\" in examples and \"response\" in examples : text = f \" { examples [ 'instruction' ][ i ] } \\n { examples [ 'response' ][ i ] } \" elif \"text\" in examples : text = examples [ \"text\" ][ i ] else : # Use first text-like column first_col = list ( examples . keys ())[ 0 ] text = str ( examples [ first_col ][ i ]) texts . append ( text ) return { \"text\" : texts } return dataset . map ( simple_format , batched = True , remove_columns = dataset . column_names ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL SFTTrainer with task-aware configuration.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig logger . info ( f \"Setting up TRL SFTTrainer for task: { self . task_type . value } \" ) # Get training parameters num_epochs = getattr ( self . config , 'num_epochs' , None ) or \\ getattr ( self . config . train , 'epochs' , 3 ) if hasattr ( self . config , 'train' ) else 3 batch_size = getattr ( self . config , 'batch_size' , None ) or \\ getattr ( self . config . train , 'per_device_batch_size' , 2 ) if hasattr ( self . config , 'train' ) else 2 grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) if hasattr ( self . config , 'train' ) else 1 lr = getattr ( self . config . train , 'learning_rate' , 2e-4 ) if hasattr ( self . config , 'train' ) else 2e-4 save_steps = getattr ( self . config . train , 'save_interval' , 500 ) if hasattr ( self . config , 'train' ) else 500 output_dir = getattr ( self . config . logging , 'output_dir' , './output' ) if hasattr ( self . config , 'logging' ) else './output' # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Create SFT configuration # Create SFT configuration self . training_config = SFTConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , gradient_accumulation_steps = grad_accum , learning_rate = lr , logging_steps = 10 , save_steps = save_steps , warmup_ratio = 0.1 , lr_scheduler_type = \"cosine\" , ** precision_args , dataloader_pin_memory = False , report_to = \"none\" if not hasattr ( self . config , 'logging' ) else \"tensorboard\" ) # Get max_seq_length max_seq_len = getattr ( self . config . model , 'max_seq_length' , 2048 ) if hasattr ( self . config , 'model' ) else 2048 # Task-specific trainer settings packing = False # Generally disable packing for better quality if self . task_type == TaskType . TEXT_GENERATION : packing = True # Can enable packing for simple text generation # Create trainer self . trainer = SFTTrainer ( model = self . unsloth_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = self . training_config , dataset_text_field = \"text\" , max_seq_length = max_seq_len , dataset_num_proc = 2 , packing = packing , ) logger . info ( f \"TRL SFTTrainer setup completed for { self . task_type . value } \" ) logger . info ( f \"Training config: { num_epochs } epochs, batch_size= { batch_size } , lr= { lr } \" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute training with Unsloth optimizations.\"\"\" try : logger . info ( f \"Starting Unsloth SFT training for task: { self . task_type . value } \" ) start_time = time . time () # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Start training training_result = self . trainer . train () # Save model output_dir = self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"task_type\" : self . task_type . value , \"training_time\" : training_time , \"final_loss\" : training_result . training_loss , \"total_steps\" : training_result . global_step , \"model_path\" : output_dir , \"training_history\" : self . training_history , } logger . info ( f \"Unsloth SFT training completed in { training_time : .2f } seconds\" ) logger . info ( f \"Task: { self . task_type . value } , Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise def evaluate ( self , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the trained model with task-specific metrics.\"\"\" try : if not self . eval_dataset : logger . warning ( \"No evaluation dataset available\" ) return {} if eval_dataset : self . eval_dataset = eval_dataset logger . info ( f \"Evaluating Unsloth SFT model for task: { self . task_type . value } \" ) # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation # It doesn't affect functionality, just makes gradient accumulation slightly less accurate with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Run basic evaluation from TRL trainer eval_results = self . trainer . evaluate () # Add task-specific metrics eval_results [ \"task_type\" ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator try : from aligntune.core.sft.evaluator import SFTEvaluator if self . unsloth_model and self . tokenizer and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . unsloth_model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) logger . info ( f \"Evaluation results: { len ( eval_results ) } metrics computed\" ) return eval_results except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate task-specific sample outputs.\"\"\" try : logger . info ( f \"Generating { num_samples } samples for task: { self . task_type . value } \" ) # Get task-specific prompts sample_prompts = self . _get_sample_prompts_for_task ( num_samples ) samples = [] for i , prompt in enumerate ( sample_prompts ): try : # Tokenize input inputs = self . tokenizer ( prompt , return_tensors = \"pt\" , truncation = True , max_length = 512 ) # Move to device if torch . cuda . is_available (): inputs = { k : v . cuda () for k , v in inputs . items ()} # Generate response with torch . no_grad (): outputs = self . unsloth_model . generate ( ** inputs , max_new_tokens = 100 , temperature = 0.7 , do_sample = True , pad_token_id = self . tokenizer . eos_token_id ) # Decode response response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : response , \"sample_id\" : i + 1 }) except Exception as e : logger . warning ( f \"Failed to generate sample { i + 1 } : { e } \" ) samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : f \"Generation failed: { e } \" , \"sample_id\" : i + 1 }) logger . info ( f \"Generated { len ( samples ) } samples\" ) return samples except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return [] def _get_sample_prompts_for_task ( self , num_samples : int ) -> List [ str ]: \"\"\"Get task-specific sample prompts for generation.\"\"\" if self . task_type == TaskType . INSTRUCTION_FOLLOWING : prompts = [ \"<|im_start|>user \\n Explain machine learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Write a Python function.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n What is AI?<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n Describe deep learning.<|im_end|> \\n <|im_start|>assistant \\n \" , \"<|im_start|>user \\n How does training work?<|im_end|> \\n <|im_start|>assistant \\n \" ] elif self . task_type == TaskType . CHAT_COMPLETION : prompts = [ \"Human: Hello! \\n Assistant:\" , \"Human: How are you? \\n Assistant:\" , \"Human: Explain quantum computing. \\n Assistant:\" , \"Human: Tell me a joke. \\n Assistant:\" , \"Human: What's the weather? \\n Assistant:\" ] elif self . task_type == TaskType . TEXT_GENERATION : prompts = [ \"The future of AI is\" , \"In a world where\" , \"The key to success is\" , \"Technology has changed\" , \"The most important skill is\" ] else : # SUPERVISED_FINE_TUNING prompts = [ \"### Instruction: \\n Explain machine learning. \\n\\n ### Response: \\n \" , \"### Instruction: \\n Write code. \\n\\n ### Response: \\n \" , \"### Instruction: \\n What is AI? \\n\\n ### Response: \\n \" , \"### Instruction: \\n Describe NLP. \\n\\n ### Response: \\n \" , \"### Instruction: \\n How to train models? \\n\\n ### Response: \\n \" ] return prompts [: num_samples ] def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained model with task metadata.\"\"\" try : save_path = path or ( self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' ) logger . info ( f \"Saving Unsloth model to: { save_path } \" ) # Save using Unsloth's optimized saving self . unsloth_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration with task type config_path = Path ( save_path ) / \"training_config.yaml\" config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else {} config_dict [ 'task_type' ] = self . task_type . value with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"Model saved successfully to: { save_path } \" ) logger . info ( f \"Task type: { self . task_type . value } \" ) # Store for push_to_hub self . _last_save_path = save_path return save_path except Exception as e : logger . error ( f \"Failed to save model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model with task type detection.\"\"\" try : logger . info ( f \"Loading Unsloth model from: { path } \" ) # Try to load task type from config config_path = Path ( path ) / \"training_config.yaml\" if config_path . exists (): with open ( config_path , 'r' ) as f : saved_config = yaml . safe_load ( f ) if 'task_type' in saved_config : self . task_type = TaskType ( saved_config [ 'task_type' ]) logger . info ( f \"Loaded task type: { self . task_type . value } \" ) import unsloth from unsloth import FastLanguageModel # Load model and tokenizer self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = self . config . model . max_seq_length if hasattr ( self . config , 'model' ) else 2048 , dtype = None , load_in_4bit = True , ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"UnslothSFTTrainer"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.evaluate","text":"Evaluate the trained model with task-specific metrics. Source code in src/aligntune/backends/unsloth/sft/sft.py 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 def evaluate ( self , eval_dataset = None ) -> Dict [ str , Any ]: \"\"\"Evaluate the trained model with task-specific metrics.\"\"\" try : if not self . eval_dataset : logger . warning ( \"No evaluation dataset available\" ) return {} if eval_dataset : self . eval_dataset = eval_dataset logger . info ( f \"Evaluating Unsloth SFT model for task: { self . task_type . value } \" ) # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation # It doesn't affect functionality, just makes gradient accumulation slightly less accurate with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Run basic evaluation from TRL trainer eval_results = self . trainer . evaluate () # Add task-specific metrics eval_results [ \"task_type\" ] = self . task_type . value # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_results : try : eval_results [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_results [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) # Add comprehensive quality metrics using SFTEvaluator try : from aligntune.core.sft.evaluator import SFTEvaluator if self . unsloth_model and self . tokenizer and self . eval_dataset : evaluator = SFTEvaluator ( self . config ) quality_metrics = evaluator . evaluate ( model = self . unsloth_model , tokenizer = self . tokenizer , dataset = self . eval_dataset , config = self . config ) # Merge quality metrics (avoid duplicates) for key , value in quality_metrics . items (): if key not in eval_results : # Don't override existing metrics eval_results [ key ] = value logger . info ( f \"Added { len ( quality_metrics ) } quality metrics\" ) except Exception as e : logger . warning ( f \"Could not compute quality metrics: { e } \" ) logger . info ( f \"Evaluation results: { len ( eval_results ) } metrics computed\" ) return eval_results except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise","title":"evaluate"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.generate_samples","text":"Generate task-specific sample outputs. Source code in src/aligntune/backends/unsloth/sft/sft.py 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 def generate_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate task-specific sample outputs.\"\"\" try : logger . info ( f \"Generating { num_samples } samples for task: { self . task_type . value } \" ) # Get task-specific prompts sample_prompts = self . _get_sample_prompts_for_task ( num_samples ) samples = [] for i , prompt in enumerate ( sample_prompts ): try : # Tokenize input inputs = self . tokenizer ( prompt , return_tensors = \"pt\" , truncation = True , max_length = 512 ) # Move to device if torch . cuda . is_available (): inputs = { k : v . cuda () for k , v in inputs . items ()} # Generate response with torch . no_grad (): outputs = self . unsloth_model . generate ( ** inputs , max_new_tokens = 100 , temperature = 0.7 , do_sample = True , pad_token_id = self . tokenizer . eos_token_id ) # Decode response response = self . tokenizer . decode ( outputs [ 0 ], skip_special_tokens = True ) response = response [ len ( prompt ):] . strip () samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : response , \"sample_id\" : i + 1 }) except Exception as e : logger . warning ( f \"Failed to generate sample { i + 1 } : { e } \" ) samples . append ({ \"task_type\" : self . task_type . value , \"prompt\" : prompt , \"response\" : f \"Generation failed: { e } \" , \"sample_id\" : i + 1 }) logger . info ( f \"Generated { len ( samples ) } samples\" ) return samples except Exception as e : logger . error ( f \"Sample generation failed: { e } \" ) return []","title":"generate_samples"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.is_available","text":"Check if Unsloth is available. Source code in src/aligntune/backends/unsloth/sft/sft.py 241 242 243 244 245 246 247 248 249 250 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth is available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTTrainer , SFTConfig , ModelConfig return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.load_model","text":"Load a trained model with task type detection. Source code in src/aligntune/backends/unsloth/sft/sft.py 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model with task type detection.\"\"\" try : logger . info ( f \"Loading Unsloth model from: { path } \" ) # Try to load task type from config config_path = Path ( path ) / \"training_config.yaml\" if config_path . exists (): with open ( config_path , 'r' ) as f : saved_config = yaml . safe_load ( f ) if 'task_type' in saved_config : self . task_type = TaskType ( saved_config [ 'task_type' ]) logger . info ( f \"Loaded task type: { self . task_type . value } \" ) import unsloth from unsloth import FastLanguageModel # Load model and tokenizer self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = self . config . model . max_seq_length if hasattr ( self . config , 'model' ) else 2048 , dtype = None , load_in_4bit = True , ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.save_model","text":"Save the trained model with task metadata. Source code in src/aligntune/backends/unsloth/sft/sft.py 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained model with task metadata.\"\"\" try : save_path = path or ( self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' ) logger . info ( f \"Saving Unsloth model to: { save_path } \" ) # Save using Unsloth's optimized saving self . unsloth_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration with task type config_path = Path ( save_path ) / \"training_config.yaml\" config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else {} config_dict [ 'task_type' ] = self . task_type . value with open ( config_path , \"w\" ) as f : yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"Model saved successfully to: { save_path } \" ) logger . info ( f \"Task type: { self . task_type . value } \" ) # Store for push_to_hub self . _last_save_path = save_path return save_path except Exception as e : logger . error ( f \"Failed to save model: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.setup_data","text":"Setup and prepare dataset for training (required by abstract base class). Source code in src/aligntune/backends/unsloth/sft/sft.py 594 595 596 def setup_data ( self ) -> None : \"\"\"Setup and prepare dataset for training (required by abstract base class).\"\"\" self . setup_dataset ()","title":"setup_data"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.setup_dataset","text":"Setup and prepare dataset with task-aware formatting. Source code in src/aligntune/backends/unsloth/sft/sft.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare dataset with task-aware formatting.\"\"\" try : from datasets import load_dataset ds_cfg = self . config . dataset logger . info ( f \"Loading dataset: { ds_cfg . name } for task: { self . task_type . value } \" ) # Prepare load_dataset arguments load_kwargs = {} # Add split parameter if hasattr ( ds_cfg , 'split' ) and ds_cfg . split : load_kwargs [ 'split' ] = ds_cfg . split else : load_kwargs [ 'split' ] = \"train\" # Handle dataset config/subset (for datasets like wikitext that require it) dataset_name = ds_cfg . name dataset_config = None if hasattr ( ds_cfg , 'subset' ) and ds_cfg . subset : dataset_config = ds_cfg . subset logger . info ( f \"Using dataset config/subset: { dataset_config } \" ) elif hasattr ( ds_cfg , 'config' ) and ds_cfg . config : dataset_config = ds_cfg . config logger . info ( f \"Using dataset config: { dataset_config } \" ) # Load dataset with or without config # Special handling for SQuAD (both v1.1 and v2.0) which use deprecated 'List' feature type if \"squad\" in dataset_name . lower (): logger . info ( f \"Detected SQuAD dataset. Using workaround for deprecated 'List' feature type...\" ) try : # Method 1: Try loading with builder to bypass feature validation from datasets import load_dataset_builder builder = load_dataset_builder ( dataset_name , dataset_config if dataset_config else None ) # Download and prepare builder . download_and_prepare () # Load from cache, bypassing feature type validation split_name = load_kwargs . get ( 'split' , 'train' ) dataset = builder . as_dataset ( split = split_name ) logger . info ( f \"Successfully loaded { dataset_name } using builder workaround\" ) except Exception as e_builder : logger . warning ( f \"Builder method failed: { e_builder } . Trying alternative approach...\" ) try : # Method 2: Try loading with ignore_verifications (if supported) load_kwargs_alt = load_kwargs . copy () load_kwargs_alt [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_alt ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_alt ) logger . info ( f \"Successfully loaded { dataset_name } with force_redownload\" ) except Exception as e2 : logger . warning ( f \"Force redownload failed: { e2 } . Trying feature patching...\" ) try : # Method 3: Patch Features.from_dict to handle List -> Sequence conversion import datasets import json original_from_dict = datasets . Features . from_dict def patched_from_dict ( features_dict ): \"\"\"Patch to convert 'List' to 'Sequence' in feature dict.\"\"\" # Convert to string, replace, convert back features_str = json . dumps ( features_dict ) features_str = features_str . replace ( '\"List\"' , '\"Sequence\"' ) features_str = features_str . replace ( \"'List'\" , \"'Sequence'\" ) features_dict = json . loads ( features_str ) return original_from_dict ( features_dict ) # Temporarily patch datasets . Features . from_dict = patched_from_dict try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) logger . info ( f \"Successfully loaded { dataset_name } with feature patching\" ) finally : # Restore original datasets . Features . from_dict = original_from_dict except Exception as e3 : logger . error ( f \"All SQuAD loading methods failed. Last error: { e3 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type that cannot be automatically fixed. \" f \"Please try: pip install --upgrade datasets>=2.14.0, \" f \"or consider using an alternative Q&A dataset like 'allenai/sciq' or 'deepmind/code_contests'. \" f \"Error: { str ( e3 )[: 200 ] } \" ) from e3 else : try : if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs ) else : dataset = load_dataset ( dataset_name , ** load_kwargs ) except ( ValueError , TypeError ) as e : error_msg = str ( e ) # Handle deprecated 'List' feature type for other datasets if \"Feature type 'List' not found\" in error_msg or \"'List'\" in error_msg : logger . warning ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Trying to load with download_mode='force_redownload'...\" ) try : load_kwargs_retry = load_kwargs . copy () load_kwargs_retry [ 'download_mode' ] = 'force_redownload' if dataset_config : dataset = load_dataset ( dataset_name , dataset_config , ** load_kwargs_retry ) else : dataset = load_dataset ( dataset_name , ** load_kwargs_retry ) logger . info ( f \"Successfully loaded dataset with updated features\" ) except Exception as e2 : logger . error ( f \"Failed to load dataset: { e2 } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' uses deprecated 'List' feature type. \" f \"Please try: pip install --upgrade datasets>=2.14.0. \" f \"Original error: { error_msg [: 200 ] } \" ) from e elif \"Config name is missing\" in error_msg or \"config\" in error_msg . lower (): logger . error ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Add 'subset' or 'config' parameter to your DatasetConfig. \" f \"Error: { error_msg } \" ) raise ValueError ( f \"Dataset ' { dataset_name } ' requires a config/subset name. \" f \"Please add 'subset' or 'config' to your DatasetConfig. \" f \"Original error: { error_msg } \" ) else : raise logger . info ( f \"Dataset loaded: { len ( dataset ) } samples\" ) # Apply subset selection if hasattr ( ds_cfg , 'max_samples' ) and ds_cfg . max_samples : original_len = len ( dataset ) dataset = dataset . select ( range ( min ( ds_cfg . max_samples , len ( dataset )))) logger . info ( f \"Applied max_samples limit: { original_len } -> { len ( dataset ) } samples\" ) elif hasattr ( ds_cfg , 'percent' ) and ds_cfg . percent : original_len = len ( dataset ) max_samples = int ( len ( dataset ) * ds_cfg . percent / 100 ) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Applied percent limit ( { ds_cfg . percent } %): { original_len } -> { len ( dataset ) } samples\" ) # Auto-detect schema if enabled if hasattr ( ds_cfg , 'auto_detect_fields' ) and ds_cfg . auto_detect_fields : logger . info ( \"Auto-detecting dataset schema...\" ) detected_format = schema_detector . detect_format ( dataset ) if detected_format : logger . info ( f \"Auto-detected dataset format: { detected_format } \" ) if hasattr ( ds_cfg , 'format_type' ): ds_cfg . format_type = detected_format # Apply field mappings field_mappings = None if hasattr ( ds_cfg , 'field_mappings' ) and ds_cfg . field_mappings : field_mappings = ds_cfg . field_mappings logger . info ( f \"Applying field mappings: { field_mappings } \" ) elif hasattr ( ds_cfg , 'column_mapping' ) and ds_cfg . column_mapping : field_mappings = ds_cfg . column_mapping logger . info ( f \"Applying column mappings: { field_mappings } \" ) if field_mappings : # Only rename columns that exist in the dataset valid_mappings = { k : v for k , v in field_mappings . items () if k in dataset . column_names } if valid_mappings : dataset = dataset . rename_columns ( valid_mappings ) logger . info ( f \"Renamed columns: { valid_mappings } \" ) else : logger . warning ( f \"No valid column mappings found. Available columns: { dataset . column_names } \" ) # Log dataset structure logger . info ( f \"Dataset columns: { dataset . column_names } \" ) if len ( dataset ) > 0 : logger . info ( f \"Sample data (first item): { list ( dataset [ 0 ] . keys ()) } \" ) # Task-specific formatting dataset = self . _format_dataset_for_task ( dataset , ds_cfg ) # Split train/test # Always create eval split if dataset has at least 2 samples (minimum for split) # For very small datasets, use a smaller test_size to ensure eval set has at least 1 sample if len ( dataset ) >= 2 : if len ( dataset ) >= 10 : test_size = 0.1 # 10% for larger datasets elif len ( dataset ) >= 5 : test_size = 0.2 # 20% for medium datasets (ensures at least 1 eval sample) else : test_size = 0.2 # 20% for very small datasets (2-4 samples -> at least 1 eval) split_ds = dataset . train_test_split ( test_size = test_size , seed = 42 ) self . train_dataset = split_ds [ \"train\" ] self . eval_dataset = split_ds [ \"test\" ] logger . info ( f \"Split dataset: { len ( self . train_dataset ) } train, { len ( self . eval_dataset ) } eval\" ) else : # Only skip split if dataset has less than 2 samples self . train_dataset = dataset self . eval_dataset = None logger . warning ( f \"Dataset too small for split (only { len ( dataset ) } samples), using all for training. No eval dataset created.\" ) logger . info ( f \"Dataset prepared: { len ( self . train_dataset ) } training samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise","title":"setup_dataset"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.setup_model","text":"Setup Unsloth-optimized model with task-aware configuration. Source code in src/aligntune/backends/unsloth/sft/sft.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model with task-aware configuration.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import SFTConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth model for task: { self . task_type . value } \" ) logger . info ( f \"Model: { self . config . model . name_or_path } \" ) # Configure Unsloth model parameters # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"Unsloth SFT\" ) dtype = PrecisionHandler . get_torch_dtype ( precision ) # Configure Unsloth model parameters # Unsloth only accepts load_in_4bit as boolean, not BitsAndBytesConfig parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : dtype , # Use dtype from PrecisionHandler instead of None \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , False ) if self . config . model . quantization else False , } # Unsloth handles quantization internally - don't pass BitsAndBytesConfig parameters # Parameters like bnb_4bit_compute_dtype, bnb_4bit_quant_type, etc. are not supported # Load model with Unsloth optimizations try : self . unsloth_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = self . config . model . name_or_path , ** model_kwargs ) except ( SyntaxError , RuntimeError , Exception ) as e : # Handle Unsloth compiled module syntax errors (may be wrapped in RuntimeError) error_msg = str ( e ) if \"unsloth_compiled_module\" in error_msg or \"unexpected indent\" in error_msg . lower (): logger . error ( f \"Unsloth compilation error: { e } \" ) logger . info ( \"This is a known Unsloth issue with compiled modules.\" ) logger . info ( \"Suggested fixes:\" ) logger . info ( \" 1. Clear Unsloth cache: rm -rf ~/.cache/unsloth/\" ) logger . info ( \" 2. Retry training after clearing cache\" ) logger . info ( \" 3. Use TRL backend instead: backend='trl'\" ) raise RuntimeError ( f \"Unsloth compiled module syntax error: { e } \\n \" f \"This is a known Unsloth library issue. \" f \"Please clear the Unsloth cache and retry, or use TRL backend instead.\" ) from e else : # Re-raise if it's a different error raise # Detect model architecture for appropriate target modules model_type = self . unsloth_model . config . model_type . lower () target_modules = self . _get_target_modules_for_architecture ( model_type ) # Validate target modules exist in the model available_modules = [ name for name , _ in self . unsloth_model . named_modules ()] valid_target_modules = [ tm for tm in target_modules if any ( tm in name for name in available_modules )] if not valid_target_modules : # Try to auto-detect target modules logger . warning ( f \"None of the target modules { target_modules } found in model. Attempting auto-detection...\" ) valid_target_modules = self . _auto_detect_target_modules ( available_modules , model_type ) if not valid_target_modules : raise ValueError ( f \"Target modules { target_modules } not found in the base model (type: { model_type } ). \" f \"Available modules include: { available_modules [: 10 ] } ... \" f \"Please check the target modules and try again, or use a different model architecture.\" ) logger . info ( f \"Using target modules for { model_type } : { valid_target_modules } \" ) # Configure model for training with LoRA self . unsloth_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , # LoRA rank target_modules = valid_target_modules , lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Setup tokenizer based on task type self . _setup_tokenizer_for_task () logger . info ( \"Unsloth model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth model: { e } \" ) raise","title":"setup_model"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.setup_trainer","text":"Setup TRL SFTTrainer with task-aware configuration. Source code in src/aligntune/backends/unsloth/sft/sft.py 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 def setup_trainer ( self ) -> None : \"\"\"Setup TRL SFTTrainer with task-aware configuration.\"\"\" try : from trl import SFTTrainer , SFTConfig , ModelConfig logger . info ( f \"Setting up TRL SFTTrainer for task: { self . task_type . value } \" ) # Get training parameters num_epochs = getattr ( self . config , 'num_epochs' , None ) or \\ getattr ( self . config . train , 'epochs' , 3 ) if hasattr ( self . config , 'train' ) else 3 batch_size = getattr ( self . config , 'batch_size' , None ) or \\ getattr ( self . config . train , 'per_device_batch_size' , 2 ) if hasattr ( self . config , 'train' ) else 2 grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) if hasattr ( self . config , 'train' ) else 1 lr = getattr ( self . config . train , 'learning_rate' , 2e-4 ) if hasattr ( self . config , 'train' ) else 2e-4 save_steps = getattr ( self . config . train , 'save_interval' , 500 ) if hasattr ( self . config , 'train' ) else 500 output_dir = getattr ( self . config . logging , 'output_dir' , './output' ) if hasattr ( self . config , 'logging' ) else './output' # === UNIFIED PRECISION HANDLING === precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Create SFT configuration # Create SFT configuration self . training_config = SFTConfig ( output_dir = output_dir , num_train_epochs = num_epochs , per_device_train_batch_size = batch_size , gradient_accumulation_steps = grad_accum , learning_rate = lr , logging_steps = 10 , save_steps = save_steps , warmup_ratio = 0.1 , lr_scheduler_type = \"cosine\" , ** precision_args , dataloader_pin_memory = False , report_to = \"none\" if not hasattr ( self . config , 'logging' ) else \"tensorboard\" ) # Get max_seq_length max_seq_len = getattr ( self . config . model , 'max_seq_length' , 2048 ) if hasattr ( self . config , 'model' ) else 2048 # Task-specific trainer settings packing = False # Generally disable packing for better quality if self . task_type == TaskType . TEXT_GENERATION : packing = True # Can enable packing for simple text generation # Create trainer self . trainer = SFTTrainer ( model = self . unsloth_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = self . training_config , dataset_text_field = \"text\" , max_seq_length = max_seq_len , dataset_num_proc = 2 , packing = packing , ) logger . info ( f \"TRL SFTTrainer setup completed for { self . task_type . value } \" ) logger . info ( f \"Training config: { num_epochs } epochs, batch_size= { batch_size } , lr= { lr } \" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.train","text":"Execute training with Unsloth optimizations. Source code in src/aligntune/backends/unsloth/sft/sft.py 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute training with Unsloth optimizations.\"\"\" try : logger . info ( f \"Starting Unsloth SFT training for task: { self . task_type . value } \" ) start_time = time . time () # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Suppress Unsloth's informational warning about num_items_in_batch # This is a known limitation with Qwen2 models and gradient accumulation with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , message = \".*num_items_in_batch.*\" , category = UserWarning ) warnings . filterwarnings ( \"ignore\" , message = \".*Qwen2ForCausalLM does not accept.*\" , category = UserWarning ) # Start training training_result = self . trainer . train () # Save model output_dir = self . config . logging . output_dir if hasattr ( self . config , 'logging' ) else './output' self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"task_type\" : self . task_type . value , \"training_time\" : training_time , \"final_loss\" : training_result . training_loss , \"total_steps\" : training_result . global_step , \"model_path\" : output_dir , \"training_history\" : self . training_history , } logger . info ( f \"Unsloth SFT training completed in { training_time : .2f } seconds\" ) logger . info ( f \"Task: { self . task_type . value } , Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise","title":"train"},{"location":"api-reference/trainers/#backends.unsloth.sft.sft.UnslothSFTTrainer.train_step","text":"Execute a single training step (required by abstract base class). Source code in src/aligntune/backends/unsloth/sft/sft.py 598 599 600 601 602 603 604 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Execute a single training step (required by abstract base class).\"\"\" if self . trainer is None : raise RuntimeError ( \"Trainer not initialized. Call setup_model() first.\" ) # The actual training step is handled by TRL's SFTTrainer return { \"loss\" : 0.0 , \"learning_rate\" : 0.0 } options: show_source: true heading_level: 3 Example : from aligntune.backends.unsloth.sft.sft import UnslothSFTTrainer from aligntune.core.sft.config import SFTConfig config = SFTConfig ( ... ) trainer = UnslothSFTTrainer ( config ) trainer . train ()","title":"train_step"},{"location":"api-reference/trainers/#unslothdpotrainer","text":"Unsloth backend for Direct Preference Optimization. Bases: TrainerBase Unsloth-optimized DPO trainer using TRL's DPOTrainer. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 class UnslothDPOTrainer ( TrainerBase ): \"\"\"Unsloth-optimized DPO trainer using TRL's DPOTrainer.\"\"\" def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize Unsloth DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . custom_evaluator = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth DPO trainer is available.\"\"\" try : import unsloth from trl import DPOTrainer , DPOConfig , ModelConfig return True except ImportError : return False # Required abstract methods implementation def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset () def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO.\"\"\" pass def train_step ( self ) -> Dict [ str , Any ]: \"\"\"Single training step - handled by TRL internally.\"\"\" raise NotImplementedError ( \"Use train() method instead\" ) def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for DPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import DPOConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth DPO model: { self . config . model . name_or_path } \" ) device_map = self . config . model . device_map or 'auto' # Configure Unsloth model parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : None , # Auto-detect \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , True ), \"device_map\" : device_map } # Add quantization parameters if specified # FIXED: Only add parameters that are valid for # FastLanguageModel.from_pretrained if self . config . model . quantization : # These are the parameters that Unsloth's FastLanguageModel # accepts valid_quant_params = { 'load_in_4bit' , 'use_gradient_checkpointing' , 'rope_scaling' , 'fix_tokenizer' , 'trust_remote_code' , 'use_cache' , 'token' } for key , value in self . config . model . quantization . items (): if key in valid_quant_params and key != \"load_in_4bit\" : model_kwargs [ key ] = value # Silently skip unsupported parameters like bnb_4bit_compute_dtype # These are handled internally by Unsloth # Load model with Unsloth optimizations self . model , self . tokenizer = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Configure model for DPO training self . model = FastLanguageModel . get_peft_model ( self . model , r = 16 , # LoRA rank target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Create reference model for DPO self . reference_model , _ = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( \"Unsloth DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth DPO model: { e } \" ) raise def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : logger . info ( \"Setting up DPO dataset with DataManager...\" ) # Extract dataset configuration if not hasattr ( self . config , 'datasets' ) or len ( self . config . datasets ) == 0 : raise ValueError ( \"No dataset configuration found\" ) dataset_config = self . config . datasets [ 0 ] # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading DPO dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for DPO task from aligntune.data.manager import DataManager enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) logger . info ( f \"Dataset columns: { available_columns } \" ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column # structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \\ \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer with Unsloth model.\"\"\" try : from trl import DPOTrainer , DPOConfig , ModelConfig logger . info ( \"Setting up TRL DPOTrainer with Unsloth model\" ) from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , default = 1 ) if num_epochs is None : num_epochs = 1 per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , default = 4 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , default = 5e-5 ) max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default =- 1 ) if max_steps is None : max_steps = - 1 # Get output and logging parameters output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/unsloth_dpo' ) run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_dpo' ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None load_best_model_at_end = False metric_for_best_model = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = False ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) # DPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , default = 0.1 ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) label_smoothing = self . _get_config_value ( self . config . train , 'label_smoothing' , default = 0.0 ) truncation_mode = self . _get_config_value ( self . config . train , 'truncation_mode' , default = 'keep_end' ) # Sequence lengths max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ) max_length = self . _get_config_value ( self . config . train , 'max_length' , default = max_seq_length ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = max_seq_length // 2 ) # Adjust save strategy to save on epoch if self . eval_dataset : save_strategy = \"epoch\" # Create DPO configuration dpo_config = DPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_steps , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , max_steps = max_steps , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # DPO specific parameters beta = beta , loss_type = loss_type , label_smoothing = label_smoothing , max_length = max_length , max_prompt_length = max_prompt_length , truncation_mode = truncation_mode , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting Unsloth DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"Unsloth DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 )} except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained model.\"\"\" # try: # if not self.trainer or not self.eval_dataset: # logger.warning(\"No trainer or evaluation dataset available\") # return {} # logger.info(\"Running DPO evaluation\") # # Run evaluation # eval_result = self.trainer.evaluate() # logger.info(\"DPO evaluation completed\") # return { # \"eval_loss\": eval_result.get(\"eval_loss\", 0), # \"eval_metrics\": eval_result # } # except Exception as e: # logger.error(f\"DPO evaluation failed: {e}\") # return {} # def evaluate( # self, # eval_dataset=None, # metric_key_prefix: str = \"eval\", # use_custom_evaluator: bool = True, # **kwargs # ) -> Dict[str, float]: # \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # # Auto-setup evaluators on first call # if self.base_evaluator is None and self.rl_evaluator is None: # logger.info(\"Auto-initializing evaluators for first evaluation...\") # self.setup_custom_evaluator(evaluator_type=\"auto\") # # Call parent's unified evaluate method # return super().evaluate( # eval_dataset=eval_dataset, # metric_key_prefix=metric_key_prefix, # use_custom_evaluator=use_custom_evaluator, # **kwargs # ) def generate_preference_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate sample preference data for testing.\"\"\" try : if not self . tokenizer : logger . warning ( \"No tokenizer available for generation\" ) return [] # Sample prompts prompts = [ \"Explain the concept of machine learning\" , \"Write a short story about a robot\" , \"Describe the benefits of renewable energy\" , \"What are the key principles of good software design?\" , \"Explain quantum computing in simple terms\" ] samples = [] for i , prompt in enumerate ( prompts [: num_samples ]): samples . append ({ \"prompt\" : prompt , \"chosen\" : f \"Sample chosen response { i + 1 } \" , \"rejected\" : f \"Sample rejected response { i + 1 } \" }) return samples except Exception as e : logger . error ( f \"Failed to generate preference samples: { e } \" ) return [] def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"UnslothDPOTrainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.__init__","text":"Initialize Unsloth DPO trainer. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , config : UnifiedConfig ): \"\"\"Initialize Unsloth DPO trainer.\"\"\" super () . __init__ ( config ) self . model = None self . reference_model = None self . tokenizer = None self . trainer = None self . train_dataset = None self . eval_dataset = None self . custom_evaluator = None self . dataset_dict = None","title":"__init__"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.generate_preference_samples","text":"Generate sample preference data for testing. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def generate_preference_samples ( self , num_samples : int = 5 ) -> List [ Dict [ str , str ]]: \"\"\"Generate sample preference data for testing.\"\"\" try : if not self . tokenizer : logger . warning ( \"No tokenizer available for generation\" ) return [] # Sample prompts prompts = [ \"Explain the concept of machine learning\" , \"Write a short story about a robot\" , \"Describe the benefits of renewable energy\" , \"What are the key principles of good software design?\" , \"Explain quantum computing in simple terms\" ] samples = [] for i , prompt in enumerate ( prompts [: num_samples ]): samples . append ({ \"prompt\" : prompt , \"chosen\" : f \"Sample chosen response { i + 1 } \" , \"rejected\" : f \"Sample rejected response { i + 1 } \" }) return samples except Exception as e : logger . error ( f \"Failed to generate preference samples: { e } \" ) return []","title":"generate_preference_samples"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.is_available","text":"Check if Unsloth DPO trainer is available. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 39 40 41 42 43 44 45 46 47 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth DPO trainer is available.\"\"\" try : import unsloth from trl import DPOTrainer , DPOConfig , ModelConfig return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.load_model","text":"Load a trained model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 def load_model ( self , path : str ) -> None : \"\"\"Load a trained model.\"\"\" try : logger . info ( f \"Loading model from: { path } \" ) # Load model and tokenizer from transformers import AutoModelForCausalLM , AutoTokenizer self . model = AutoModelForCausalLM . from_pretrained ( path ) self . tokenizer = AutoTokenizer . from_pretrained ( path ) logger . info ( \"Model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.save_model","text":"Save model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/dpo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" import yaml with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.setup_data","text":"Setup data - delegates to setup_dataset. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 62 63 64 def setup_data ( self ) -> None : \"\"\"Setup data - delegates to setup_dataset.\"\"\" self . setup_dataset ()","title":"setup_data"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.setup_dataset","text":"Setup and prepare preference dataset for DPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def setup_dataset ( self ) -> None : \"\"\"Setup and prepare preference dataset for DPO training using unified DataManager.\"\"\" try : logger . info ( \"Setting up DPO dataset with DataManager...\" ) # Extract dataset configuration if not hasattr ( self . config , 'datasets' ) or len ( self . config . datasets ) == 0 : raise ValueError ( \"No dataset configuration found\" ) dataset_config = self . config . datasets [ 0 ] # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) logger . info ( f \"Loading DPO dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for DPO task from aligntune.data.manager import DataManager enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) manager = DataManager ( task_type = \"dpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict # === ROBUST COLUMN DETECTION === # Check what columns actually exist available_columns = set ( self . train_dataset . column_names ) logger . info ( f \"Dataset columns: { available_columns } \" ) # Detect the column structure has_prompt = \"prompt\" in available_columns has_chosen = \"chosen\" in available_columns has_rejected = \"rejected\" in available_columns # Log first sample to understand the structure if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( f \"Sample keys: { list ( sample . keys ()) } \" ) for key , value in sample . items (): if isinstance ( value , str ): logger . info ( f \" { key } : { value [: 100 ] } ...\" if len ( value ) > 100 else f \" { key } : { value } \" ) # Define a robust filtering function that handles different column # structures def filter_valid_examples ( example ): \"\"\"Filter out invalid examples with flexible column detection.\"\"\" try : # If we have the standard DPO format if has_prompt and has_chosen and has_rejected : return ( len ( example . get ( \"prompt\" , \"\" )) > 0 and len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # Alternative: dataset might have 'chosen' and 'rejected' only # (TRL can handle this format too) elif has_chosen and has_rejected and not has_prompt : return ( len ( example . get ( \"chosen\" , \"\" )) > 0 and len ( example . get ( \"rejected\" , \"\" )) > 0 ) # If columns don't match expected format, log and skip else : logger . warning ( f \"Unexpected dataset format. Available columns: { available_columns } \" ) return False except Exception as e : logger . warning ( f \"Error filtering example: { e } \" ) return False # Filter empty examples if len ( self . train_dataset ) > 0 : initial_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid examples\" ) filtered_count = initial_size - len ( self . train_dataset ) if filtered_count > 0 : logger . info ( f \"Filtered out { filtered_count } invalid examples\" ) # Filter by approximate token count max_length = self . config . model . max_seq_length def is_valid_length ( example ): \"\"\"Check if example fits within max sequence length.\"\"\" try : # Rough approximation: 1 token \u2248 4 characters # Handle different column structures if has_prompt : prompt_tokens = len ( example . get ( \"prompt\" , \"\" )) // 4 else : # If no prompt, we'll check chosen/rejected only prompt_tokens = 0 chosen_tokens = len ( example . get ( \"chosen\" , \"\" )) // 4 rejected_tokens = len ( example . get ( \"rejected\" , \"\" )) // 4 # Each response is concatenated with prompt, so check both return ( prompt_tokens + chosen_tokens < max_length and prompt_tokens + rejected_tokens < max_length ) except Exception as e : logger . warning ( f \"Error checking length: { e } \" ) return False original_size = len ( self . train_dataset ) self . train_dataset = self . train_dataset . filter ( is_valid_length , desc = \"Filtering long sequences\" ) filtered_size = len ( self . train_dataset ) if original_size - filtered_size > 0 : logger . info ( f \"Filtered { original_size - filtered_size } samples that were too long\" ) # Apply same filtering to eval dataset if it exists if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( filter_valid_examples , desc = \"Filtering invalid eval examples\" ) self . eval_dataset = self . eval_dataset . filter ( is_valid_length , desc = \"Filtering long eval sequences\" ) logger . info ( f \"DPO dataset prepared: { len ( self . train_dataset ) } train samples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } samples\" ) # Log final sample for debugging if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] logger . info ( \"Final dataset structure:\" ) for key in sample . keys (): value = sample [ key ] if isinstance ( value , str ): preview = value [: 100 ] + \\ \"...\" if len ( value ) > 100 else value logger . info ( f \" { key } : { preview } \" ) except Exception as e : logger . error ( f \"Failed to setup dataset: { e } \" ) raise","title":"setup_dataset"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.setup_model","text":"Setup Unsloth-optimized model and tokenizer for DPO. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for DPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import DPOConfig , ModelConfig from transformers import AutoTokenizer logger . info ( f \"Setting up Unsloth DPO model: { self . config . model . name_or_path } \" ) device_map = self . config . model . device_map or 'auto' # Configure Unsloth model parameters model_kwargs = { \"max_seq_length\" : self . config . model . max_seq_length , \"dtype\" : None , # Auto-detect \"load_in_4bit\" : self . config . model . quantization . get ( \"load_in_4bit\" , True ), \"device_map\" : device_map } # Add quantization parameters if specified # FIXED: Only add parameters that are valid for # FastLanguageModel.from_pretrained if self . config . model . quantization : # These are the parameters that Unsloth's FastLanguageModel # accepts valid_quant_params = { 'load_in_4bit' , 'use_gradient_checkpointing' , 'rope_scaling' , 'fix_tokenizer' , 'trust_remote_code' , 'use_cache' , 'token' } for key , value in self . config . model . quantization . items (): if key in valid_quant_params and key != \"load_in_4bit\" : model_kwargs [ key ] = value # Silently skip unsupported parameters like bnb_4bit_compute_dtype # These are handled internally by Unsloth # Load model with Unsloth optimizations self . model , self . tokenizer = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) # Configure model for DPO training self . model = FastLanguageModel . get_peft_model ( self . model , r = 16 , # LoRA rank target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = \"unsloth\" , random_state = 3407 , use_rslora = False , loftq_config = None , ) # Create reference model for DPO self . reference_model , _ = FastLanguageModel . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( \"Unsloth DPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth DPO model: { e } \" ) raise","title":"setup_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.setup_rewards","text":"Setup rewards - not used in DPO. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 66 67 68 def setup_rewards ( self ) -> None : \"\"\"Setup rewards - not used in DPO.\"\"\" pass","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.setup_trainer","text":"Setup TRL DPOTrainer with Unsloth model. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def setup_trainer ( self ) -> None : \"\"\"Setup TRL DPOTrainer with Unsloth model.\"\"\" try : from trl import DPOTrainer , DPOConfig , ModelConfig logger . info ( \"Setting up TRL DPOTrainer with Unsloth model\" ) from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Get training parameters num_epochs = self . _get_config_value ( self . config . train , 'epochs' , 'num_epochs' , default = 1 ) if num_epochs is None : num_epochs = 1 per_device_batch_size = self . _get_config_value ( self . config . train , 'per_device_batch_size' , default = 4 ) gradient_accumulation_steps = self . _get_config_value ( self . config . train , 'gradient_accumulation_steps' , default = 1 ) learning_rate = self . _get_config_value ( self . config . train , 'learning_rate' , default = 5e-5 ) max_steps = self . _get_config_value ( self . config . train , 'max_steps' , default =- 1 ) if max_steps is None : max_steps = - 1 # Get output and logging parameters output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , default = './output/unsloth_dpo' ) run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_dpo' ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = 100 ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = 'eval_loss' ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = True ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None load_best_model_at_end = False metric_for_best_model = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_steps = self . _get_config_value ( self . config . train , 'save_steps' , default = 100 ) save_strategy = self . _get_config_value ( self . config . train , 'save_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) max_grad_norm = self . _get_config_value ( self . config . train , 'max_grad_norm' , default = 1.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = False ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) # DPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , default = 0.1 ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) label_smoothing = self . _get_config_value ( self . config . train , 'label_smoothing' , default = 0.0 ) truncation_mode = self . _get_config_value ( self . config . train , 'truncation_mode' , default = 'keep_end' ) # Sequence lengths max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , default = 512 ) max_length = self . _get_config_value ( self . config . train , 'max_length' , default = max_seq_length ) max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = max_seq_length // 2 ) # Adjust save strategy to save on epoch if self . eval_dataset : save_strategy = \"epoch\" # Create DPO configuration dpo_config = DPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_steps , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , max_steps = max_steps , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = max_grad_norm , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # DPO specific parameters beta = beta , loss_type = loss_type , label_smoothing = label_smoothing , max_length = max_length , max_prompt_length = max_prompt_length , truncation_mode = truncation_mode , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = dpo_config , config = self . config , algorithm = 'dpo' ) for key , value in missing . items (): setattr ( dpo_config , key , value ) # Create trainer self . trainer = DPOTrainer ( model = self . model , ref_model = self . reference_model , tokenizer = self . tokenizer , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , args = dpo_config , ) logger . info ( \"TRL DPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup DPO trainer: { e } \" ) raise","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.train","text":"Run DPO training. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 def train ( self ) -> Dict [ str , Any ]: \"\"\"Run DPO training.\"\"\" try : logger . info ( \"Starting Unsloth DPO training\" ) # Setup components self . setup_model () self . setup_dataset () self . setup_trainer () # Run training training_result = self . trainer . train () # Save model model_path = self . save_model () logger . info ( \"Unsloth DPO training completed successfully\" ) return { \"training_time\" : training_result . metrics . get ( \"train_runtime\" , 0 ), \"final_loss\" : training_result . metrics . get ( \"train_loss\" , 0 ), \"model_path\" : model_path , \"total_steps\" : training_result . metrics . get ( \"train_steps\" , 0 )} except Exception as e : logger . error ( f \"DPO training failed: { e } \" ) raise","title":"train"},{"location":"api-reference/trainers/#backends.unsloth.rl.dpo.dpo.UnslothDPOTrainer.train_step","text":"Single training step - handled by TRL internally. Source code in src/aligntune/backends/unsloth/rl/dpo/dpo.py 70 71 72 def train_step ( self ) -> Dict [ str , Any ]: \"\"\"Single training step - handled by TRL internally.\"\"\" raise NotImplementedError ( \"Use train() method instead\" ) options: show_source: true heading_level: 3","title":"train_step"},{"location":"api-reference/trainers/#unslothppotrainer","text":"Unsloth backend for Proximal Policy Optimization. Bases: TrainerBase PPO trainer with hybrid Unsloth architecture. Model Architecture: - Policy Model: Unsloth FastLanguageModel + LoRA (trained, optimized) - Reference Model: Unsloth FastLanguageModel (frozen, memory efficient) - Reward Model: Standard HuggingFace (frozen, any architecture) - Value Model: Standard HuggingFace (trained alongside policy) Key Insight: Unsloth models need manual patches for apply_qkv method. Standard models work naturally with TRL without patches. Reward Model Options: 1. Pretrained: Load from HuggingFace (DeBERTa, RoBERTa, etc.) 2. Custom trained: Train using TRL's RewardTrainer 3. Hybrid: Fine-tune pretrained with custom reward functions Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 class UnslothPPOTrainer ( TrainerBase ): \"\"\"PPO trainer with hybrid Unsloth architecture. Model Architecture: - Policy Model: Unsloth FastLanguageModel + LoRA (trained, optimized) - Reference Model: Unsloth FastLanguageModel (frozen, memory efficient) - Reward Model: Standard HuggingFace (frozen, any architecture) - Value Model: Standard HuggingFace (trained alongside policy) Key Insight: Unsloth models need manual patches for apply_qkv method. Standard models work naturally with TRL without patches. Reward Model Options: 1. Pretrained: Load from HuggingFace (DeBERTa, RoBERTa, etc.) 2. Custom trained: Train using TRL's RewardTrainer 3. Hybrid: Fine-tune pretrained with custom reward functions \"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) # Validate Unsloth is properly loaded import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : raise RuntimeError ( \"Unsloth PPO backend requires Unsloth to be imported first. \" \"This should be handled by the factory, but validation failed. \" \"Please report this as a bug.\" ) # Rest of existing initialization self . unsloth_model = None self . policy_model = None self . ref_model = None self . reward_model = None self . value_model = None self . trainer = None self . training_history = [] self . train_dataset = None self . eval_dataset = None self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available and properly configured.\"\"\" try : # Check Unsloth installation import unsloth from unsloth import FastLanguageModel # Check if Unsloth is properly configured import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : logger . warning ( \"Unsloth not properly initialized\" ) return False # Check TRL availability from trl import PPOTrainer , PPOConfig from transformers import AutoModelForSequenceClassification return True except ImportError as e : logger . warning ( f \"Unsloth PPO backend not available: { e } \" ) return False except Exception as e : logger . warning ( f \"Unsloth PPO backend configuration error: { e } \" ) return False def _get_config_value ( self , config_obj , key : str , default = None ): \"\"\"Helper to get config value from dict or object.\"\"\" if isinstance ( config_obj , dict ): return config_obj . get ( key , default ) else : return getattr ( config_obj , key , default ) def _detect_model_type ( self , model_name : str ) -> str : \"\"\"Detect if model should use Unsloth or standard loading. Returns: 'unsloth': Use Unsloth FastLanguageModel 'standard': Use standard transformers \"\"\" # Check config override first model_config = self . config . model if hasattr ( self . config , 'model' ) else {} if isinstance ( model_config , dict ): force_type = model_config . get ( 'reward_value_loading_type' ) else : force_type = getattr ( model_config , 'reward_value_loading_type' , None ) if force_type in [ 'unsloth' , 'standard' ]: logger . info ( f \"Using config-specified loading type: { force_type } \" ) return force_type # Models that CANNOT use Unsloth (encoder-only models) incompatible_models = [ 'bert' , 'roberta' , 'distilbert' , 'albert' , 'electra' , 'deberta' ] if any ( indicator in model_name . lower () for indicator in incompatible_models ): logger . info ( f \"Using standard loading for encoder model: { model_name } \" ) return 'standard' # Default to Unsloth for decoder models (Llama, GPT, Mistral, etc.) # This ensures they get proper rotary embedding patches logger . info ( f \"Using Unsloth loading for decoder model: { model_name } \" ) return 'unsloth' def _load_reward_value_models_unsloth ( self , model_name : str , max_seq_length : int , quantization : dict ): \"\"\"Load reward/value models using Unsloth for optimization.\"\"\" logger . info ( f \"Loading reward/value models with Unsloth: { model_name } \" ) # Check for integrated reward training first reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) # Check if integrated reward training is enabled if ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ): logger . info ( \"\ud83c\udfcb\ufe0f Training custom reward model before PPO...\" ) reward_model_path = self . _train_custom_reward_model () # Override reward_model_name with trained model path reward_model_name = reward_model_path logger . info ( f \"\u2705 Custom reward model trained and saved to: { reward_model_path } \" ) # Get quantization settings (configurable per model) reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , quantization ) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , quantization ) # Load reward model if reward_model_name : logger . info ( f \"Loading pre-trained reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification # Load pre-trained reward model self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch . bfloat16 , device_map = \"auto\" ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # DEBUG: Check wrapper logger . info ( f \"\ud83d\udd0d DEBUG: Wrapped reward model type: { type ( self . reward_model ) . __name__ } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Wrapped reward model has score: { hasattr ( self . reward_model , 'score' ) } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Base model type: { type ( self . reward_model . _model ) . __name__ } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Base model has classifier: { hasattr ( self . reward_model . _model , 'classifier' ) } \" ) # Verify score method exists for Unsloth PPO compatibility if not hasattr ( self . reward_model , 'score' ): logger . error ( \"Reward model wrapper missing score method!\" ) base_model = self . reward_model . _model if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to Unsloth reward model wrapper\" ) else : raise AttributeError ( \"Reward model must have score() or classifier() method\" ) else : logger . info ( \"Unsloth reward model wrapper has score method\" ) logger . info ( \"\u2705 Loaded pre-trained reward model with UniversalRewardModelWrapper\" ) else : # Create reward model from scratch (NO quantization for TRL compatibility) reward_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = False , # CRITICAL: Disable quantization for TRL compatibility ) if hasattr ( reward_base , \"config\" ): reward_base . config . output_hidden_states = True import torch.nn as nn hidden_size = reward_base . config . hidden_size base_dtype = next ( reward_base . parameters ()) . dtype self . reward_model = reward_base self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( base_dtype ) . to ( reward_base . device ) # Verify score layer was added correctly if not hasattr ( self . reward_model , 'score' ): raise RuntimeError ( \"Failed to add score layer to Unsloth reward model\" ) logger . info ( f \"Unsloth reward model score layer: { self . reward_model . score } \" ) # Load value model similarly (NO quantization to avoid .to() errors in PPOTrainer) value_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = False , # CRITICAL: Disable quantization for TRL compatibility ) if hasattr ( value_base , \"config\" ): value_base . config . output_hidden_states = True self . value_model = value_base value_dtype = next ( value_base . parameters ()) . dtype self . value_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( value_dtype ) . to ( value_base . device ) # Disable gradient checkpointing for model in [ self . reward_model , self . value_model ]: if hasattr ( model , 'gradient_checkpointing_disable' ): model . gradient_checkpointing_disable () if hasattr ( model , 'config' ): model . config . gradient_checkpointing = False model . config . use_cache = True logger . info ( \"\u2705 Reward/value models loaded with Unsloth optimizations\" ) def _load_reward_value_models_standard ( self , model_name : str ): \"\"\"Load reward/value models using standard transformers (no Unsloth patches).\"\"\" logger . info ( f \"Loading reward/value models with standard transformers: { model_name } \" ) from transformers import AutoModelForSequenceClassification # Get quantization settings (configurable per model) reward_quant = self . _get_config_value ( self . config . model , 'reward_model_quantization' , {}) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , {}) # Get precision from config precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 # Load reward model self . reward_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = reward_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . reward_model , 'config' ): self . reward_model . config . output_hidden_states = True # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # Verify score method for standard transformers path if not hasattr ( self . reward_model , 'score' ): base_model = self . reward_model . _model if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to standard reward model wrapper\" ) else : logger . info ( \"Standard reward model wrapper has score method\" ) # Load value model self . value_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = value_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . value_model , 'config' ): self . value_model . config . output_hidden_states = True # Disable gradient checkpointing for model in [ self . reward_model , self . value_model ]: if hasattr ( model , 'gradient_checkpointing_disable' ): model . gradient_checkpointing_disable () if hasattr ( model , 'config' ): model . config . gradient_checkpointing = False model . config . use_cache = True # Standard HuggingFace models don't need Unsloth patches # They work naturally with TRL's PPOTrainer logger . info ( \"\u2705 Reward/value models loaded with standard transformers\" ) def _load_value_model_unsloth ( self , model_name : str , max_seq_length : int , quantization : dict ): \"\"\"Load only value model using Unsloth.\"\"\" logger . info ( f \"Loading value model with Unsloth: { model_name } \" ) value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , quantization ) value_base , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , load_in_4bit = value_quant . get ( \"load_in_4bit\" , True ), ) if hasattr ( value_base , \"config\" ): value_base . config . output_hidden_states = True import torch.nn as nn hidden_size = value_base . config . hidden_size self . value_model = value_base base_dtype = next ( value_base . parameters ()) . dtype self . value_model . score = nn . Linear ( hidden_size , 1 , bias = False ) . to ( base_dtype ) . to ( value_base . device ) if hasattr ( self . value_model , 'gradient_checkpointing_disable' ): self . value_model . gradient_checkpointing_disable () if hasattr ( self . value_model , 'config' ): self . value_model . config . gradient_checkpointing = False self . value_model . config . use_cache = True logger . info ( \"\u2705 Value model loaded with Unsloth optimizations\" ) def _load_value_model_standard ( self , model_name : str ): \"\"\"Load only value model using standard transformers (NO Unsloth patches).\"\"\" logger . info ( f \"Loading value model with standard transformers: { model_name } \" ) # Warn if trying to load Unsloth model with standard transformers if \"unsloth\" in model_name . lower (): logger . warning ( f \"\u26a0\ufe0f Loading Unsloth model ' { model_name } ' with standard transformers\" ) logger . warning ( \"\u26a0\ufe0f This may cause compatibility issues with Unsloth's PPOTrainer\" ) logger . warning ( \"\u26a0\ufe0f Consider using a standard HuggingFace model or set reward_value_loading_type='unsloth'\" ) from transformers import AutoModelForSequenceClassification value_quant = self . _get_config_value ( self . config . model , 'value_model_quantization' , {}) # Get precision from config precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 self . value_model = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , load_in_4bit = value_quant . get ( \"load_in_4bit\" , False ), ) if hasattr ( self . value_model , 'config' ): self . value_model . config . output_hidden_states = True # DEBUG: Check value model dtype after loading if DEBUG : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model loaded with dtype: { value_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) if hasattr ( self . value_model , 'gradient_checkpointing_disable' ): self . value_model . gradient_checkpointing_disable () if hasattr ( self . value_model , 'config' ): self . value_model . config . gradient_checkpointing = False self . value_model . config . use_cache = True # DO NOT apply Unsloth patches to standard models logger . info ( \"\u2705 Value model loaded with standard transformers (no Unsloth patches)\" ) def _get_config_hash ( self ): \"\"\"Generate hash of relevant configuration for cache invalidation.\"\"\" import hashlib import json # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) gradient_checkpointing = self . _get_config_value ( self . config . model , \"gradient_checkpointing\" , True ) reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , \"meta-llama/Llama-3.2-1B-Instruct\" ) # Get library versions try : import unsloth unsloth_version = getattr ( unsloth , \"__version__\" , \"unknown\" ) except : unsloth_version = \"not_installed\" try : import transformers transformers_version = getattr ( transformers , \"__version__\" , \"unknown\" ) except : transformers_version = \"not_installed\" try : import torch torch_version = getattr ( torch , \"__version__\" , \"unknown\" ) except : torch_version = \"not_installed\" config_dict = { 'model_name' : model_name , 'max_seq_length' : max_seq_length , 'quantization' : quantization , 'gradient_checkpointing' : gradient_checkpointing , 'reward_value_model' : reward_value_model , 'unsloth_version' : unsloth_version , 'transformers_version' : transformers_version , 'torch_version' : torch_version , } config_str = json . dumps ( config_dict , sort_keys = True ) return hashlib . md5 ( config_str . encode ()) . hexdigest () def _clear_unsloth_cache ( self , force : bool = False ): \"\"\"Clear Unsloth compiled cache to force recompilation. Args: force: If True, always clear cache. If False, only clear if config changed. \"\"\" import shutil from pathlib import Path # Clear local cache cache_dir = Path . cwd () / \"unsloth_compiled_cache\" if cache_dir . exists (): if force : logger . info ( \"\ud83d\uddd1\ufe0f Force clearing Unsloth compiled cache...\" ) shutil . rmtree ( cache_dir ) logger . info ( f \"\u2705 Cleared local cache: { cache_dir } \" ) else : logger . info ( f \"Cache exists at: { cache_dir } \" ) # Clear global Unsloth cache if set import os if 'UNSLOTH_COMPILED_CACHE' in os . environ : global_cache = Path ( os . environ [ 'UNSLOTH_COMPILED_CACHE' ]) if global_cache . exists (): if force : logger . info ( f \"\ud83d\uddd1\ufe0f Force clearing global Unsloth cache...\" ) shutil . rmtree ( global_cache ) logger . info ( f \"\u2705 Cleared global cache: { global_cache } \" ) else : logger . info ( f \"Global cache exists at: { global_cache } \" ) # Set environment variable to force recompilation if force : os . environ [ 'UNSLOTH_FORCE_RECOMPILE' ] = '1' logger . info ( \"\u2705 Set UNSLOTH_FORCE_RECOMPILE=1\" ) def _maybe_invalidate_unsloth_cache ( self ): \"\"\"Invalidate Unsloth cache if configuration has changed.\"\"\" from pathlib import Path import shutil cache_dir = Path . cwd () / \"unsloth_compiled_cache\" config_file = cache_dir / \".config_hash\" current_hash = self . _get_config_hash () logger . info ( f \"Current config hash: { current_hash [: 8 ] } ...\" ) if config_file . exists (): stored_hash = config_file . read_text () . strip () if stored_hash != current_hash : logger . info ( f \"Config changed ( { stored_hash [: 8 ] } ... -> { current_hash [: 8 ] } ...), invalidating cache\" ) if cache_dir . exists (): shutil . rmtree ( cache_dir ) logger . info ( f \"\u2713 Cleared Unsloth cache: { cache_dir } \" ) else : logger . info ( f \"Config unchanged ( { current_hash [: 8 ] } ...), reusing cache\" ) else : logger . info ( \"No previous cache found, will create new cache\" ) # Ensure cache dir exists and store new hash cache_dir . mkdir ( exist_ok = True ) config_file . write_text ( current_hash ) def _validate_model_compatibility ( self ): \"\"\"Validate that model types are compatible with loading strategy.\"\"\" # Check value model model_name = self . _get_config_value ( self . config . model , \"reward_value_model\" , \"\" ) loading_type = self . _get_config_value ( self . config . model , \"reward_value_loading_type\" , \"auto\" ) if loading_type == \"standard\" and \"unsloth\" in model_name . lower (): raise ValueError ( f \"Incompatible configuration: reward_value_model=' { model_name } ' \" f \"with reward_value_loading_type='standard'. \" f \"Either use a standard HuggingFace model or set loading_type='unsloth'\" ) # Check reward model reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) if reward_model_name and \"unsloth\" in reward_model_name . lower (): logger . warning ( f \"\u26a0\ufe0f Using Unsloth reward model ' { reward_model_name } ' with standard loading\" ) logger . warning ( \"\u26a0\ufe0f This may cause compatibility issues. Consider using a standard HuggingFace reward model\" ) def _disable_gradient_checkpointing_for_all ( self ): \"\"\"Disable gradient checkpointing on all models - UNSLOTH-SAFE VERSION.\"\"\" def _safe_replace_gc_attr ( obj , name = \"(unknown)\" , indent = \"\" ): \"\"\"Replace gradient checkpointing attributes safely.\"\"\" try : # DON'T replace _gradient_checkpointing_func - just disable the flag if hasattr ( obj , \"gradient_checkpointing\" ): obj . gradient_checkpointing = False # For config objects if hasattr ( obj , \"config\" ): cfg = getattr ( obj , \"config\" , None ) if cfg and hasattr ( cfg , \"gradient_checkpointing\" ): cfg . gradient_checkpointing = False if cfg and hasattr ( cfg , \"use_cache\" ): cfg . use_cache = True logger . info ( f \" { indent } \u2713 Disabled GC flags on { name } \" ) except Exception as e : logger . warning ( f \" { indent } \u2717 Could not disable GC on { name } : { e } \" ) def _recursive_disable ( model , model_name = \"model\" , depth = 0 ): if model is None or depth > 6 : return indent = \" \" * depth logger . info ( f \" { indent } Disabling GC on { model_name } (depth= { depth } )\" ) # 1. Use official API if available if hasattr ( model , \"gradient_checkpointing_disable\" ): try : model . gradient_checkpointing_disable () logger . info ( f \" { indent } \u2713 Called gradient_checkpointing_disable()\" ) except Exception as e : logger . warning ( f \" { indent } \u2717 gradient_checkpointing_disable failed: { e } \" ) # 2. Disable flags (but DON'T replace functions) _safe_replace_gc_attr ( model , model_name , indent ) # 3. Recurse into known wrappers for attr_name in [ \"model\" , \"base_model\" , \"transformer\" , \"bert\" , \"roberta\" ]: inner = getattr ( model , attr_name , None ) if inner is not None and inner is not model : _recursive_disable ( inner , f \" { model_name } . { attr_name } \" , depth + 1 ) logger . info ( \"=\" * 80 ) logger . info ( \"\ud83d\udd27 UNSLOTH-SAFE GRADIENT CHECKPOINTING DISABLE\" ) logger . info ( \"=\" * 80 ) for model_name , model in [ ( \"policy_model\" , getattr ( self , \"policy_model\" , None )), ( \"ref_model\" , getattr ( self , \"ref_model\" , None )), ( \"value_model\" , getattr ( self , \"value_model\" , None )), ( \"reward_model\" , getattr ( self , \"reward_model\" , None )), ]: if model is not None : logger . info ( f \" \\n >>> Processing { model_name } \" ) _recursive_disable ( model , model_name , 0 ) logger . info ( \"=\" * 80 ) logger . info ( \"\u2705 UNSLOTH-SAFE DISABLE COMPLETE\" ) logger . info ( \"=\" * 80 ) for model_name , model in [ ( \"policy_model\" , getattr ( self , \"policy_model\" , None )), ( \"ref_model\" , getattr ( self , \"ref_model\" , None )), ( \"value_model\" , getattr ( self , \"value_model\" , None )), ( \"reward_model\" , getattr ( self , \"reward_model\" , None )), ]: if model is not None : logger . info ( f \" \\n >>> Processing { model_name } \" ) _recursive_disable ( model , model_name , 0 ) logger . info ( \"=\" * 80 ) logger . info ( \"\u2705 NUCLEAR DISABLE COMPLETE (SAFE MODE)\" ) logger . info ( \"=\" * 80 ) # ========================================================================================= # FORCE DISABLE (used for specific model) # ========================================================================================= def _force_disable_gradient_checkpointing_on_model ( self , model , model_name = \"model\" ): \"\"\"Forcefully disable gradient checkpointing - UNSLOTH SAFE.\"\"\" if model is None : return try : # 1. Call disable() wherever possible for target in [ model , getattr ( model , \"base_model\" , None ), getattr ( model , \"model\" , None ), ]: if target and hasattr ( target , \"gradient_checkpointing_disable\" ): target . gradient_checkpointing_disable () logger . info ( f \"\u2713 { model_name } : called gradient_checkpointing_disable()\" ) # 2. Disable config flags for candidate in [ model , getattr ( model , \"base_model\" , None )]: cfg = getattr ( candidate , \"config\" , None ) if cfg : if hasattr ( cfg , \"gradient_checkpointing\" ): cfg . gradient_checkpointing = False if hasattr ( cfg , \"use_cache\" ): cfg . use_cache = True # 3. DON'T replace _gradient_checkpointing_func - Unsloth needs it! # Just set the flag to False on layers inner = getattr ( model , \"model\" , model ) layers = getattr ( inner , \"layers\" , None ) if layers : for layer in layers : if hasattr ( layer , \"gradient_checkpointing\" ): layer . gradient_checkpointing = False logger . info ( f \"\u2713 { model_name } : disabled GC flags on { len ( layers ) } layers\" ) logger . info ( f \"\u2705 Force-disabled GC on { model_name } (Unsloth-safe)\" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Error disabling GC on { model_name } : { e } \" ) def _patch_transformers_validation ( self ): \"\"\"Global patch for transformers validation to ignore num_logits_to_keep.\"\"\" try : from transformers.generation.utils import GenerationMixin if getattr ( GenerationMixin , \"_is_patched_validation\" , False ): return original_validate = GenerationMixin . _validate_model_kwargs def patched_validate ( self , model_kwargs ): # Strip num_logits_to_keep if present to avoid ValueError if 'num_logits_to_keep' in model_kwargs : # logger.info(\"Global Patch: Stripped num_logits_to_keep from validation\") model_kwargs . pop ( 'num_logits_to_keep' ) return original_validate ( self , model_kwargs ) GenerationMixin . _validate_model_kwargs = patched_validate GenerationMixin . _is_patched_validation = True logger . info ( \"\u2705 Globally patched transformers._validate_model_kwargs\" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Failed to patch transformers validation: { e } \" ) def _patch_model_forward_for_compatibility ( self , model , model_name = \"model\" ): \"\"\"Patch model's forward method to ignore incompatible kwargs (like position_ids).\"\"\" if model is None : return # Determine target model (handle wrappers) target_model = model if hasattr ( model , \"model\" ): target_model = model . model elif hasattr ( model , \"base_model\" ): target_model = model . base_model if not hasattr ( target_model , \"forward\" ): logger . warning ( f \"Cannot patch forward: { model_name } has no forward method\" ) return # Check if model is Decoder (Llama, GPT, etc.) or Encoder (BERT, RoBERTa, DistilBERT) # Only patch decoders conditionally; ALWAYS strip position_ids for encoders is_decoder = False is_encoder = False if hasattr ( model , 'config' ): model_type = getattr ( model . config , 'model_type' , '' ) . lower () decoder_models = [ 'llama' , 'gpt' , 'mistral' , 'gemma' , 'qwen' , 'opt' , 'bloom' ] encoder_models = [ 'bert' , 'roberta' , 'distilbert' , 'albert' , 'electra' , 'deberta' ] if any ( dec in model_type for dec in decoder_models ): is_decoder = True elif any ( enc in model_type for enc in encoder_models ): is_encoder = True if not is_decoder and not is_encoder : return # Patch the forward method of the model itself original_forward = target_model . forward def patched_forward ( self , * args , ** kwargs ): # For encoders: ALWAYS strip position_ids and use_cache (they don't support them) # For decoders: Only strip position_ids during training/eval (not inference) if is_encoder : if 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) if 'use_cache' in kwargs : kwargs . pop ( 'use_cache' ) else : # is_decoder is_inference = kwargs . get ( 'use_cache' , False ) or 'past_key_values' in kwargs if not is_inference and 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) return original_forward ( * args , ** kwargs ) import types target_model . forward = types . MethodType ( patched_forward , target_model ) logger . info ( f \"\u2705 Patched { model_name } forward to conditionally ignore position_ids\" ) # CRITICAL: Also patch the BACKBONE forward because get_reward calls it directly! # get_reward does: lm_backbone = getattr(model, model.base_model_prefix); lm_backbone(...) if hasattr ( model , \"base_model_prefix\" ): backbone_name = model . base_model_prefix if hasattr ( model , backbone_name ): backbone = getattr ( model , backbone_name ) if hasattr ( backbone , \"forward\" ) and backbone is not target_model : original_backbone_forward = backbone . forward def patched_backbone_forward ( self , * args , ** kwargs ): # For encoders: ALWAYS strip position_ids and use_cache # For decoders: Only strip position_ids during training/eval if is_encoder : if 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) if 'use_cache' in kwargs : kwargs . pop ( 'use_cache' ) else : # is_decoder is_inference = kwargs . get ( 'use_cache' , False ) or 'past_key_values' in kwargs if not is_inference and 'position_ids' in kwargs : kwargs . pop ( 'position_ids' ) return original_backbone_forward ( * args , ** kwargs ) backbone . forward = types . MethodType ( patched_backbone_forward , backbone ) logger . info ( f \"\u2705 Patched backbone ( { backbone_name } ) forward to conditionally ignore position_ids\" ) def _patch_model_generate_for_compatibility ( self , model , model_name = \"model\" ): \"\"\"Patch model's generate method to remove incompatible kwargs.\"\"\" if model is None : return # Recursive patching for wrappers (PeftModel, etc.) if hasattr ( model , \"base_model\" ) and model . base_model is not model : self . _patch_model_generate_for_compatibility ( model . base_model , f \" { model_name } .base_model\" ) if not hasattr ( model , \"generate\" ): return # Avoid double patching if getattr ( model . generate , \"_is_patched_compatibility\" , False ): return original_generate = model . generate def patched_generate ( self , * args , ** kwargs ): # Strip num_logits_to_keep if present (causes transformers validation error) if 'num_logits_to_keep' in kwargs : kwargs . pop ( 'num_logits_to_keep' ) # DEBUG: Force keys_to_ignore_at_inference to include num_logits_to_keep if hasattr ( self , 'config' ) and hasattr ( self . config , 'keys_to_ignore_at_inference' ): if 'num_logits_to_keep' not in self . config . keys_to_ignore_at_inference : # self.config.keys_to_ignore_at_inference.append('num_logits_to_keep') # Modify in place # Some configs use tuples? if isinstance ( self . config . keys_to_ignore_at_inference , tuple ): self . config . keys_to_ignore_at_inference = list ( self . config . keys_to_ignore_at_inference ) self . config . keys_to_ignore_at_inference . append ( 'num_logits_to_keep' ) return original_generate ( * args , ** kwargs ) patched_generate . _is_patched_compatibility = True import types model . generate = types . MethodType ( patched_generate , model ) logger . info ( f \"\u2705 Patched { model_name } generate to ignore num_logits_to_keep\" ) # CRITICAL: If model has _old_generate (Unsloth), patch it too! # Unsloth's generate might ADD num_logits_to_keep and then call _old_generate if hasattr ( model , \"_old_generate\" ): logger . info ( f \"Found _old_generate on { model_name } . Patching it.\" ) original_old_generate = model . _old_generate def patched_old_generate ( self , * args , ** kwargs ): if 'num_logits_to_keep' in kwargs : # logger.info(f\"Stripped num_logits_to_keep in _old_generate of {model_name}\") kwargs . pop ( 'num_logits_to_keep' ) return original_old_generate ( * args , ** kwargs ) import types model . _old_generate = types . MethodType ( patched_old_generate , model ) logger . info ( f \"\u2705 Patched { model_name } _old_generate to ignore num_logits_to_keep\" ) else : logger . info ( f \"\u274c _old_generate NOT found on { model_name } (Type: { type ( model ) . __name__ } )\" ) # ========================================================================================= # Integration inside setup_model (simplified relevant part) # ========================================================================================= def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for PPO.\"\"\" try : from unsloth import FastLanguageModel # CRITICAL: Clear cache and patch classes BEFORE any model loading from .unsloth_patches import clear_all_unsloth_caches , patch_attention_classes_globally logger . info ( \"Step 1: Clearing Unsloth caches...\" ) clear_all_unsloth_caches () logger . info ( \"Step 2: Patching attention classes at class level...\" ) patch_attention_classes_globally () # Validate model compatibility self . _validate_model_compatibility () # Check if user wants to clear cache (additional clearing if requested) clear_cache = self . _get_config_value ( self . config . model , 'clear_unsloth_cache' , False ) if clear_cache : logger . info ( \"\ud83d\uddd1\ufe0f Additional cache clearing requested (clear_unsloth_cache=True)\" ) self . _clear_unsloth_cache ( force = True ) else : # Smart cache invalidation based on config changes self . _maybe_invalidate_unsloth_cache () # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) # Get precision from config to ensure consistency across all models precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value logger . info ( f \"Using precision: { precision } for all models\" ) logger . info ( f \"Setting up Unsloth PPO model: { model_name } \" ) # Load tokenizer first (before any model loading) from transformers import AutoTokenizer from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE self . tokenizer = AutoTokenizer . from_pretrained ( model_name , padding_side = \"left\" , trust_remote_code = False ) # Tokenizer setup if self . tokenizer . pad_token is None : if self . tokenizer . eos_token : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : self . tokenizer . add_special_tokens ({ \"pad_token\" : \"[PAD]\" }) if self . tokenizer . pad_token_id is None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if self . tokenizer . chat_template is None : logger . info ( \"Applying Unsloth chat template for PPO...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = self . _get_config_value ( self . config . dataset , 'chat_template' , None ) # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to SIMPLE_CHAT_TEMPLATE.\" ) self . tokenizer . chat_template = SIMPLE_CHAT_TEMPLATE logger . info ( f \"Tokenizer ready (vocab= { len ( self . tokenizer ) } )\" ) # Check if separate reward model is specified reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) try : if self . config . model . reward_model_source . source_type == \"custom_trained\" : self . config . model . reward_model_source . training_config . base_model_name = self . _train_and_load_custom_reward_model ( self . config . model . reward_model_source . training_config ) except : pass if reward_model_name : # Check if reward model is already loaded and wrapped if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \"Reward model already loaded: { type ( self . reward_model ) . __name__ } \" ) return # Load specialized reward model (e.g., DeBERTa, RoBERTa, or Llama-based) logger . info ( f \"Loading specialized reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification , AutoTokenizer # Detect if reward model is a decoder (Llama, Mistral, etc.) or encoder (BERT, RoBERTa, etc.) reward_model_type = self . _detect_model_type ( reward_model_name ) # Load reward model with config precision torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 if reward_model_type == 'unsloth' : # Load decoder-based reward model via Unsloth logger . info ( f \"Loading reward model via Unsloth (decoder model): { reward_model_name } \" ) from unsloth import FastLanguageModel self . reward_model , _ = FastLanguageModel . from_pretrained ( model_name = reward_model_name , max_seq_length = max_seq_length , dtype = torch_dtype , load_in_4bit = False , # Disable quantization for reward model ) logger . info ( f \"\u2705 Reward model loaded via Unsloth\" ) # Unsloth loads as CausalLM. We need to add a score head if it's missing. import torch.nn as nn if not hasattr ( self . reward_model , 'score' ) and not hasattr ( self . reward_model , 'classifier' ): logger . info ( \"Adding ad-hoc score head to Unsloth model\" ) hidden_size = self . reward_model . config . hidden_size self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) self . reward_model . score . to ( self . reward_model . device ) . to ( torch_dtype ) logger . info ( \"\u2705 Ad-hoc score head added\" ) else : # Load encoder-based reward model via standard transformers logger . info ( f \"Loading reward model via standard transformers (encoder model): { reward_model_name } \" ) self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , ignore_mismatched_sizes = True , ) logger . info ( f \"\u2705 Reward model loaded via standard transformers\" ) # CRITICAL: Ensure ALL parameters are converted to the correct dtype if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Before conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Convert the entire model to the target dtype self . reward_model = self . reward_model . to ( torch_dtype ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: After .to() conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Also ensure all submodules are converted for name , module in self . reward_model . named_modules (): if hasattr ( module , 'weight' ) and module . weight is not None : old_dtype = module . weight . data . dtype module . weight . data = module . weight . data . to ( torch_dtype ) new_dtype = module . weight . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .weight from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .weight already { new_dtype } \" ) if hasattr ( module , 'bias' ) and module . bias is not None : old_dtype = module . bias . data . dtype module . bias . data = module . bias . data . to ( torch_dtype ) new_dtype = module . bias . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .bias from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .bias already { new_dtype } \" ) # CRITICAL: Check the classification head specifically if DEBUG : if hasattr ( self . reward_model , 'classifier' ): logger . info ( f \"\ud83d\udd0d DEBUG: Classifier weight dtype: { self . reward_model . classifier . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Classifier bias dtype: { self . reward_model . classifier . bias . dtype if self . reward_model . classifier . bias is not None else 'None' } \" ) if hasattr ( self . reward_model , 'score' ): logger . info ( f \"\ud83d\udd0d DEBUG: Score weight dtype: { self . reward_model . score . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Score bias dtype: { self . reward_model . score . bias . dtype if self . reward_model . score . bias is not None else 'None' } \" ) # Final verification final_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Final reward model dtype: { final_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) logger . info ( f \"\u2705 Reward model loaded with { precision } precision\" ) # CRITICAL: Check if classifier is actually in the right dtype # Handle different model architectures for head layer head_layer = None if hasattr ( self . reward_model , 'classifier' ): head_layer = self . reward_model . classifier if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using classifier - weight dtype: { head_layer . weight . dtype } \" ) elif hasattr ( self . reward_model , 'score' ): head_layer = self . reward_model . score if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using score - weight dtype: { head_layer . weight . dtype } \" ) else : raise AttributeError ( f \"Model { type ( self . reward_model ) } has no classifier or score layer\" ) # CRITICAL: Ensure head layer is in the right dtype try : # Use .to() which handles recursion for complex heads (like RoBERTa) head_layer . to ( torch_dtype ) logger . info ( f \"\u2705 Converted head layer to { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Could not convert head layer to { torch_dtype } : { e } \" ) # Load reward tokenizer self . reward_tokenizer = AutoTokenizer . from_pretrained ( reward_model_name ) # Load value model using same model as policy reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , model_name # Use policy model for value model ) if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_value_model_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_value_model_standard ( reward_value_model ) # DEBUG: Check value model dtype if DEBUG and hasattr ( self , 'value_model' ) and self . value_model is not None : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model dtype: { value_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) else : # Original behavior: use same model for reward/value reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , None # Default to None, we will set to model_name if missing ) # Fallback to policy model if not specified if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Reward/value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_reward_value_models_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_reward_value_models_standard ( reward_value_model ) # NOW build policy/ref models (Unsloth already imported at top) logger . info ( \"Building policy/ref models with Unsloth...\" ) # Load Unsloth policy model self . unsloth_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Explicitly set dtype to bfloat16 load_in_4bit = quantization . get ( \"load_in_4bit\" , True ), ) # CRITICAL FIX: Apply LoRA with use_gradient_checkpointing=False logger . info ( \"Applying LoRA WITHOUT gradient checkpointing...\" ) self . policy_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = False , # CHANGED: Set to False directly random_state = 3407 , use_rslora = False , ) # Using class-level patches applied before model loading from .unsloth_patches import verify_attention_patches logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . policy_model ) # FIX: Add num_logits_to_keep to ignored keys to prevent generation error # Apply to all reachable configs to be safe models_to_fix = [ self . policy_model , self . ref_model ] if hasattr ( self . policy_model , \"base_model\" ): models_to_fix . append ( self . policy_model . base_model ) if hasattr ( self . policy_model , \"model\" ): models_to_fix . append ( self . policy_model . model ) for m in models_to_fix : if hasattr ( m , \"config\" ): if not hasattr ( m . config , \"keys_to_ignore_at_inference\" ): m . config . keys_to_ignore_at_inference = [] if \"num_logits_to_keep\" not in m . config . keys_to_ignore_at_inference : m . config . keys_to_ignore_at_inference . append ( \"num_logits_to_keep\" ) logger . info ( f \"\u2705 Added num_logits_to_keep to keys_to_ignore_at_inference for { type ( m ) . __name__ } \" ) logger . info ( \"\u2705 Policy model ready\" ) # DEBUG: Check policy model dtype if DEBUG : try : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model dtype: { policy_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get policy model dtype: { e } \" ) # Reference model (no quantization, no GC) - USE SAME DTYPE AS POLICY logger . info ( \"Loading reference model...\" ) self . ref_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Use bfloat16 to match policy model load_in_4bit = False , ) for p in self . ref_model . parameters (): p . requires_grad = False # Using class-level patches applied before model loading logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . ref_model ) logger . info ( \"\u2705 Reference model ready\" ) # DEBUG: Check ref model dtype if DEBUG : try : ref_dtype = next ( self . ref_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reference model dtype: { ref_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get ref model dtype: { e } \" ) logger . info ( \"\u2705 All models loaded successfully\" ) # Log model architecture logger . info ( \"=\" * 60 ) logger . info ( \"Model Architecture:\" ) logger . info ( f \" Policy: { type ( self . policy_model ) . __name__ } (Unsloth + LoRA)\" ) logger . info ( f \" Reference: { type ( self . ref_model ) . __name__ } (Unsloth frozen)\" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \" Reward: { type ( self . reward_model ) . __name__ } (Standard HF)\" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : logger . info ( f \" Value: { type ( self . value_model ) . __name__ } (Standard HF)\" ) logger . info ( \"=\" * 60 ) # Load reward model if specified self . _load_reward_model_strict () # CRITICAL: Patch models to ignore position_ids (for compatibility with TRL get_reward) # Apply to ALL models including reward model (especially important for encoder models like BERT/RoBERTa/DistilBERT) self . _patch_model_forward_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_forward_for_compatibility ( self . ref_model , \"ref_model\" ) self . _patch_model_forward_for_compatibility ( self . value_model , \"value_model\" ) # CRITICAL FIX: Patch reward model attention layers with apply_qkv from policy model # This is necessary when loading multiple Unsloth models, as the second instance might miss the patch if hasattr ( self , 'reward_model' ) and self . reward_model is not None and hasattr ( self , 'policy_model' ) and self . policy_model is not None : try : policy_attn = None for module in self . policy_model . modules (): if 'Attention' in module . __class__ . __name__ : policy_attn = module break if policy_attn is not None and hasattr ( policy_attn , 'apply_qkv' ): logger . info ( \"Patching reward model attention layers with Unsloth methods from policy model...\" ) count = 0 for module in self . reward_model . modules (): if 'Attention' in module . __class__ . __name__ : patched_module = False # Copy all apply_* methods (apply_qkv, apply_o, etc.) for attr_name in dir ( policy_attn ): if attr_name . startswith ( 'apply_' ): if not hasattr ( module , attr_name ): setattr ( module , attr_name , getattr ( policy_attn , attr_name )) patched_module = True if patched_module : count += 1 if count > 0 : logger . info ( f \"\u2705 Patched { count } attention layers in reward model\" ) else : logger . debug ( \"Policy model has no apply_qkv to copy\" ) except Exception as e : logger . warning ( f \"Failed to patch apply_qkv on reward model: { e } \" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : # For wrapped reward models, patch the underlying model if hasattr ( self . reward_model , '_model' ): self . _patch_model_forward_for_compatibility ( self . reward_model . _model , \"reward_model._model\" ) else : self . _patch_model_forward_for_compatibility ( self . reward_model , \"reward_model\" ) # CRITICAL: Patch generate to ignore num_logits_to_keep self . _patch_model_generate_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_generate_for_compatibility ( self . ref_model , \"ref_model\" ) # Although ref_model usually doesn't generate # GLOBAL: Patch transformers validation as a last resort self . _patch_transformers_validation () except Exception as e : logger . error ( f \"\u274c Failed to setup model: { e } \" ) raise def _load_reward_model_strict ( self ): \"\"\"Load reward model with strict validation, NO fallbacks.\"\"\" if not hasattr ( self . config . model , 'reward_model_source' ): logger . info ( \"No reward_model_source in config, skipping custom reward model\" ) return if not self . config . model . reward_model_source : logger . info ( \"reward_model_source is None, skipping custom reward model\" ) return reward_source = self . config . model . reward_model_source # Validate source before loading from aligntune.rewards.training import RewardModelValidator RewardModelValidator . validate_reward_source ( reward_source ) logger . info ( f \"Loading reward model: source_type= { reward_source . source_type } \" ) try : if reward_source . source_type == \"pretrained_hf\" : self . reward_model = self . _load_pretrained_hf_reward_model ( reward_source . model_name ) elif reward_source . source_type == \"pretrained_local\" : self . reward_model = self . _load_local_reward_model ( reward_source . model_path ) elif reward_source . source_type == \"custom_trained\" : # self.reward_model = self._train_and_load_custom_reward_model(reward_source.training_config) self . reward_model = self . _load_local_reward_model ( reward_source . training_config . base_model_name ) else : raise ValueError ( f \"Invalid source_type: { reward_source . source_type } \" ) # CRITICAL: Wrap with UniversalRewardModelWrapper for TRL compatibility if not isinstance ( self . reward_model , UniversalRewardModelWrapper ): self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) # CRITICAL: Add score method directly to the model for TRL compatibility # TRL's get_reward might bypass the wrapper and call model.score() directly base_model = self . reward_model . _model if not hasattr ( base_model , 'score' ): if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) base_model . score = score_method logger . info ( \"\u2705 Added score method to base reward model\" ) elif hasattr ( base_model , 'score' ): pass # Already has it else : logger . warning ( \"\u26a0\ufe0f Reward model has no classifier or score layer\" ) # Verify wrapper score method exists if not hasattr ( self . reward_model , 'score' ): logger . error ( \"Reward model wrapper missing score method!\" ) if hasattr ( base_model , 'classifier' ): def score_method ( hidden_states ): return base_model . classifier ( hidden_states ) self . reward_model . score = score_method logger . info ( \"Added score method to reward model wrapper\" ) else : raise AttributeError ( \"Reward model must have score() or classifier() method\" ) else : logger . info ( \"\u2705 Reward model wrapper has score method\" ) logger . info ( \"\u2705 Reward model loaded successfully\" ) except Exception as e : logger . error ( f \"\u274c Failed to load reward model: { e } \" ) raise RuntimeError ( f \"Reward model loading failed: { e } \" ) from e def _load_pretrained_hf_reward_model ( self , model_name : str ): \"\"\"Load pre-trained reward model from HuggingFace Hub.\"\"\" from aligntune.rewards.training import RewardModelLoader logger . info ( f \"Loading HF reward model: { model_name } \" ) loader = RewardModelLoader () # Use ignore_mismatched_sizes to handle models trained with different num_labels return loader . load_from_huggingface ( model_name , ignore_mismatched_sizes = True ) def _load_local_reward_model ( self , model_path : str ): \"\"\"Load reward model from local path.\"\"\" from aligntune.rewards.training import RewardModelLoader logger . info ( f \"Loading local reward model: { model_path } \" ) loader = RewardModelLoader () return loader . load_from_local ( model_path ) def _train_and_load_custom_reward_model ( self , training_config ): \"\"\"Train custom reward model from reward functions.\"\"\" from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry logger . info ( \"Training custom reward model from reward functions\" ) logger . info ( f \"Reward functions: { training_config . reward_functions } \" ) logger . info ( f \"Training texts: { len ( training_config . training_texts ) } samples\" ) # Get reward functions from registry reward_funcs = [ RewardRegistry . get_reward_function ( name ) for name in training_config . reward_functions ] # Create trainer trainer = RewardModelTrainer ( base_model_name = training_config . base_model_name , reward_functions = reward_funcs , composite_weights = training_config . reward_weights ) # Generate training data training_data = trainer . generate_training_data ( texts = training_config . training_texts , references = training_config . reference_texts , batch_size = training_config . batch_size ) # Train model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = training_config . output_dir , num_epochs = training_config . num_epochs , learning_rate = training_config . learning_rate , batch_size = training_config . batch_size , gradient_accumulation_steps = training_config . gradient_accumulation_steps ) logger . info ( f \"\u2705 Custom reward model trained: { model_path } \" ) # Load trained model # return self._load_local_reward_model(model_path) return model_path def setup_rewards ( self ) -> None : \"\"\"Setup reward functions based on configuration mode.\"\"\" logger . info ( \"Setting up Unsloth PPO reward functions...\" ) # Determine mode has_pretrained = hasattr ( self , 'reward_model' ) and self . reward_model is not None has_reward_funcs = self . config . rewards and len ( self . config . rewards ) > 0 fine_tune_mode = ( has_pretrained and has_reward_funcs and hasattr ( self . config . model , 'reward_model_source' ) and self . config . model . reward_model_source and getattr ( self . config . model . reward_model_source , 'fine_tune_with_rewards' , False ) ) # Check if reward training is enabled should_train_model = ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ) # Mode 1: Pretrained only if has_pretrained and not fine_tune_mode : logger . info ( \"\u2705 Mode: Pretrained reward model (using as-is)\" ) logger . info ( \" Reward functions will be ignored\" ) self . reward_functions = [] return # Mode 2: Custom trained (handled in _load_reward_model_strict) if not has_pretrained and has_reward_funcs : if should_train_model : logger . info ( \"\u2705 Mode: Custom reward model training\" ) logger . info ( \" Will train new model using reward functions\" ) # Setup reward functions for training self . _setup_reward_functions () return else : # NEW: Use reward functions directly with FunctionBasedRewardModel logger . info ( \"\u2705 Mode: Direct reward function usage\" ) logger . info ( \" Reward functions will be wrapped in FunctionBasedRewardModel\" ) self . _setup_reward_functions () return # Mode 3: Hybrid (fine-tune) if fine_tune_mode : logger . info ( \"\u2705 Mode: Hybrid (fine-tune pretrained with reward functions)\" ) self . _setup_reward_functions () self . _fine_tune_reward_model () return # No rewards configured logger . info ( \"No reward model or functions configured\" ) self . reward_functions = [] def _setup_reward_functions ( self ): \"\"\"Internal method to setup reward functions.\"\"\" self . reward_functions = [] for reward_config in self . config . rewards : reward_type = self . _get_config_value ( reward_config , 'type' ) reward_params = self . _get_config_value ( reward_config , 'params' , {}) reward_weight = self . _get_config_value ( reward_config , 'weight' , 1.0 ) reward_clip = self . _get_config_value ( reward_config , 'clip' , None ) # Get reward function from registry reward_func = RewardRegistry . get_reward_function ( reward_type ) # Apply weight if reward_weight != 1.0 : def weighted_reward ( text , weight = reward_weight , func = reward_func , ** kwargs ): return weight * func ( text , ** kwargs ) reward_func = weighted_reward # Apply clipping if reward_clip is not None : def clipped_reward ( text , clip = reward_clip , func = reward_func , ** kwargs ): return max ( - clip , min ( clip , func ( text , ** kwargs ))) reward_func = clipped_reward self . reward_functions . append ( reward_func ) logger . info ( f \"Configured { len ( self . reward_functions ) } reward functions\" ) def _fine_tune_reward_model ( self ): \"\"\"Fine-tune pretrained reward model with reward functions.\"\"\" logger . info ( \"Fine-tuning pretrained reward model...\" ) # Use reward functions to generate additional training data # Fine-tune the loaded model # This is optional advanced feature logger . warning ( \"\u26a0\ufe0f Fine-tuning not yet implemented, using pretrained as-is\" ) def _compute_rewards ( self , texts : List [ str ], references : Optional [ List [ str ]] = None ) -> List [ float ]: \"\"\"Compute rewards for given texts using configured reward functions.\"\"\" if not self . reward_functions : logger . warning ( \"No reward functions configured, returning zero rewards\" ) return [ 0.0 ] * len ( texts ) rewards = [] for text in texts : total_reward = 0.0 for reward_func in self . reward_functions : try : reward = reward_func ( text , reference = references [ texts . index ( text )] if references else None ) total_reward += reward except Exception as e : logger . warning ( f \"Error computing reward for text: { e } \" ) total_reward += 0.0 rewards . append ( total_reward ) return rewards def _detect_dataset_format ( self , sample ): \"\"\"Detect dataset format.\"\"\" if \"messages\" in sample : return \"chat_format\" elif \"chosen\" in sample and \"rejected\" in sample : return \"hh_rlhf_with_prompt\" if \"prompt\" in sample else \"hh_rlhf_no_prompt\" elif \"instruction\" in sample : return \"alpaca_format\" elif \"text\" in sample : return \"simple_text\" elif \"input\" in sample and \"output\" in sample : return \"input_output\" else : return \"unknown\" def _parse_element_by_format ( self , element , format_type , tokenizer ): \"\"\"Parse dataset element - EXACT SAME AS WORKING CODE.\"\"\" if format_type == \"chat_format\" : input_ids = tokenizer . apply_chat_template ( element [ \"messages\" ][: 1 ], padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"hh_rlhf_with_prompt\" : query_text = element [ \"prompt\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"hh_rlhf_no_prompt\" : chosen_text = element [ \"chosen\" ] . strip () if \" \\n\\n Human:\" in chosen_text and \" \\n\\n Assistant:\" in chosen_text : parts = chosen_text . split ( \" \\n\\n Assistant:\" ) human_part = parts [ 0 ] . replace ( \" \\n\\n Human:\" , \"\" ) . strip () if len ( parts ) > 1 else chosen_text . split ( \" \\n\\n Human:\" )[ - 1 ] . strip () elif \"Human:\" in chosen_text and \"Assistant:\" in chosen_text : parts = chosen_text . split ( \"Assistant:\" ) human_part = parts [ 0 ] . replace ( \"Human:\" , \"\" ) . strip () if len ( parts ) > 1 else chosen_text . split ( \"Human:\" )[ - 1 ] . strip () else : lines = chosen_text . split ( ' \\n ' ) human_part = lines [ 0 ] . strip () if lines else chosen_text [: 100 ] . strip () if len ( human_part ) < 10 : human_part = chosen_text [: 200 ] . strip () input_ids = tokenizer . encode ( human_part , add_special_tokens = True , return_tensors = None ) if len ( input_ids ) < 5 : padding_ids = tokenizer . encode ( \" What do you think about this?\" , add_special_tokens = False ) input_ids . extend ( padding_ids ) elif format_type == \"alpaca_format\" : instruction = element [ \"instruction\" ] . strip () if \"input\" in element and element [ \"input\" ] and element [ \"input\" ] . strip (): query_text = f \" { instruction } \\n\\n Input: { element [ 'input' ] . strip () } \" else : query_text = instruction messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"simple_text\" : query_text = element [ \"text\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) elif format_type == \"input_output\" : query_text = element [ \"input\" ] . strip () messages = [{ \"role\" : \"user\" , \"content\" : query_text }] input_ids = tokenizer . apply_chat_template ( messages , padding = False , add_generation_prompt = True , tokenize = True , ) else : raise ValueError ( f \"Unknown format: { format_type } \" ) # Remove trailing EOS try : eos_id = tokenizer . eos_token_id if eos_id is not None and input_ids and input_ids [ - 1 ] == eos_id : input_ids = input_ids [: - 1 ] except Exception : pass # Ensure minimum length if len ( input_ids ) < 5 : padding_ids = tokenizer . encode ( \" Please elaborate.\" , add_special_tokens = False ) input_ids . extend ( padding_ids ) return input_ids def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched ) # Load dataset - DataManager handles everything dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) tokenized = self . tokenizer ( prompts , padding = False , truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL PPOTrainer.\"\"\" # Set global reference for patched_get_reward global GLOBAL_UNSLOTH_TRAINER_REF GLOBAL_UNSLOTH_TRAINER_REF = self try : from trl import PPOTrainer , PPOConfig logger . info ( \"Setting up PPOTrainer\" ) # Logging configuration with defaults output_dir = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) run_name = getattr ( self . config . logging , 'run_name' , None ) or 'ppo_experiment' report_to = getattr ( self . config . logging , 'loggers' , 'none' ) # Training parameters from config with defaults num_epochs = getattr ( self . config . train , 'epochs' , None ) or 1 batch_size = getattr ( self . config . train , 'per_device_batch_size' , 1 ) grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 32 ) lr = getattr ( self . config . train , 'learning_rate' , 1e-6 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.05 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) cliprange_value = getattr ( self . config . train , 'cliprange_value' , 0.2 ) vf_coef = getattr ( self . config . train , 'vf_coef' , 0.1 ) gamma = getattr ( self . config . train , 'gamma' , 1.0 ) lam = getattr ( self . config . train , 'lam' , 0.95 ) max_grad_norm = getattr ( self . config . train , 'max_grad_norm' , 0.5 ) num_ppo_epochs = getattr ( self . config . train , 'num_ppo_epochs' , None ) or 4 whiten_rewards = getattr ( self . config . train , 'whiten_rewards' , False ) response_length = getattr ( self . config . train , 'response_length' , 53 ) stop_token = getattr ( self . config . train , 'stop_token' , 'eos' ) missing_eos_penalty = getattr ( self . config . train , 'missing_eos_penalty' , 1.0 ) save_steps = getattr ( self . config . train , 'save_steps' , 500 ) save_strategy = getattr ( self . config . train , 'save_strategy' , 'steps' ) save_total_limit = getattr ( self . config . train , 'save_total_limit' , 5 ) eval_strategy = getattr ( self . config . train , 'eval_strategy' , 'steps' ) eval_steps = getattr ( self . config . train , 'eval_steps' , None ) or 100 logging_steps = getattr ( self . config . train , 'logging_steps' , 10 ) gradient_checkpointing = getattr ( self . config . model , 'gradient_checkpointing' , False ) gradient_checkpointing_kwargs = getattr ( self . config . train , 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }) # Seed with default seed = getattr ( self . config . distributed , 'seed' , 42 ) # Precision handling - direct string comparison with default precision_str = getattr ( self . config . model , 'precision' , 'auto' ) if hasattr ( precision_str , 'value' ): # Handle enum precision_str = precision_str . value bf16 = precision_str in [ 'bf16' , 'auto' ] fp16 = precision_str == 'fp16' # Calculate total episodes total_episodes = len ( self . train_dataset ) * num_epochs # Create PPOConfig ppo_config = PPOConfig ( exp_name = run_name , learning_rate = lr , batch_size = batch_size , mini_batch_size = batch_size , gradient_accumulation_steps = grad_accum , total_episodes = total_episodes , num_ppo_epochs = num_ppo_epochs , max_grad_norm = max_grad_norm , seed = seed , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , kl_coef = kl_coef , whiten_rewards = whiten_rewards , gamma = gamma , lam = lam , response_length = response_length , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , local_rollout_forward_batch_size = batch_size , output_dir = output_dir , save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , eval_strategy = eval_strategy , eval_steps = eval_steps , logging_steps = logging_steps , bf16 = bf16 , fp16 = fp16 , remove_unused_columns = False , run_name = run_name , gradient_checkpointing = False , report_to = report_to , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # DEBUG: Final dtype check before creating trainer if DEBUG : logger . info ( \"\ud83d\udd0d DEBUG: Final dtype check before PPOTrainer creation:\" ) try : if hasattr ( self , 'reward_model' ) and self . reward_model is not None : reward_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reward model final dtype: { reward_dtype } \" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model final dtype: { value_dtype } \" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model final dtype: { policy_dtype } \" ) # CRITICAL: Test a forward pass to check hidden states dtype logger . info ( \"\ud83d\udd0d DEBUG: Testing forward pass dtypes...\" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : test_input = torch . tensor ([[ 1 , 2 , 3 , 4 , 5 ]], dtype = torch . long ) with torch . no_grad (): try : outputs = self . policy_model ( test_input ) if hasattr ( outputs , 'hidden_states' ) and outputs . hidden_states : hidden_dtype = outputs . hidden_states [ - 1 ] . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model hidden states dtype: { hidden_dtype } \" ) else : logger . info ( f \"\ud83d\udd0d DEBUG: Policy model outputs type: { type ( outputs ) } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not test policy model forward pass: { e } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get final dtypes: { e } \" ) # Check if we should use function-based reward model if not hasattr ( self , 'reward_model' ) or self . reward_model is None : if hasattr ( self , 'reward_functions' ) and self . reward_functions and len ( self . reward_functions ) > 0 : # Use function-based reward model from aligntune.core.rl.function_based_reward_model import FunctionBasedRewardModel logger . info ( \"=\" * 60 ) logger . info ( \"\u2705 Using function-based reward model with reward functions\" ) logger . info ( f \" { len ( self . reward_functions ) } reward function(s) configured\" ) logger . info ( \"=\" * 60 ) # Get device and dtype from policy model device = next ( self . policy_model . parameters ()) . device dtype = next ( self . policy_model . parameters ()) . dtype self . reward_model = FunctionBasedRewardModel ( reward_functions = self . reward_functions , tokenizer = self . tokenizer , device = str ( device ), dtype = dtype ) self . reward_model = self . reward_model . to ( device ) logger . info ( \"\u2705 FunctionBasedRewardModel created and moved to device\" ) else : raise ValueError ( \"TRL PPOTrainer requires either: \\n \" \" - reward_model: A neural network reward model (pretrained or custom trained) \\n \" \" Examples: 'OpenAssistant/reward-model-deberta-v3-large-v2', 'Skywork/Skywork-Reward-V2-Qwen3-0.6B' \\n \" \" - reward_functions: Rule-based reward functions (will be wrapped in FunctionBasedRewardModel) \\n \" \" Example: [{'type': 'length', 'weight': 1.0}, {'type': 'sentiment', 'weight': 0.5}] \\n \" \" - reward_training.enabled=True: Train a custom reward model from reward functions \\n \" \"Please provide one of these options.\" ) else : # Neural reward model provided logger . info ( \"\u2705 Using neural network reward model\" ) logger . info ( f \" Reward model type: { type ( self . reward_model ) . __name__ } \" ) # Create trainer with strict generation limits self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = None , ) # Patch PolicyAndValueWrapper (keep this) self . _patch_policy_wrapper () logger . info ( \"PPOTrainer created successfully\" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise def _patch_policy_wrapper ( self ): \"\"\"Patch PolicyAndValueWrapper to add missing methods for Unsloth compatibility.\"\"\" try : wrapper = getattr ( self . trainer , 'model' , None ) if wrapper is None : return # Add gradient_checkpointing_disable method if missing if not hasattr ( wrapper , 'gradient_checkpointing_disable' ): def _disable_gc (): \"\"\"Disable gradient checkpointing on underlying models.\"\"\" try : policy = getattr ( wrapper , 'policy_model' , None ) if policy and hasattr ( policy , 'gradient_checkpointing_disable' ): policy . gradient_checkpointing_disable () base = getattr ( wrapper , 'model' , None ) if base and hasattr ( base , 'gradient_checkpointing_disable' ): base . gradient_checkpointing_disable () except Exception : pass setattr ( wrapper , 'gradient_checkpointing_disable' , _disable_gc ) logger . info ( \"Added gradient_checkpointing_disable() to PolicyAndValueWrapper\" ) # Add gradient_checkpointing_enable method if missing if not hasattr ( wrapper , 'gradient_checkpointing_enable' ): def _enable_gc ( gradient_checkpointing_kwargs = None ): \"\"\"Enable gradient checkpointing on underlying models.\"\"\" try : policy = getattr ( wrapper , 'policy_model' , None ) if policy and hasattr ( policy , 'gradient_checkpointing_enable' ): if gradient_checkpointing_kwargs : policy . gradient_checkpointing_enable ( gradient_checkpointing_kwargs = gradient_checkpointing_kwargs ) else : policy . gradient_checkpointing_enable () base = getattr ( wrapper , 'model' , None ) if base and hasattr ( base , 'gradient_checkpointing_enable' ): if gradient_checkpointing_kwargs : base . gradient_checkpointing_enable ( gradient_checkpointing_kwargs = gradient_checkpointing_kwargs ) else : base . gradient_checkpointing_enable () except Exception : pass setattr ( wrapper , 'gradient_checkpointing_enable' , _enable_gc ) logger . info ( \"Added gradient_checkpointing_enable() to PolicyAndValueWrapper\" ) # Add generate method if missing (for Unsloth) if not hasattr ( wrapper , 'generate' ): policy = getattr ( wrapper , 'policy_model' , None ) base = getattr ( wrapper , 'model' , None ) if policy and hasattr ( policy , 'generate' ): target = policy . generate elif base and hasattr ( base , 'generate' ): target = base . generate else : return def _delegate_generate ( * args , ** kwargs ): return target ( * args , ** kwargs ) setattr ( wrapper , 'generate' , _delegate_generate ) logger . info ( \"Added generate() to PolicyAndValueWrapper\" ) except Exception as e : logger . warning ( f \"Could not patch PolicyAndValueWrapper: { e } \" ) def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Not used - TRL handles training loop.\"\"\" return { \"loss\" : 0.0 } def create_data_loader ( self ): \"\"\"Not used - TRL handles data loading.\"\"\" return None def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" try : logger . info ( \"Starting Unsloth PPO training\" ) start_time = time . time () # Setup everything self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Train logger . info ( \"Running PPO training...\" ) training_result = self . trainer . train () try : generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , getattr ( self , 'reward_functions' , None ), stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Save output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) logger . info ( f \"Saving to: { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # TRL PPOTrainer.train() may return None (or an object without global_step). # Use training_history length as a robust fallback so examples don't report 0 steps. fallback_steps = 0 try : if hasattr ( self , \"training_history\" ) and self . training_history : fallback_steps = len ( self . training_history ) except Exception : fallback_steps = 0 results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else fallback_steps , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : 0 , \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Training completed in { training_time : .2f } s\" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise def _train_custom_reward_model ( self ) -> str : \"\"\"Train a custom reward model using the reward_training config.\"\"\" from aligntune.rewards import RewardModelTrainer from aligntune.rewards.factory import create_reward_functions reward_config = self . config . reward_training # Create reward functions from config if reward_config . reward_functions : reward_functions = create_reward_functions ( reward_config . reward_functions ) else : # Default reward functions if none specified reward_functions = create_reward_functions ([ { 'type' : 'length' , 'weight' : 0.25 , 'params' : { 'min_length' : 10 , 'max_length' : 200 }}, { 'type' : 'sentiment' , 'weight' : 0.25 , 'params' : { 'positive_weight' : 1.0 }}, { 'type' : 'coherence' , 'weight' : 0.25 , 'params' : { 'threshold' : 0.7 }}, { 'type' : 'safety' , 'weight' : 0.25 , 'params' : { 'strict' : True }} ]) # Create composite weights composite_weights = [ 1.0 ] * len ( reward_functions ) # Determine base model for reward training base_model_name = reward_config . base_model or self . config . model . name_or_path # Create reward model trainer trainer = RewardModelTrainer ( base_model_name = base_model_name , reward_functions = reward_functions , composite_weights = composite_weights ) # Load training data training_texts = trainer . load_training_data ( texts = reward_config . training_texts , dataset_name = reward_config . dataset_name , dataset_path = reward_config . dataset_path , dataset_split = reward_config . dataset_split , text_column = reward_config . text_column , max_samples = reward_config . max_samples ) # Generate training dataset training_data = trainer . generate_training_data ( texts = training_texts , batch_size = reward_config . batch_size ) # Train the reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = reward_config . output_dir , num_epochs = reward_config . num_epochs , learning_rate = reward_config . learning_rate , batch_size = reward_config . batch_size , save_steps = reward_config . save_steps , logging_steps = reward_config . logging_steps ) logger . info ( f \"\u2705 Custom reward model training completed: { model_path } \" ) return model_path def evaluate ( self ) -> Dict [ str , Any ]: \"\"\"Evaluate model with fallback for missing memory tracker.\"\"\" try : if not self . eval_dataset : logger . info ( \"No evaluation dataset provided\" ) return {} logger . info ( \"Evaluating...\" ) # Initialize memory tracker if missing if not hasattr ( self . trainer , '_memory_tracker' ): from transformers.trainer_utils import TrainerMemoryTracker self . trainer . _memory_tracker = TrainerMemoryTracker ( skip_memory_metrics = True ) logger . info ( \"Initialized missing memory tracker\" ) eval_results = self . trainer . evaluate () return eval_results except AttributeError as e : if '_memory_tracker' in str ( e ): logger . warning ( f \"Memory tracker error during evaluation: { e } \" ) logger . warning ( \"Skipping evaluation - this is a known TRL/Transformers compatibility issue\" ) return { \"eval_skipped\" : True , \"reason\" : \"memory_tracker_missing\" } else : logger . error ( f \"Evaluation failed: { e } \" ) raise except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . policy_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load model.\"\"\" try : from unsloth import FastLanguageModel logger . info ( f \"Loading from: { path } \" ) max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , 2048 ) self . policy_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"Loaded successfully\" ) except Exception as e : logger . error ( f \"Load failed: { e } \" ) raise","title":"UnslothPPOTrainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.create_data_loader","text":"Not used - TRL handles data loading. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2147 2148 2149 def create_data_loader ( self ): \"\"\"Not used - TRL handles data loading.\"\"\" return None","title":"create_data_loader"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.evaluate","text":"Evaluate model with fallback for missing memory tracker. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 def evaluate ( self ) -> Dict [ str , Any ]: \"\"\"Evaluate model with fallback for missing memory tracker.\"\"\" try : if not self . eval_dataset : logger . info ( \"No evaluation dataset provided\" ) return {} logger . info ( \"Evaluating...\" ) # Initialize memory tracker if missing if not hasattr ( self . trainer , '_memory_tracker' ): from transformers.trainer_utils import TrainerMemoryTracker self . trainer . _memory_tracker = TrainerMemoryTracker ( skip_memory_metrics = True ) logger . info ( \"Initialized missing memory tracker\" ) eval_results = self . trainer . evaluate () return eval_results except AttributeError as e : if '_memory_tracker' in str ( e ): logger . warning ( f \"Memory tracker error during evaluation: { e } \" ) logger . warning ( \"Skipping evaluation - this is a known TRL/Transformers compatibility issue\" ) return { \"eval_skipped\" : True , \"reason\" : \"memory_tracker_missing\" } else : logger . error ( f \"Evaluation failed: { e } \" ) raise except Exception as e : logger . error ( f \"Evaluation failed: { e } \" ) raise","title":"evaluate"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.is_available","text":"Check if Unsloth and TRL are available and properly configured. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available and properly configured.\"\"\" try : # Check Unsloth installation import unsloth from unsloth import FastLanguageModel # Check if Unsloth is properly configured import os if os . environ . get ( \"UNSLOTH_IS_PRESENT\" ) != \"1\" : logger . warning ( \"Unsloth not properly initialized\" ) return False # Check TRL availability from trl import PPOTrainer , PPOConfig from transformers import AutoModelForSequenceClassification return True except ImportError as e : logger . warning ( f \"Unsloth PPO backend not available: { e } \" ) return False except Exception as e : logger . warning ( f \"Unsloth PPO backend configuration error: { e } \" ) return False","title":"is_available"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.load_model","text":"Load model. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 def load_model ( self , path : str ) -> None : \"\"\"Load model.\"\"\" try : from unsloth import FastLanguageModel logger . info ( f \"Loading from: { path } \" ) max_seq_length = self . _get_config_value ( self . config . model , 'max_seq_length' , 2048 ) self . policy_model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"Loaded successfully\" ) except Exception as e : logger . error ( f \"Load failed: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.save_model","text":"Save model. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save model.\"\"\" try : default_path = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) save_path = path or default_path logger . info ( f \"Saving to: { save_path } \" ) self . policy_model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) config_path = Path ( save_path ) / \"training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( \"Saved successfully\" ) return save_path except Exception as e : logger . error ( f \"Save failed: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.setup_data","text":"Setup datasets for PPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 def setup_data ( self ) -> None : \"\"\"Setup datasets for PPO training using unified DataManager.\"\"\" logger . info ( \"Setting up PPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ) and self . config . dataset is not None : dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] if len ( self . config . datasets ) > 1 : logger . warning ( f \"Multiple datasets provided, using first one\" ) else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = 'train' ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( dataset_config , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) percent = self . _get_config_value ( dataset_config , 'percent' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for PPO task from aligntune.data.manager import DataManager manager = DataManager ( task_type = \"sft\" , system_prompt = system_prompt , tokenizer = self . tokenizer , enable_thinking = enable_thinking , column_mapping = column_mapping , processing_fn = processing_fn , max_samples = max_samples , processing_batched = processing_batched ) # Load dataset - DataManager handles everything dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , ) max_eval_samples = self . _get_config_value ( dataset_config , 'max_eval_samples' , default = None ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Tokenize dataset for PPO max_prompt_length = self . _get_config_value ( self . config . train , 'max_prompt_length' , default = 512 ) def tokenize_function ( examples ): \"\"\"Tokenize prompts for PPO training.\"\"\" prompts = examples . get ( \"prompt\" , examples . get ( \"query\" , [])) tokenized = self . tokenizer ( prompts , padding = False , truncation = True , max_length = max_prompt_length , ) input_ids = tokenized [ \"input_ids\" ] lengths = [ len ( ids ) for ids in input_ids ] return { \"input_ids\" : input_ids , \"lengths\" : lengths } # Tokenize datasets self . train_dataset = self . train_dataset . map ( tokenize_function , batched = True , remove_columns = self . train_dataset . column_names , desc = \"Tokenizing train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . map ( tokenize_function , batched = True , remove_columns = self . eval_dataset . column_names , desc = \"Tokenizing eval dataset\" , ) # Filter by length logger . info ( f \"Filtering sequences longer than { max_prompt_length } tokens...\" ) self . train_dataset = self . train_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering train dataset\" , ) if self . eval_dataset : self . eval_dataset = self . eval_dataset . filter ( lambda x : x [ \"lengths\" ] <= max_prompt_length , desc = \"Filtering eval dataset\" , ) logger . info ( f \"Final dataset sizes - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) if self . eval_dataset else 0 } \" ) # Verify dataset structure assert self . train_dataset [ 0 ][ \"input_ids\" ][ - 1 ] != self . tokenizer . eos_token_id , \\ \"The last token should not be an EOS token\" # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] decoded = self . tokenizer . decode ( sample [ \"input_ids\" ], skip_special_tokens = False ) logger . info ( f \"Sample tokenized prompt (first 100 chars): { decoded [: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" )","title":"setup_data"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.setup_model","text":"Setup Unsloth-optimized model and tokenizer for PPO. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for PPO.\"\"\" try : from unsloth import FastLanguageModel # CRITICAL: Clear cache and patch classes BEFORE any model loading from .unsloth_patches import clear_all_unsloth_caches , patch_attention_classes_globally logger . info ( \"Step 1: Clearing Unsloth caches...\" ) clear_all_unsloth_caches () logger . info ( \"Step 2: Patching attention classes at class level...\" ) patch_attention_classes_globally () # Validate model compatibility self . _validate_model_compatibility () # Check if user wants to clear cache (additional clearing if requested) clear_cache = self . _get_config_value ( self . config . model , 'clear_unsloth_cache' , False ) if clear_cache : logger . info ( \"\ud83d\uddd1\ufe0f Additional cache clearing requested (clear_unsloth_cache=True)\" ) self . _clear_unsloth_cache ( force = True ) else : # Smart cache invalidation based on config changes self . _maybe_invalidate_unsloth_cache () # Get config values model_name = self . _get_config_value ( self . config . model , \"name_or_path\" ) max_seq_length = self . _get_config_value ( self . config . model , \"max_seq_length\" , 512 ) quantization = self . _get_config_value ( self . config . model , \"quantization\" , {}) # Get precision from config to ensure consistency across all models precision = self . _get_config_value ( self . config . model , \"precision\" , \"bfloat16\" ) if hasattr ( precision , 'value' ): precision = precision . value logger . info ( f \"Using precision: { precision } for all models\" ) logger . info ( f \"Setting up Unsloth PPO model: { model_name } \" ) # Load tokenizer first (before any model loading) from transformers import AutoTokenizer from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE self . tokenizer = AutoTokenizer . from_pretrained ( model_name , padding_side = \"left\" , trust_remote_code = False ) # Tokenizer setup if self . tokenizer . pad_token is None : if self . tokenizer . eos_token : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : self . tokenizer . add_special_tokens ({ \"pad_token\" : \"[PAD]\" }) if self . tokenizer . pad_token_id is None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if self . tokenizer . chat_template is None : logger . info ( \"Applying Unsloth chat template for PPO...\" ) from unsloth import FastLanguageModel # Get chat template from config if available chat_template = self . _get_config_value ( self . config . dataset , 'chat_template' , None ) # Default mapping mapping = { \"role\" : \"role\" , \"content\" : \"content\" , \"user\" : \"user\" , \"assistant\" : \"assistant\" } try : self . tokenizer = FastLanguageModel . get_chat_template ( self . tokenizer , chat_template = chat_template if chat_template else \"llama-3\" , # Default to llama-3 mapping = mapping , ) logger . info ( f \"Applied chat template: { chat_template if chat_template else 'llama-3 (default)' } \" ) except Exception as e : logger . warning ( f \"Failed to apply Unsloth chat template: { e } . Falling back to SIMPLE_CHAT_TEMPLATE.\" ) self . tokenizer . chat_template = SIMPLE_CHAT_TEMPLATE logger . info ( f \"Tokenizer ready (vocab= { len ( self . tokenizer ) } )\" ) # Check if separate reward model is specified reward_model_name = self . _get_config_value ( self . config . model , \"reward_model_name\" , None ) try : if self . config . model . reward_model_source . source_type == \"custom_trained\" : self . config . model . reward_model_source . training_config . base_model_name = self . _train_and_load_custom_reward_model ( self . config . model . reward_model_source . training_config ) except : pass if reward_model_name : # Check if reward model is already loaded and wrapped if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \"Reward model already loaded: { type ( self . reward_model ) . __name__ } \" ) return # Load specialized reward model (e.g., DeBERTa, RoBERTa, or Llama-based) logger . info ( f \"Loading specialized reward model: { reward_model_name } \" ) from transformers import AutoModelForSequenceClassification , AutoTokenizer # Detect if reward model is a decoder (Llama, Mistral, etc.) or encoder (BERT, RoBERTa, etc.) reward_model_type = self . _detect_model_type ( reward_model_name ) # Load reward model with config precision torch_dtype = getattr ( torch , precision ) if precision in [ 'float16' , 'bfloat16' , 'float32' ] else torch . bfloat16 if reward_model_type == 'unsloth' : # Load decoder-based reward model via Unsloth logger . info ( f \"Loading reward model via Unsloth (decoder model): { reward_model_name } \" ) from unsloth import FastLanguageModel self . reward_model , _ = FastLanguageModel . from_pretrained ( model_name = reward_model_name , max_seq_length = max_seq_length , dtype = torch_dtype , load_in_4bit = False , # Disable quantization for reward model ) logger . info ( f \"\u2705 Reward model loaded via Unsloth\" ) # Unsloth loads as CausalLM. We need to add a score head if it's missing. import torch.nn as nn if not hasattr ( self . reward_model , 'score' ) and not hasattr ( self . reward_model , 'classifier' ): logger . info ( \"Adding ad-hoc score head to Unsloth model\" ) hidden_size = self . reward_model . config . hidden_size self . reward_model . score = nn . Linear ( hidden_size , 1 , bias = False ) self . reward_model . score . to ( self . reward_model . device ) . to ( torch_dtype ) logger . info ( \"\u2705 Ad-hoc score head added\" ) else : # Load encoder-based reward model via standard transformers logger . info ( f \"Loading reward model via standard transformers (encoder model): { reward_model_name } \" ) self . reward_model = AutoModelForSequenceClassification . from_pretrained ( reward_model_name , num_labels = 1 , torch_dtype = torch_dtype , device_map = \"auto\" , ignore_mismatched_sizes = True , ) logger . info ( f \"\u2705 Reward model loaded via standard transformers\" ) # CRITICAL: Ensure ALL parameters are converted to the correct dtype if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Before conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Convert the entire model to the target dtype self . reward_model = self . reward_model . to ( torch_dtype ) # Wrap with UniversalRewardModelWrapper for compatibility self . reward_model = UniversalRewardModelWrapper ( self . reward_model ) if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: After .to() conversion - reward model dtype: { next ( self . reward_model . parameters ()) . dtype } \" ) # Also ensure all submodules are converted for name , module in self . reward_model . named_modules (): if hasattr ( module , 'weight' ) and module . weight is not None : old_dtype = module . weight . data . dtype module . weight . data = module . weight . data . to ( torch_dtype ) new_dtype = module . weight . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .weight from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .weight already { new_dtype } \" ) if hasattr ( module , 'bias' ) and module . bias is not None : old_dtype = module . bias . data . dtype module . bias . data = module . bias . data . to ( torch_dtype ) new_dtype = module . bias . data . dtype if DEBUG and old_dtype != new_dtype : logger . info ( f \"\ud83d\udd0d DEBUG: Converted { name } .bias from { old_dtype } to { new_dtype } \" ) elif DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: { name } .bias already { new_dtype } \" ) # CRITICAL: Check the classification head specifically if DEBUG : if hasattr ( self . reward_model , 'classifier' ): logger . info ( f \"\ud83d\udd0d DEBUG: Classifier weight dtype: { self . reward_model . classifier . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Classifier bias dtype: { self . reward_model . classifier . bias . dtype if self . reward_model . classifier . bias is not None else 'None' } \" ) if hasattr ( self . reward_model , 'score' ): logger . info ( f \"\ud83d\udd0d DEBUG: Score weight dtype: { self . reward_model . score . weight . dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Score bias dtype: { self . reward_model . score . bias . dtype if self . reward_model . score . bias is not None else 'None' } \" ) # Final verification final_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Final reward model dtype: { final_dtype } \" ) logger . info ( f \"\ud83d\udd0d DEBUG: Target dtype was: { torch_dtype } \" ) logger . info ( f \"\u2705 Reward model loaded with { precision } precision\" ) # CRITICAL: Check if classifier is actually in the right dtype # Handle different model architectures for head layer head_layer = None if hasattr ( self . reward_model , 'classifier' ): head_layer = self . reward_model . classifier if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using classifier - weight dtype: { head_layer . weight . dtype } \" ) elif hasattr ( self . reward_model , 'score' ): head_layer = self . reward_model . score if DEBUG : logger . info ( f \"\ud83d\udd0d DEBUG: Using score - weight dtype: { head_layer . weight . dtype } \" ) else : raise AttributeError ( f \"Model { type ( self . reward_model ) } has no classifier or score layer\" ) # CRITICAL: Ensure head layer is in the right dtype try : # Use .to() which handles recursion for complex heads (like RoBERTa) head_layer . to ( torch_dtype ) logger . info ( f \"\u2705 Converted head layer to { torch_dtype } \" ) except Exception as e : logger . warning ( f \"\u26a0\ufe0f Could not convert head layer to { torch_dtype } : { e } \" ) # Load reward tokenizer self . reward_tokenizer = AutoTokenizer . from_pretrained ( reward_model_name ) # Load value model using same model as policy reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , model_name # Use policy model for value model ) if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_value_model_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_value_model_standard ( reward_value_model ) # DEBUG: Check value model dtype if DEBUG and hasattr ( self , 'value_model' ) and self . value_model is not None : try : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model dtype: { value_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get value model dtype: { e } \" ) else : # Original behavior: use same model for reward/value reward_value_model = self . _get_config_value ( self . config . model , \"reward_value_model\" , None # Default to None, we will set to model_name if missing ) # Fallback to policy model if not specified if reward_value_model is None : reward_value_model = model_name logger . info ( f \"Reward/value model: { reward_value_model } \" ) model_type = self . _detect_model_type ( reward_value_model ) if model_type == 'unsloth' : self . _load_reward_value_models_unsloth ( reward_value_model , max_seq_length , quantization ) else : self . _load_reward_value_models_standard ( reward_value_model ) # NOW build policy/ref models (Unsloth already imported at top) logger . info ( \"Building policy/ref models with Unsloth...\" ) # Load Unsloth policy model self . unsloth_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Explicitly set dtype to bfloat16 load_in_4bit = quantization . get ( \"load_in_4bit\" , True ), ) # CRITICAL FIX: Apply LoRA with use_gradient_checkpointing=False logger . info ( \"Applying LoRA WITHOUT gradient checkpointing...\" ) self . policy_model = FastLanguageModel . get_peft_model ( self . unsloth_model , r = 16 , target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], lora_alpha = 16 , lora_dropout = 0 , bias = \"none\" , use_gradient_checkpointing = False , # CHANGED: Set to False directly random_state = 3407 , use_rslora = False , ) # Using class-level patches applied before model loading from .unsloth_patches import verify_attention_patches logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . policy_model ) # FIX: Add num_logits_to_keep to ignored keys to prevent generation error # Apply to all reachable configs to be safe models_to_fix = [ self . policy_model , self . ref_model ] if hasattr ( self . policy_model , \"base_model\" ): models_to_fix . append ( self . policy_model . base_model ) if hasattr ( self . policy_model , \"model\" ): models_to_fix . append ( self . policy_model . model ) for m in models_to_fix : if hasattr ( m , \"config\" ): if not hasattr ( m . config , \"keys_to_ignore_at_inference\" ): m . config . keys_to_ignore_at_inference = [] if \"num_logits_to_keep\" not in m . config . keys_to_ignore_at_inference : m . config . keys_to_ignore_at_inference . append ( \"num_logits_to_keep\" ) logger . info ( f \"\u2705 Added num_logits_to_keep to keys_to_ignore_at_inference for { type ( m ) . __name__ } \" ) logger . info ( \"\u2705 Policy model ready\" ) # DEBUG: Check policy model dtype if DEBUG : try : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model dtype: { policy_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get policy model dtype: { e } \" ) # Reference model (no quantization, no GC) - USE SAME DTYPE AS POLICY logger . info ( \"Loading reference model...\" ) self . ref_model , _ = FastLanguageModel . from_pretrained ( model_name = model_name , max_seq_length = max_seq_length , dtype = torch . bfloat16 , # FIXED: Use bfloat16 to match policy model load_in_4bit = False , ) for p in self . ref_model . parameters (): p . requires_grad = False # Using class-level patches applied before model loading logger . info ( \"Using class-level patches applied before model loading\" ) verify_attention_patches ( self . ref_model ) logger . info ( \"\u2705 Reference model ready\" ) # DEBUG: Check ref model dtype if DEBUG : try : ref_dtype = next ( self . ref_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reference model dtype: { ref_dtype } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get ref model dtype: { e } \" ) logger . info ( \"\u2705 All models loaded successfully\" ) # Log model architecture logger . info ( \"=\" * 60 ) logger . info ( \"Model Architecture:\" ) logger . info ( f \" Policy: { type ( self . policy_model ) . __name__ } (Unsloth + LoRA)\" ) logger . info ( f \" Reference: { type ( self . ref_model ) . __name__ } (Unsloth frozen)\" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : logger . info ( f \" Reward: { type ( self . reward_model ) . __name__ } (Standard HF)\" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : logger . info ( f \" Value: { type ( self . value_model ) . __name__ } (Standard HF)\" ) logger . info ( \"=\" * 60 ) # Load reward model if specified self . _load_reward_model_strict () # CRITICAL: Patch models to ignore position_ids (for compatibility with TRL get_reward) # Apply to ALL models including reward model (especially important for encoder models like BERT/RoBERTa/DistilBERT) self . _patch_model_forward_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_forward_for_compatibility ( self . ref_model , \"ref_model\" ) self . _patch_model_forward_for_compatibility ( self . value_model , \"value_model\" ) # CRITICAL FIX: Patch reward model attention layers with apply_qkv from policy model # This is necessary when loading multiple Unsloth models, as the second instance might miss the patch if hasattr ( self , 'reward_model' ) and self . reward_model is not None and hasattr ( self , 'policy_model' ) and self . policy_model is not None : try : policy_attn = None for module in self . policy_model . modules (): if 'Attention' in module . __class__ . __name__ : policy_attn = module break if policy_attn is not None and hasattr ( policy_attn , 'apply_qkv' ): logger . info ( \"Patching reward model attention layers with Unsloth methods from policy model...\" ) count = 0 for module in self . reward_model . modules (): if 'Attention' in module . __class__ . __name__ : patched_module = False # Copy all apply_* methods (apply_qkv, apply_o, etc.) for attr_name in dir ( policy_attn ): if attr_name . startswith ( 'apply_' ): if not hasattr ( module , attr_name ): setattr ( module , attr_name , getattr ( policy_attn , attr_name )) patched_module = True if patched_module : count += 1 if count > 0 : logger . info ( f \"\u2705 Patched { count } attention layers in reward model\" ) else : logger . debug ( \"Policy model has no apply_qkv to copy\" ) except Exception as e : logger . warning ( f \"Failed to patch apply_qkv on reward model: { e } \" ) if hasattr ( self , 'reward_model' ) and self . reward_model is not None : # For wrapped reward models, patch the underlying model if hasattr ( self . reward_model , '_model' ): self . _patch_model_forward_for_compatibility ( self . reward_model . _model , \"reward_model._model\" ) else : self . _patch_model_forward_for_compatibility ( self . reward_model , \"reward_model\" ) # CRITICAL: Patch generate to ignore num_logits_to_keep self . _patch_model_generate_for_compatibility ( self . policy_model , \"policy_model\" ) self . _patch_model_generate_for_compatibility ( self . ref_model , \"ref_model\" ) # Although ref_model usually doesn't generate # GLOBAL: Patch transformers validation as a last resort self . _patch_transformers_validation () except Exception as e : logger . error ( f \"\u274c Failed to setup model: { e } \" ) raise","title":"setup_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.setup_rewards","text":"Setup reward functions based on configuration mode. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions based on configuration mode.\"\"\" logger . info ( \"Setting up Unsloth PPO reward functions...\" ) # Determine mode has_pretrained = hasattr ( self , 'reward_model' ) and self . reward_model is not None has_reward_funcs = self . config . rewards and len ( self . config . rewards ) > 0 fine_tune_mode = ( has_pretrained and has_reward_funcs and hasattr ( self . config . model , 'reward_model_source' ) and self . config . model . reward_model_source and getattr ( self . config . model . reward_model_source , 'fine_tune_with_rewards' , False ) ) # Check if reward training is enabled should_train_model = ( hasattr ( self . config , 'reward_training' ) and self . config . reward_training and self . config . reward_training . enabled ) # Mode 1: Pretrained only if has_pretrained and not fine_tune_mode : logger . info ( \"\u2705 Mode: Pretrained reward model (using as-is)\" ) logger . info ( \" Reward functions will be ignored\" ) self . reward_functions = [] return # Mode 2: Custom trained (handled in _load_reward_model_strict) if not has_pretrained and has_reward_funcs : if should_train_model : logger . info ( \"\u2705 Mode: Custom reward model training\" ) logger . info ( \" Will train new model using reward functions\" ) # Setup reward functions for training self . _setup_reward_functions () return else : # NEW: Use reward functions directly with FunctionBasedRewardModel logger . info ( \"\u2705 Mode: Direct reward function usage\" ) logger . info ( \" Reward functions will be wrapped in FunctionBasedRewardModel\" ) self . _setup_reward_functions () return # Mode 3: Hybrid (fine-tune) if fine_tune_mode : logger . info ( \"\u2705 Mode: Hybrid (fine-tune pretrained with reward functions)\" ) self . _setup_reward_functions () self . _fine_tune_reward_model () return # No rewards configured logger . info ( \"No reward model or functions configured\" ) self . reward_functions = []","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.setup_trainer","text":"Setup TRL PPOTrainer. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 def setup_trainer ( self ) -> None : \"\"\"Setup TRL PPOTrainer.\"\"\" # Set global reference for patched_get_reward global GLOBAL_UNSLOTH_TRAINER_REF GLOBAL_UNSLOTH_TRAINER_REF = self try : from trl import PPOTrainer , PPOConfig logger . info ( \"Setting up PPOTrainer\" ) # Logging configuration with defaults output_dir = getattr ( self . config . logging , 'output_dir' , './output/ppo' ) run_name = getattr ( self . config . logging , 'run_name' , None ) or 'ppo_experiment' report_to = getattr ( self . config . logging , 'loggers' , 'none' ) # Training parameters from config with defaults num_epochs = getattr ( self . config . train , 'epochs' , None ) or 1 batch_size = getattr ( self . config . train , 'per_device_batch_size' , 1 ) grad_accum = getattr ( self . config . train , 'gradient_accumulation_steps' , 32 ) lr = getattr ( self . config . train , 'learning_rate' , 1e-6 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.05 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) cliprange_value = getattr ( self . config . train , 'cliprange_value' , 0.2 ) vf_coef = getattr ( self . config . train , 'vf_coef' , 0.1 ) gamma = getattr ( self . config . train , 'gamma' , 1.0 ) lam = getattr ( self . config . train , 'lam' , 0.95 ) max_grad_norm = getattr ( self . config . train , 'max_grad_norm' , 0.5 ) num_ppo_epochs = getattr ( self . config . train , 'num_ppo_epochs' , None ) or 4 whiten_rewards = getattr ( self . config . train , 'whiten_rewards' , False ) response_length = getattr ( self . config . train , 'response_length' , 53 ) stop_token = getattr ( self . config . train , 'stop_token' , 'eos' ) missing_eos_penalty = getattr ( self . config . train , 'missing_eos_penalty' , 1.0 ) save_steps = getattr ( self . config . train , 'save_steps' , 500 ) save_strategy = getattr ( self . config . train , 'save_strategy' , 'steps' ) save_total_limit = getattr ( self . config . train , 'save_total_limit' , 5 ) eval_strategy = getattr ( self . config . train , 'eval_strategy' , 'steps' ) eval_steps = getattr ( self . config . train , 'eval_steps' , None ) or 100 logging_steps = getattr ( self . config . train , 'logging_steps' , 10 ) gradient_checkpointing = getattr ( self . config . model , 'gradient_checkpointing' , False ) gradient_checkpointing_kwargs = getattr ( self . config . train , 'gradient_checkpointing_kwargs' , { \"use_reentrant\" : False }) # Seed with default seed = getattr ( self . config . distributed , 'seed' , 42 ) # Precision handling - direct string comparison with default precision_str = getattr ( self . config . model , 'precision' , 'auto' ) if hasattr ( precision_str , 'value' ): # Handle enum precision_str = precision_str . value bf16 = precision_str in [ 'bf16' , 'auto' ] fp16 = precision_str == 'fp16' # Calculate total episodes total_episodes = len ( self . train_dataset ) * num_epochs # Create PPOConfig ppo_config = PPOConfig ( exp_name = run_name , learning_rate = lr , batch_size = batch_size , mini_batch_size = batch_size , gradient_accumulation_steps = grad_accum , total_episodes = total_episodes , num_ppo_epochs = num_ppo_epochs , max_grad_norm = max_grad_norm , seed = seed , cliprange = cliprange , cliprange_value = cliprange_value , vf_coef = vf_coef , kl_coef = kl_coef , whiten_rewards = whiten_rewards , gamma = gamma , lam = lam , response_length = response_length , stop_token = stop_token , missing_eos_penalty = missing_eos_penalty , local_rollout_forward_batch_size = batch_size , output_dir = output_dir , save_strategy = save_strategy , save_steps = save_steps , save_total_limit = save_total_limit , eval_strategy = eval_strategy , eval_steps = eval_steps , logging_steps = logging_steps , bf16 = bf16 , fp16 = fp16 , remove_unused_columns = False , run_name = run_name , gradient_checkpointing = False , report_to = report_to , ) missing = extract_extra_and_missing_params ( backend_config = ppo_config , config = self . config , algorithm = 'ppo' ) for key , value in missing . items (): setattr ( ppo_config , key , value ) # DEBUG: Final dtype check before creating trainer if DEBUG : logger . info ( \"\ud83d\udd0d DEBUG: Final dtype check before PPOTrainer creation:\" ) try : if hasattr ( self , 'reward_model' ) and self . reward_model is not None : reward_dtype = next ( self . reward_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Reward model final dtype: { reward_dtype } \" ) if hasattr ( self , 'value_model' ) and self . value_model is not None : value_dtype = next ( self . value_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Value model final dtype: { value_dtype } \" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : policy_dtype = next ( self . policy_model . parameters ()) . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model final dtype: { policy_dtype } \" ) # CRITICAL: Test a forward pass to check hidden states dtype logger . info ( \"\ud83d\udd0d DEBUG: Testing forward pass dtypes...\" ) if hasattr ( self , 'policy_model' ) and self . policy_model is not None : test_input = torch . tensor ([[ 1 , 2 , 3 , 4 , 5 ]], dtype = torch . long ) with torch . no_grad (): try : outputs = self . policy_model ( test_input ) if hasattr ( outputs , 'hidden_states' ) and outputs . hidden_states : hidden_dtype = outputs . hidden_states [ - 1 ] . dtype logger . info ( f \"\ud83d\udd0d DEBUG: Policy model hidden states dtype: { hidden_dtype } \" ) else : logger . info ( f \"\ud83d\udd0d DEBUG: Policy model outputs type: { type ( outputs ) } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not test policy model forward pass: { e } \" ) except Exception as e : logger . warning ( f \"\ud83d\udd0d DEBUG: Could not get final dtypes: { e } \" ) # Check if we should use function-based reward model if not hasattr ( self , 'reward_model' ) or self . reward_model is None : if hasattr ( self , 'reward_functions' ) and self . reward_functions and len ( self . reward_functions ) > 0 : # Use function-based reward model from aligntune.core.rl.function_based_reward_model import FunctionBasedRewardModel logger . info ( \"=\" * 60 ) logger . info ( \"\u2705 Using function-based reward model with reward functions\" ) logger . info ( f \" { len ( self . reward_functions ) } reward function(s) configured\" ) logger . info ( \"=\" * 60 ) # Get device and dtype from policy model device = next ( self . policy_model . parameters ()) . device dtype = next ( self . policy_model . parameters ()) . dtype self . reward_model = FunctionBasedRewardModel ( reward_functions = self . reward_functions , tokenizer = self . tokenizer , device = str ( device ), dtype = dtype ) self . reward_model = self . reward_model . to ( device ) logger . info ( \"\u2705 FunctionBasedRewardModel created and moved to device\" ) else : raise ValueError ( \"TRL PPOTrainer requires either: \\n \" \" - reward_model: A neural network reward model (pretrained or custom trained) \\n \" \" Examples: 'OpenAssistant/reward-model-deberta-v3-large-v2', 'Skywork/Skywork-Reward-V2-Qwen3-0.6B' \\n \" \" - reward_functions: Rule-based reward functions (will be wrapped in FunctionBasedRewardModel) \\n \" \" Example: [{'type': 'length', 'weight': 1.0}, {'type': 'sentiment', 'weight': 0.5}] \\n \" \" - reward_training.enabled=True: Train a custom reward model from reward functions \\n \" \"Please provide one of these options.\" ) else : # Neural reward model provided logger . info ( \"\u2705 Using neural network reward model\" ) logger . info ( f \" Reward model type: { type ( self . reward_model ) . __name__ } \" ) # Create trainer with strict generation limits self . trainer = PPOTrainer ( args = ppo_config , processing_class = self . tokenizer , model = self . policy_model , ref_model = self . ref_model , reward_model = self . reward_model , value_model = self . value_model , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , peft_config = None , ) # Patch PolicyAndValueWrapper (keep this) self . _patch_policy_wrapper () logger . info ( \"PPOTrainer created successfully\" ) except Exception as e : logger . error ( f \"Failed to setup trainer: { e } \" ) raise","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.train","text":"Execute PPO training. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute PPO training.\"\"\" try : logger . info ( \"Starting Unsloth PPO training\" ) start_time = time . time () # Setup everything self . setup_model () self . setup_rewards () self . setup_data () self . setup_trainer () # Train logger . info ( \"Running PPO training...\" ) training_result = self . trainer . train () try : generate_and_log_samples ( self . config . logging . sample_logging , self . policy_model , self . tokenizer , getattr ( self , 'reward_functions' , None ), stage = \"post-train\" , log = logger , ) except Exception as sample_error : logger . warning ( f \"Unable to log qualitative samples: { sample_error } \" ) # Save output_dir = self . _get_config_value ( self . config . logging , 'output_dir' , './output/ppo' ) logger . info ( f \"Saving to: { output_dir } \" ) self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # TRL PPOTrainer.train() may return None (or an object without global_step). # Use training_history length as a robust fallback so examples don't report 0 steps. fallback_steps = 0 try : if hasattr ( self , \"training_history\" ) and self . training_history : fallback_steps = len ( self . training_history ) except Exception : fallback_steps = 0 results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else fallback_steps , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : 0 , \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Training completed in { training_time : .2f } s\" ) return results except Exception as e : logger . error ( f \"Training failed: { e } \" ) raise","title":"train"},{"location":"api-reference/trainers/#backends.unsloth.rl.ppo.ppo.UnslothPPOTrainer.train_step","text":"Not used - TRL handles training loop. Source code in src/aligntune/backends/unsloth/rl/ppo/ppo.py 2143 2144 2145 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\"Not used - TRL handles training loop.\"\"\" return { \"loss\" : 0.0 } options: show_source: true heading_level: 3","title":"train_step"},{"location":"api-reference/trainers/#unslothgrpotrainer","text":"Unsloth backend for Group Relative Policy Optimization. Bases: TrainerBase GRPO trainer using Unsloth's FastLanguageModel for optimized training. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 class UnslothGRPOTrainer ( TrainerBase ): \"\"\"GRPO trainer using Unsloth's FastLanguageModel for optimized training.\"\"\" def __init__ ( self , config : UnifiedConfig ): super () . __init__ ( config ) self . model = None self . tokenizer = None self . trainer = None self . dataset_cache = None self . training_history = [] self . logging_manager = None self . reward_functions = [] self . train_dataset = None self . eval_dataset = None self . reward_configs = [] self . custom_evaluator = None # self . dataset_dict = None @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import GRPOTrainer , GRPOConfig return True except ImportError : return False def _get_config_value ( self , config_obj , * attr_names , default = None ): \"\"\"Safely get config value from multiple possible attribute names.\"\"\" if isinstance ( config_obj , dict ): for attr_name in attr_names : if attr_name in config_obj : return config_obj [ attr_name ] else : for attr_name in attr_names : if hasattr ( config_obj , attr_name ): return getattr ( config_obj , attr_name ) return default def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for GRPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): model_name = self . config . model . get ( 'name_or_path' ) max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) quantization = self . config . model . get ( 'quantization' , {}) else : model_name = self . config . model . name_or_path max_seq_length = self . config . model . max_seq_length quantization = getattr ( self . config . model , 'quantization' , {}) logger . info ( f \"Setting up Unsloth GRPO model: { model_name } \" ) # Configure Unsloth model parameters # Unsloth only accepts: max_seq_length, dtype, and load_in_4bit # Other quantization params are NOT passed to from_pretrained device_map = self . config . model . device_map or 'auto' model_kwargs = { \"max_seq_length\" : max_seq_length , # Auto-detect (Unsloth will use bfloat16 automatically) \"dtype\" : None , \"load_in_4bit\" : quantization . get ( \"load_in_4bit\" , True ) if isinstance ( quantization , dict ) else True , \"device_map\" : device_map , } logger . info ( f \"Loading model with kwargs: { model_kwargs } \" ) # Load model with Unsloth optimizations # NOTE: Unsloth handles all quantization internally - don't pass # bnb_4bit params self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = model_name , ** model_kwargs ) # Set pad token if not present if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Configure model for GRPO training with LoRA (use config values) # NOTE: AlignTune configs may be dataclasses or dicts depending on how the trainer was created. if isinstance ( self . config . model , dict ): use_peft = self . config . model . get ( \"use_peft\" , True ) lora_r = self . config . model . get ( \"lora_r\" , 16 ) lora_alpha = self . config . model . get ( \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = self . config . model . get ( \"lora_dropout\" , 0.0 ) target_modules = self . config . model . get ( \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = self . config . model . get ( \"gradient_checkpointing\" , True ) else : use_peft = getattr ( self . config . model , \"use_peft\" , True ) lora_r = getattr ( self . config . model , \"lora_r\" , 16 ) lora_alpha = getattr ( self . config . model , \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = getattr ( self . config . model , \"lora_dropout\" , 0.0 ) target_modules = getattr ( self . config . model , \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = getattr ( self . config . model , \"gradient_checkpointing\" , True ) if use_peft : self . model = FastLanguageModel . get_peft_model ( self . model , r = int ( lora_r ), target_modules = target_modules , lora_alpha = int ( lora_alpha ), lora_dropout = float ( lora_dropout ), bias = \"none\" , use_gradient_checkpointing = \"unsloth\" if gradient_checkpointing else False , random_state = 3407 , use_rslora = False , loftq_config = None , ) logger . info ( \"Unsloth GRPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth GRPO model: { e } \" ) raise def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) try : # \u2728 SPECIAL CASE: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"\u2713 Loaded custom reward function (weight: { weight } )\" ) # Store directly - no registry needed self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : \"custom\" }) continue # Skip registry lookup # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" ) def _combined_reward_function ( self , completions : List [ str ], ** kwargs ) -> List [ float ]: \"\"\"Combined reward function that applies all registered rewards. Args: completions: List of generated completions **kwargs: Additional arguments from TRL including: - prompts: List of prompts - test_list: List of test cases (for code datasets like MBPP) - answer/solution/reference: Reference answers (for math datasets) \"\"\" if not completions : return [] # Debug: log what TRL is passing (once) if not hasattr ( self , '_logged_kwargs' ): logger . info ( f \"[DEBUG] _combined_reward_function kwargs keys: { list ( kwargs . keys ()) } \" ) if 'test_list' in kwargs : sample = kwargs [ 'test_list' ][: 1 ] if kwargs [ 'test_list' ] else 'empty' logger . info ( f \"[DEBUG] test_list found, sample: { sample } \" ) else : logger . info ( \"[DEBUG] test_list NOT in kwargs!\" ) self . _logged_kwargs = True batch_rewards = [] # Extract batch data from kwargs (TRL passes these as lists) test_lists = kwargs . get ( 'test_list' , [ None ] * len ( completions )) # CHECK MULTIPLE POSSIBLE REFERENCE COLUMN NAMES # Try different column names in order of preference references = None for ref_key in [ 'answer' , 'solution' , 'reference' , 'response' , 'ground_truth' , 'target' ]: if ref_key in kwargs : references = kwargs [ ref_key ] # print(f\"[INFO] Found reference data in column: '{ref_key}'\") break # If no reference column found, use None for all completions if references is None : references = [ None ] * len ( completions ) print ( f \"[WARNING] No reference column found. Checked: answer, solution, reference, ground_truth, target\" ) # Ensure lists match completion length if not isinstance ( test_lists , list ): test_lists = [ test_lists ] * len ( completions ) if not isinstance ( references , list ): references = [ references ] * len ( completions ) for idx , completion in enumerate ( completions ): total_reward = 0.0 # Get per-sample data test_cases = test_lists [ idx ] if idx < len ( test_lists ) else None reference = references [ idx ] if idx < len ( references ) else None for rf in self . reward_functions : try : reward_func = rf [ \"function\" ] weight = rf [ \"weight\" ] # Handle different reward function signatures if callable ( reward_func ): # Try different calling patterns try : # Pattern 1: text + test_cases (for code execution) reward = reward_func ( completion , test_cases = test_cases ) except TypeError : try : # Pattern 2: Just text reward = reward_func ( completion ) except TypeError : try : # Pattern 3: text + reference reward = reward_func ( completion , reference = reference ) except TypeError : try : # Pattern 4: text + reference + context reward = reward_func ( completion , reference = reference , context = {}) except BaseException : logger . debug ( f \"Could not call reward function { rf [ 'name' ] } , returning 0\" ) reward = 0.0 else : logger . warning ( f \"Reward function { rf [ 'name' ] } is not callable\" ) reward = 0.0 # Apply weight weighted_reward = reward * weight total_reward += weighted_reward logger . debug ( f \"Reward { rf [ 'name' ] } : { reward : .4f } (weighted: { weighted_reward : .4f } )\" ) except Exception as e : logger . warning ( f \"Error computing reward { rf [ 'name' ] } : { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) batch_rewards . append ( total_reward ) # Log batch statistics with completions summary if batch_rewards : successful = sum ( 1 for r in batch_rewards if r > 0.5 ) partial = sum ( 1 for r in batch_rewards if 0 < r <= 0.5 ) failed = sum ( 1 for r in batch_rewards if r <= 0 ) print ( f \" \\n { '=' * 60 } \" ) print ( f \"BATCH REWARDS: { successful } passed | { partial } partial | { failed } failed | total= { len ( batch_rewards ) } \" ) print ( f \"Reward stats: min= { min ( batch_rewards ) : .2f } , max= { max ( batch_rewards ) : .2f } , mean= { sum ( batch_rewards ) / len ( batch_rewards ) : .2f } \" ) print ( f \" { '=' * 60 } \\n \" ) return batch_rewards def setup_data ( self ) -> None : \"\"\"Setup datasets - compatible with base trainer interface.\"\"\" self . setup_dataset () def setup_dataset ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( self . config . train , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager # Columns to preserve for reward computation (test cases, solutions, # etc.) preserve_columns = { 'test_list' , 'test' , 'answer' , 'solution' , 'code' , 'canonical_solution' } manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset - DataManager handles all the complexity dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , max_samples = max_samples ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" ) def setup_trainer ( self ) -> None : \"\"\"Setup TRL GRPOTrainer with Unsloth model.\"\"\" try : from trl import GRPOTrainer , GRPOConfig logger . info ( \"Setting up TRL GRPOTrainer with Unsloth model\" ) # Handle both dict and object config if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' if isinstance ( self . config . train , dict ): num_epochs = self . config . train . get ( 'epochs' , 1 ) per_device_batch_size = self . config . train . get ( 'per_device_batch_size' , 4 ) gradient_accumulation_steps = self . config . train . get ( 'gradient_accumulation_steps' , 1 ) learning_rate = self . config . train . get ( 'learning_rate' , 2e-4 ) save_interval = self . config . train . get ( 'save_interval' , 500 ) eval_interval = self . config . train . get ( 'eval_interval' , 500 ) kl_coef = self . config . train . get ( 'kl_coef' , 0.1 ) cliprange = self . config . train . get ( 'cliprange' , 0.2 ) else : num_epochs = getattr ( self . config . train , 'epochs' , 1 ) per_device_batch_size = getattr ( self . config . train , 'per_device_batch_size' , 4 ) gradient_accumulation_steps = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) learning_rate = getattr ( self . config . train , 'learning_rate' , 2e-4 ) save_interval = getattr ( self . config . train , 'save_interval' , 500 ) eval_interval = getattr ( self . config . train , 'eval_interval' , 500 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.1 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) # Get max sequence length from model config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # CRITICAL: Calculate max_prompt_length and max_completion_length max_prompt_length = int ( max_seq_length * 0.6 ) # 60% for prompt max_completion_length = int ( max_seq_length * 0.4 ) # 40% for completion # Determine evaluation and save strategy based on eval_dataset has_eval = self . eval_dataset is not None and len ( self . eval_dataset ) > 0 # Set save strategy save_strategy = \"steps\" from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = None ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = False ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Run name run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_grpo' ) # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = True ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # GRPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = kl_coef ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = cliprange ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) scale_rewards = self . _get_config_value ( self . config . train , 'scale_rewards' , default = 'group' ) mask_truncated_completions = self . _get_config_value ( self . config . train , 'mask_truncated_completions' , default = True ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.9 ) num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = per_device_batch_size ) # TRL GRPOConfig supports GRPO variants: \"grpo\", \"dapo\", \"dr_grpo\" VALID_GRPO_LOSS_TYPES = { \"grpo\" , \"dapo\" , \"dr_grpo\" } if loss_type is None : loss_type = \"grpo\" else : loss_type = str ( loss_type ) . lower () . strip () # \"sigmoid\" is a DPO loss type, not valid for GRPO - raise error if loss_type == \"sigmoid\" : raise ValueError ( f \"loss_type='sigmoid' is a DPO/IPO loss type and is not valid for GRPO training. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } use grpo\" ) if loss_type not in VALID_GRPO_LOSS_TYPES : raise ValueError ( f \"Unknown loss_type=' { loss_type } ' for GRPO. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } \" ) # Adjust save strategy to save on epoch save_strategy = \"epoch\" if self . eval_dataset else \"steps\" # Create GRPO configuration grpo_config = GRPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , max_steps = max_steps , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_interval , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = 0.5 , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # GRPO specific parameters max_prompt_length = max_prompt_length , max_completion_length = max_completion_length , num_generations = num_generations , temperature = temperature , top_p = top_p , loss_type = loss_type , beta = beta , epsilon = epsilon , scale_rewards = scale_rewards , mask_truncated_completions = mask_truncated_completions , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , group_by_length = group_by_length , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Rest of your code stays the same... # Get max_seq_length for text truncation in reward functions # Use conservative limit: most reward models (e.g., toxic-bert) have max_length=512 # Truncate to min(512, model_max_seq_length) to prevent tensor # shape mismatches model_max_seq_length = 512 # Default if hasattr ( self . config , 'model' ): if isinstance ( self . config . model , dict ): model_max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : model_max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # Use conservative 512 limit for reward functions (most reward # models expect this) max_seq_length = min ( 512 , model_max_seq_length ) # # Create trainer with reward functions # self.trainer = GRPOTrainer( # model=self.model, # tokenizer=self.tokenizer, # train_dataset=self.train_dataset, # eval_dataset=self.eval_dataset, # args=grpo_config, # reward_funcs=self._combined_reward_function if self.reward_functions else None, # ) # # Create GRPO trainer use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if use_rewards_directly : # Use functions directly (no wrapper) reward_funcs = use_rewards_directly logger . info ( f \"\u2713 Using { len ( reward_funcs ) } reward functions DIRECTLY\" ) else : # Use combined wrapper (default) reward_funcs = self . _combined_reward_function logger . info ( \"\u2713 Using _combined_reward_function wrapper\" ) import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs if isinstance ( reward_funcs , list ) else [ reward_funcs ], ) else : try : import unsloth logger . info ( \"Detected Unsloth environment\" ) # Unsloth expects a list of functions if isinstance ( reward_funcs , list ): # Already a list (direct mode) logger . info ( f \"Passing { len ( reward_funcs ) } functions directly\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List of functions ) else : # Single function (wrapper mode) - wrap it for Unsloth # logger.info(\"Wrapping _combined_reward_function for Unsloth\") # def unsloth_reward_wrapper(prompts=None, completions=None, **kwargs): # if completions is None: # return [0.0] * (len(prompts) if prompts else 1) # return reward_funcs(completions, **kwargs) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List with wrapper ) except ImportError : logger . info ( \"Using pure TRL (no Unsloth)\" ) # Pure TRL can accept either format if isinstance ( reward_funcs , list ): # Direct mode: pass list as-is self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , ) else : # Wrapper mode: wrap in list self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ reward_funcs ], # Wrap single function in list ) logger . info ( \"GRPO trainer setup completed successfully!\" ) logger . info ( \"TRL GRPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup GRPO trainer: { e } \" ) raise def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\" Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. \"\"\" # This method is required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles the training loop internally logger . debug ( \"train_step() called but Unsloth GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\" Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. \"\"\" # This method might be required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles data loading internally logger . debug ( \"create_data_loader() called but Unsloth GRPO uses TRL's internal data loading\" ) return None def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training with Unsloth optimizations.\"\"\" try : logger . info ( \"Starting Unsloth GRPO training\" ) start_time = time . time () # Setup components self . setup_model () self . setup_data () use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if not use_rewards_directly : self . setup_rewards () else : logger . warning ( \"Skipping setup_rewards() - using reward functions directly from config\" ) self . setup_trainer () # Start training training_result = self . trainer . train () # Get output directory if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' # Save model self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Unsloth GRPO training completed in { training_time : .2f } seconds\" ) if hasattr ( training_result , 'training_loss' ): logger . info ( f \"Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"GRPO training failed: { e } \" ) raise # def evaluate(self) -> Dict[str, Any]: # \"\"\"Evaluate the trained GRPO model.\"\"\" # try: # if not self.eval_dataset: # logger.warning(\"No evaluation dataset available\") # return {} # logger.info(\"Evaluating Unsloth GRPO model\") # # Run evaluation # eval_results = self.trainer.evaluate() # logger.info(f\"GRPO evaluation results: {eval_results}\") # return eval_results # except Exception as e: # logger.error(f\"GRPO evaluation failed: {e}\") # raise def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = True , ** kwargs ) -> Dict [ str , float ]: \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # Auto-setup evaluators on first call if self . base_evaluator is None and self . rl_evaluator is None : logger . info ( \"Auto-initializing evaluators for first evaluation...\" ) self . setup_custom_evaluator ( evaluator_type = \"auto\" ) # Call parent's unified evaluate method return super () . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = metric_key_prefix , use_custom_evaluator = use_custom_evaluator , ** kwargs ) def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise","title":"UnslothGRPOTrainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.create_data_loader","text":"Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 924 925 926 927 928 929 930 931 932 933 934 935 936 def create_data_loader ( self ) -> Optional [ DataLoader ]: \"\"\" Create data loader for training. NOTE: For Unsloth GRPO, data loading is handled by TRL's GRPOTrainer, so this method returns None. It's implemented to satisfy the TrainerBase abstract method requirement if it exists. \"\"\" # This method might be required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles data loading internally logger . debug ( \"create_data_loader() called but Unsloth GRPO uses TRL's internal data loading\" ) return None","title":"create_data_loader"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.evaluate","text":"GRPO-specific evaluation - auto-setup evaluators and delegate to parent. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 def evaluate ( self , eval_dataset = None , metric_key_prefix : str = \"eval\" , use_custom_evaluator : bool = True , ** kwargs ) -> Dict [ str , float ]: \"\"\"GRPO-specific evaluation - auto-setup evaluators and delegate to parent.\"\"\" # Auto-setup evaluators on first call if self . base_evaluator is None and self . rl_evaluator is None : logger . info ( \"Auto-initializing evaluators for first evaluation...\" ) self . setup_custom_evaluator ( evaluator_type = \"auto\" ) # Call parent's unified evaluate method return super () . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = metric_key_prefix , use_custom_evaluator = use_custom_evaluator , ** kwargs )","title":"evaluate"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.is_available","text":"Check if Unsloth and TRL are available. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 47 48 49 50 51 52 53 54 55 56 @classmethod def is_available ( cls ) -> bool : \"\"\"Check if Unsloth and TRL are available.\"\"\" try : import unsloth from unsloth import FastLanguageModel from trl import GRPOTrainer , GRPOConfig return True except ImportError : return False","title":"is_available"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.load_model","text":"Load a trained GRPO model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 def load_model ( self , path : str ) -> None : \"\"\"Load a trained GRPO model.\"\"\" try : logger . info ( f \"Loading Unsloth GRPO model from: { path } \" ) import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 2048 ) # Load model and tokenizer self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = path , max_seq_length = max_seq_length , dtype = None , load_in_4bit = True , ) logger . info ( \"GRPO model loaded successfully\" ) except Exception as e : logger . error ( f \"Failed to load GRPO model: { e } \" ) raise","title":"load_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.save_model","text":"Save the trained GRPO model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 def save_model ( self , path : Optional [ str ] = None ) -> str : \"\"\"Save the trained GRPO model.\"\"\" try : if isinstance ( self . config . logging , dict ): default_path = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : default_path = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' save_path = path or default_path logger . info ( f \"Saving Unsloth GRPO model to: { save_path } \" ) # Save using Unsloth's optimized saving self . model . save_pretrained ( save_path ) self . tokenizer . save_pretrained ( save_path ) # Save training configuration config_path = Path ( save_path ) / \"grpo_training_config.yaml\" with open ( config_path , \"w\" ) as f : config_dict = self . config . to_dict () if hasattr ( self . config , 'to_dict' ) else self . config yaml . dump ( config_dict , f , default_flow_style = False ) logger . info ( f \"GRPO model saved successfully to: { save_path } \" ) return save_path except Exception as e : logger . error ( f \"Failed to save GRPO model: { e } \" ) raise","title":"save_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.setup_data","text":"Setup datasets - compatible with base trainer interface. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 443 444 445 def setup_data ( self ) -> None : \"\"\"Setup datasets - compatible with base trainer interface.\"\"\" self . setup_dataset ()","title":"setup_data"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.setup_dataset","text":"Setup datasets for GRPO training using unified DataManager. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 def setup_dataset ( self ) -> None : \"\"\"Setup datasets for GRPO training using unified DataManager.\"\"\" logger . info ( \"Setting up GRPO datasets with DataManager...\" ) # Extract dataset configuration dataset_config = None if hasattr ( self . config , 'dataset' ): dataset_config = self . config . dataset elif hasattr ( self . config , 'datasets' ) and len ( self . config . datasets ) > 0 : dataset_config = self . config . datasets [ 0 ] else : raise ValueError ( \"No dataset configuration found\" ) # Extract parameters dataset_name = self . _get_config_value ( dataset_config , 'name' , 'dataset_name' , default = 'imdb' ) split = self . _get_config_value ( dataset_config , 'split' , default = None ) config_name = self . _get_config_value ( dataset_config , 'config_name' , default = None ) system_prompt = self . _get_config_value ( self . config . train , 'system_prompt' , default = None ) enable_thinking = self . _get_config_value ( self . config . train , 'enable_thinking' , default = False ) # Advanced DataManager features column_mapping = self . _get_config_value ( dataset_config , 'column_mapping' , default = None ) processing_fn = self . _get_config_value ( dataset_config , 'processing_fn' , default = None ) processing_batched = self . _get_config_value ( dataset_config , 'processing_batched' , default = False ) max_samples = self . _get_config_value ( dataset_config , 'max_samples' , default = None ) logger . info ( f \"Loading dataset: { dataset_name } (split: { split } , config: { config_name } )\" ) # Initialize DataManager for GRPO task from aligntune.data.manager import DataManager # Columns to preserve for reward computation (test cases, solutions, # etc.) preserve_columns = { 'test_list' , 'test' , 'answer' , 'solution' , 'code' , 'canonical_solution' } manager = DataManager ( task_type = \"grpo\" , system_prompt = system_prompt , column_mapping = column_mapping , processing_fn = processing_fn , processing_batched = processing_batched , max_samples = max_samples , tokenizer = self . tokenizer , # \u2705 ADD THIS - Pass tokenizer for chat template enable_thinking = enable_thinking , ) # Load dataset - DataManager handles all the complexity dataset_dict = manager . load_dataset ( dataset_name , config_name = config_name , split = split , max_samples = max_samples ) # Extract train and validation splits self . train_dataset = dataset_dict . get ( \"train\" , None ) self . eval_dataset = dataset_dict . get ( \"validation\" , None ) self . dataset_dict = dataset_dict logger . info ( f \"Dataset loaded: { len ( self . train_dataset ) } train examples\" ) if self . eval_dataset : logger . info ( f \"Evaluation dataset: { len ( self . eval_dataset ) } examples\" ) # Log sample if len ( self . train_dataset ) > 0 : sample = self . train_dataset [ 0 ] prompt_col = \"prompt\" if \"prompt\" in sample else \"query\" logger . info ( f \"Sample prompt (first 100 chars): { sample [ prompt_col ][: 100 ] } ...\" ) logger . info ( f \"Dataset columns: { self . train_dataset . column_names } \" )","title":"setup_dataset"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.setup_model","text":"Setup Unsloth-optimized model and tokenizer for GRPO. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def setup_model ( self ) -> None : \"\"\"Setup Unsloth-optimized model and tokenizer for GRPO.\"\"\" try : import unsloth from unsloth import FastLanguageModel # Handle both dict and object config if isinstance ( self . config . model , dict ): model_name = self . config . model . get ( 'name_or_path' ) max_seq_length = self . config . model . get ( 'max_seq_length' , 2048 ) quantization = self . config . model . get ( 'quantization' , {}) else : model_name = self . config . model . name_or_path max_seq_length = self . config . model . max_seq_length quantization = getattr ( self . config . model , 'quantization' , {}) logger . info ( f \"Setting up Unsloth GRPO model: { model_name } \" ) # Configure Unsloth model parameters # Unsloth only accepts: max_seq_length, dtype, and load_in_4bit # Other quantization params are NOT passed to from_pretrained device_map = self . config . model . device_map or 'auto' model_kwargs = { \"max_seq_length\" : max_seq_length , # Auto-detect (Unsloth will use bfloat16 automatically) \"dtype\" : None , \"load_in_4bit\" : quantization . get ( \"load_in_4bit\" , True ) if isinstance ( quantization , dict ) else True , \"device_map\" : device_map , } logger . info ( f \"Loading model with kwargs: { model_kwargs } \" ) # Load model with Unsloth optimizations # NOTE: Unsloth handles all quantization internally - don't pass # bnb_4bit params self . model , self . tokenizer = FastLanguageModel . from_pretrained ( model_name = model_name , ** model_kwargs ) # Set pad token if not present if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # Configure model for GRPO training with LoRA (use config values) # NOTE: AlignTune configs may be dataclasses or dicts depending on how the trainer was created. if isinstance ( self . config . model , dict ): use_peft = self . config . model . get ( \"use_peft\" , True ) lora_r = self . config . model . get ( \"lora_r\" , 16 ) lora_alpha = self . config . model . get ( \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = self . config . model . get ( \"lora_dropout\" , 0.0 ) target_modules = self . config . model . get ( \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = self . config . model . get ( \"gradient_checkpointing\" , True ) else : use_peft = getattr ( self . config . model , \"use_peft\" , True ) lora_r = getattr ( self . config . model , \"lora_r\" , 16 ) lora_alpha = getattr ( self . config . model , \"lora_alpha\" , max ( 16 , int ( lora_r ))) lora_dropout = getattr ( self . config . model , \"lora_dropout\" , 0.0 ) target_modules = getattr ( self . config . model , \"lora_target_modules\" , [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" , \"gate_proj\" , \"up_proj\" , \"down_proj\" ], ) gradient_checkpointing = getattr ( self . config . model , \"gradient_checkpointing\" , True ) if use_peft : self . model = FastLanguageModel . get_peft_model ( self . model , r = int ( lora_r ), target_modules = target_modules , lora_alpha = int ( lora_alpha ), lora_dropout = float ( lora_dropout ), bias = \"none\" , use_gradient_checkpointing = \"unsloth\" if gradient_checkpointing else False , random_state = 3407 , use_rslora = False , loftq_config = None , ) logger . info ( \"Unsloth GRPO model setup completed successfully\" ) except Exception as e : logger . error ( f \"Failed to setup Unsloth GRPO model: { e } \" ) raise","title":"setup_model"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.setup_rewards","text":"Setup reward functions using the centralized registry system. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def setup_rewards ( self ) -> None : \"\"\"Setup reward functions using the centralized registry system.\"\"\" logger . info ( \"Setting up reward functions for GRPO...\" ) # Get reward configurations rewards_config = [] if hasattr ( self . config , 'rewards' ): rewards_config = self . config . rewards if isinstance ( self . config . rewards , list ) else [] if not rewards_config : logger . warning ( \"No reward configurations found, using default rewards\" ) rewards_config = [ { \"type\" : \"length\" , \"weight\" : 0.2 , \"params\" : { \"min_length\" : 20 , \"max_length\" : 200 }}, { \"type\" : \"sentiment\" , \"weight\" : 0.2 , \"params\" : { \"positive_weight\" : 1.0 }}, { \"type\" : \"safety\" , \"weight\" : 0.2 , \"params\" : { \"strict\" : True }}, { \"type\" : \"diversity\" , \"weight\" : 0.2 , \"params\" : {}}, { \"type\" : \"fluency\" , \"weight\" : 0.2 , \"params\" : {}}, ] # Load reward functions from the registry for reward_config in rewards_config : reward_type = reward_config . get ( 'type' , 'length' ) weight = reward_config . get ( 'weight' , 1.0 ) params = reward_config . get ( 'params' , {}) try : # \u2728 SPECIAL CASE: custom reward function passed directly if reward_type == 'custom' and 'reward_function' in params : reward_func = params [ 'reward_function' ] logger . info ( f \"\u2713 Loaded custom reward function (weight: { weight } )\" ) # Store directly - no registry needed self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : \"custom\" }) continue # Skip registry lookup # Use the rewards registry to get reward functions from aligntune.rewards.registry import RewardRegistry as RewardsRegistry from aligntune.rewards.core import RewardConfig , RewardType # Map common variations to standard names reward_type_mapping = { 'math' : 'math_reasoning' , 'code' : 'code_quality' , } reward_type = reward_type_mapping . get ( reward_type , reward_type ) try : # Convert reward type string to enum reward_type_enum = RewardType [ reward_type . upper ()] # Create RewardConfig with weight and params reward_cfg = RewardConfig ( reward_type = reward_type_enum , weight = 1.0 , # Weight will be applied separately params = params ) # Get reward function from registry reward_func_obj = RewardsRegistry . get_reward_function ( reward_type , reward_cfg ) # Extract the callable compute method if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward from registry (weight: { weight } )\" ) except KeyError : # Reward type not in enum, try registry by name logger . warning ( f \"Reward type ' { reward_type } ' not in RewardType enum, trying registry by name\" ) reward_func_obj = RewardsRegistry . get_reward_function ( reward_type ) if hasattr ( reward_func_obj , 'compute' ): reward_func = reward_func_obj . compute elif callable ( reward_func_obj ): reward_func = reward_func_obj else : logger . warning ( f \"Reward function ' { reward_type } ' is not callable, skipping\" ) continue logger . info ( f \"\u2713 Loaded { reward_type } reward by name (weight: { weight } )\" ) # Store reward function with metadata self . reward_functions . append ({ \"function\" : reward_func , \"weight\" : weight , \"name\" : reward_type }) except Exception as e : logger . warning ( f \"Failed to load reward function ' { reward_type } ': { e } \" ) logger . debug ( f \"Error details:\" , exc_info = True ) continue if not self . reward_functions : logger . error ( \"No reward functions were loaded! Adding a simple default length reward.\" ) # Add a simple fallback reward so training doesn't fail def default_length_reward ( text , reference = None , ** kwargs ): length = len ( text . split ()) if length < 20 : return length / 20.0 * 0.5 elif length > 200 : return max ( 0.0 , 1.0 - ( length - 200 ) / 200.0 ) else : return 1.0 self . reward_functions . append ({ \"function\" : default_length_reward , \"weight\" : 1.0 , \"name\" : \"default_length\" }) logger . info ( \"Added default length reward as fallback\" ) logger . info ( f \"\u2713 Configured { len ( self . reward_functions ) } reward functions successfully\" ) # Log summary of loaded rewards reward_summary = \", \" . join ( [ f \" { rf [ 'name' ] } ( { rf [ 'weight' ] : .2f } )\" for rf in self . reward_functions ]) logger . info ( f \"Reward functions: { reward_summary } \" )","title":"setup_rewards"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.setup_trainer","text":"Setup TRL GRPOTrainer with Unsloth model. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 def setup_trainer ( self ) -> None : \"\"\"Setup TRL GRPOTrainer with Unsloth model.\"\"\" try : from trl import GRPOTrainer , GRPOConfig logger . info ( \"Setting up TRL GRPOTrainer with Unsloth model\" ) # Handle both dict and object config if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' if isinstance ( self . config . train , dict ): num_epochs = self . config . train . get ( 'epochs' , 1 ) per_device_batch_size = self . config . train . get ( 'per_device_batch_size' , 4 ) gradient_accumulation_steps = self . config . train . get ( 'gradient_accumulation_steps' , 1 ) learning_rate = self . config . train . get ( 'learning_rate' , 2e-4 ) save_interval = self . config . train . get ( 'save_interval' , 500 ) eval_interval = self . config . train . get ( 'eval_interval' , 500 ) kl_coef = self . config . train . get ( 'kl_coef' , 0.1 ) cliprange = self . config . train . get ( 'cliprange' , 0.2 ) else : num_epochs = getattr ( self . config . train , 'epochs' , 1 ) per_device_batch_size = getattr ( self . config . train , 'per_device_batch_size' , 4 ) gradient_accumulation_steps = getattr ( self . config . train , 'gradient_accumulation_steps' , 1 ) learning_rate = getattr ( self . config . train , 'learning_rate' , 2e-4 ) save_interval = getattr ( self . config . train , 'save_interval' , 500 ) eval_interval = getattr ( self . config . train , 'eval_interval' , 500 ) kl_coef = getattr ( self . config . train , 'kl_coef' , 0.1 ) cliprange = getattr ( self . config . train , 'cliprange' , 0.2 ) # Get max sequence length from model config if isinstance ( self . config . model , dict ): max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # CRITICAL: Calculate max_prompt_length and max_completion_length max_prompt_length = int ( max_seq_length * 0.6 ) # 60% for prompt max_completion_length = int ( max_seq_length * 0.4 ) # 40% for completion # Determine evaluation and save strategy based on eval_dataset has_eval = self . eval_dataset is not None and len ( self . eval_dataset ) > 0 # Set save strategy save_strategy = \"steps\" from aligntune.core.precision_handler import PrecisionHandler precision = PrecisionHandler . get_precision_from_config ( self . config , default = \"auto\" ) precision = PrecisionHandler . validate_precision ( precision ) PrecisionHandler . log_precision_info ( precision , \"GRPO (Unsloth)\" ) precision_args = PrecisionHandler . get_training_args_precision ( precision ) # Evaluation parameters eval_strategy = self . _get_config_value ( self . config . train , 'eval_strategy' , default = 'steps' ) eval_steps = self . _get_config_value ( self . config . train , 'eval_steps' , default = None ) per_device_eval_batch_size = self . _get_config_value ( self . config . train , 'per_device_eval_batch_size' , default = per_device_batch_size ) metric_for_best_model = self . _get_config_value ( self . config . train , 'metric_for_best_model' , default = None ) greater_is_better = self . _get_config_value ( self . config . train , 'greater_is_better' , default = False ) load_best_model_at_end = self . _get_config_value ( self . config . train , 'load_best_model_at_end' , default = False ) # Adjust eval strategy based on eval_dataset availability if self . eval_dataset : eval_strategy = eval_strategy if eval_strategy != 'no' else 'epoch' else : eval_strategy = 'no' eval_steps = None # Logging parameters logging_steps = self . _get_config_value ( self . config . train , 'logging_steps' , default = 10 ) logging_strategy = self . _get_config_value ( self . config . train , 'logging_strategy' , default = 'steps' ) save_total_limit = self . _get_config_value ( self . config . train , 'save_total_limit' , default = None ) # Report to report_to = self . _get_config_value ( self . config . logging , 'report_to' , default = 'none' ) if isinstance ( self . config . logging , dict ): loggers = self . config . logging . get ( 'loggers' , []) else : loggers = getattr ( self . config . logging , 'loggers' , []) if loggers and report_to == 'none' : report_to = loggers # Run name run_name = self . _get_config_value ( self . config . logging , 'run_name' , default = 'unsloth_grpo' ) # Optimizer parameters optimizer = self . _get_config_value ( self . config . train , 'optimizer' , default = 'adamw_torch' ) lr_scheduler_type = self . _get_config_value ( self . config . train , 'lr_scheduler' , default = 'cosine' ) warmup_ratio = self . _get_config_value ( self . config . train , 'warmup_ratio' , default = 0.1 ) warmup_steps = self . _get_config_value ( self . config . train , 'warmup_steps' , default = 0 ) weight_decay = self . _get_config_value ( self . config . train , 'weight_decay' , default = 0.0 ) # Additional training parameters gradient_checkpointing = self . _get_config_value ( self . config . train , 'use_gradient_checkpointing' , 'gradient_checkpointing' , default = True ) group_by_length = self . _get_config_value ( self . config . train , 'group_by_length' , default = True ) seed = self . _get_config_value ( self . config . train , 'seed' , default = 42 ) data_seed = self . _get_config_value ( self . config . train , 'data_seed' , default = 47 ) max_steps = self . _get_config_value ( self . config . train , \"max_steps\" , 500 ) # GRPO specific parameters beta = self . _get_config_value ( self . config . train , 'beta' , 'kl_coef' , default = kl_coef ) epsilon = self . _get_config_value ( self . config . train , 'epsilon' , 'cliprange' , default = cliprange ) loss_type = self . _get_config_value ( self . config . train , 'loss_type' , default = 'sigmoid' ) scale_rewards = self . _get_config_value ( self . config . train , 'scale_rewards' , default = 'group' ) mask_truncated_completions = self . _get_config_value ( self . config . train , 'mask_truncated_completions' , default = True ) temperature = self . _get_config_value ( self . config . train , 'temperature' , default = 0.7 ) top_p = self . _get_config_value ( self . config . train , 'top_p' , default = 0.9 ) num_generations = self . _get_config_value ( self . config . train , 'num_generations' , default = per_device_batch_size ) # TRL GRPOConfig supports GRPO variants: \"grpo\", \"dapo\", \"dr_grpo\" VALID_GRPO_LOSS_TYPES = { \"grpo\" , \"dapo\" , \"dr_grpo\" } if loss_type is None : loss_type = \"grpo\" else : loss_type = str ( loss_type ) . lower () . strip () # \"sigmoid\" is a DPO loss type, not valid for GRPO - raise error if loss_type == \"sigmoid\" : raise ValueError ( f \"loss_type='sigmoid' is a DPO/IPO loss type and is not valid for GRPO training. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } use grpo\" ) if loss_type not in VALID_GRPO_LOSS_TYPES : raise ValueError ( f \"Unknown loss_type=' { loss_type } ' for GRPO. \" f \"Valid GRPO loss types are: { VALID_GRPO_LOSS_TYPES } \" ) # Adjust save strategy to save on epoch save_strategy = \"epoch\" if self . eval_dataset else \"steps\" # Create GRPO configuration grpo_config = GRPOConfig ( # Output and logging output_dir = output_dir , run_name = run_name , logging_steps = logging_steps , logging_strategy = logging_strategy , report_to = report_to , max_steps = max_steps , # Evaluation eval_strategy = eval_strategy , eval_steps = eval_steps , per_device_eval_batch_size = per_device_eval_batch_size , metric_for_best_model = metric_for_best_model , greater_is_better = greater_is_better , load_best_model_at_end = load_best_model_at_end , # Checkpointing save_steps = save_interval , save_strategy = save_strategy , save_total_limit = save_total_limit , # Training parameters num_train_epochs = num_epochs , per_device_train_batch_size = per_device_batch_size , gradient_accumulation_steps = gradient_accumulation_steps , learning_rate = learning_rate , warmup_ratio = warmup_ratio , warmup_steps = warmup_steps , weight_decay = weight_decay , max_grad_norm = 0.5 , # Optimizer and scheduler optim = optimizer , lr_scheduler_type = lr_scheduler_type , # GRPO specific parameters max_prompt_length = max_prompt_length , max_completion_length = max_completion_length , num_generations = num_generations , temperature = temperature , top_p = top_p , loss_type = loss_type , beta = beta , epsilon = epsilon , scale_rewards = scale_rewards , mask_truncated_completions = mask_truncated_completions , # Seeds seed = seed , data_seed = data_seed , # Performance gradient_checkpointing = gradient_checkpointing , group_by_length = group_by_length , dataloader_pin_memory = False , # Precision ** precision_args , # Other settings remove_unused_columns = False , ) missing = extract_extra_and_missing_params ( backend_config = grpo_config , config = self . config , algorithm = 'grpo' ) for key , value in missing . items (): setattr ( grpo_config , key , value ) # Rest of your code stays the same... # Get max_seq_length for text truncation in reward functions # Use conservative limit: most reward models (e.g., toxic-bert) have max_length=512 # Truncate to min(512, model_max_seq_length) to prevent tensor # shape mismatches model_max_seq_length = 512 # Default if hasattr ( self . config , 'model' ): if isinstance ( self . config . model , dict ): model_max_seq_length = self . config . model . get ( 'max_seq_length' , 512 ) else : model_max_seq_length = getattr ( self . config . model , 'max_seq_length' , 512 ) # Use conservative 512 limit for reward functions (most reward # models expect this) max_seq_length = min ( 512 , model_max_seq_length ) # # Create trainer with reward functions # self.trainer = GRPOTrainer( # model=self.model, # tokenizer=self.tokenizer, # train_dataset=self.train_dataset, # eval_dataset=self.eval_dataset, # args=grpo_config, # reward_funcs=self._combined_reward_function if self.reward_functions else None, # ) # # Create GRPO trainer use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if use_rewards_directly : # Use functions directly (no wrapper) reward_funcs = use_rewards_directly logger . info ( f \"\u2713 Using { len ( reward_funcs ) } reward functions DIRECTLY\" ) else : # Use combined wrapper (default) reward_funcs = self . _combined_reward_function logger . info ( \"\u2713 Using _combined_reward_function wrapper\" ) import os should_use_pure_trl = os . environ . get ( 'PURE_TRL_MODE' , '0' ) == '1' if should_use_pure_trl : logger . info ( \"PURE_TRL_MODE enabled - using pure TRL API\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs if isinstance ( reward_funcs , list ) else [ reward_funcs ], ) else : try : import unsloth logger . info ( \"Detected Unsloth environment\" ) # Unsloth expects a list of functions if isinstance ( reward_funcs , list ): # Already a list (direct mode) logger . info ( f \"Passing { len ( reward_funcs ) } functions directly\" ) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List of functions ) else : # Single function (wrapper mode) - wrap it for Unsloth # logger.info(\"Wrapping _combined_reward_function for Unsloth\") # def unsloth_reward_wrapper(prompts=None, completions=None, **kwargs): # if completions is None: # return [0.0] * (len(prompts) if prompts else 1) # return reward_funcs(completions, **kwargs) self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , # List with wrapper ) except ImportError : logger . info ( \"Using pure TRL (no Unsloth)\" ) # Pure TRL can accept either format if isinstance ( reward_funcs , list ): # Direct mode: pass list as-is self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = reward_funcs , ) else : # Wrapper mode: wrap in list self . trainer = GRPOTrainer ( model = self . model , args = grpo_config , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , processing_class = self . tokenizer , reward_funcs = [ reward_funcs ], # Wrap single function in list ) logger . info ( \"GRPO trainer setup completed successfully!\" ) logger . info ( \"TRL GRPOTrainer setup completed\" ) except Exception as e : logger . error ( f \"Failed to setup GRPO trainer: { e } \" ) raise","title":"setup_trainer"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.train","text":"Execute GRPO training with Unsloth optimizations. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 def train ( self ) -> Dict [ str , Any ]: \"\"\"Execute GRPO training with Unsloth optimizations.\"\"\" try : logger . info ( \"Starting Unsloth GRPO training\" ) start_time = time . time () # Setup components self . setup_model () self . setup_data () use_rewards_directly = self . _get_config_value ( self . config . train , 'use_rewards_directly' , default = None ) if not use_rewards_directly : self . setup_rewards () else : logger . warning ( \"Skipping setup_rewards() - using reward functions directly from config\" ) self . setup_trainer () # Start training training_result = self . trainer . train () # Get output directory if isinstance ( self . config . logging , dict ): output_dir = self . config . logging . get ( 'output_dir' , './output/grpo' ) else : output_dir = getattr ( self . config . logging , 'output_dir' , './output/grpo' ) if hasattr ( self . config , 'logging' ) else './output/grpo' # Save model self . trainer . save_model ( output_dir ) self . tokenizer . save_pretrained ( output_dir ) training_time = time . time () - start_time # Compile results results = { \"training_time\" : training_time , \"final_loss\" : training_result . training_loss if hasattr ( training_result , 'training_loss' ) else 0.0 , \"total_steps\" : training_result . global_step if hasattr ( training_result , 'global_step' ) else 0 , \"model_path\" : output_dir , \"training_history\" : self . training_history , \"num_reward_functions\" : len ( self . reward_functions ), \"num_datasets\" : len ( self . config . datasets ) if hasattr ( self . config , 'datasets' ) else 0 , } logger . info ( f \"Unsloth GRPO training completed in { training_time : .2f } seconds\" ) if hasattr ( training_result , 'training_loss' ): logger . info ( f \"Final loss: { training_result . training_loss : .4f } \" ) return results except Exception as e : logger . error ( f \"GRPO training failed: { e } \" ) raise","title":"train"},{"location":"api-reference/trainers/#backends.unsloth.rl.grpo.grpo.UnslothGRPOTrainer.train_step","text":"Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. Source code in src/aligntune/backends/unsloth/rl/grpo/grpo.py 910 911 912 913 914 915 916 917 918 919 920 921 922 def train_step ( self , batch : Dict [ str , Any ]) -> Dict [ str , float ]: \"\"\" Execute a single training step. NOTE: For Unsloth GRPO, the actual training is handled by TRL's GRPOTrainer, so this method is not used during training. It's implemented to satisfy the TrainerBase abstract method requirement. \"\"\" # This method is required by TrainerBase but not used in Unsloth GRPO # because TRL's GRPOTrainer handles the training loop internally logger . debug ( \"train_step() called but Unsloth GRPO uses TRL's internal training loop\" ) return { \"loss\" : 0.0 } options: show_source: true heading_level: 3","title":"train_step"},{"location":"api-reference/trainers/#specialized-trainers","text":"","title":"Specialized Trainers"},{"location":"api-reference/trainers/#classificationtrainer","text":"Specialized trainer for text classification tasks. Complete Classification Trainer for text and token classification. Supports: - Text Classification (sentiment, topic classification, etc.) - Token Classification (NER, POS tagging, etc.) Uses standard Transformers Trainer (NOT TRL) for proper classification. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 class ClassificationTrainer : \"\"\" Complete Classification Trainer for text and token classification. Supports: - Text Classification (sentiment, topic classification, etc.) - Token Classification (NER, POS tagging, etc.) Uses standard Transformers Trainer (NOT TRL) for proper classification. \"\"\" def __init__ ( self , config ): \"\"\"Initialize trainer with config.\"\"\" self . config = config self . model = None self . tokenizer = None self . train_dataset = None self . eval_dataset = None self . trainer = None self . task_type = config . dataset . task_type logger . info ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) print ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) def setup_model ( self ): \"\"\"Load model and tokenizer based on task type.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading model: { self . config . model . name_or_path } \" ) logger . info ( f \"Loading model: { self . config . model . name_or_path } \" ) # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , use_fast = True ) # Add padding token if missing if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Ensure pad_token_id is set (required for data collator batching) # For Llama and similar tokenizers, we need to set it on both tokenizer and config pad_token_id = None if hasattr ( self . tokenizer , 'pad_token_id' ): pad_token_id = self . tokenizer . pad_token_id if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : # If pad_token was added, get its ID from the tokenizer try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id from tokenizer: { e } \" ) # Set pad_token_id on tokenizer (multiple ways to ensure it sticks) if pad_token_id is not None : # Set directly on tokenizer if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id # Also set on tokenizer's special_tokens_map if it exists if hasattr ( self . tokenizer , 'special_tokens_map' ): if 'pad_token' not in self . tokenizer . special_tokens_map : self . tokenizer . special_tokens_map [ 'pad_token' ] = self . tokenizer . pad_token # Set on tokenizer config if it exists if hasattr ( self . tokenizer , 'init_kwargs' ) and 'pad_token_id' in self . tokenizer . init_kwargs : self . tokenizer . init_kwargs [ 'pad_token_id' ] = pad_token_id # Also try setting via __setattr__ to ensure it persists try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : logger . error ( \"pad_token_id is still None after setup! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Determine precision and quantization settings use_8bit = False torch_dtype = None device_map = \"cpu\" # Keep on CPU initially # Check if precision is set and configure accordingly if hasattr ( self . config . model , 'precision' ) and self . config . model . precision : precision_value = self . config . model . precision . value if hasattr ( self . config . model . precision , 'value' ) else str ( self . config . model . precision ) if precision_value == \"bf16\" and torch . cuda . is_available () and torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 elif precision_value == \"fp16\" : torch_dtype = torch . float16 # For 4B+ models, use lower precision dtype to reduce memory # NOTE: 8-bit quantization (BitsAndBytes) requires PEFT adapters for fine-tuning # Since we're not using PEFT for classification, we skip 8-bit quantization # and rely on BF16/FP16 precision instead model_size_estimate = \"4B\" if \"4B\" in self . config . model . name_or_path or \"4b\" in self . config . model . name_or_path . lower () else \"unknown\" use_8bit = False # Disable 8-bit quantization for classification (requires PEFT) quantization_config = None # Ensure we use lower precision dtype for 4B models if torch_dtype is None and torch . cuda . is_available (): # Default to BF16 if supported, else FP16 if torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 logger . info ( \"Using BF16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) else : torch_dtype = torch . float16 logger . info ( \"Using FP16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) # Load appropriate model based on task # Load on CPU first with appropriate precision/quantization to avoid OOM model_kwargs = { \"num_labels\" : self . config . model . num_labels , \"ignore_mismatched_sizes\" : True , \"device_map\" : device_map , # Keep on CPU \"torch_dtype\" : torch_dtype if torch_dtype else None } # Add quantization config if using 8-bit if use_8bit and quantization_config : model_kwargs [ \"quantization_config\" ] = quantization_config # Remove None values model_kwargs = { k : v for k , v in model_kwargs . items () if v is not None } if self . task_type == TaskType . TEXT_CLASSIFICATION : self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForSequenceClassification with { self . config . model . num_labels } labels\" ) else : # TOKEN_CLASSIFICATION self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForTokenClassification with { self . config . model . num_labels } labels\" ) # Ensure model stays on CPU (device_map should handle this, but double-check) if self . model is not None and not use_8bit : # 8-bit models handle device_map automatically try : # Only move to CPU if not already there and not using device_map current_device = next ( self . model . parameters ()) . device if current_device . type != \"cpu\" : self . model = self . model . to ( \"cpu\" ) logger . info ( \"Model moved to CPU to avoid OOM during setup\" ) else : logger . info ( \"Model already on CPU (via device_map)\" ) except Exception as move_error : logger . warning ( f \"Could not verify/move model to CPU: { move_error } \" ) # Resize token embeddings if we added tokens if len ( self . tokenizer ) != self . model . config . vocab_size : self . model . resize_token_embeddings ( len ( self . tokenizer )) # CRITICAL: Set pad_token_id on model config (required for DataCollatorWithPadding) # This is often the missing piece that causes \"Cannot handle batch sizes > 1\" errors pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is not None and hasattr ( self . model , 'config' ): if hasattr ( self . model . config , 'pad_token_id' ): self . model . config . pad_token_id = pad_token_id # Also try setting on model.config if it's a dict-like object try : setattr ( self . model . config , 'pad_token_id' , pad_token_id ) except : pass logger . info ( f \"Set model.config.pad_token_id = { pad_token_id } \" ) # Enable gradient checkpointing to reduce memory usage (helps with OOM) if hasattr ( self . model , 'gradient_checkpointing_enable' ): try : self . model . gradient_checkpointing_enable () logger . info ( \"Gradient checkpointing enabled to reduce memory usage\" ) except Exception as e : logger . warning ( f \"Could not enable gradient checkpointing: { e } \" ) # Apply PEFT (LoRA) if enabled if self . config . model . peft_enabled : from peft import LoraConfig , get_peft_model # Get target modules from config or auto-detect target_modules = self . config . model . target_modules if target_modules is None : # Auto-detect based on model architecture if \"distilbert\" in self . config . model . name_or_path . lower (): target_modules = [ \"q_lin\" , \"k_lin\" , \"v_lin\" , \"out_lin\" ] elif \"bert\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] elif \"roberta\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] else : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] logger . info ( f \"Auto-detected target modules: { target_modules } \" ) # Create PEFT config peft_config = LoraConfig ( r = self . config . model . lora_rank , lora_alpha = self . config . model . lora_alpha , lora_dropout = self . config . model . lora_dropout , bias = self . config . model . bias , task_type = \"SEQ_CLS\" if self . task_type == TaskType . TEXT_CLASSIFICATION else \"TOKEN_CLS\" , target_modules = target_modules ) # Apply PEFT self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () print ( f \"\u2713 Model loaded successfully\" ) def setup_data ( self ): \"\"\"Load and prepare dataset.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading dataset: { self . config . dataset . name } \" ) logger . info ( f \"Loading dataset: { self . config . dataset . name } \" ) # Load dataset with trust_remote_code for compatibility try : if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split , trust_remote_code = True ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split , trust_remote_code = True ) except Exception as e : logger . error ( f \"Failed to load dataset with trust_remote_code: { e } \" ) # Fallback without trust_remote_code if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split ) # Limit samples if specified if self . config . dataset . max_samples : max_samples = min ( self . config . dataset . max_samples , len ( dataset )) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Limited dataset to { max_samples } samples\" ) # Apply column mapping if provided (must be done before splitting) if self . config . dataset . column_mapping : logger . info ( f \"Applying column mapping: { self . config . dataset . column_mapping } \" ) # Rename columns: old_name -> new_name dataset = dataset . rename_columns ( self . config . dataset . column_mapping ) logger . info ( f \"Renamed columns: { self . config . dataset . column_mapping } \" ) # Split into train/eval (90/10) split = dataset . train_test_split ( test_size = 0.1 , seed = 42 ) train_dataset = split [ 'train' ] eval_dataset = split [ 'test' ] # Tokenize based on task type if self . task_type == TaskType . TEXT_CLASSIFICATION : def tokenize_function ( examples ): \"\"\"Tokenize text classification data.\"\"\" result = self . tokenizer ( examples [ self . config . dataset . text_column ], truncation = True , padding = False , max_length = self . config . model . max_seq_length ) # Get labels and convert to 0-indexed if needed (PyTorch expects 0-indexed) labels = examples [ self . config . dataset . label_column ] if isinstance ( labels , list ) and len ( labels ) > 0 : min_label = min ( labels ) if min_label > 0 : # Labels are 1-indexed, convert to 0-indexed labels = [ l - 1 for l in labels ] result [ 'labels' ] = labels return result else : # TOKEN_CLASSIFICATION def tokenize_function ( examples ): \"\"\"Tokenize token classification data with label alignment.\"\"\" tokenized = self . tokenizer ( examples [ self . config . dataset . tokens_column ], truncation = True , is_split_into_words = True , padding = False , max_length = self . config . model . max_seq_length ) # Align labels with tokens labels = [] for i , label in enumerate ( examples [ self . config . dataset . tags_column ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : # Special tokens get -100 label_ids . append ( - 100 ) elif word_idx != previous_word_idx : # First token of a word gets the label label_ids . append ( label [ word_idx ]) else : # Other tokens of the same word get -100 label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ 'labels' ] = labels return tokenized # Apply tokenization self . train_dataset = train_dataset . map ( tokenize_function , batched = True , remove_columns = train_dataset . column_names , desc = \"Tokenizing train dataset\" ) self . eval_dataset = eval_dataset . map ( tokenize_function , batched = True , remove_columns = eval_dataset . column_names , desc = \"Tokenizing eval dataset\" ) print ( f \"\u2713 Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) logger . info ( f \"Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) def setup_training ( self ): \"\"\"Setup trainer with appropriate settings.\"\"\" from aligntune.core.sft.config import TaskType print ( \" \\n Configuring trainer...\" ) logger . info ( \"Configuring trainer...\" ) # Clear GPU cache before creating trainer to avoid OOM errors try : if torch . cuda . is_available (): torch . cuda . empty_cache () except Exception as cache_error : logger . warning ( f \"Could not clear GPU cache: { cache_error } \" ) # Training arguments # For quick testing: use max_steps=10, then revert to full training max_steps_for_testing = None # Set to None for full training, or 10 for quick testing training_args_dict = { \"output_dir\" : self . config . logging . output_dir , \"run_name\" : self . config . logging . run_name , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size , \"per_device_eval_batch_size\" : self . config . train . per_device_batch_size , \"learning_rate\" : self . config . train . learning_rate , \"weight_decay\" : self . config . train . weight_decay , \"warmup_ratio\" : self . config . train . warmup_ratio , \"eval_strategy\" : \"steps\" , \"eval_steps\" : self . config . train . eval_interval , \"save_strategy\" : \"steps\" , \"save_steps\" : self . config . train . save_interval , \"load_best_model_at_end\" : self . config . train . load_best_model_at_end , \"metric_for_best_model\" : self . config . train . metric_for_best_model , \"greater_is_better\" : self . config . train . greater_is_better , \"logging_steps\" : 10 , \"logging_dir\" : f \" { self . config . logging . output_dir } /logs\" , # Use precision from model config, default to BF16 if CUDA supports it (for memory efficiency) \"fp16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"fp16\" ) if hasattr ( self . config . model , 'precision' ) else False , \"bf16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"bf16\" ) if hasattr ( self . config . model , 'precision' ) else ( torch . cuda . is_available () and torch . cuda . is_bf16_supported ()), # Auto-enable BF16 if supported \"report_to\" : \"none\" , # Disable wandb/tensorboard \"save_total_limit\" : 2 , # Keep only 2 checkpoints \"push_to_hub\" : False } # Add gradient_accumulation_steps if specified in config (helps with OOM) if hasattr ( self . config . train , 'gradient_accumulation_steps' ) and self . config . train . gradient_accumulation_steps is not None : training_args_dict [ \"gradient_accumulation_steps\" ] = self . config . train . gradient_accumulation_steps # Set either max_steps (for quick testing) or num_train_epochs (for full training) # Don't pass num_train_epochs at all when max_steps is set (TrainingArguments doesn't like both) if max_steps_for_testing is not None : training_args_dict [ \"max_steps\" ] = max_steps_for_testing # Don't include num_train_epochs when max_steps is set else : training_args_dict [ \"num_train_epochs\" ] = self . config . train . epochs # Don't include max_steps when num_train_epochs is set training_args = TrainingArguments ( ** training_args_dict ) # Ensure pad_token_id is set before creating data collator (required for batching) if self . tokenizer is not None : if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id != self . tokenizer . unk_token_id : # Make sure it's not the unk token self . tokenizer . pad_token_id = pad_token_id else : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not set pad_token_id: { e } \" ) # Final check: ensure pad_token_id is set (critical for batching) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : logger . error ( \"pad_token_id is still None before creating data collator! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Choose data collator based on task # Explicitly pass pad_token_id to ensure it's used pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : raise ValueError ( \"pad_token_id must be set before creating data collator\" ) # Create a wrapper class to ensure pad_token_id is always set before batching class SafeDataCollatorWithPadding ( DataCollatorWithPadding ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : # Try to set it from eos_token_id if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # If still None, try to get it from pad_token if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) class SafeDataCollatorForTokenClassification ( DataCollatorForTokenClassification ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) if self . task_type == TaskType . TEXT_CLASSIFICATION : data_collator = SafeDataCollatorWithPadding ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 # Optimize for tensor cores ) else : # TOKEN_CLASSIFICATION data_collator = SafeDataCollatorForTokenClassification ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 ) # Compute metrics function def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics.\"\"\" predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) # For token classification, flatten and filter if self . task_type == TaskType . TOKEN_CLASSIFICATION : predictions = predictions . flatten () labels = labels . flatten () # Remove ignored indices (-100) mask = labels != - 100 predictions = predictions [ mask ] labels = labels [ mask ] # Calculate metrics return { 'accuracy' : accuracy_score ( labels , predictions ), 'f1' : f1_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'precision' : precision_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'recall' : recall_score ( labels , predictions , average = 'weighted' , zero_division = 0 ) } # Clear CUDA cache before creating trainer if torch . cuda . is_available (): torch . cuda . empty_cache () # Create trainer with error handling for OOM try : self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , data_collator = data_collator , compute_metrics = compute_metrics , tokenizer = self . tokenizer ) except ( torch . cuda . OutOfMemoryError , RuntimeError ) as oom_error : # Check if it's actually an OOM error is_oom = isinstance ( oom_error , torch . cuda . OutOfMemoryError ) or \"out of memory\" in str ( oom_error ) . lower () if is_oom : # Clear cache and try to provide helpful error message if torch . cuda . is_available (): torch . cuda . empty_cache () error_msg = ( f \"CUDA out of memory when creating trainer. \" f \"This usually means other processes are using GPU memory. \" f \"Try: (1) Free GPU memory by stopping other processes, \" f \"(2) Reduce batch_size in config, (3) Use a smaller model. \" f \"Original error: { str ( oom_error )[: 200 ] } \" ) logger . error ( error_msg ) raise RuntimeError ( error_msg ) from oom_error else : # Re-raise if it's not an OOM error raise print ( \"\u2713 Trainer configured\" ) logger . info ( \"Trainer configured successfully\" ) def train ( self ): \"\"\"Train the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"TRAINING\" ) print ( \"=\" * 80 ) self . setup_model () self . setup_data () self . setup_training () # Clear CUDA cache before training to avoid OOM if torch . cuda . is_available (): torch . cuda . empty_cache () logger . info ( \"Cleared CUDA cache before training\" ) logger . info ( \"Starting training...\" ) try : train_result = self . trainer . train () except RuntimeError as e : if \"out of memory\" in str ( e ) . lower () or \"OOM\" in str ( e ): if torch . cuda . is_available (): torch . cuda . empty_cache () logger . error ( f \"CUDA OOM during training: { e } \" ) print ( f \" \\n \u274c CUDA Out of Memory error during training.\" ) print ( \" Suggestions:\" ) print ( \" - Reduce batch_size in your config\" ) print ( \" - Increase gradient_accumulation_steps to maintain effective batch size\" ) print ( \" - Reduce max_seq_length\" ) print ( \" - Use a smaller model\" ) raise else : raise print ( \" \\n \u2713 Training completed\" ) logger . info ( \"Training completed\" ) return train_result def evaluate ( self ): \"\"\"Evaluate the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"EVALUATION\" ) print ( \"=\" * 80 ) # Ensure pad_token is set before evaluation (required for batching) if self . tokenizer is not None and self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Also ensure pad_token_id is set (critical for batching) if self . tokenizer is not None : pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id in evaluate(): { e } \" ) # Set pad_token_id if we found one if pad_token_id is not None : if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : error_msg = \"pad_token_id is still None in evaluate()! Cannot handle batch sizes > 1 without padding token.\" logger . error ( error_msg ) raise ValueError ( error_msg ) logger . info ( \"Starting evaluation...\" ) eval_result = self . trainer . evaluate () # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_result : try : eval_result [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_result [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) print ( \" \\n Evaluation Results:\" ) for key , value in eval_result . items (): if isinstance ( value , float ): print ( f \" { key } : { value : .4f } \" ) else : print ( f \" { key } : { value } \" ) logger . info ( f \"Evaluation completed: { len ( eval_result ) } metrics computed\" ) return eval_result def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : save_path = f \" { self . config . logging . output_dir } / { self . config . logging . run_name } /final_model\" else : save_path = output_dir print ( f \" \\n Saving model to { save_path } ...\" ) logger . info ( f \"Saving model to { save_path } \" ) self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) print ( f \"\u2713 Model saved\" ) logger . info ( \"Model saved successfully\" ) # Store for push_to_hub self . _last_save_path = save_path return save_path def run_experiment ( self ): \"\"\" Run complete training experiment. This is the main method that backend_factory calls! \"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( f \"CLASSIFICATION TRAINING - { self . task_type . value . upper () } \" ) print ( \"=\" * 80 ) logger . info ( f \"Starting classification experiment: { self . task_type . value } \" ) try : # Setup self . setup_model () self . setup_data () self . setup_training () # Train train_result = self . train () # Evaluate eval_result = self . evaluate () # Save model_path = self . save_model () # Return complete results results = { 'task_type' : self . task_type . value , 'train_loss' : train_result . training_loss , 'eval_results' : eval_result , 'model_path' : model_path , 'steps' : train_result . global_step } print ( \" \\n \" + \"=\" * 80 ) print ( \"\u2705 EXPERIMENT COMPLETE\" ) print ( \"=\" * 80 ) print ( f \" Task: { results [ 'task_type' ] } \" ) print ( f \" Train Loss: { results [ 'train_loss' ] : .4f } \" ) print ( f \" Eval Accuracy: { eval_result . get ( 'eval_accuracy' , 0 ) : .4f } \" ) print ( f \" Eval F1: { eval_result . get ( 'eval_f1' , 0 ) : .4f } \" ) print ( f \" Model: { model_path } \" ) print ( \"=\" * 80 ) logger . info ( f \"Experiment completed successfully: { results } \" ) return results except Exception as e : logger . error ( f \"Experiment failed: { e } \" , exc_info = True ) raise","title":"ClassificationTrainer"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.__init__","text":"Initialize trainer with config. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , config ): \"\"\"Initialize trainer with config.\"\"\" self . config = config self . model = None self . tokenizer = None self . train_dataset = None self . eval_dataset = None self . trainer = None self . task_type = config . dataset . task_type logger . info ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" ) print ( f \"\u2713 ClassificationTrainer initialized for { self . task_type . value } \" )","title":"__init__"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.evaluate","text":"Evaluate the model. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def evaluate ( self ): \"\"\"Evaluate the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"EVALUATION\" ) print ( \"=\" * 80 ) # Ensure pad_token is set before evaluation (required for batching) if self . tokenizer is not None and self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Also ensure pad_token_id is set (critical for batching) if self . tokenizer is not None : pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id in evaluate(): { e } \" ) # Set pad_token_id if we found one if pad_token_id is not None : if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : error_msg = \"pad_token_id is still None in evaluate()! Cannot handle batch sizes > 1 without padding token.\" logger . error ( error_msg ) raise ValueError ( error_msg ) logger . info ( \"Starting evaluation...\" ) eval_result = self . trainer . evaluate () # Calculate perplexity from loss (if available) if \"eval_loss\" in eval_result : try : eval_result [ \"eval_perplexity\" ] = float ( torch . exp ( torch . tensor ( float ( eval_result [ \"eval_loss\" ]))) . item () ) except Exception as e : logger . debug ( f \"Could not calculate perplexity: { e } \" ) print ( \" \\n Evaluation Results:\" ) for key , value in eval_result . items (): if isinstance ( value , float ): print ( f \" { key } : { value : .4f } \" ) else : print ( f \" { key } : { value } \" ) logger . info ( f \"Evaluation completed: { len ( eval_result ) } metrics computed\" ) return eval_result","title":"evaluate"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.run_experiment","text":"Run complete training experiment. This is the main method that backend_factory calls! Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 def run_experiment ( self ): \"\"\" Run complete training experiment. This is the main method that backend_factory calls! \"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( f \"CLASSIFICATION TRAINING - { self . task_type . value . upper () } \" ) print ( \"=\" * 80 ) logger . info ( f \"Starting classification experiment: { self . task_type . value } \" ) try : # Setup self . setup_model () self . setup_data () self . setup_training () # Train train_result = self . train () # Evaluate eval_result = self . evaluate () # Save model_path = self . save_model () # Return complete results results = { 'task_type' : self . task_type . value , 'train_loss' : train_result . training_loss , 'eval_results' : eval_result , 'model_path' : model_path , 'steps' : train_result . global_step } print ( \" \\n \" + \"=\" * 80 ) print ( \"\u2705 EXPERIMENT COMPLETE\" ) print ( \"=\" * 80 ) print ( f \" Task: { results [ 'task_type' ] } \" ) print ( f \" Train Loss: { results [ 'train_loss' ] : .4f } \" ) print ( f \" Eval Accuracy: { eval_result . get ( 'eval_accuracy' , 0 ) : .4f } \" ) print ( f \" Eval F1: { eval_result . get ( 'eval_f1' , 0 ) : .4f } \" ) print ( f \" Model: { model_path } \" ) print ( \"=\" * 80 ) logger . info ( f \"Experiment completed successfully: { results } \" ) return results except Exception as e : logger . error ( f \"Experiment failed: { e } \" , exc_info = True ) raise","title":"run_experiment"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.save_model","text":"Save the trained model. Parameters: Name Type Description Default output_dir Optional [ str ] Directory to save to (defaults to config output_dir) None Returns: Type Description str Path where model was saved Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 def save_model ( self , output_dir : Optional [ str ] = None ) -> str : \"\"\"Save the trained model. Args: output_dir: Directory to save to (defaults to config output_dir) Returns: Path where model was saved \"\"\" if output_dir is None : save_path = f \" { self . config . logging . output_dir } / { self . config . logging . run_name } /final_model\" else : save_path = output_dir print ( f \" \\n Saving model to { save_path } ...\" ) logger . info ( f \"Saving model to { save_path } \" ) self . trainer . save_model ( save_path ) self . tokenizer . save_pretrained ( save_path ) print ( f \"\u2713 Model saved\" ) logger . info ( \"Model saved successfully\" ) # Store for push_to_hub self . _last_save_path = save_path return save_path","title":"save_model"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.setup_data","text":"Load and prepare dataset. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def setup_data ( self ): \"\"\"Load and prepare dataset.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading dataset: { self . config . dataset . name } \" ) logger . info ( f \"Loading dataset: { self . config . dataset . name } \" ) # Load dataset with trust_remote_code for compatibility try : if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split , trust_remote_code = True ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split , trust_remote_code = True ) except Exception as e : logger . error ( f \"Failed to load dataset with trust_remote_code: { e } \" ) # Fallback without trust_remote_code if self . config . dataset . subset : dataset = load_dataset ( self . config . dataset . name , self . config . dataset . subset , split = self . config . dataset . split ) else : dataset = load_dataset ( self . config . dataset . name , split = self . config . dataset . split ) # Limit samples if specified if self . config . dataset . max_samples : max_samples = min ( self . config . dataset . max_samples , len ( dataset )) dataset = dataset . select ( range ( max_samples )) logger . info ( f \"Limited dataset to { max_samples } samples\" ) # Apply column mapping if provided (must be done before splitting) if self . config . dataset . column_mapping : logger . info ( f \"Applying column mapping: { self . config . dataset . column_mapping } \" ) # Rename columns: old_name -> new_name dataset = dataset . rename_columns ( self . config . dataset . column_mapping ) logger . info ( f \"Renamed columns: { self . config . dataset . column_mapping } \" ) # Split into train/eval (90/10) split = dataset . train_test_split ( test_size = 0.1 , seed = 42 ) train_dataset = split [ 'train' ] eval_dataset = split [ 'test' ] # Tokenize based on task type if self . task_type == TaskType . TEXT_CLASSIFICATION : def tokenize_function ( examples ): \"\"\"Tokenize text classification data.\"\"\" result = self . tokenizer ( examples [ self . config . dataset . text_column ], truncation = True , padding = False , max_length = self . config . model . max_seq_length ) # Get labels and convert to 0-indexed if needed (PyTorch expects 0-indexed) labels = examples [ self . config . dataset . label_column ] if isinstance ( labels , list ) and len ( labels ) > 0 : min_label = min ( labels ) if min_label > 0 : # Labels are 1-indexed, convert to 0-indexed labels = [ l - 1 for l in labels ] result [ 'labels' ] = labels return result else : # TOKEN_CLASSIFICATION def tokenize_function ( examples ): \"\"\"Tokenize token classification data with label alignment.\"\"\" tokenized = self . tokenizer ( examples [ self . config . dataset . tokens_column ], truncation = True , is_split_into_words = True , padding = False , max_length = self . config . model . max_seq_length ) # Align labels with tokens labels = [] for i , label in enumerate ( examples [ self . config . dataset . tags_column ]): word_ids = tokenized . word_ids ( batch_index = i ) label_ids = [] previous_word_idx = None for word_idx in word_ids : if word_idx is None : # Special tokens get -100 label_ids . append ( - 100 ) elif word_idx != previous_word_idx : # First token of a word gets the label label_ids . append ( label [ word_idx ]) else : # Other tokens of the same word get -100 label_ids . append ( - 100 ) previous_word_idx = word_idx labels . append ( label_ids ) tokenized [ 'labels' ] = labels return tokenized # Apply tokenization self . train_dataset = train_dataset . map ( tokenize_function , batched = True , remove_columns = train_dataset . column_names , desc = \"Tokenizing train dataset\" ) self . eval_dataset = eval_dataset . map ( tokenize_function , batched = True , remove_columns = eval_dataset . column_names , desc = \"Tokenizing eval dataset\" ) print ( f \"\u2713 Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" ) logger . info ( f \"Dataset loaded - Train: { len ( self . train_dataset ) } , Eval: { len ( self . eval_dataset ) } \" )","title":"setup_data"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.setup_model","text":"Load model and tokenizer based on task type. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def setup_model ( self ): \"\"\"Load model and tokenizer based on task type.\"\"\" from aligntune.core.sft.config import TaskType print ( f \" \\n Loading model: { self . config . model . name_or_path } \" ) logger . info ( f \"Loading model: { self . config . model . name_or_path } \" ) # Load tokenizer self . tokenizer = AutoTokenizer . from_pretrained ( self . config . model . name_or_path , use_fast = True ) # Add padding token if missing if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) # Ensure pad_token_id is set (required for data collator batching) # For Llama and similar tokenizers, we need to set it on both tokenizer and config pad_token_id = None if hasattr ( self . tokenizer , 'pad_token_id' ): pad_token_id = self . tokenizer . pad_token_id if pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : # If pad_token was added, get its ID from the tokenizer try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id == self . tokenizer . unk_token_id : # If it's the unk token, use eos instead if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not get pad_token_id from tokenizer: { e } \" ) # Set pad_token_id on tokenizer (multiple ways to ensure it sticks) if pad_token_id is not None : # Set directly on tokenizer if hasattr ( self . tokenizer , 'pad_token_id' ): self . tokenizer . pad_token_id = pad_token_id # Also set on tokenizer's special_tokens_map if it exists if hasattr ( self . tokenizer , 'special_tokens_map' ): if 'pad_token' not in self . tokenizer . special_tokens_map : self . tokenizer . special_tokens_map [ 'pad_token' ] = self . tokenizer . pad_token # Set on tokenizer config if it exists if hasattr ( self . tokenizer , 'init_kwargs' ) and 'pad_token_id' in self . tokenizer . init_kwargs : self . tokenizer . init_kwargs [ 'pad_token_id' ] = pad_token_id # Also try setting via __setattr__ to ensure it persists try : object . __setattr__ ( self . tokenizer , 'pad_token_id' , pad_token_id ) except : pass # Final check: ensure pad_token_id is set (critical for batching) final_pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if final_pad_token_id is None : logger . error ( \"pad_token_id is still None after setup! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Determine precision and quantization settings use_8bit = False torch_dtype = None device_map = \"cpu\" # Keep on CPU initially # Check if precision is set and configure accordingly if hasattr ( self . config . model , 'precision' ) and self . config . model . precision : precision_value = self . config . model . precision . value if hasattr ( self . config . model . precision , 'value' ) else str ( self . config . model . precision ) if precision_value == \"bf16\" and torch . cuda . is_available () and torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 elif precision_value == \"fp16\" : torch_dtype = torch . float16 # For 4B+ models, use lower precision dtype to reduce memory # NOTE: 8-bit quantization (BitsAndBytes) requires PEFT adapters for fine-tuning # Since we're not using PEFT for classification, we skip 8-bit quantization # and rely on BF16/FP16 precision instead model_size_estimate = \"4B\" if \"4B\" in self . config . model . name_or_path or \"4b\" in self . config . model . name_or_path . lower () else \"unknown\" use_8bit = False # Disable 8-bit quantization for classification (requires PEFT) quantization_config = None # Ensure we use lower precision dtype for 4B models if torch_dtype is None and torch . cuda . is_available (): # Default to BF16 if supported, else FP16 if torch . cuda . is_bf16_supported (): torch_dtype = torch . bfloat16 logger . info ( \"Using BF16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) else : torch_dtype = torch . float16 logger . info ( \"Using FP16 precision for 4B model to reduce memory usage (8-bit quantization disabled - requires PEFT)\" ) # Load appropriate model based on task # Load on CPU first with appropriate precision/quantization to avoid OOM model_kwargs = { \"num_labels\" : self . config . model . num_labels , \"ignore_mismatched_sizes\" : True , \"device_map\" : device_map , # Keep on CPU \"torch_dtype\" : torch_dtype if torch_dtype else None } # Add quantization config if using 8-bit if use_8bit and quantization_config : model_kwargs [ \"quantization_config\" ] = quantization_config # Remove None values model_kwargs = { k : v for k , v in model_kwargs . items () if v is not None } if self . task_type == TaskType . TEXT_CLASSIFICATION : self . model = AutoModelForSequenceClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForSequenceClassification with { self . config . model . num_labels } labels\" ) else : # TOKEN_CLASSIFICATION self . model = AutoModelForTokenClassification . from_pretrained ( self . config . model . name_or_path , ** model_kwargs ) logger . info ( f \"Loaded AutoModelForTokenClassification with { self . config . model . num_labels } labels\" ) # Ensure model stays on CPU (device_map should handle this, but double-check) if self . model is not None and not use_8bit : # 8-bit models handle device_map automatically try : # Only move to CPU if not already there and not using device_map current_device = next ( self . model . parameters ()) . device if current_device . type != \"cpu\" : self . model = self . model . to ( \"cpu\" ) logger . info ( \"Model moved to CPU to avoid OOM during setup\" ) else : logger . info ( \"Model already on CPU (via device_map)\" ) except Exception as move_error : logger . warning ( f \"Could not verify/move model to CPU: { move_error } \" ) # Resize token embeddings if we added tokens if len ( self . tokenizer ) != self . model . config . vocab_size : self . model . resize_token_embeddings ( len ( self . tokenizer )) # CRITICAL: Set pad_token_id on model config (required for DataCollatorWithPadding) # This is often the missing piece that causes \"Cannot handle batch sizes > 1\" errors pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is not None and hasattr ( self . model , 'config' ): if hasattr ( self . model . config , 'pad_token_id' ): self . model . config . pad_token_id = pad_token_id # Also try setting on model.config if it's a dict-like object try : setattr ( self . model . config , 'pad_token_id' , pad_token_id ) except : pass logger . info ( f \"Set model.config.pad_token_id = { pad_token_id } \" ) # Enable gradient checkpointing to reduce memory usage (helps with OOM) if hasattr ( self . model , 'gradient_checkpointing_enable' ): try : self . model . gradient_checkpointing_enable () logger . info ( \"Gradient checkpointing enabled to reduce memory usage\" ) except Exception as e : logger . warning ( f \"Could not enable gradient checkpointing: { e } \" ) # Apply PEFT (LoRA) if enabled if self . config . model . peft_enabled : from peft import LoraConfig , get_peft_model # Get target modules from config or auto-detect target_modules = self . config . model . target_modules if target_modules is None : # Auto-detect based on model architecture if \"distilbert\" in self . config . model . name_or_path . lower (): target_modules = [ \"q_lin\" , \"k_lin\" , \"v_lin\" , \"out_lin\" ] elif \"bert\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] elif \"roberta\" in self . config . model . name_or_path . lower (): target_modules = [ \"query\" , \"key\" , \"value\" , \"dense\" ] else : target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ] logger . info ( f \"Auto-detected target modules: { target_modules } \" ) # Create PEFT config peft_config = LoraConfig ( r = self . config . model . lora_rank , lora_alpha = self . config . model . lora_alpha , lora_dropout = self . config . model . lora_dropout , bias = self . config . model . bias , task_type = \"SEQ_CLS\" if self . task_type == TaskType . TEXT_CLASSIFICATION else \"TOKEN_CLS\" , target_modules = target_modules ) # Apply PEFT self . model = get_peft_model ( self . model , peft_config ) self . model . print_trainable_parameters () print ( f \"\u2713 Model loaded successfully\" )","title":"setup_model"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.setup_training","text":"Setup trainer with appropriate settings. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def setup_training ( self ): \"\"\"Setup trainer with appropriate settings.\"\"\" from aligntune.core.sft.config import TaskType print ( \" \\n Configuring trainer...\" ) logger . info ( \"Configuring trainer...\" ) # Clear GPU cache before creating trainer to avoid OOM errors try : if torch . cuda . is_available (): torch . cuda . empty_cache () except Exception as cache_error : logger . warning ( f \"Could not clear GPU cache: { cache_error } \" ) # Training arguments # For quick testing: use max_steps=10, then revert to full training max_steps_for_testing = None # Set to None for full training, or 10 for quick testing training_args_dict = { \"output_dir\" : self . config . logging . output_dir , \"run_name\" : self . config . logging . run_name , \"per_device_train_batch_size\" : self . config . train . per_device_batch_size , \"per_device_eval_batch_size\" : self . config . train . per_device_batch_size , \"learning_rate\" : self . config . train . learning_rate , \"weight_decay\" : self . config . train . weight_decay , \"warmup_ratio\" : self . config . train . warmup_ratio , \"eval_strategy\" : \"steps\" , \"eval_steps\" : self . config . train . eval_interval , \"save_strategy\" : \"steps\" , \"save_steps\" : self . config . train . save_interval , \"load_best_model_at_end\" : self . config . train . load_best_model_at_end , \"metric_for_best_model\" : self . config . train . metric_for_best_model , \"greater_is_better\" : self . config . train . greater_is_better , \"logging_steps\" : 10 , \"logging_dir\" : f \" { self . config . logging . output_dir } /logs\" , # Use precision from model config, default to BF16 if CUDA supports it (for memory efficiency) \"fp16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"fp16\" ) if hasattr ( self . config . model , 'precision' ) else False , \"bf16\" : ( hasattr ( self . config . model , 'precision' ) and hasattr ( self . config . model . precision , 'value' ) and self . config . model . precision . value == \"bf16\" ) if hasattr ( self . config . model , 'precision' ) else ( torch . cuda . is_available () and torch . cuda . is_bf16_supported ()), # Auto-enable BF16 if supported \"report_to\" : \"none\" , # Disable wandb/tensorboard \"save_total_limit\" : 2 , # Keep only 2 checkpoints \"push_to_hub\" : False } # Add gradient_accumulation_steps if specified in config (helps with OOM) if hasattr ( self . config . train , 'gradient_accumulation_steps' ) and self . config . train . gradient_accumulation_steps is not None : training_args_dict [ \"gradient_accumulation_steps\" ] = self . config . train . gradient_accumulation_steps # Set either max_steps (for quick testing) or num_train_epochs (for full training) # Don't pass num_train_epochs at all when max_steps is set (TrainingArguments doesn't like both) if max_steps_for_testing is not None : training_args_dict [ \"max_steps\" ] = max_steps_for_testing # Don't include num_train_epochs when max_steps is set else : training_args_dict [ \"num_train_epochs\" ] = self . config . train . epochs # Don't include max_steps when num_train_epochs is set training_args = TrainingArguments ( ** training_args_dict ) # Ensure pad_token_id is set before creating data collator (required for batching) if self . tokenizer is not None : if self . tokenizer . pad_token is None : self . tokenizer . pad_token = self . tokenizer . eos_token if self . tokenizer . pad_token is None : self . tokenizer . add_special_tokens ({ 'pad_token' : '[PAD]' }) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id elif self . tokenizer . pad_token is not None : try : pad_token_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_token_id != self . tokenizer . unk_token_id : # Make sure it's not the unk token self . tokenizer . pad_token_id = pad_token_id else : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except Exception as e : # Fallback: use eos_token_id if available if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id else : logger . warning ( f \"Could not set pad_token_id: { e } \" ) # Final check: ensure pad_token_id is set (critical for batching) if hasattr ( self . tokenizer , 'pad_token_id' ) and self . tokenizer . pad_token_id is None : logger . error ( \"pad_token_id is still None before creating data collator! This will cause batching errors.\" ) raise ValueError ( \"pad_token_id must be set for batching. Tokenizer configuration issue.\" ) # Choose data collator based on task # Explicitly pass pad_token_id to ensure it's used pad_token_id = getattr ( self . tokenizer , 'pad_token_id' , None ) if pad_token_id is None : raise ValueError ( \"pad_token_id must be set before creating data collator\" ) # Create a wrapper class to ensure pad_token_id is always set before batching class SafeDataCollatorWithPadding ( DataCollatorWithPadding ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : # Try to set it from eos_token_id if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id # If still None, try to get it from pad_token if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) class SafeDataCollatorForTokenClassification ( DataCollatorForTokenClassification ): def __call__ ( self , features ): # Ensure pad_token_id is set before batching (critical fix) if self . tokenizer is not None : if getattr ( self . tokenizer , 'pad_token_id' , None ) is None : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id if getattr ( self . tokenizer , 'pad_token_id' , None ) is None and self . tokenizer . pad_token is not None : try : pad_id = self . tokenizer . convert_tokens_to_ids ( self . tokenizer . pad_token ) if pad_id != self . tokenizer . unk_token_id : self . tokenizer . pad_token_id = pad_id elif hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id except : if hasattr ( self . tokenizer , 'eos_token_id' ) and self . tokenizer . eos_token_id is not None : self . tokenizer . pad_token_id = self . tokenizer . eos_token_id return super () . __call__ ( features ) if self . task_type == TaskType . TEXT_CLASSIFICATION : data_collator = SafeDataCollatorWithPadding ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 # Optimize for tensor cores ) else : # TOKEN_CLASSIFICATION data_collator = SafeDataCollatorForTokenClassification ( tokenizer = self . tokenizer , pad_to_multiple_of = 8 ) # Compute metrics function def compute_metrics ( eval_pred ): \"\"\"Compute classification metrics.\"\"\" predictions , labels = eval_pred predictions = np . argmax ( predictions , axis =- 1 ) # For token classification, flatten and filter if self . task_type == TaskType . TOKEN_CLASSIFICATION : predictions = predictions . flatten () labels = labels . flatten () # Remove ignored indices (-100) mask = labels != - 100 predictions = predictions [ mask ] labels = labels [ mask ] # Calculate metrics return { 'accuracy' : accuracy_score ( labels , predictions ), 'f1' : f1_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'precision' : precision_score ( labels , predictions , average = 'weighted' , zero_division = 0 ), 'recall' : recall_score ( labels , predictions , average = 'weighted' , zero_division = 0 ) } # Clear CUDA cache before creating trainer if torch . cuda . is_available (): torch . cuda . empty_cache () # Create trainer with error handling for OOM try : self . trainer = Trainer ( model = self . model , args = training_args , train_dataset = self . train_dataset , eval_dataset = self . eval_dataset , data_collator = data_collator , compute_metrics = compute_metrics , tokenizer = self . tokenizer ) except ( torch . cuda . OutOfMemoryError , RuntimeError ) as oom_error : # Check if it's actually an OOM error is_oom = isinstance ( oom_error , torch . cuda . OutOfMemoryError ) or \"out of memory\" in str ( oom_error ) . lower () if is_oom : # Clear cache and try to provide helpful error message if torch . cuda . is_available (): torch . cuda . empty_cache () error_msg = ( f \"CUDA out of memory when creating trainer. \" f \"This usually means other processes are using GPU memory. \" f \"Try: (1) Free GPU memory by stopping other processes, \" f \"(2) Reduce batch_size in config, (3) Use a smaller model. \" f \"Original error: { str ( oom_error )[: 200 ] } \" ) logger . error ( error_msg ) raise RuntimeError ( error_msg ) from oom_error else : # Re-raise if it's not an OOM error raise print ( \"\u2713 Trainer configured\" ) logger . info ( \"Trainer configured successfully\" )","title":"setup_training"},{"location":"api-reference/trainers/#backends.trl.sft.Classification_trainer.ClassificationTrainer.train","text":"Train the model. Source code in src/aligntune/backends/trl/sft/Classification_trainer.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 def train ( self ): \"\"\"Train the model.\"\"\" print ( \" \\n \" + \"=\" * 80 ) print ( \"TRAINING\" ) print ( \"=\" * 80 ) self . setup_model () self . setup_data () self . setup_training () # Clear CUDA cache before training to avoid OOM if torch . cuda . is_available (): torch . cuda . empty_cache () logger . info ( \"Cleared CUDA cache before training\" ) logger . info ( \"Starting training...\" ) try : train_result = self . trainer . train () except RuntimeError as e : if \"out of memory\" in str ( e ) . lower () or \"OOM\" in str ( e ): if torch . cuda . is_available (): torch . cuda . empty_cache () logger . error ( f \"CUDA OOM during training: { e } \" ) print ( f \" \\n \u274c CUDA Out of Memory error during training.\" ) print ( \" Suggestions:\" ) print ( \" - Reduce batch_size in your config\" ) print ( \" - Increase gradient_accumulation_steps to maintain effective batch size\" ) print ( \" - Reduce max_seq_length\" ) print ( \" - Use a smaller model\" ) raise else : raise print ( \" \\n \u2713 Training completed\" ) logger . info ( \"Training completed\" ) return train_result options: show_source: true heading_level: 3 Note: Only available for TRL backend.","title":"train"},{"location":"api-reference/trainers/#usage-examples","text":"","title":"Usage Examples"},{"location":"api-reference/trainers/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Predict result = trainer . predict ( \"What is AI?\" )","title":"SFT Training"},{"location":"api-reference/trainers/#rl-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) # Train results = trainer . train () # Evaluate metrics = trainer . evaluate () # Save model_path = trainer . save_model () # Push to Hub url = trainer . push_to_hub ( \"username/my-model\" )","title":"RL Training"},{"location":"api-reference/trainers/#next-steps","text":"Backend Factory - Trainer creation functions Configuration Classes - Configuration options User Guide - Usage guide","title":"Next Steps"},{"location":"api-reference/unified-api/","text":"Unified RLHF API Documentation \u00b6 This document describes the unified RLHF training system that provides a consistent interface for training with PPO, DPO, GRPO, GSPO, DAPO, Dr. GRPO, GBMPO, Counterfactual GRPO, and BOLT algorithms. Overview \u00b6 The unified system provides: - Consistent API : Same interface for all RLHF algorithms - No Interactive Prompts : Everything is configuration-driven - No Placeholder Defaults : All critical parameters must be explicitly provided - Distributed Training : Support for DDP, FSDP, and DeepSpeed - Comprehensive Logging : TensorBoard and WandB integration - Dataset Caching : Efficient caching with hash-based keys - Extensible : Easy to add new algorithms, datasets, and rewards Quick Start \u00b6 Python Library Usage \u00b6 from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , ConfigLoader , TrainerFactory ) # Create configuration config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-small\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , percent = 1 )], train = TrainingConfig ( max_steps = 100 ) ) # Create and run trainer trainer = TrainerFactory . create_trainer ( config ) trainer . train () CLI Usage \u00b6 # Train with YAML config python -m aligntune.cli.train_unified --config examples/configs/ppo_minimal.yaml # Train with CLI arguments python -m aligntune.cli.train_unified \\ --algo ppo \\ --model microsoft/DialoGPT-small \\ --dataset Anthropic/hh-rlhf:train#percent = 1 \\ --max-steps 100 Configuration \u00b6 UnifiedConfig \u00b6 The main configuration class that contains all training parameters: @dataclass class UnifiedConfig : algo : AlgorithmType # Algorithm to use model : ModelConfig # Model configuration datasets : List [ DatasetConfig ] # Dataset configurations tasks : List [ Dict [ str , Any ]] # Task definitions rewards : List [ RewardConfig ] # Reward functions train : TrainingConfig # Training parameters distributed : DistributedConfig # Distributed training logging : LoggingConfig # Logging configuration chat_template : Optional [ str ] # Chat template caching : Dict [ str , Any ] # Caching configuration Algorithm Types \u00b6 class AlgorithmType ( Enum ): PPO = \"ppo\" # Proximal Policy Optimization DPO = \"dpo\" # Direct Preference Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective DAPO = \"dapo\" # Decouple Clip and Dynamic sAmpling Policy Optimization DRGRPO = \"drgrpo\" # Dr. GRPO (GRPO Done Right) GBMPO = \"gbmpo\" # Group-Based Mirror Policy Optimization COUNTERFACT_GRPO = \"counterfact_grpo\" # Counterfactual GRPO BOLT = \"bolt\" # Baseline-Optimized Learning Technique Model Configuration \u00b6 @dataclass class ModelConfig : name_or_path : str # Model name or path (required) sft_path : Optional [ str ] # SFT checkpoint path reward_path : Optional [ str ] # Reward model path precision : PrecisionType # Model precision quantization : Dict [ str , Any ] # Quantization settings attn_implementation : str # Attention implementation gradient_checkpointing : bool # Enable gradient checkpointing max_memory : Optional [ Dict [ str , str ]] # Memory limits per device Dataset Configuration \u00b6 @dataclass class DatasetConfig : name : str # Dataset name (required) split : str # Dataset split percent : Optional [ float ] # Percentage of dataset to use max_samples : Optional [ int ] # Maximum number of samples column_mapping : Dict [ str , str ] # Column mapping task_type : str # Task type weight : float # Dataset weight chat_template : Optional [ str ] # Chat template Training Configuration \u00b6 @dataclass class TrainingConfig : per_device_batch_size : int # Batch size per device gradient_accumulation_steps : int # Gradient accumulation steps max_steps : Optional [ int ] # Maximum training steps epochs : Optional [ int ] # Number of epochs eval_interval : int # Evaluation interval save_interval : int # Save interval rollout_batch_size : int # Rollout batch size kl_coef : float # KL coefficient cliprange : float # PPO clip range num_ppo_epochs : Optional [ int ] # PPO epochs learning_rate : float # Learning rate Dataset Specifications \u00b6 Format \u00b6 name[:split][#percent=N|max=N][?map.key=value] Examples \u00b6 # Basic dataset Anthropic/hh-rlhf:train#percent = 25 # With column mapping HuggingFaceH4/ultrafeedback_binarized:train?map.prompt = prompt & map.chosen = chosen & map.rejected = rejected # With max samples gsm8k:train#max = 1000 Reward Specifications \u00b6 Format \u00b6 type:weight:param1=value1:param2=value2 Examples \u00b6 # Length reward length:weight = 1 .0:min_length = 10 :max_length = 200 # Math reward math:weight = 0 .8:tolerance = 1e-6 # Safety reward with shield safety:weight = 0 .5:strict = true Built-in Components \u00b6 Dataset Loaders \u00b6 huggingface : Load from HuggingFace Hub local : Load from local files Schema Adapters \u00b6 chat : Chat format (messages) hh-rlhf : Anthropic HH-RLHF format ultrafeedback : UltraFeedback format alpaca : Alpaca format Reward Functions \u00b6 length : Text length reward sentiment : Sentiment analysis reward coherence : Text coherence reward math : Mathematical correctness reward code : Code quality reward safety : Safety check reward Tasks \u00b6 conversation : Conversational AI classification : Text classification summarization : Text summarization math : Mathematical reasoning code : Code generation Distributed Training \u00b6 Backends \u00b6 single : Single GPU/CPU training ddp : DistributedDataParallel via accelerate fsdp : FullyShardedDataParallel deepspeed : DeepSpeed ZeRO-2/3 Example Configuration \u00b6 distributed : backend : ddp nodes : 1 gpus_per_node : 2 seed : 42 Logging \u00b6 Supported Loggers \u00b6 tensorboard : TensorBoard logging wandb : Weights & Biases logging Example Configuration \u00b6 logging : loggers : [ \"tensorboard\" , \"wandb\" ] output_dir : \"./output\" run_name : \"my_training_run\" Caching \u00b6 Configuration \u00b6 caching : root : \"./cache\" enabled : true Features \u00b6 Hash-based cache keys Automatic cache validation Cache cleanup utilities Support for multiple datasets Examples \u00b6 Minimal PPO Training \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-small\" precision : bf16 datasets : - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 1 train : per_device_batch_size : 1 max_steps : 100 eval_interval : 50 distributed : backend : single logging : output_dir : \"./output\" Multi-task Training \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" precision : bf16 datasets : - name : \"gsm8k\" split : \"train\" max_samples : 5000 task_type : \"math\" weight : 0.4 - name : \"sahil2801/CodeAlpaca-20k\" split : \"train\" percent : 20 task_type : \"code\" weight : 0.3 - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 10 task_type : \"conversation\" weight : 0.3 tasks : - name : \"math\" weight : 0.4 - name : \"code\" weight : 0.3 - name : \"conversation\" weight : 0.3 rewards : - type : \"math\" weight : 0.4 - type : \"code\" weight : 0.3 - type : \"length\" weight : 0.2 - type : \"safety\" weight : 0.1 shield : true Migration Guide \u00b6 From Old API \u00b6 Replace interactive prompts : Use YAML configuration files Update imports : Use unified system imports Update configuration : Use UnifiedConfig instead of algorithm-specific configs Update training : Use TrainerFactory.create_trainer() Example Migration \u00b6 Old (Interactive) : from aligntune import PPOTrainer trainer = PPOTrainer () trainer . setup_interactive () # Interactive prompts trainer . train () New (Unified) : from aligntune import UnifiedConfig , TrainerFactory config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"model_name\" ), datasets = [ DatasetConfig ( name = \"dataset_name\" )], train = TrainingConfig ( max_steps = 1000 ) ) trainer = TrainerFactory . create_trainer ( config ) trainer . train () Troubleshooting \u00b6 Common Issues \u00b6 Missing required fields : All critical parameters must be explicitly provided Invalid dataset specs : Check dataset specification format Invalid reward specs : Check reward specification format Import errors : Ensure all dependencies are installed Debug Mode \u00b6 Enable debug logging: import logging logging . basicConfig ( level = logging . DEBUG ) Validation \u00b6 Validate configuration before training: from aligntune import ConfigLoader ConfigLoader . validate_config ( config ) API Reference \u00b6 Core Classes \u00b6 UnifiedConfig : Main configuration class ConfigLoader : Configuration loading and validation TrainerFactory : Trainer creation factory TrainerBase : Abstract base trainer class Registries \u00b6 DatasetRegistry : Dataset loader registry RewardRegistry : Reward function registry TaskRegistry : Task definition registry Utilities \u00b6 UnifiedLogger : Logging system UnifiedEvaluator : Evaluation system DatasetCache : Dataset caching system RolloutEngine : Rollout generation engine Model Wrappers \u00b6 PolicyModel : Policy model wrapper ReferenceModel : Reference model wrapper ValueModel : Value model wrapper","title":"Unified RLHF API Documentation"},{"location":"api-reference/unified-api/#unified-rlhf-api-documentation","text":"This document describes the unified RLHF training system that provides a consistent interface for training with PPO, DPO, GRPO, GSPO, DAPO, Dr. GRPO, GBMPO, Counterfactual GRPO, and BOLT algorithms.","title":"Unified RLHF API Documentation"},{"location":"api-reference/unified-api/#overview","text":"The unified system provides: - Consistent API : Same interface for all RLHF algorithms - No Interactive Prompts : Everything is configuration-driven - No Placeholder Defaults : All critical parameters must be explicitly provided - Distributed Training : Support for DDP, FSDP, and DeepSpeed - Comprehensive Logging : TensorBoard and WandB integration - Dataset Caching : Efficient caching with hash-based keys - Extensible : Easy to add new algorithms, datasets, and rewards","title":"Overview"},{"location":"api-reference/unified-api/#quick-start","text":"","title":"Quick Start"},{"location":"api-reference/unified-api/#python-library-usage","text":"from aligntune import ( UnifiedConfig , AlgorithmType , ModelConfig , DatasetConfig , TrainingConfig , ConfigLoader , TrainerFactory ) # Create configuration config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"microsoft/DialoGPT-small\" ), datasets = [ DatasetConfig ( name = \"Anthropic/hh-rlhf\" , percent = 1 )], train = TrainingConfig ( max_steps = 100 ) ) # Create and run trainer trainer = TrainerFactory . create_trainer ( config ) trainer . train ()","title":"Python Library Usage"},{"location":"api-reference/unified-api/#cli-usage","text":"# Train with YAML config python -m aligntune.cli.train_unified --config examples/configs/ppo_minimal.yaml # Train with CLI arguments python -m aligntune.cli.train_unified \\ --algo ppo \\ --model microsoft/DialoGPT-small \\ --dataset Anthropic/hh-rlhf:train#percent = 1 \\ --max-steps 100","title":"CLI Usage"},{"location":"api-reference/unified-api/#configuration","text":"","title":"Configuration"},{"location":"api-reference/unified-api/#unifiedconfig","text":"The main configuration class that contains all training parameters: @dataclass class UnifiedConfig : algo : AlgorithmType # Algorithm to use model : ModelConfig # Model configuration datasets : List [ DatasetConfig ] # Dataset configurations tasks : List [ Dict [ str , Any ]] # Task definitions rewards : List [ RewardConfig ] # Reward functions train : TrainingConfig # Training parameters distributed : DistributedConfig # Distributed training logging : LoggingConfig # Logging configuration chat_template : Optional [ str ] # Chat template caching : Dict [ str , Any ] # Caching configuration","title":"UnifiedConfig"},{"location":"api-reference/unified-api/#algorithm-types","text":"class AlgorithmType ( Enum ): PPO = \"ppo\" # Proximal Policy Optimization DPO = \"dpo\" # Direct Preference Optimization GRPO = \"grpo\" # Group Relative Policy Optimization GSPO = \"gspo\" # Generalized Scoring Proximal Objective DAPO = \"dapo\" # Decouple Clip and Dynamic sAmpling Policy Optimization DRGRPO = \"drgrpo\" # Dr. GRPO (GRPO Done Right) GBMPO = \"gbmpo\" # Group-Based Mirror Policy Optimization COUNTERFACT_GRPO = \"counterfact_grpo\" # Counterfactual GRPO BOLT = \"bolt\" # Baseline-Optimized Learning Technique","title":"Algorithm Types"},{"location":"api-reference/unified-api/#model-configuration","text":"@dataclass class ModelConfig : name_or_path : str # Model name or path (required) sft_path : Optional [ str ] # SFT checkpoint path reward_path : Optional [ str ] # Reward model path precision : PrecisionType # Model precision quantization : Dict [ str , Any ] # Quantization settings attn_implementation : str # Attention implementation gradient_checkpointing : bool # Enable gradient checkpointing max_memory : Optional [ Dict [ str , str ]] # Memory limits per device","title":"Model Configuration"},{"location":"api-reference/unified-api/#dataset-configuration","text":"@dataclass class DatasetConfig : name : str # Dataset name (required) split : str # Dataset split percent : Optional [ float ] # Percentage of dataset to use max_samples : Optional [ int ] # Maximum number of samples column_mapping : Dict [ str , str ] # Column mapping task_type : str # Task type weight : float # Dataset weight chat_template : Optional [ str ] # Chat template","title":"Dataset Configuration"},{"location":"api-reference/unified-api/#training-configuration","text":"@dataclass class TrainingConfig : per_device_batch_size : int # Batch size per device gradient_accumulation_steps : int # Gradient accumulation steps max_steps : Optional [ int ] # Maximum training steps epochs : Optional [ int ] # Number of epochs eval_interval : int # Evaluation interval save_interval : int # Save interval rollout_batch_size : int # Rollout batch size kl_coef : float # KL coefficient cliprange : float # PPO clip range num_ppo_epochs : Optional [ int ] # PPO epochs learning_rate : float # Learning rate","title":"Training Configuration"},{"location":"api-reference/unified-api/#dataset-specifications","text":"","title":"Dataset Specifications"},{"location":"api-reference/unified-api/#format","text":"name[:split][#percent=N|max=N][?map.key=value]","title":"Format"},{"location":"api-reference/unified-api/#examples","text":"# Basic dataset Anthropic/hh-rlhf:train#percent = 25 # With column mapping HuggingFaceH4/ultrafeedback_binarized:train?map.prompt = prompt & map.chosen = chosen & map.rejected = rejected # With max samples gsm8k:train#max = 1000","title":"Examples"},{"location":"api-reference/unified-api/#reward-specifications","text":"","title":"Reward Specifications"},{"location":"api-reference/unified-api/#format_1","text":"type:weight:param1=value1:param2=value2","title":"Format"},{"location":"api-reference/unified-api/#examples_1","text":"# Length reward length:weight = 1 .0:min_length = 10 :max_length = 200 # Math reward math:weight = 0 .8:tolerance = 1e-6 # Safety reward with shield safety:weight = 0 .5:strict = true","title":"Examples"},{"location":"api-reference/unified-api/#built-in-components","text":"","title":"Built-in Components"},{"location":"api-reference/unified-api/#dataset-loaders","text":"huggingface : Load from HuggingFace Hub local : Load from local files","title":"Dataset Loaders"},{"location":"api-reference/unified-api/#schema-adapters","text":"chat : Chat format (messages) hh-rlhf : Anthropic HH-RLHF format ultrafeedback : UltraFeedback format alpaca : Alpaca format","title":"Schema Adapters"},{"location":"api-reference/unified-api/#reward-functions","text":"length : Text length reward sentiment : Sentiment analysis reward coherence : Text coherence reward math : Mathematical correctness reward code : Code quality reward safety : Safety check reward","title":"Reward Functions"},{"location":"api-reference/unified-api/#tasks","text":"conversation : Conversational AI classification : Text classification summarization : Text summarization math : Mathematical reasoning code : Code generation","title":"Tasks"},{"location":"api-reference/unified-api/#distributed-training","text":"","title":"Distributed Training"},{"location":"api-reference/unified-api/#backends","text":"single : Single GPU/CPU training ddp : DistributedDataParallel via accelerate fsdp : FullyShardedDataParallel deepspeed : DeepSpeed ZeRO-2/3","title":"Backends"},{"location":"api-reference/unified-api/#example-configuration","text":"distributed : backend : ddp nodes : 1 gpus_per_node : 2 seed : 42","title":"Example Configuration"},{"location":"api-reference/unified-api/#logging","text":"","title":"Logging"},{"location":"api-reference/unified-api/#supported-loggers","text":"tensorboard : TensorBoard logging wandb : Weights & Biases logging","title":"Supported Loggers"},{"location":"api-reference/unified-api/#example-configuration_1","text":"logging : loggers : [ \"tensorboard\" , \"wandb\" ] output_dir : \"./output\" run_name : \"my_training_run\"","title":"Example Configuration"},{"location":"api-reference/unified-api/#caching","text":"","title":"Caching"},{"location":"api-reference/unified-api/#configuration_1","text":"caching : root : \"./cache\" enabled : true","title":"Configuration"},{"location":"api-reference/unified-api/#features","text":"Hash-based cache keys Automatic cache validation Cache cleanup utilities Support for multiple datasets","title":"Features"},{"location":"api-reference/unified-api/#examples_2","text":"","title":"Examples"},{"location":"api-reference/unified-api/#minimal-ppo-training","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-small\" precision : bf16 datasets : - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 1 train : per_device_batch_size : 1 max_steps : 100 eval_interval : 50 distributed : backend : single logging : output_dir : \"./output\"","title":"Minimal PPO Training"},{"location":"api-reference/unified-api/#multi-task-training","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" precision : bf16 datasets : - name : \"gsm8k\" split : \"train\" max_samples : 5000 task_type : \"math\" weight : 0.4 - name : \"sahil2801/CodeAlpaca-20k\" split : \"train\" percent : 20 task_type : \"code\" weight : 0.3 - name : \"Anthropic/hh-rlhf\" split : \"train\" percent : 10 task_type : \"conversation\" weight : 0.3 tasks : - name : \"math\" weight : 0.4 - name : \"code\" weight : 0.3 - name : \"conversation\" weight : 0.3 rewards : - type : \"math\" weight : 0.4 - type : \"code\" weight : 0.3 - type : \"length\" weight : 0.2 - type : \"safety\" weight : 0.1 shield : true","title":"Multi-task Training"},{"location":"api-reference/unified-api/#migration-guide","text":"","title":"Migration Guide"},{"location":"api-reference/unified-api/#from-old-api","text":"Replace interactive prompts : Use YAML configuration files Update imports : Use unified system imports Update configuration : Use UnifiedConfig instead of algorithm-specific configs Update training : Use TrainerFactory.create_trainer()","title":"From Old API"},{"location":"api-reference/unified-api/#example-migration","text":"Old (Interactive) : from aligntune import PPOTrainer trainer = PPOTrainer () trainer . setup_interactive () # Interactive prompts trainer . train () New (Unified) : from aligntune import UnifiedConfig , TrainerFactory config = UnifiedConfig ( algo = AlgorithmType . PPO , model = ModelConfig ( name_or_path = \"model_name\" ), datasets = [ DatasetConfig ( name = \"dataset_name\" )], train = TrainingConfig ( max_steps = 1000 ) ) trainer = TrainerFactory . create_trainer ( config ) trainer . train ()","title":"Example Migration"},{"location":"api-reference/unified-api/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api-reference/unified-api/#common-issues","text":"Missing required fields : All critical parameters must be explicitly provided Invalid dataset specs : Check dataset specification format Invalid reward specs : Check reward specification format Import errors : Ensure all dependencies are installed","title":"Common Issues"},{"location":"api-reference/unified-api/#debug-mode","text":"Enable debug logging: import logging logging . basicConfig ( level = logging . DEBUG )","title":"Debug Mode"},{"location":"api-reference/unified-api/#validation","text":"Validate configuration before training: from aligntune import ConfigLoader ConfigLoader . validate_config ( config )","title":"Validation"},{"location":"api-reference/unified-api/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/unified-api/#core-classes","text":"UnifiedConfig : Main configuration class ConfigLoader : Configuration loading and validation TrainerFactory : Trainer creation factory TrainerBase : Abstract base trainer class","title":"Core Classes"},{"location":"api-reference/unified-api/#registries","text":"DatasetRegistry : Dataset loader registry RewardRegistry : Reward function registry TaskRegistry : Task definition registry","title":"Registries"},{"location":"api-reference/unified-api/#utilities","text":"UnifiedLogger : Logging system UnifiedEvaluator : Evaluation system DatasetCache : Dataset caching system RolloutEngine : Rollout generation engine","title":"Utilities"},{"location":"api-reference/unified-api/#model-wrappers","text":"PolicyModel : Policy model wrapper ReferenceModel : Reference model wrapper ValueModel : Value model wrapper","title":"Model Wrappers"},{"location":"backends/comparison/","text":"Backend Comparison \u00b6 Detailed side-by-side comparison of TRL and Unsloth backends. Feature Comparison \u00b6 Feature TRL Backend Unsloth Backend Winner Training Speed Baseline faster Unsloth Memory Usage Baseline Less memory usage Unsloth Reliability Battle-tested Production-ready Tie Algorithm Support All algorithms Most algorithms TRL CPU Support Yes No TRL Setup Complexity Low Medium TRL Model Compatibility All models Optimized models TRL Error Handling Comprehensive Comprehensive Tie Algorithm Support \u00b6 Complete Support Matrix \u00b6 Algorithm TRL Unsloth Notes SFT Yes Yes Both fully supported DPO Yes Yes Both fully supported PPO Yes Yes Both fully supported GRPO Yes Yes Both fully supported GSPO Yes No TRL only DAPO Yes Yes Both fully supported Dr. GRPO Yes Yes Both fully supported Use Case Recommendations \u00b6 Use TRL Backend When: \u00b6 Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow Use Unsloth Backend When: \u00b6 Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup Code Examples \u00b6 TRL Backend Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () Unsloth Backend Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () Migration Scenarios \u00b6 Scenario 1: Speed Optimization \u00b6 Current : TRL backend, slow training Goal : Faster training Solution : Switch to Unsloth backend backend = \"unsloth\" # faster Scenario 2: Memory Issues \u00b6 Current : TRL backend, OOM errors Goal : Reduce memory usage Solution : Switch to Unsloth with QLoRA backend = \"unsloth\" model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" # 4-bit quantized Scenario 3: Algorithm Support \u00b6 Current : Using GSPO with Unsloth (not supported) Goal : Use GSPO Solution : Switch to TRL backend backend = \"trl\" # GSPO only supported on TRL Decision Tree \u00b6 flowchart TD Start[Start] --> GSPO{Need GSPO?} GSPO -->|Yes| TRL1[Use TRL Backend] GSPO -->|No| GPU{Have CUDA GPU?} GPU -->|No| TRL2[Use TRL Backend] GPU -->|Yes| Speed{Need Maximum Speed?} Speed -->|Yes| Unsloth1[Use Unsloth Backend] Speed -->|No| Memory{Memory Constrained?} Memory -->|Yes| Unsloth2[Use Unsloth Backend] Memory -->|No| TRL3[Use TRL Backend] Summary \u00b6 Choose TRL Backend if: - You need maximum reliability - You're using GSPO - You're on CPU or have CUDA issues - You want the simplest setup Choose Unsloth Backend if: - You have a CUDA GPU - You need maximum speed - You're memory constrained - You're using supported algorithms Use Auto Selection if: - You want AlignTune to choose - You're unsure which to use - You want automatic fallback Next Steps \u00b6 TRL Backend - Detailed TRL guide Unsloth Backend - Detailed Unsloth guide Backend Selection - Selection guide","title":"Comparison"},{"location":"backends/comparison/#backend-comparison","text":"Detailed side-by-side comparison of TRL and Unsloth backends.","title":"Backend Comparison"},{"location":"backends/comparison/#feature-comparison","text":"Feature TRL Backend Unsloth Backend Winner Training Speed Baseline faster Unsloth Memory Usage Baseline Less memory usage Unsloth Reliability Battle-tested Production-ready Tie Algorithm Support All algorithms Most algorithms TRL CPU Support Yes No TRL Setup Complexity Low Medium TRL Model Compatibility All models Optimized models TRL Error Handling Comprehensive Comprehensive Tie","title":"Feature Comparison"},{"location":"backends/comparison/#algorithm-support","text":"","title":"Algorithm Support"},{"location":"backends/comparison/#complete-support-matrix","text":"Algorithm TRL Unsloth Notes SFT Yes Yes Both fully supported DPO Yes Yes Both fully supported PPO Yes Yes Both fully supported GRPO Yes Yes Both fully supported GSPO Yes No TRL only DAPO Yes Yes Both fully supported Dr. GRPO Yes Yes Both fully supported","title":"Complete Support Matrix"},{"location":"backends/comparison/#use-case-recommendations","text":"","title":"Use Case Recommendations"},{"location":"backends/comparison/#use-trl-backend-when","text":"Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow","title":"Use TRL Backend When:"},{"location":"backends/comparison/#use-unsloth-backend-when","text":"Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup","title":"Use Unsloth Backend When:"},{"location":"backends/comparison/#code-examples","text":"","title":"Code Examples"},{"location":"backends/comparison/#trl-backend-example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"TRL Backend Example"},{"location":"backends/comparison/#unsloth-backend-example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"Unsloth Backend Example"},{"location":"backends/comparison/#migration-scenarios","text":"","title":"Migration Scenarios"},{"location":"backends/comparison/#scenario-1-speed-optimization","text":"Current : TRL backend, slow training Goal : Faster training Solution : Switch to Unsloth backend backend = \"unsloth\" # faster","title":"Scenario 1: Speed Optimization"},{"location":"backends/comparison/#scenario-2-memory-issues","text":"Current : TRL backend, OOM errors Goal : Reduce memory usage Solution : Switch to Unsloth with QLoRA backend = \"unsloth\" model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" # 4-bit quantized","title":"Scenario 2: Memory Issues"},{"location":"backends/comparison/#scenario-3-algorithm-support","text":"Current : Using GSPO with Unsloth (not supported) Goal : Use GSPO Solution : Switch to TRL backend backend = \"trl\" # GSPO only supported on TRL","title":"Scenario 3: Algorithm Support"},{"location":"backends/comparison/#decision-tree","text":"flowchart TD Start[Start] --> GSPO{Need GSPO?} GSPO -->|Yes| TRL1[Use TRL Backend] GSPO -->|No| GPU{Have CUDA GPU?} GPU -->|No| TRL2[Use TRL Backend] GPU -->|Yes| Speed{Need Maximum Speed?} Speed -->|Yes| Unsloth1[Use Unsloth Backend] Speed -->|No| Memory{Memory Constrained?} Memory -->|Yes| Unsloth2[Use Unsloth Backend] Memory -->|No| TRL3[Use TRL Backend]","title":"Decision Tree"},{"location":"backends/comparison/#summary","text":"Choose TRL Backend if: - You need maximum reliability - You're using GSPO - You're on CPU or have CUDA issues - You want the simplest setup Choose Unsloth Backend if: - You have a CUDA GPU - You need maximum speed - You're memory constrained - You're using supported algorithms Use Auto Selection if: - You want AlignTune to choose - You're unsure which to use - You want automatic fallback","title":"Summary"},{"location":"backends/comparison/#next-steps","text":"TRL Backend - Detailed TRL guide Unsloth Backend - Detailed Unsloth guide Backend Selection - Selection guide","title":"Next Steps"},{"location":"backends/overview/","text":"Backends Overview \u00b6 AlignTune provides two backend implementations for training: TRL (reliable, battle-tested) and Unsloth (faster, memory efficient). Backend Comparison \u00b6 Feature TRL Backend Unsloth Backend Speed Standard Faster Faster Memory Standard Optimized Reliability Battle-tested Production-ready GPU Required Works on CPU CUDA required Algorithm Support All algorithms Most algorithms Model Compatibility All HuggingFace models Optimized models Setup Complexity Low Medium TRL Backend \u00b6 Overview \u00b6 The TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation based on HuggingFace's TRL library. Key Features \u00b6 Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues When to Use TRL \u00b6 You need maximum reliability You're using algorithms not supported by Unsloth (GSPO) You're on CPU or have CUDA compatibility issues You want the most stable training experience Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Explicit TRL backend num_epochs = 3 ) Unsloth Backend \u00b6 Overview \u00b6 The Unsloth backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. Key Features \u00b6 Fast : faster training Memory Efficient : Memory reduction GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported When to Use Unsloth \u00b6 You have a CUDA-capable GPU You want maximum training speed You're using supported algorithms (DPO, PPO, GRPO, etc.) You need memory-efficient training Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth backend num_epochs = 3 ) Algorithm Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Backend Selection \u00b6 Automatic Selection \u00b6 AlignTune can automatically select the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatic selection ) Selection Logic : 1. Checks if Unsloth is available and compatible 2. Falls back to TRL if Unsloth unavailable 3. Considers algorithm support Explicit Selection \u00b6 You can explicitly choose a backend: # TRL backend trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Unsloth backend trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Performance Comparison \u00b6 Training Speed \u00b6 Model Size TRL Backend Unsloth Backend Speedup Small (100M-1B) Baseline faster 2-3x Medium (1B-7B) Baseline faster 3-4x Large (7B+) Baseline faster Faster Memory Usage \u00b6 Configuration TRL Backend Unsloth Backend Reduction Full Fine-tuning Baseline 60-70% less 60-70% LoRA Baseline 70-80% less 70-80% QLoRA (4-bit) Baseline 80-90% less 80-90% Best Practices \u00b6 1. Use String-Based Selection \u00b6 Always use string-based backend selection to prevent interference: # CORRECT backend = \"trl\" # or \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference 2. Check Backend Availability \u00b6 # Run diagnostics aligntune diagnose # List available backends aligntune list-backends-cmd 3. Handle Backend Errors \u00b6 try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ImportError as e : # Fallback to TRL print ( f \"Unsloth not available: { e } \" ) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) Migration Guide \u00b6 From TRL to Unsloth \u00b6 Check Compatibility : aligntune diagnose Update Model Name : # TRL model_name = \"microsoft/DialoGPT-small\" # Unsloth (if available) model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" Update Backend : backend = \"unsloth\" Test Training : # Start with small dataset max_samples = 100 Troubleshooting \u00b6 Unsloth Not Available \u00b6 See Troubleshooting Guide for solutions. Backend Interference \u00b6 See Troubleshooting Guide for solutions. Next Steps \u00b6 TRL Backend - Detailed TRL backend guide Unsloth Backend - Detailed Unsloth backend guide Backend Comparison - Side-by-side comparison Backend Selection - Choose the right backend","title":"Overview"},{"location":"backends/overview/#backends-overview","text":"AlignTune provides two backend implementations for training: TRL (reliable, battle-tested) and Unsloth (faster, memory efficient).","title":"Backends Overview"},{"location":"backends/overview/#backend-comparison","text":"Feature TRL Backend Unsloth Backend Speed Standard Faster Faster Memory Standard Optimized Reliability Battle-tested Production-ready GPU Required Works on CPU CUDA required Algorithm Support All algorithms Most algorithms Model Compatibility All HuggingFace models Optimized models Setup Complexity Low Medium","title":"Backend Comparison"},{"location":"backends/overview/#trl-backend","text":"","title":"TRL Backend"},{"location":"backends/overview/#overview","text":"The TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation based on HuggingFace's TRL library.","title":"Overview"},{"location":"backends/overview/#key-features","text":"Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues","title":"Key Features"},{"location":"backends/overview/#when-to-use-trl","text":"You need maximum reliability You're using algorithms not supported by Unsloth (GSPO) You're on CPU or have CUDA compatibility issues You want the most stable training experience","title":"When to Use TRL"},{"location":"backends/overview/#example","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Explicit TRL backend num_epochs = 3 )","title":"Example"},{"location":"backends/overview/#unsloth-backend","text":"","title":"Unsloth Backend"},{"location":"backends/overview/#overview_1","text":"The Unsloth backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations.","title":"Overview"},{"location":"backends/overview/#key-features_1","text":"Fast : faster training Memory Efficient : Memory reduction GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported","title":"Key Features"},{"location":"backends/overview/#when-to-use-unsloth","text":"You have a CUDA-capable GPU You want maximum training speed You're using supported algorithms (DPO, PPO, GRPO, etc.) You need memory-efficient training","title":"When to Use Unsloth"},{"location":"backends/overview/#example_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth backend num_epochs = 3 )","title":"Example"},{"location":"backends/overview/#algorithm-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Algorithm Support Matrix"},{"location":"backends/overview/#backend-selection","text":"","title":"Backend Selection"},{"location":"backends/overview/#automatic-selection","text":"AlignTune can automatically select the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatic selection ) Selection Logic : 1. Checks if Unsloth is available and compatible 2. Falls back to TRL if Unsloth unavailable 3. Considers algorithm support","title":"Automatic Selection"},{"location":"backends/overview/#explicit-selection","text":"You can explicitly choose a backend: # TRL backend trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Unsloth backend trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" )","title":"Explicit Selection"},{"location":"backends/overview/#performance-comparison","text":"","title":"Performance Comparison"},{"location":"backends/overview/#training-speed","text":"Model Size TRL Backend Unsloth Backend Speedup Small (100M-1B) Baseline faster 2-3x Medium (1B-7B) Baseline faster 3-4x Large (7B+) Baseline faster Faster","title":"Training Speed"},{"location":"backends/overview/#memory-usage","text":"Configuration TRL Backend Unsloth Backend Reduction Full Fine-tuning Baseline 60-70% less 60-70% LoRA Baseline 70-80% less 70-80% QLoRA (4-bit) Baseline 80-90% less 80-90%","title":"Memory Usage"},{"location":"backends/overview/#best-practices","text":"","title":"Best Practices"},{"location":"backends/overview/#1-use-string-based-selection","text":"Always use string-based backend selection to prevent interference: # CORRECT backend = \"trl\" # or \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference","title":"1. Use String-Based Selection"},{"location":"backends/overview/#2-check-backend-availability","text":"# Run diagnostics aligntune diagnose # List available backends aligntune list-backends-cmd","title":"2. Check Backend Availability"},{"location":"backends/overview/#3-handle-backend-errors","text":"try : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) except ImportError as e : # Fallback to TRL print ( f \"Unsloth not available: { e } \" ) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" )","title":"3. Handle Backend Errors"},{"location":"backends/overview/#migration-guide","text":"","title":"Migration Guide"},{"location":"backends/overview/#from-trl-to-unsloth","text":"Check Compatibility : aligntune diagnose Update Model Name : # TRL model_name = \"microsoft/DialoGPT-small\" # Unsloth (if available) model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" Update Backend : backend = \"unsloth\" Test Training : # Start with small dataset max_samples = 100","title":"From TRL to Unsloth"},{"location":"backends/overview/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/overview/#unsloth-not-available","text":"See Troubleshooting Guide for solutions.","title":"Unsloth Not Available"},{"location":"backends/overview/#backend-interference","text":"See Troubleshooting Guide for solutions.","title":"Backend Interference"},{"location":"backends/overview/#next-steps","text":"TRL Backend - Detailed TRL backend guide Unsloth Backend - Detailed Unsloth backend guide Backend Comparison - Side-by-side comparison Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"backends/trl/","text":"TRL Backend \u00b6 Complete guide to using the TRL (Transformers Reinforcement Learning) backend in AlignTune. Overview \u00b6 The TRL Backend is the standard, battle-tested implementation based on HuggingFace's TRL library. It provides reliable, production-ready training for all supported algorithms. Key Features \u00b6 Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues When to Use TRL Backend \u00b6 Use TRL backend when: Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO (Unsloth doesn't support) Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow Usage \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () Algorithm Support \u00b6 TRL backend supports all algorithms: Algorithm Support Notes SFT Full support DPO Full support PPO Full support GRPO Full support GSPO Only TRL supports GSPO DAPO Full support Dr. GRPO Full support Configuration \u00b6 Basic Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 ) Advanced Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , gradient_checkpointing = True , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 , warmup_steps = 100 , weight_decay = 0.01 ) Performance \u00b6 Training Speed \u00b6 TRL backend provides standard training speed: - Baseline performance - No special optimizations - Reliable and consistent Memory Usage \u00b6 TRL backend uses standard memory: - Full fine-tuning: Baseline memory usage - LoRA: Reduced memory with PEFT - Gradient checkpointing: Further memory reduction Best Practices \u00b6 1. Use String-Based Selection \u00b6 # CORRECT backend = \"trl\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference 2. Enable Gradient Checkpointing \u00b6 gradient_checkpointing = True # Reduces memory usage 3. Use LoRA for Large Models \u00b6 use_peft = True , lora_r = 8 , lora_alpha = 16 4. Monitor Training \u00b6 logging_steps = 10 , # Log every 10 steps save_steps = 500 , # Save checkpoint every 500 steps Troubleshooting \u00b6 Common Issues \u00b6 Slow Training : Normal for TRL backend, consider Unsloth for speed Memory Issues : Enable gradient checkpointing or use LoRA CUDA Errors : TRL is more compatible, check PyTorch version See Troubleshooting Guide for more details. Comparison with Unsloth \u00b6 Feature TRL Unsloth Speed Baseline faster Memory Baseline 60-80% less Algorithm Support All Most CPU Support Yes No Reliability Battle-tested Production-ready See Backend Comparison for detailed comparison. Next Steps \u00b6 Backend Comparison - Compare with Unsloth Unsloth Backend - Learn about Unsloth backend Backend Selection - Choose the right backend","title":"TRL Backend"},{"location":"backends/trl/#trl-backend","text":"Complete guide to using the TRL (Transformers Reinforcement Learning) backend in AlignTune.","title":"TRL Backend"},{"location":"backends/trl/#overview","text":"The TRL Backend is the standard, battle-tested implementation based on HuggingFace's TRL library. It provides reliable, production-ready training for all supported algorithms.","title":"Overview"},{"location":"backends/trl/#key-features","text":"Reliable : Extensively tested, production-ready Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models CPU Support : Can run on CPU (slower) Stable : No compatibility issues","title":"Key Features"},{"location":"backends/trl/#when-to-use-trl-backend","text":"Use TRL backend when: Maximum Reliability Needed Production deployments Critical applications Long-running training jobs Algorithm Requirements Using GSPO (Unsloth doesn't support) Need all algorithm support Hardware Constraints CPU-only environments CUDA compatibility issues Limited GPU memory Simplicity Minimal setup required Standard HuggingFace workflow","title":"When to Use TRL Backend"},{"location":"backends/trl/#usage","text":"","title":"Usage"},{"location":"backends/trl/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"SFT Training"},{"location":"backends/trl/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"DPO Training"},{"location":"backends/trl/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"trl\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO Training"},{"location":"backends/trl/#algorithm-support","text":"TRL backend supports all algorithms: Algorithm Support Notes SFT Full support DPO Full support PPO Full support GRPO Full support GSPO Only TRL supports GSPO DAPO Full support Dr. GRPO Full support","title":"Algorithm Support"},{"location":"backends/trl/#configuration","text":"","title":"Configuration"},{"location":"backends/trl/#basic-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 )","title":"Basic Configuration"},{"location":"backends/trl/#advanced-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , gradient_checkpointing = True , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 , warmup_steps = 100 , weight_decay = 0.01 )","title":"Advanced Configuration"},{"location":"backends/trl/#performance","text":"","title":"Performance"},{"location":"backends/trl/#training-speed","text":"TRL backend provides standard training speed: - Baseline performance - No special optimizations - Reliable and consistent","title":"Training Speed"},{"location":"backends/trl/#memory-usage","text":"TRL backend uses standard memory: - Full fine-tuning: Baseline memory usage - LoRA: Reduced memory with PEFT - Gradient checkpointing: Further memory reduction","title":"Memory Usage"},{"location":"backends/trl/#best-practices","text":"","title":"Best Practices"},{"location":"backends/trl/#1-use-string-based-selection","text":"# CORRECT backend = \"trl\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . TRL # May cause Unsloth interference","title":"1. Use String-Based Selection"},{"location":"backends/trl/#2-enable-gradient-checkpointing","text":"gradient_checkpointing = True # Reduces memory usage","title":"2. Enable Gradient Checkpointing"},{"location":"backends/trl/#3-use-lora-for-large-models","text":"use_peft = True , lora_r = 8 , lora_alpha = 16","title":"3. Use LoRA for Large Models"},{"location":"backends/trl/#4-monitor-training","text":"logging_steps = 10 , # Log every 10 steps save_steps = 500 , # Save checkpoint every 500 steps","title":"4. Monitor Training"},{"location":"backends/trl/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/trl/#common-issues","text":"Slow Training : Normal for TRL backend, consider Unsloth for speed Memory Issues : Enable gradient checkpointing or use LoRA CUDA Errors : TRL is more compatible, check PyTorch version See Troubleshooting Guide for more details.","title":"Common Issues"},{"location":"backends/trl/#comparison-with-unsloth","text":"Feature TRL Unsloth Speed Baseline faster Memory Baseline 60-80% less Algorithm Support All Most CPU Support Yes No Reliability Battle-tested Production-ready See Backend Comparison for detailed comparison.","title":"Comparison with Unsloth"},{"location":"backends/trl/#next-steps","text":"Backend Comparison - Compare with Unsloth Unsloth Backend - Learn about Unsloth backend Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"backends/unsloth/","text":"Unsloth Backend \u00b6 Complete guide to using the Unsloth backend in AlignTune for faster training. Overview \u00b6 The Unsloth Backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. It's ideal for GPU-based training when speed is critical. Key Features \u00b6 Fast : faster training Memory Efficient : Reduction in memory GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported When to Use Unsloth Backend \u00b6 Use Unsloth backend when: Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup Usage \u00b6 SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train () DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train () Algorithm Support \u00b6 Unsloth backend supports most algorithms: Algorithm Support Notes SFT Yes Full support DPO Yes Full support PPO Yes Full support GRPO Yes Full support GSPO No Not supported DAPO Yes Full support Dr. GRPO Yes Full support Configuration \u00b6 Basic Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) Advanced Configuration with QLoRA \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 ) Performance \u00b6 Training Speed \u00b6 Unsloth backend provides faster training: - Small models (100M-1B) : faster - Medium models (1B-7B) : faster - Large models (7B+) : faster Memory Usage \u00b6 Unsloth backend uses less memory overall Best Practices \u00b6 1. Use Quantized Models \u00b6 # Use 4-bit quantized models for maximum memory efficiency model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" 2. Enable LoRA/QLoRA \u00b6 use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 3. Optimize Batch Size \u00b6 # Start with small batch size, increase if memory allows batch_size = 1 # For large models batch_size = 4 # For smaller models 4. Use String-Based Selection \u00b6 # CORRECT backend = \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . UNSLOTH # May cause interference Troubleshooting \u00b6 Common Issues \u00b6 Unsloth Not Available : Check CUDA installation and compatibility CUDA Errors : Verify PyTorch and CUDA versions match Memory Issues : Use quantized models or reduce batch size See Troubleshooting Guide and Unsloth Compatibility for more details. Comparison with TRL \u00b6 Feature Unsloth TRL Speed faster Baseline Memory Less memory Baseline Algorithm Support Most All CPU Support No Yes GPU Required Yes No See Backend Comparison for detailed comparison. Next Steps \u00b6 Backend Comparison - Compare with TRL TRL Backend - Learn about TRL backend Unsloth Compatibility - Compatibility guide Backend Selection - Choose the right backend","title":"Unsloth Backend"},{"location":"backends/unsloth/#unsloth-backend","text":"Complete guide to using the Unsloth backend in AlignTune for faster training.","title":"Unsloth Backend"},{"location":"backends/unsloth/#overview","text":"The Unsloth Backend provides optimized training with significant speed improvements and memory efficiency through CUDA-specific optimizations. It's ideal for GPU-based training when speed is critical.","title":"Overview"},{"location":"backends/unsloth/#key-features","text":"Fast : faster training Memory Efficient : Reduction in memory GPU Optimized : CUDA-specific optimizations LoRA/QLoRA : Automatic PEFT configuration GPU Required : Needs CUDA-capable GPU Limited : Some algorithms not supported","title":"Key Features"},{"location":"backends/unsloth/#when-to-use-unsloth-backend","text":"Use Unsloth backend when: Speed is Critical Fast iteration needed Large-scale training Time-constrained projects Memory Constraints Limited GPU memory Training large models Need memory efficiency Supported Algorithms Using DPO, PPO, GRPO, etc. Not using GSPO GPU Available CUDA-capable GPU Optimized CUDA setup","title":"When to Use Unsloth Backend"},{"location":"backends/unsloth/#usage","text":"","title":"Usage"},{"location":"backends/unsloth/#sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) trainer . train ()","title":"SFT Training"},{"location":"backends/unsloth/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"DPO Training"},{"location":"backends/unsloth/#ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 ) trainer . train ()","title":"PPO Training"},{"location":"backends/unsloth/#algorithm-support","text":"Unsloth backend supports most algorithms: Algorithm Support Notes SFT Yes Full support DPO Yes Full support PPO Yes Full support GRPO Yes Full support GSPO No Not supported DAPO Yes Full support Dr. GRPO Yes Full support","title":"Algorithm Support"},{"location":"backends/unsloth/#configuration","text":"","title":"Configuration"},{"location":"backends/unsloth/#basic-configuration","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 )","title":"Basic Configuration"},{"location":"backends/unsloth/#advanced-configuration-with-qlora","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05 )","title":"Advanced Configuration with QLoRA"},{"location":"backends/unsloth/#performance","text":"","title":"Performance"},{"location":"backends/unsloth/#training-speed","text":"Unsloth backend provides faster training: - Small models (100M-1B) : faster - Medium models (1B-7B) : faster - Large models (7B+) : faster","title":"Training Speed"},{"location":"backends/unsloth/#memory-usage","text":"Unsloth backend uses less memory overall","title":"Memory Usage"},{"location":"backends/unsloth/#best-practices","text":"","title":"Best Practices"},{"location":"backends/unsloth/#1-use-quantized-models","text":"# Use 4-bit quantized models for maximum memory efficiency model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"","title":"1. Use Quantized Models"},{"location":"backends/unsloth/#2-enable-loraqlora","text":"use_peft = True , lora_r = 8 , lora_alpha = 16 , lora_dropout = 0.05","title":"2. Enable LoRA/QLoRA"},{"location":"backends/unsloth/#3-optimize-batch-size","text":"# Start with small batch size, increase if memory allows batch_size = 1 # For large models batch_size = 4 # For smaller models","title":"3. Optimize Batch Size"},{"location":"backends/unsloth/#4-use-string-based-selection","text":"# CORRECT backend = \"unsloth\" # AVOID from aligntune.core.backend_factory import BackendType backend = BackendType . UNSLOTH # May cause interference","title":"4. Use String-Based Selection"},{"location":"backends/unsloth/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"backends/unsloth/#common-issues","text":"Unsloth Not Available : Check CUDA installation and compatibility CUDA Errors : Verify PyTorch and CUDA versions match Memory Issues : Use quantized models or reduce batch size See Troubleshooting Guide and Unsloth Compatibility for more details.","title":"Common Issues"},{"location":"backends/unsloth/#comparison-with-trl","text":"Feature Unsloth TRL Speed faster Baseline Memory Less memory Baseline Algorithm Support Most All CPU Support No Yes GPU Required Yes No See Backend Comparison for detailed comparison.","title":"Comparison with TRL"},{"location":"backends/unsloth/#next-steps","text":"Backend Comparison - Compare with TRL TRL Backend - Learn about TRL backend Unsloth Compatibility - Compatibility guide Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"cli/commands/","text":"CLI Commands \u00b6 Detailed reference for all AlignTune CLI commands and options. Core Commands \u00b6 aligntune info \u00b6 Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status aligntune list-backends-cmd \u00b6 List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status Training Commands \u00b6 aligntune train \u00b6 Main training command with flexible configuration options. aligntune train [ OPTIONS ] aligntune train --help # To get all parameters Required Options \u00b6 Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type --type sft Optional Options \u00b6 Option Default Description --backend auto Backend to use (trl, unsloth, auto) --epochs 1 Number of training epochs --batch-size 4 Batch size per device --learning-rate 5e-5 Learning rate --max-length 512 Maximum sequence length --output-dir ./output Output directory --seed 42 Random seed --config - YAML configuration file Examples \u00b6 # Basic SFT training aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ # DPO training with specific backend aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --backend trl \\ --epochs 1 \\ --batch-size 2 \\ --learning-rate 1e-6 # Training with configuration file aligntune train --config examples/configs/dpo_minimal.yaml Configuration File Support \u00b6 The CLI supports YAML configuration files for complex training setups: # config.yaml algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Error Handling \u00b6 The CLI provides helpful error messages and suggestions: Model not found : Lists similar available models Backend unavailable : Shows installation instructions Configuration errors : Points to specific validation issues Memory issues : Suggests batch size adjustments See Also \u00b6 CLI Overview - Main CLI overview Configuration Files - YAML configuration format Getting Started - Quick start guide","title":"CLI Commands"},{"location":"cli/commands/#cli-commands","text":"Detailed reference for all AlignTune CLI commands and options.","title":"CLI Commands"},{"location":"cli/commands/#core-commands","text":"","title":"Core Commands"},{"location":"cli/commands/#aligntune-info","text":"Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status","title":"aligntune info"},{"location":"cli/commands/#aligntune-list-backends-cmd","text":"List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status","title":"aligntune list-backends-cmd"},{"location":"cli/commands/#training-commands","text":"","title":"Training Commands"},{"location":"cli/commands/#aligntune-train","text":"Main training command with flexible configuration options. aligntune train [ OPTIONS ] aligntune train --help # To get all parameters","title":"aligntune train"},{"location":"cli/commands/#required-options","text":"Option Description Example --model Model name or path --model \"microsoft/DialoGPT-small\" --dataset Dataset name --dataset \"tatsu-lab/alpaca\" --type Training type --type sft","title":"Required Options"},{"location":"cli/commands/#optional-options","text":"Option Default Description --backend auto Backend to use (trl, unsloth, auto) --epochs 1 Number of training epochs --batch-size 4 Batch size per device --learning-rate 5e-5 Learning rate --max-length 512 Maximum sequence length --output-dir ./output Output directory --seed 42 Random seed --config - YAML configuration file","title":"Optional Options"},{"location":"cli/commands/#examples","text":"# Basic SFT training aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ # DPO training with specific backend aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --backend trl \\ --epochs 1 \\ --batch-size 2 \\ --learning-rate 1e-6 # Training with configuration file aligntune train --config examples/configs/dpo_minimal.yaml","title":"Examples"},{"location":"cli/commands/#configuration-file-support","text":"The CLI supports YAML configuration files for complex training setups: # config.yaml algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Configuration File Support"},{"location":"cli/commands/#error-handling","text":"The CLI provides helpful error messages and suggestions: Model not found : Lists similar available models Backend unavailable : Shows installation instructions Configuration errors : Points to specific validation issues Memory issues : Suggests batch size adjustments","title":"Error Handling"},{"location":"cli/commands/#see-also","text":"CLI Overview - Main CLI overview Configuration Files - YAML configuration format Getting Started - Quick start guide","title":"See Also"},{"location":"cli/configuration/","text":"Configuration Files \u00b6 Detailed guide for creating and using YAML configuration files with the AlignTune CLI. Overview \u00b6 Configuration files provide a declarative way to specify complex training setups, making it easy to: Share training configurations Version control experiments Reproduce results Manage complex setups Basic Structure \u00b6 # Basic configuration structure algo : dpo # Algorithm (dpo, ppo, grpo, etc.) model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Configuration Sections \u00b6 Algorithm Configuration \u00b6 algo : dpo # Options: dpo, ppo, grpo, gspo, dapo, dr-grpo, gbmpo, counterfactual-grpo, bolt Model Configuration \u00b6 model : name_or_path : \"microsoft/DialoGPT-medium\" # Required: Model identifier sft_path : null # Optional: Pre-trained SFT model path reward_path : null # Optional: Reward model path precision : \"bf16\" # Optional: bf16, fp16, fp32 max_seq_length : 2048 # Optional: Maximum sequence length use_unsloth : false # Optional: Enable Unsloth gradient_checkpointing : true # Optional: Enable gradient checkpointing Dataset Configuration \u00b6 datasets : - name : \"Anthropic/hh-rlhf\" # Required: Dataset name split : \"train\" # Optional: Dataset split percent : 100 # Optional: Percentage to use (1-100) max_samples : null # Optional: Maximum samples task_type : \"conversation\" # Optional: Task type weight : 1.0 # Optional: Dataset weight for multi-dataset training Training Configuration \u00b6 train : per_device_batch_size : 4 # Required: Batch size per device gradient_accumulation_steps : 1 # Optional: Gradient accumulation max_steps : 1000 # Optional: Maximum training steps learning_rate : 5e-5 # Optional: Learning rate weight_decay : 0.01 # Optional: Weight decay warmup_steps : 100 # Optional: Warmup steps max_length : 512 # Optional: Maximum sequence length temperature : 0.7 # Optional: Generation temperature # RL-specific parameters beta : 0.1 # Optional: DPO beta parameter kl_coef : 0.1 # Optional: PPO KL coefficient cliprange : 0.2 # Optional: PPO clip range Reward Configuration (RL only) \u00b6 rewards : - reward_type : \"safety\" # Required: Reward function type weight : 1.0 # Optional: Reward weight params : {} # Optional: Reward parameters model_name : null # Optional: Model for reward function device : \"auto\" # Optional: Device for computation Logging Configuration \u00b6 logging : output_dir : \"./output\" # Required: Output directory run_name : null # Optional: Run name loggers : [ \"tensorboard\" ] # Optional: Logging backends level : \"INFO\" # Optional: Log level save_steps : 500 # Optional: Checkpoint save interval eval_steps : 100 # Optional: Evaluation interval report_to : [ \"tensorboard\" ] # Optional: Reporting backends push_to_hub : false # Optional: Push to HuggingFace Hub hub_model_id : null # Optional: Hub model ID Sample Logging Configuration \u00b6 sample_logging : enabled : true # Enable qualitative sample logging num_samples : 2 # Number of samples to generate max_new_tokens : 64 # Maximum tokens per sample interval_steps : null # Log every N steps percent_of_max_steps : 0.5 # Log at percentage of training prompts : null # Custom prompts (uses defaults if null) temperature : 0.7 # Sampling temperature top_p : 0.9 # Nucleus sampling parameter Complete Examples \u00b6 SFT Configuration \u00b6 algo : sft model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : per_device_batch_size : 4 max_steps : 1000 learning_rate : 5e-5 weight_decay : 0.01 warmup_steps : 100 logging : output_dir : \"./output/sft_training\" run_name : \"sft_experiment\" save_steps : 500 eval_steps : 100 DPO Configuration \u00b6 algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : per_device_batch_size : 2 max_steps : 1000 learning_rate : 1e-6 beta : 0.1 weight_decay : 0.01 warmup_steps : 50 rewards : - reward_type : \"safety\" weight : 2.0 - reward_type : \"helpfulness\" weight : 1.5 logging : output_dir : \"./output/dpo_training\" run_name : \"dpo_experiment\" save_steps : 500 eval_steps : 100 PPO Configuration \u00b6 algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 500 train : per_device_batch_size : 1 max_steps : 1000 learning_rate : 1e-6 kl_coef : 0.1 cliprange : 0.2 vf_coef : 0.1 ent_coef : 0.0 rewards : - reward_type : \"length\" weight : 0.5 - reward_type : \"sentiment\" weight : 1.0 - reward_type : \"safety\" weight : 2.0 sample_logging : enabled : true num_samples : 2 max_new_tokens : 64 percent_of_max_steps : 0.5 logging : output_dir : \"./output/ppo_training\" run_name : \"ppo_experiment\" save_steps : 500 eval_steps : 100 Using Configuration Files \u00b6 Command Line Usage \u00b6 # Train with configuration file aligntune train --config config.yaml # Override specific values aligntune train --config config.yaml --max-steps 2000 --learning-rate 1e-5 Best Practices \u00b6 Organization \u00b6 Use descriptive filenames: dpo_experiment_001.yaml Group related configs in directories Version control your configurations Parameter Tuning \u00b6 Start with provided examples Gradually modify parameters Keep records of what worked Reproducibility \u00b6 Set explicit random seeds Record exact configurations used Include environment information See Also \u00b6 CLI Commands - Command line interface CLI Overview - Main CLI overview Examples - Configuration examples","title":"CLI Configuration Files"},{"location":"cli/configuration/#configuration-files","text":"Detailed guide for creating and using YAML configuration files with the AlignTune CLI.","title":"Configuration Files"},{"location":"cli/configuration/#overview","text":"Configuration files provide a declarative way to specify complex training setups, making it easy to: Share training configurations Version control experiments Reproduce results Manage complex setups","title":"Overview"},{"location":"cli/configuration/#basic-structure","text":"# Basic configuration structure algo : dpo # Algorithm (dpo, ppo, grpo, etc.) model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"Basic Structure"},{"location":"cli/configuration/#configuration-sections","text":"","title":"Configuration Sections"},{"location":"cli/configuration/#algorithm-configuration","text":"algo : dpo # Options: dpo, ppo, grpo, gspo, dapo, dr-grpo, gbmpo, counterfactual-grpo, bolt","title":"Algorithm Configuration"},{"location":"cli/configuration/#model-configuration","text":"model : name_or_path : \"microsoft/DialoGPT-medium\" # Required: Model identifier sft_path : null # Optional: Pre-trained SFT model path reward_path : null # Optional: Reward model path precision : \"bf16\" # Optional: bf16, fp16, fp32 max_seq_length : 2048 # Optional: Maximum sequence length use_unsloth : false # Optional: Enable Unsloth gradient_checkpointing : true # Optional: Enable gradient checkpointing","title":"Model Configuration"},{"location":"cli/configuration/#dataset-configuration","text":"datasets : - name : \"Anthropic/hh-rlhf\" # Required: Dataset name split : \"train\" # Optional: Dataset split percent : 100 # Optional: Percentage to use (1-100) max_samples : null # Optional: Maximum samples task_type : \"conversation\" # Optional: Task type weight : 1.0 # Optional: Dataset weight for multi-dataset training","title":"Dataset Configuration"},{"location":"cli/configuration/#training-configuration","text":"train : per_device_batch_size : 4 # Required: Batch size per device gradient_accumulation_steps : 1 # Optional: Gradient accumulation max_steps : 1000 # Optional: Maximum training steps learning_rate : 5e-5 # Optional: Learning rate weight_decay : 0.01 # Optional: Weight decay warmup_steps : 100 # Optional: Warmup steps max_length : 512 # Optional: Maximum sequence length temperature : 0.7 # Optional: Generation temperature # RL-specific parameters beta : 0.1 # Optional: DPO beta parameter kl_coef : 0.1 # Optional: PPO KL coefficient cliprange : 0.2 # Optional: PPO clip range","title":"Training Configuration"},{"location":"cli/configuration/#reward-configuration-rl-only","text":"rewards : - reward_type : \"safety\" # Required: Reward function type weight : 1.0 # Optional: Reward weight params : {} # Optional: Reward parameters model_name : null # Optional: Model for reward function device : \"auto\" # Optional: Device for computation","title":"Reward Configuration (RL only)"},{"location":"cli/configuration/#logging-configuration","text":"logging : output_dir : \"./output\" # Required: Output directory run_name : null # Optional: Run name loggers : [ \"tensorboard\" ] # Optional: Logging backends level : \"INFO\" # Optional: Log level save_steps : 500 # Optional: Checkpoint save interval eval_steps : 100 # Optional: Evaluation interval report_to : [ \"tensorboard\" ] # Optional: Reporting backends push_to_hub : false # Optional: Push to HuggingFace Hub hub_model_id : null # Optional: Hub model ID","title":"Logging Configuration"},{"location":"cli/configuration/#sample-logging-configuration","text":"sample_logging : enabled : true # Enable qualitative sample logging num_samples : 2 # Number of samples to generate max_new_tokens : 64 # Maximum tokens per sample interval_steps : null # Log every N steps percent_of_max_steps : 0.5 # Log at percentage of training prompts : null # Custom prompts (uses defaults if null) temperature : 0.7 # Sampling temperature top_p : 0.9 # Nucleus sampling parameter","title":"Sample Logging Configuration"},{"location":"cli/configuration/#complete-examples","text":"","title":"Complete Examples"},{"location":"cli/configuration/#sft-configuration","text":"algo : sft model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : per_device_batch_size : 4 max_steps : 1000 learning_rate : 5e-5 weight_decay : 0.01 warmup_steps : 100 logging : output_dir : \"./output/sft_training\" run_name : \"sft_experiment\" save_steps : 500 eval_steps : 100","title":"SFT Configuration"},{"location":"cli/configuration/#dpo-configuration","text":"algo : dpo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : per_device_batch_size : 2 max_steps : 1000 learning_rate : 1e-6 beta : 0.1 weight_decay : 0.01 warmup_steps : 50 rewards : - reward_type : \"safety\" weight : 2.0 - reward_type : \"helpfulness\" weight : 1.5 logging : output_dir : \"./output/dpo_training\" run_name : \"dpo_experiment\" save_steps : 500 eval_steps : 100","title":"DPO Configuration"},{"location":"cli/configuration/#ppo-configuration","text":"algo : ppo model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 500 train : per_device_batch_size : 1 max_steps : 1000 learning_rate : 1e-6 kl_coef : 0.1 cliprange : 0.2 vf_coef : 0.1 ent_coef : 0.0 rewards : - reward_type : \"length\" weight : 0.5 - reward_type : \"sentiment\" weight : 1.0 - reward_type : \"safety\" weight : 2.0 sample_logging : enabled : true num_samples : 2 max_new_tokens : 64 percent_of_max_steps : 0.5 logging : output_dir : \"./output/ppo_training\" run_name : \"ppo_experiment\" save_steps : 500 eval_steps : 100","title":"PPO Configuration"},{"location":"cli/configuration/#using-configuration-files","text":"","title":"Using Configuration Files"},{"location":"cli/configuration/#command-line-usage","text":"# Train with configuration file aligntune train --config config.yaml # Override specific values aligntune train --config config.yaml --max-steps 2000 --learning-rate 1e-5","title":"Command Line Usage"},{"location":"cli/configuration/#best-practices","text":"","title":"Best Practices"},{"location":"cli/configuration/#organization","text":"Use descriptive filenames: dpo_experiment_001.yaml Group related configs in directories Version control your configurations","title":"Organization"},{"location":"cli/configuration/#parameter-tuning","text":"Start with provided examples Gradually modify parameters Keep records of what worked","title":"Parameter Tuning"},{"location":"cli/configuration/#reproducibility","text":"Set explicit random seeds Record exact configurations used Include environment information","title":"Reproducibility"},{"location":"cli/configuration/#see-also","text":"CLI Commands - Command line interface CLI Overview - Main CLI overview Examples - Configuration examples","title":"See Also"},{"location":"cli/overview/","text":"CLI Overview \u00b6 AlignTune provides a comprehensive command-line interface for training and managing models without writing Python code. Quick Start \u00b6 # Show system information aligntune info # Train a model aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" Main Commands \u00b6 aligntune info \u00b6 Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status aligntune list-backends-cmd \u00b6 List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status aligntune train \u00b6 Train models with comprehensive options. aligntune train --help # To get all parameters # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" --type sft # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl --split train [ :100 ] # PPO Training aligntune train --model \"Qwen/Qwen3-0.6B\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth --split \"train[:100]\" Common Options: Option Description Example --model , -m Model name or path --model \"microsoft/DialoGPT-small\" --dataset , -d Dataset name --dataset \"tatsu-lab/alpaca\" --type , -t Training type (sft, dpo, ppo, etc.) --type dpo --backend , -b Backend (trl, unsloth, auto) --backend auto --epochs , -e Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate , --lr Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir , -o Output directory --output-dir ./output --4bit Load model in 4-bit quantization --4bit --8bit Load model in 8-bit quantization --8bit --wandb-project Weights & Biases project name --wandb-project my-project Configuration Files \u00b6 The CLI supports YAML configuration files for complex setups. See Configuration Files for detailed format documentation. Example configuration: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Using configuration files: # Train with config file aligntune train --config my_config.yaml Examples \u00b6 Quick SFT Training \u00b6 aligntune train examples/sft_customer_support_unsloth/config_sft_customer_support.yaml Quick RL Training \u00b6 aligntune train examples/grpo_gsm8k_trl/config_grpo_gsm8k.yaml Advanced Usage \u00b6 For detailed command reference, see: - CLI Commands - Complete command reference - CLI Configuration - Configuration file format See Also \u00b6 Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference Examples - Code examples","title":"CLI Overview"},{"location":"cli/overview/#cli-overview","text":"AlignTune provides a comprehensive command-line interface for training and managing models without writing Python code.","title":"CLI Overview"},{"location":"cli/overview/#quick-start","text":"# Show system information aligntune info # Train a model aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\"","title":"Quick Start"},{"location":"cli/overview/#main-commands","text":"","title":"Main Commands"},{"location":"cli/overview/#aligntune-info","text":"Display system information, versions, and compatibility status. aligntune info Output includes: - AlignTune version - Python version - Available backends (TRL, Unsloth) - GPU information - Memory status","title":"aligntune info"},{"location":"cli/overview/#aligntune-list-backends-cmd","text":"List all available backends and their current status. aligntune list-backends-cmd Shows: - Backend availability - Version information - GPU compatibility - Installation status","title":"aligntune list-backends-cmd"},{"location":"cli/overview/#aligntune-train","text":"Train models with comprehensive options. aligntune train --help # To get all parameters # SFT Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"tatsu-lab/alpaca\" --type sft # DPO Training aligntune train --model \"microsoft/DialoGPT-small\" --dataset \"Anthropic/hh-rlhf\" --type dpo --backend trl --split train [ :100 ] # PPO Training aligntune train --model \"Qwen/Qwen3-0.6B\" --dataset \"Anthropic/hh-rlhf\" --type ppo --backend unsloth --split \"train[:100]\" Common Options: Option Description Example --model , -m Model name or path --model \"microsoft/DialoGPT-small\" --dataset , -d Dataset name --dataset \"tatsu-lab/alpaca\" --type , -t Training type (sft, dpo, ppo, etc.) --type dpo --backend , -b Backend (trl, unsloth, auto) --backend auto --epochs , -e Number of epochs --epochs 3 --batch-size Batch size --batch-size 4 --learning-rate , --lr Learning rate --learning-rate 5e-5 --max-length Maximum sequence length --max-length 512 --output-dir , -o Output directory --output-dir ./output --4bit Load model in 4-bit quantization --4bit --8bit Load model in 8-bit quantization --8bit --wandb-project Weights & Biases project name --wandb-project my-project","title":"aligntune train"},{"location":"cli/overview/#configuration-files","text":"The CLI supports YAML configuration files for complex setups. See Configuration Files for detailed format documentation. Example configuration: algo : dpo model : name_or_path : \"microsoft/DialoGPT-small\" max_seq_length : 512 datasets : - name : \"Anthropic/hh-rlhf\" max_samples : 1000 train : max_steps : 1000 learning_rate : 5e-5 per_device_batch_size : 4 logging : output_dir : \"./output\" run_name : \"my_experiment\" Using configuration files: # Train with config file aligntune train --config my_config.yaml","title":"Configuration Files"},{"location":"cli/overview/#examples","text":"","title":"Examples"},{"location":"cli/overview/#quick-sft-training","text":"aligntune train examples/sft_customer_support_unsloth/config_sft_customer_support.yaml","title":"Quick SFT Training"},{"location":"cli/overview/#quick-rl-training","text":"aligntune train examples/grpo_gsm8k_trl/config_grpo_gsm8k.yaml","title":"Quick RL Training"},{"location":"cli/overview/#advanced-usage","text":"For detailed command reference, see: - CLI Commands - Complete command reference - CLI Configuration - Configuration file format","title":"Advanced Usage"},{"location":"cli/overview/#see-also","text":"Getting Started - Installation and setup User Guide - Detailed usage guides API Reference - Python API reference Examples - Code examples","title":"See Also"},{"location":"compatibility/backend-matrix/","text":"Backend Support Matrix \u00b6 Complete compatibility matrix for AlignTune backends and algorithms. Algorithm Support \u00b6 Algorithm TRL Backend Unsloth Backend Notes SFT Yes Yes Both backends fully supported DPO Yes Yes Both backends fully supported PPO Yes Yes Both backends fully supported GRPO Yes Yes Both backends fully supported GSPO Yes No TRL only, Unsloth not supported DAPO Yes Yes Both backends fully supported Dr. GRPO Yes Yes Both backends fully supported Backend Comparison \u00b6 TRL Backend \u00b6 Advantages: - Maximum compatibility - Supports all algorithms (including GSPO) - Battle-tested reliability - Works on CPU and GPU - No special requirements Use When: - Need GSPO support - Maximum compatibility required - Working with standard models - CPU-only environments Unsloth Backend \u00b6 Advantages: - faster training - Memory efficient - Optimized kernels - Automatic optimizations Limitations: - GSPO not supported - Requires GPU - CUDA compatibility needed Use When: - Need faster training - Working with large models - Memory constraints - GPU available Model Compatibility \u00b6 TRL Backend \u00b6 Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Yes Full support Encoder-Decoder Yes Full support Classification Yes Full support Unsloth Backend \u00b6 Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Limited Limited support Encoder-Decoder Limited Limited support Classification No Not recommended Task Type Support \u00b6 SFT Tasks \u00b6 Task Type TRL Unsloth Notes Instruction Following Yes Yes Both supported Supervised Fine-Tuning Yes Yes Both supported Text Classification Yes Limited TRL recommended Token Classification Yes Limited TRL recommended Text Generation Yes Yes Both supported Chat Completion Yes Yes Both supported Requirements \u00b6 TRL Backend \u00b6 Python 3.8+ PyTorch 1.13+ Transformers 4.35+ TRL 0.7+ CPU or GPU Unsloth Backend \u00b6 Python 3.8+ PyTorch 2.0+ (CUDA) CUDA 11.8+ or 12.1+ GPU required Unsloth package Performance Comparison \u00b6 Training Speed \u00b6 Model Size TRL Unsloth Speedup Small (<1B) Baseline 2-3x 2-3x Medium (1-7B) Baseline 3-4x 3-4x Large (7B+) Baseline Faster Faster Memory Usage \u00b6 Configuration TRL Unsloth Reduction Full Precision Baseline -20% 20% LoRA Baseline -30% 30% QLoRA (4-bit) Baseline -50% 50% Migration Guide \u00b6 From TRL to Unsloth \u00b6 # TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"trl\" ) # Unsloth (if compatible) trainer = create_sft_trainer ( model_name = \"unsloth/model-bnb-4bit\" , # Use Unsloth model backend = \"unsloth\" ) From Unsloth to TRL \u00b6 # Unsloth trainer = create_sft_trainer ( model_name = \"unsloth/model\" , backend = \"unsloth\" ) # TRL (always works) trainer = create_sft_trainer ( model_name = \"model\" , # Standard model backend = \"trl\" ) Troubleshooting \u00b6 Unsloth Not Available \u00b6 # Automatic fallback to TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"auto\" # Falls back to TRL if Unsloth unavailable ) GSPO with Unsloth \u00b6 # This will fail trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"unsloth\" # Error: GSPO not supported ) # Use TRL instead trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"trl\" # Works ) Next Steps \u00b6 Backend Selection - Choosing backends Unsloth Compatibility - Unsloth setup Performance - Performance optimization","title":"Backend Support Matrix"},{"location":"compatibility/backend-matrix/#backend-support-matrix","text":"Complete compatibility matrix for AlignTune backends and algorithms.","title":"Backend Support Matrix"},{"location":"compatibility/backend-matrix/#algorithm-support","text":"Algorithm TRL Backend Unsloth Backend Notes SFT Yes Yes Both backends fully supported DPO Yes Yes Both backends fully supported PPO Yes Yes Both backends fully supported GRPO Yes Yes Both backends fully supported GSPO Yes No TRL only, Unsloth not supported DAPO Yes Yes Both backends fully supported Dr. GRPO Yes Yes Both backends fully supported","title":"Algorithm Support"},{"location":"compatibility/backend-matrix/#backend-comparison","text":"","title":"Backend Comparison"},{"location":"compatibility/backend-matrix/#trl-backend","text":"Advantages: - Maximum compatibility - Supports all algorithms (including GSPO) - Battle-tested reliability - Works on CPU and GPU - No special requirements Use When: - Need GSPO support - Maximum compatibility required - Working with standard models - CPU-only environments","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend","text":"Advantages: - faster training - Memory efficient - Optimized kernels - Automatic optimizations Limitations: - GSPO not supported - Requires GPU - CUDA compatibility needed Use When: - Need faster training - Working with large models - Memory constraints - GPU available","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#model-compatibility","text":"","title":"Model Compatibility"},{"location":"compatibility/backend-matrix/#trl-backend_1","text":"Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Yes Full support Encoder-Decoder Yes Full support Classification Yes Full support","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend_1","text":"Model Type Support Notes Causal LM Yes Full support Sequence-to-Sequence Limited Limited support Encoder-Decoder Limited Limited support Classification No Not recommended","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#task-type-support","text":"","title":"Task Type Support"},{"location":"compatibility/backend-matrix/#sft-tasks","text":"Task Type TRL Unsloth Notes Instruction Following Yes Yes Both supported Supervised Fine-Tuning Yes Yes Both supported Text Classification Yes Limited TRL recommended Token Classification Yes Limited TRL recommended Text Generation Yes Yes Both supported Chat Completion Yes Yes Both supported","title":"SFT Tasks"},{"location":"compatibility/backend-matrix/#requirements","text":"","title":"Requirements"},{"location":"compatibility/backend-matrix/#trl-backend_2","text":"Python 3.8+ PyTorch 1.13+ Transformers 4.35+ TRL 0.7+ CPU or GPU","title":"TRL Backend"},{"location":"compatibility/backend-matrix/#unsloth-backend_2","text":"Python 3.8+ PyTorch 2.0+ (CUDA) CUDA 11.8+ or 12.1+ GPU required Unsloth package","title":"Unsloth Backend"},{"location":"compatibility/backend-matrix/#performance-comparison","text":"","title":"Performance Comparison"},{"location":"compatibility/backend-matrix/#training-speed","text":"Model Size TRL Unsloth Speedup Small (<1B) Baseline 2-3x 2-3x Medium (1-7B) Baseline 3-4x 3-4x Large (7B+) Baseline Faster Faster","title":"Training Speed"},{"location":"compatibility/backend-matrix/#memory-usage","text":"Configuration TRL Unsloth Reduction Full Precision Baseline -20% 20% LoRA Baseline -30% 30% QLoRA (4-bit) Baseline -50% 50%","title":"Memory Usage"},{"location":"compatibility/backend-matrix/#migration-guide","text":"","title":"Migration Guide"},{"location":"compatibility/backend-matrix/#from-trl-to-unsloth","text":"# TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"trl\" ) # Unsloth (if compatible) trainer = create_sft_trainer ( model_name = \"unsloth/model-bnb-4bit\" , # Use Unsloth model backend = \"unsloth\" )","title":"From TRL to Unsloth"},{"location":"compatibility/backend-matrix/#from-unsloth-to-trl","text":"# Unsloth trainer = create_sft_trainer ( model_name = \"unsloth/model\" , backend = \"unsloth\" ) # TRL (always works) trainer = create_sft_trainer ( model_name = \"model\" , # Standard model backend = \"trl\" )","title":"From Unsloth to TRL"},{"location":"compatibility/backend-matrix/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"compatibility/backend-matrix/#unsloth-not-available","text":"# Automatic fallback to TRL trainer = create_sft_trainer ( model_name = \"model\" , backend = \"auto\" # Falls back to TRL if Unsloth unavailable )","title":"Unsloth Not Available"},{"location":"compatibility/backend-matrix/#gspo-with-unsloth","text":"# This will fail trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"unsloth\" # Error: GSPO not supported ) # Use TRL instead trainer = create_rl_trainer ( algorithm = \"gspo\" , backend = \"trl\" # Works )","title":"GSPO with Unsloth"},{"location":"compatibility/backend-matrix/#next-steps","text":"Backend Selection - Choosing backends Unsloth Compatibility - Unsloth setup Performance - Performance optimization","title":"Next Steps"},{"location":"contributing/code-style/","text":"Code Style Guide \u00b6 Code style guidelines for contributing to AlignTune. Overview \u00b6 AlignTune follows Python best practices and uses automated formatting tools to ensure consistent code style. Code Formatting \u00b6 Black \u00b6 We use Black for code formatting. Configuration: - Line length: 100 characters - Target Python versions: 3.8, 3.9, 3.10, 3.11 Usage: # Format code black src/ # Check formatting black --check src/ isort \u00b6 We use isort for import sorting. Configuration: - Profile: black - Line length: 100 - Multi-line output: 3 Usage: # Sort imports isort src/ # Check imports isort --check src/ Linting \u00b6 Flake8 \u00b6 We use Flake8 for linting. Configuration: - Line length: 100 - Ignore: E501 (handled by Black) Usage: flake8 src/ Ruff (Alternative) \u00b6 We also support Ruff as a modern alternative. Usage: ruff check src/ ruff format src/ Type Hints \u00b6 Type Annotations \u00b6 Always use type hints for function signatures: from typing import Dict , List , Optional , Union def create_trainer ( model_name : str , dataset_name : str , backend : str = \"auto\" , max_samples : Optional [ int ] = None ) -> TrainerBase : \"\"\"Create trainer with specified parameters.\"\"\" pass Type Checking \u00b6 We use mypy for type checking. Usage: mypy src/ Documentation \u00b6 Docstrings \u00b6 Use Google-style docstrings: def train_model ( model : Model , dataset : Dataset , epochs : int = 3 ) -> Dict [ str , Any ]: \"\"\"Train model on dataset. Args: model: Model to train dataset: Training dataset epochs: Number of training epochs Returns: Dictionary with training results Raises: TrainingError: If training fails \"\"\" pass Inline Comments \u00b6 Use comments to explain \"why\", not \"what\" Keep comments up-to-date with code Remove commented-out code Naming Conventions \u00b6 Variables and Functions \u00b6 Use snake_case for variables and functions Use descriptive names Avoid abbreviations # Good def create_sft_trainer (): ... model_name = \"microsoft/DialoGPT-medium\" # Bad def createSFT (): ... mn = \"microsoft/DialoGPT-medium\" Classes \u00b6 Use PascalCase for classes Use descriptive names # Good class SFTTrainerBase : ... class BackendFactory : ... # Bad class Trainer : ... class Factory : ... Constants \u00b6 Use UPPER_SNAKE_CASE for constants # Good MAX_SEQUENCE_LENGTH = 2048 DEFAULT_BATCH_SIZE = 4 # Bad maxSequenceLength = 2048 default_batch_size = 4 Code Organization \u00b6 Imports \u00b6 Order imports as follows: Standard library Third-party packages Local imports # Standard library import logging from typing import Dict , List # Third-party import torch from transformers import AutoModel # Local from aligntune.core.backend_factory import create_sft_trainer File Structure \u00b6 \"\"\"Module docstring.\"\"\" # Imports import ... # Constants DEFAULT_VALUE = ... # Classes class MyClass : ... # Functions def my_function (): ... # Main (if applicable) if __name__ == \"__main__\" : ... Error Handling \u00b6 Exceptions \u00b6 Use custom exceptions with clear messages: from aligntune.utils.errors import AlignTuneError class ConfigurationError ( AlignTuneError ): \"\"\"Configuration error.\"\"\" pass # Usage if not config . model . name : raise ConfigurationError ( \"Model name is required\" ) Error Messages \u00b6 Be specific and actionable Include context Suggest solutions # Good raise ValueError ( f \"Invalid backend ' { backend } '. \" f \"Available backends: { available_backends } . \" f \"Use 'auto' for automatic selection.\" ) # Bad raise ValueError ( \"Invalid backend\" ) Testing \u00b6 Test Structure \u00b6 One test file per module Test file: test_<module_name>.py Test class: Test<ClassName> Test function: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass Pre-commit Hooks \u00b6 Setup \u00b6 pip install pre-commit pre-commit install Hooks \u00b6 We use pre-commit hooks for: - Black formatting - isort import sorting - Flake8 linting - mypy type checking Best Practices \u00b6 Format Before Committing : Run Black and isort Type Hints : Add type hints to all functions Documentation : Document public APIs Error Handling : Use custom exceptions Testing : Write tests for new features Next Steps \u00b6 Testing Guide - Testing guidelines Contributing Guide - General contributing guide","title":"Code Style"},{"location":"contributing/code-style/#code-style-guide","text":"Code style guidelines for contributing to AlignTune.","title":"Code Style Guide"},{"location":"contributing/code-style/#overview","text":"AlignTune follows Python best practices and uses automated formatting tools to ensure consistent code style.","title":"Overview"},{"location":"contributing/code-style/#code-formatting","text":"","title":"Code Formatting"},{"location":"contributing/code-style/#black","text":"We use Black for code formatting. Configuration: - Line length: 100 characters - Target Python versions: 3.8, 3.9, 3.10, 3.11 Usage: # Format code black src/ # Check formatting black --check src/","title":"Black"},{"location":"contributing/code-style/#isort","text":"We use isort for import sorting. Configuration: - Profile: black - Line length: 100 - Multi-line output: 3 Usage: # Sort imports isort src/ # Check imports isort --check src/","title":"isort"},{"location":"contributing/code-style/#linting","text":"","title":"Linting"},{"location":"contributing/code-style/#flake8","text":"We use Flake8 for linting. Configuration: - Line length: 100 - Ignore: E501 (handled by Black) Usage: flake8 src/","title":"Flake8"},{"location":"contributing/code-style/#ruff-alternative","text":"We also support Ruff as a modern alternative. Usage: ruff check src/ ruff format src/","title":"Ruff (Alternative)"},{"location":"contributing/code-style/#type-hints","text":"","title":"Type Hints"},{"location":"contributing/code-style/#type-annotations","text":"Always use type hints for function signatures: from typing import Dict , List , Optional , Union def create_trainer ( model_name : str , dataset_name : str , backend : str = \"auto\" , max_samples : Optional [ int ] = None ) -> TrainerBase : \"\"\"Create trainer with specified parameters.\"\"\" pass","title":"Type Annotations"},{"location":"contributing/code-style/#type-checking","text":"We use mypy for type checking. Usage: mypy src/","title":"Type Checking"},{"location":"contributing/code-style/#documentation","text":"","title":"Documentation"},{"location":"contributing/code-style/#docstrings","text":"Use Google-style docstrings: def train_model ( model : Model , dataset : Dataset , epochs : int = 3 ) -> Dict [ str , Any ]: \"\"\"Train model on dataset. Args: model: Model to train dataset: Training dataset epochs: Number of training epochs Returns: Dictionary with training results Raises: TrainingError: If training fails \"\"\" pass","title":"Docstrings"},{"location":"contributing/code-style/#inline-comments","text":"Use comments to explain \"why\", not \"what\" Keep comments up-to-date with code Remove commented-out code","title":"Inline Comments"},{"location":"contributing/code-style/#naming-conventions","text":"","title":"Naming Conventions"},{"location":"contributing/code-style/#variables-and-functions","text":"Use snake_case for variables and functions Use descriptive names Avoid abbreviations # Good def create_sft_trainer (): ... model_name = \"microsoft/DialoGPT-medium\" # Bad def createSFT (): ... mn = \"microsoft/DialoGPT-medium\"","title":"Variables and Functions"},{"location":"contributing/code-style/#classes","text":"Use PascalCase for classes Use descriptive names # Good class SFTTrainerBase : ... class BackendFactory : ... # Bad class Trainer : ... class Factory : ...","title":"Classes"},{"location":"contributing/code-style/#constants","text":"Use UPPER_SNAKE_CASE for constants # Good MAX_SEQUENCE_LENGTH = 2048 DEFAULT_BATCH_SIZE = 4 # Bad maxSequenceLength = 2048 default_batch_size = 4","title":"Constants"},{"location":"contributing/code-style/#code-organization","text":"","title":"Code Organization"},{"location":"contributing/code-style/#imports","text":"Order imports as follows: Standard library Third-party packages Local imports # Standard library import logging from typing import Dict , List # Third-party import torch from transformers import AutoModel # Local from aligntune.core.backend_factory import create_sft_trainer","title":"Imports"},{"location":"contributing/code-style/#file-structure","text":"\"\"\"Module docstring.\"\"\" # Imports import ... # Constants DEFAULT_VALUE = ... # Classes class MyClass : ... # Functions def my_function (): ... # Main (if applicable) if __name__ == \"__main__\" : ...","title":"File Structure"},{"location":"contributing/code-style/#error-handling","text":"","title":"Error Handling"},{"location":"contributing/code-style/#exceptions","text":"Use custom exceptions with clear messages: from aligntune.utils.errors import AlignTuneError class ConfigurationError ( AlignTuneError ): \"\"\"Configuration error.\"\"\" pass # Usage if not config . model . name : raise ConfigurationError ( \"Model name is required\" )","title":"Exceptions"},{"location":"contributing/code-style/#error-messages","text":"Be specific and actionable Include context Suggest solutions # Good raise ValueError ( f \"Invalid backend ' { backend } '. \" f \"Available backends: { available_backends } . \" f \"Use 'auto' for automatic selection.\" ) # Bad raise ValueError ( \"Invalid backend\" )","title":"Error Messages"},{"location":"contributing/code-style/#testing","text":"","title":"Testing"},{"location":"contributing/code-style/#test-structure","text":"One test file per module Test file: test_<module_name>.py Test class: Test<ClassName> Test function: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass","title":"Test Structure"},{"location":"contributing/code-style/#pre-commit-hooks","text":"","title":"Pre-commit Hooks"},{"location":"contributing/code-style/#setup","text":"pip install pre-commit pre-commit install","title":"Setup"},{"location":"contributing/code-style/#hooks","text":"We use pre-commit hooks for: - Black formatting - isort import sorting - Flake8 linting - mypy type checking","title":"Hooks"},{"location":"contributing/code-style/#best-practices","text":"Format Before Committing : Run Black and isort Type Hints : Add type hints to all functions Documentation : Document public APIs Error Handling : Use custom exceptions Testing : Write tests for new features","title":"Best Practices"},{"location":"contributing/code-style/#next-steps","text":"Testing Guide - Testing guidelines Contributing Guide - General contributing guide","title":"Next Steps"},{"location":"contributing/guide/","text":"Contributing Guide \u00b6 Thank you for your interest in contributing to AlignTune! This guide will help you get started. Getting Started \u00b6 1. Fork and Clone \u00b6 # Fork the repository on GitHub # Then clone your fork git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune 2. Set Up Development Environment \u00b6 # Install in development mode pip install -e \".[dev]\" # Install pre-commit hooks pre-commit install 3. Create a Branch \u00b6 git checkout -b feature/your-feature-name Contribution Areas \u00b6 Code Contributions \u00b6 Bug fixes New features Performance improvements Documentation updates Documentation Contributions \u00b6 Fix typos Improve clarity Add examples Update guides Testing Contributions \u00b6 Add test cases Improve test coverage Fix failing tests Development Workflow \u00b6 1. Make Changes \u00b6 Make your changes following the Code Style Guide . 2. Write Tests \u00b6 Add tests for your changes: # tests/test_your_feature.py def test_your_feature (): # Your test code pass 3. Run Tests \u00b6 # Run all tests pytest # Run specific test pytest tests/test_your_feature.py # Run with coverage pytest --cov = aligntune 4. Check Code Quality \u00b6 # Format code black src/ tests/ # Sort imports isort src/ tests/ # Type check mypy src/ # Lint ruff check src/ tests/ 5. Commit Changes \u00b6 git add . git commit -m \"Add: your feature description\" 6. Push and Create PR \u00b6 git push origin feature/your-feature-name # Create PR on GitHub Code Style \u00b6 See Code Style Guide for detailed guidelines. Testing \u00b6 See Testing Guide for testing guidelines. Pull Request Process \u00b6 Update Documentation : Update relevant documentation Add Tests : Add tests for new features Update Changelog : Add entry to CHANGELOG.md Ensure Tests Pass : All tests must pass Code Review : Address review comments Questions? \u00b6 Open an issue for questions Join discussions on GitHub Check existing issues and PRs Thank you for contributing!","title":"Contributing Guide"},{"location":"contributing/guide/#contributing-guide","text":"Thank you for your interest in contributing to AlignTune! This guide will help you get started.","title":"Contributing Guide"},{"location":"contributing/guide/#getting-started","text":"","title":"Getting Started"},{"location":"contributing/guide/#1-fork-and-clone","text":"# Fork the repository on GitHub # Then clone your fork git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune","title":"1. Fork and Clone"},{"location":"contributing/guide/#2-set-up-development-environment","text":"# Install in development mode pip install -e \".[dev]\" # Install pre-commit hooks pre-commit install","title":"2. Set Up Development Environment"},{"location":"contributing/guide/#3-create-a-branch","text":"git checkout -b feature/your-feature-name","title":"3. Create a Branch"},{"location":"contributing/guide/#contribution-areas","text":"","title":"Contribution Areas"},{"location":"contributing/guide/#code-contributions","text":"Bug fixes New features Performance improvements Documentation updates","title":"Code Contributions"},{"location":"contributing/guide/#documentation-contributions","text":"Fix typos Improve clarity Add examples Update guides","title":"Documentation Contributions"},{"location":"contributing/guide/#testing-contributions","text":"Add test cases Improve test coverage Fix failing tests","title":"Testing Contributions"},{"location":"contributing/guide/#development-workflow","text":"","title":"Development Workflow"},{"location":"contributing/guide/#1-make-changes","text":"Make your changes following the Code Style Guide .","title":"1. Make Changes"},{"location":"contributing/guide/#2-write-tests","text":"Add tests for your changes: # tests/test_your_feature.py def test_your_feature (): # Your test code pass","title":"2. Write Tests"},{"location":"contributing/guide/#3-run-tests","text":"# Run all tests pytest # Run specific test pytest tests/test_your_feature.py # Run with coverage pytest --cov = aligntune","title":"3. Run Tests"},{"location":"contributing/guide/#4-check-code-quality","text":"# Format code black src/ tests/ # Sort imports isort src/ tests/ # Type check mypy src/ # Lint ruff check src/ tests/","title":"4. Check Code Quality"},{"location":"contributing/guide/#5-commit-changes","text":"git add . git commit -m \"Add: your feature description\"","title":"5. Commit Changes"},{"location":"contributing/guide/#6-push-and-create-pr","text":"git push origin feature/your-feature-name # Create PR on GitHub","title":"6. Push and Create PR"},{"location":"contributing/guide/#code-style","text":"See Code Style Guide for detailed guidelines.","title":"Code Style"},{"location":"contributing/guide/#testing","text":"See Testing Guide for testing guidelines.","title":"Testing"},{"location":"contributing/guide/#pull-request-process","text":"Update Documentation : Update relevant documentation Add Tests : Add tests for new features Update Changelog : Add entry to CHANGELOG.md Ensure Tests Pass : All tests must pass Code Review : Address review comments","title":"Pull Request Process"},{"location":"contributing/guide/#questions","text":"Open an issue for questions Join discussions on GitHub Check existing issues and PRs Thank you for contributing!","title":"Questions?"},{"location":"contributing/testing/","text":"Testing Guide \u00b6 Testing guidelines for contributing to AlignTune. Overview \u00b6 AlignTune uses pytest for testing with comprehensive coverage requirements. Test Structure \u00b6 Directory Layout \u00b6 tests/ __init__.py core/ test_backend_factory.py test_config.py backends/ trl/ unsloth/ integration/ test_training_pipeline.py fixtures/ sample_data.py Test File Naming \u00b6 Test files: test_<module_name>.py Test classes: Test<ClassName> Test functions: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass Writing Tests \u00b6 Basic Test \u00b6 import pytest from aligntune.core.backend_factory import create_sft_trainer def test_create_sft_trainer (): \"\"\"Test basic SFT trainer creation.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 ) assert trainer is not None assert trainer . config . model . name_or_path == \"microsoft/DialoGPT-small\" Fixtures \u00b6 Use pytest fixtures for reusable test data: import pytest from aligntune.core.sft.config import SFTConfig @pytest . fixture def sample_config (): \"\"\"Sample SFT configuration.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = \"test-model\" ), dataset = DatasetConfig ( name = \"test-dataset\" ) ) def test_trainer_with_config ( sample_config ): \"\"\"Test trainer with sample config.\"\"\" trainer = create_sft_trainer_from_config ( sample_config ) assert trainer . config == sample_config Parametrized Tests \u00b6 Use @pytest.mark.parametrize for multiple test cases: @pytest . mark . parametrize ( \"backend\" , [ \"trl\" , \"unsloth\" ]) def test_backend_selection ( backend ): \"\"\"Test different backend selections.\"\"\" trainer = create_sft_trainer ( model_name = \"model\" , dataset_name = \"dataset\" , backend = backend ) assert trainer . backend == backend Test Markers \u00b6 Available Markers \u00b6 @pytest . mark . slow # Slow tests @pytest . mark . integration # Integration tests @pytest . mark . gpu # Requires GPU @pytest . mark . unsloth # Requires Unsloth @pytest . mark . wandb # Requires WandB Usage \u00b6 @pytest . mark . slow def test_large_model_training (): \"\"\"Test training with large model.\"\"\" pass @pytest . mark . gpu def test_unsloth_backend (): \"\"\"Test Unsloth backend (requires GPU).\"\"\" pass Running Marked Tests \u00b6 # Run all tests except slow pytest -m \"not slow\" # Run only GPU tests pytest -m gpu # Run integration tests pytest -m integration Test Coverage \u00b6 Coverage Requirements \u00b6 Target: >80% coverage Critical paths: 100% coverage New features: Must include tests Running Coverage \u00b6 # Run with coverage pytest --cov = src/aligntune --cov-report = html # View HTML report open htmlcov/index.html Mocking \u00b6 External Dependencies \u00b6 Mock external dependencies: from unittest.mock import Mock , patch @patch ( 'aligntune.core.backend_factory.load_model' ) def test_model_loading ( mock_load ): \"\"\"Test model loading with mock.\"\"\" mock_load . return_value = Mock () trainer = create_sft_trainer ( ... ) mock_load . assert_called_once () Integration Tests \u00b6 Training Pipeline \u00b6 @pytest . mark . integration def test_sft_training_pipeline (): \"\"\"Test complete SFT training pipeline.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 , num_epochs = 1 ) # Train results = trainer . train () assert \"train_loss\" in results # Evaluate metrics = trainer . evaluate () assert \"eval_loss\" in metrics # Save path = trainer . save_model () assert path is not None Best Practices \u00b6 Test Isolation : Each test should be independent Fast Tests : Keep unit tests fast (<1s) Clear Names : Use descriptive test names One Assertion : One concept per test Fixtures : Use fixtures for setup/teardown Running Tests \u00b6 All Tests \u00b6 pytest Specific Test \u00b6 pytest tests/core/test_backend_factory.py::test_create_sft_trainer Verbose Output \u00b6 pytest -v With Coverage \u00b6 pytest --cov = src/aligntune --cov-report = term-missing Continuous Integration \u00b6 Tests run automatically on: - Pull requests - Pushes to main - Scheduled runs Next Steps \u00b6 Code Style Guide - Code style guidelines Contributing Guide - General contributing guide","title":"Testing"},{"location":"contributing/testing/#testing-guide","text":"Testing guidelines for contributing to AlignTune.","title":"Testing Guide"},{"location":"contributing/testing/#overview","text":"AlignTune uses pytest for testing with comprehensive coverage requirements.","title":"Overview"},{"location":"contributing/testing/#test-structure","text":"","title":"Test Structure"},{"location":"contributing/testing/#directory-layout","text":"tests/ __init__.py core/ test_backend_factory.py test_config.py backends/ trl/ unsloth/ integration/ test_training_pipeline.py fixtures/ sample_data.py","title":"Directory Layout"},{"location":"contributing/testing/#test-file-naming","text":"Test files: test_<module_name>.py Test classes: Test<ClassName> Test functions: test_<function_name> # test_backend_factory.py class TestBackendFactory : def test_create_sft_trainer ( self ): \"\"\"Test SFT trainer creation.\"\"\" pass","title":"Test File Naming"},{"location":"contributing/testing/#writing-tests","text":"","title":"Writing Tests"},{"location":"contributing/testing/#basic-test","text":"import pytest from aligntune.core.backend_factory import create_sft_trainer def test_create_sft_trainer (): \"\"\"Test basic SFT trainer creation.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 ) assert trainer is not None assert trainer . config . model . name_or_path == \"microsoft/DialoGPT-small\"","title":"Basic Test"},{"location":"contributing/testing/#fixtures","text":"Use pytest fixtures for reusable test data: import pytest from aligntune.core.sft.config import SFTConfig @pytest . fixture def sample_config (): \"\"\"Sample SFT configuration.\"\"\" return SFTConfig ( model = ModelConfig ( name_or_path = \"test-model\" ), dataset = DatasetConfig ( name = \"test-dataset\" ) ) def test_trainer_with_config ( sample_config ): \"\"\"Test trainer with sample config.\"\"\" trainer = create_sft_trainer_from_config ( sample_config ) assert trainer . config == sample_config","title":"Fixtures"},{"location":"contributing/testing/#parametrized-tests","text":"Use @pytest.mark.parametrize for multiple test cases: @pytest . mark . parametrize ( \"backend\" , [ \"trl\" , \"unsloth\" ]) def test_backend_selection ( backend ): \"\"\"Test different backend selections.\"\"\" trainer = create_sft_trainer ( model_name = \"model\" , dataset_name = \"dataset\" , backend = backend ) assert trainer . backend == backend","title":"Parametrized Tests"},{"location":"contributing/testing/#test-markers","text":"","title":"Test Markers"},{"location":"contributing/testing/#available-markers","text":"@pytest . mark . slow # Slow tests @pytest . mark . integration # Integration tests @pytest . mark . gpu # Requires GPU @pytest . mark . unsloth # Requires Unsloth @pytest . mark . wandb # Requires WandB","title":"Available Markers"},{"location":"contributing/testing/#usage","text":"@pytest . mark . slow def test_large_model_training (): \"\"\"Test training with large model.\"\"\" pass @pytest . mark . gpu def test_unsloth_backend (): \"\"\"Test Unsloth backend (requires GPU).\"\"\" pass","title":"Usage"},{"location":"contributing/testing/#running-marked-tests","text":"# Run all tests except slow pytest -m \"not slow\" # Run only GPU tests pytest -m gpu # Run integration tests pytest -m integration","title":"Running Marked Tests"},{"location":"contributing/testing/#test-coverage","text":"","title":"Test Coverage"},{"location":"contributing/testing/#coverage-requirements","text":"Target: >80% coverage Critical paths: 100% coverage New features: Must include tests","title":"Coverage Requirements"},{"location":"contributing/testing/#running-coverage","text":"# Run with coverage pytest --cov = src/aligntune --cov-report = html # View HTML report open htmlcov/index.html","title":"Running Coverage"},{"location":"contributing/testing/#mocking","text":"","title":"Mocking"},{"location":"contributing/testing/#external-dependencies","text":"Mock external dependencies: from unittest.mock import Mock , patch @patch ( 'aligntune.core.backend_factory.load_model' ) def test_model_loading ( mock_load ): \"\"\"Test model loading with mock.\"\"\" mock_load . return_value = Mock () trainer = create_sft_trainer ( ... ) mock_load . assert_called_once ()","title":"External Dependencies"},{"location":"contributing/testing/#integration-tests","text":"","title":"Integration Tests"},{"location":"contributing/testing/#training-pipeline","text":"@pytest . mark . integration def test_sft_training_pipeline (): \"\"\"Test complete SFT training pipeline.\"\"\" trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , max_samples = 10 , num_epochs = 1 ) # Train results = trainer . train () assert \"train_loss\" in results # Evaluate metrics = trainer . evaluate () assert \"eval_loss\" in metrics # Save path = trainer . save_model () assert path is not None","title":"Training Pipeline"},{"location":"contributing/testing/#best-practices","text":"Test Isolation : Each test should be independent Fast Tests : Keep unit tests fast (<1s) Clear Names : Use descriptive test names One Assertion : One concept per test Fixtures : Use fixtures for setup/teardown","title":"Best Practices"},{"location":"contributing/testing/#running-tests","text":"","title":"Running Tests"},{"location":"contributing/testing/#all-tests","text":"pytest","title":"All Tests"},{"location":"contributing/testing/#specific-test","text":"pytest tests/core/test_backend_factory.py::test_create_sft_trainer","title":"Specific Test"},{"location":"contributing/testing/#verbose-output","text":"pytest -v","title":"Verbose Output"},{"location":"contributing/testing/#with-coverage","text":"pytest --cov = src/aligntune --cov-report = term-missing","title":"With Coverage"},{"location":"contributing/testing/#continuous-integration","text":"Tests run automatically on: - Pull requests - Pushes to main - Scheduled runs","title":"Continuous Integration"},{"location":"contributing/testing/#next-steps","text":"Code Style Guide - Code style guidelines Contributing Guide - General contributing guide","title":"Next Steps"},{"location":"examples/advanced/","text":"Advanced Examples \u00b6 Advanced examples demonstrating sophisticated use cases with AlignTune. Distributed Training \u00b6 distributed training DDP Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer import torch # Configure for DDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 2 , # Per GPU gradient_accumulation_steps = 4 ) # Run with accelerate # accelerate launch --multi_gpu train.py trainer . train () FSDP Training \u00b6 # Configure for FSDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 1 , gradient_accumulation_steps = 8 ) # Run with accelerate FSDP # accelerate launch --fsdp train.py trainer . train () Custom Reward Functions \u00b6 Creating Custom Rewards \u00b6 from aligntune.rewards.types import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry class DomainSpecificReward ( RewardFunction ): def __init__ ( self ): def compute ( self , text : str , ** kwargs ) -> float : # Your custom logic score = 0.0 if \"domain_keyword\" in text . lower (): score += 0.5 # ... more logic return score # Register RewardRegistry . register_reward ( \"domain_specific\" , DomainSpecificReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , reward_functions = [ \"domain_specific\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] ) Production Pipeline \u00b6 Complete Production Setup \u00b6 from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer import logging from pathlib import Path logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def production_pipeline (): \"\"\"Complete production training pipeline.\"\"\" # Stage 1: SFT logger . info ( \"Stage 1: SFT Training\" ) sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , output_dir = \"./output/sft\" , eval_interval = 100 , save_interval = 500 ) sft_trainer . train () sft_path = sft_trainer . save_model () logger . info ( f \"SFT model saved to: { sft_path } \" ) # Stage 2: DPO logger . info ( \"Stage 2: DPO Training\" ) dpo_trainer = create_rl_trainer ( model_name = sft_path , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , output_dir = \"./output/dpo\" ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () logger . info ( f \"DPO model saved to: { dpo_path } \" ) # Stage 3: Evaluation logger . info ( \"Stage 3: Evaluation\" ) metrics = dpo_trainer . evaluate () logger . info ( f \"Final metrics: { metrics } \" ) # Stage 4: Push to Hub logger . info ( \"Stage 4: Pushing to Hub\" ) url = dpo_trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) logger . info ( f \"Model available at: { url } \" ) if __name__ == \"__main__\" : production_pipeline () Custom Domain Training \u00b6 Medical Domain Example \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"medical-dataset\" , backend = \"trl\" , task_type = \"instruction_following\" , num_epochs = 5 , batch_size = 4 , learning_rate = 1e-5 , max_seq_length = 1024 , # Custom column mappings column_mapping = { \"instruction\" : \"medical_question\" , \"output\" : \"medical_answer\" , \"input\" : \"patient_context\" } ) trainer . train () Integration Examples \u00b6 Weights & Biases Integration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"wandb\" ], # Enable W&B run_name = \"my-experiment\" ) trainer . train () TensorBoard Integration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"tensorboard\" ], output_dir = \"./output/tensorboard\" ) trainer . train () # View with: tensorboard --logdir ./output/tensorboard Memory Optimization \u00b6 Large Model Training \u00b6 trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Memory optimizations quantization = { \"load_in_4bit\" : True }, peft_enabled = True , lora_r = 16 , lora_alpha = 32 , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 , activation_offloading = True ) trainer . train () Next Steps \u00b6 SFT Examples - Basic SFT examples RL Examples - RL training examples Advanced Topics - Architecture details","title":"Advanced Examples"},{"location":"examples/advanced/#advanced-examples","text":"Advanced examples demonstrating sophisticated use cases with AlignTune.","title":"Advanced Examples"},{"location":"examples/advanced/#distributed-training","text":"","title":"Distributed Training"},{"location":"examples/advanced/#distributed-training-ddp-training","text":"from aligntune.core.backend_factory import create_sft_trainer import torch # Configure for DDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 2 , # Per GPU gradient_accumulation_steps = 4 ) # Run with accelerate # accelerate launch --multi_gpu train.py trainer . train ()","title":"distributed training DDP Training"},{"location":"examples/advanced/#fsdp-training","text":"# Configure for FSDP trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 1 , gradient_accumulation_steps = 8 ) # Run with accelerate FSDP # accelerate launch --fsdp train.py trainer . train ()","title":"FSDP Training"},{"location":"examples/advanced/#custom-reward-functions","text":"","title":"Custom Reward Functions"},{"location":"examples/advanced/#creating-custom-rewards","text":"from aligntune.rewards.types import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry class DomainSpecificReward ( RewardFunction ): def __init__ ( self ): def compute ( self , text : str , ** kwargs ) -> float : # Your custom logic score = 0.0 if \"domain_keyword\" in text . lower (): score += 0.5 # ... more logic return score # Register RewardRegistry . register_reward ( \"domain_specific\" , DomainSpecificReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , reward_functions = [ \"domain_specific\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] )","title":"Creating Custom Rewards"},{"location":"examples/advanced/#production-pipeline","text":"","title":"Production Pipeline"},{"location":"examples/advanced/#complete-production-setup","text":"from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer import logging from pathlib import Path logging . basicConfig ( level = logging . INFO ) logger = logging . getLogger ( __name__ ) def production_pipeline (): \"\"\"Complete production training pipeline.\"\"\" # Stage 1: SFT logger . info ( \"Stage 1: SFT Training\" ) sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , output_dir = \"./output/sft\" , eval_interval = 100 , save_interval = 500 ) sft_trainer . train () sft_path = sft_trainer . save_model () logger . info ( f \"SFT model saved to: { sft_path } \" ) # Stage 2: DPO logger . info ( \"Stage 2: DPO Training\" ) dpo_trainer = create_rl_trainer ( model_name = sft_path , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , output_dir = \"./output/dpo\" ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () logger . info ( f \"DPO model saved to: { dpo_path } \" ) # Stage 3: Evaluation logger . info ( \"Stage 3: Evaluation\" ) metrics = dpo_trainer . evaluate () logger . info ( f \"Final metrics: { metrics } \" ) # Stage 4: Push to Hub logger . info ( \"Stage 4: Pushing to Hub\" ) url = dpo_trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) logger . info ( f \"Model available at: { url } \" ) if __name__ == \"__main__\" : production_pipeline ()","title":"Complete Production Setup"},{"location":"examples/advanced/#custom-domain-training","text":"","title":"Custom Domain Training"},{"location":"examples/advanced/#medical-domain-example","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"medical-dataset\" , backend = \"trl\" , task_type = \"instruction_following\" , num_epochs = 5 , batch_size = 4 , learning_rate = 1e-5 , max_seq_length = 1024 , # Custom column mappings column_mapping = { \"instruction\" : \"medical_question\" , \"output\" : \"medical_answer\" , \"input\" : \"patient_context\" } ) trainer . train ()","title":"Medical Domain Example"},{"location":"examples/advanced/#integration-examples","text":"","title":"Integration Examples"},{"location":"examples/advanced/#weights-biases-integration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"wandb\" ], # Enable W&B run_name = \"my-experiment\" ) trainer . train ()","title":"Weights &amp; Biases Integration"},{"location":"examples/advanced/#tensorboard-integration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , loggers = [ \"tensorboard\" ], output_dir = \"./output/tensorboard\" ) trainer . train () # View with: tensorboard --logdir ./output/tensorboard","title":"TensorBoard Integration"},{"location":"examples/advanced/#memory-optimization","text":"","title":"Memory Optimization"},{"location":"examples/advanced/#large-model-training","text":"trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Memory optimizations quantization = { \"load_in_4bit\" : True }, peft_enabled = True , lora_r = 16 , lora_alpha = 32 , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 , activation_offloading = True ) trainer . train ()","title":"Large Model Training"},{"location":"examples/advanced/#next-steps","text":"SFT Examples - Basic SFT examples RL Examples - RL training examples Advanced Topics - Architecture details","title":"Next Steps"},{"location":"examples/overview/","text":"Examples Overview \u00b6 This section contains comprehensive examples for using AlignTune. Quick Links \u00b6 SFT Examples - Supervised Fine-Tuning examples RL Examples - Reinforcement Learning examples Advanced Examples - Advanced use cases Example Categories \u00b6 Basic Examples \u00b6 Simple SFT training Basic DPO training Model evaluation Model saving and loading Intermediate Examples \u00b6 Custom reward functions Multi-dataset training Custom evaluation metrics Checkpoint management Advanced Examples \u00b6 Distributed training Custom backends Reward model training Performance optimization Running Examples \u00b6 All examples can be run directly: # Run SFT example python examples/sft_customer_support_trl/train_sft_direct_api.py # Run DPO example python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py Example Structure \u00b6 Examples are organized by: Task Type : SFT, DPO, PPO, GRPO, GSPO Backend : TRL, Unsloth Complexity : Basic, Intermediate, Advanced Next Steps \u00b6 SFT Examples - Start with SFT RL Examples - Learn RL training Advanced Examples - Explore advanced features","title":"Overview"},{"location":"examples/overview/#examples-overview","text":"This section contains comprehensive examples for using AlignTune.","title":"Examples Overview"},{"location":"examples/overview/#quick-links","text":"SFT Examples - Supervised Fine-Tuning examples RL Examples - Reinforcement Learning examples Advanced Examples - Advanced use cases","title":"Quick Links"},{"location":"examples/overview/#example-categories","text":"","title":"Example Categories"},{"location":"examples/overview/#basic-examples","text":"Simple SFT training Basic DPO training Model evaluation Model saving and loading","title":"Basic Examples"},{"location":"examples/overview/#intermediate-examples","text":"Custom reward functions Multi-dataset training Custom evaluation metrics Checkpoint management","title":"Intermediate Examples"},{"location":"examples/overview/#advanced-examples","text":"Distributed training Custom backends Reward model training Performance optimization","title":"Advanced Examples"},{"location":"examples/overview/#running-examples","text":"All examples can be run directly: # Run SFT example python examples/sft_customer_support_trl/train_sft_direct_api.py # Run DPO example python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py","title":"Running Examples"},{"location":"examples/overview/#example-structure","text":"Examples are organized by: Task Type : SFT, DPO, PPO, GRPO, GSPO Backend : TRL, Unsloth Complexity : Basic, Intermediate, Advanced","title":"Example Structure"},{"location":"examples/overview/#next-steps","text":"SFT Examples - Start with SFT RL Examples - Learn RL training Advanced Examples - Explore advanced features","title":"Next Steps"},{"location":"examples/rl/","text":"RL Examples \u00b6 Comprehensive examples for Reinforcement Learning training with AlignTune. Basic Examples \u00b6 1. DPO Training \u00b6 Basic DPO training example: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/1: 100%|| 250/250 [08:45<00:00, loss=0.234] Model saved to: ./output/model 2. PPO with Pre-trained Reward Model \u00b6 PPO training with HuggingFace reward model: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , cliprange = 0.2 ) trainer . train () 3. PPO with Custom Reward Model \u00b6 Train custom reward model during PPO: def load_training_texts (): return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () Advanced Examples \u00b6 4. GRPO Training \u00b6 Group Relative Policy Optimization: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' , ) trainer . train () 5. GSPO Training \u00b6 Group Sequential Policy Optimization (TRL only): trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , ) trainer . train () 6. Multi-Stage Pipeline \u00b6 SFT \u2192 DPO \u2192 PPO pipeline: # Stage 1: SFT from aligntune.core.backend_factory import create_sft_trainer sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) sft_trainer . train () sft_path = sft_trainer . save_model () # Stage 2: DPO dpo_trainer = create_rl_trainer ( model_name = sft_path , # Start from SFT model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () # Stage 3: PPO ppo_trainer = create_rl_trainer ( model_name = dpo_path , # Start from DPO model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 ) ppo_trainer . train () Complete Workflow Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer from datasets import load_dataset # Create trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ] # or set use_peft = False eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Running Examples \u00b6 From Command Line \u00b6 # DPO training python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py # PPO training python examples/ppo_ultrachat_unsloth/train_ppo_ultrachat_unsloth.py # GRPO training python examples/grpo_gsm8k_trl/train_grpo_direct_api.py Tips \u00b6 Start with DPO : Simpler setup, no reward model needed Model Family Consistency : For PPO, ensure all models same family Reward Models : Use pre-trained when available, train custom for domains Backend Selection : Unsloth for speed, TRL for compatibility/GSPO Memory Management : Use smaller batch sizes for PPO Next Steps \u00b6 SFT Examples - SFT training examples Advanced Examples - Advanced use cases RL Guide - Complete RL guide","title":"RL Examples"},{"location":"examples/rl/#rl-examples","text":"Comprehensive examples for Reinforcement Learning training with AlignTune.","title":"RL Examples"},{"location":"examples/rl/#basic-examples","text":"","title":"Basic Examples"},{"location":"examples/rl/#1-dpo-training","text":"Basic DPO training example: from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/1: 100%|| 250/250 [08:45<00:00, loss=0.234] Model saved to: ./output/model","title":"1. DPO Training"},{"location":"examples/rl/#2-ppo-with-pre-trained-reward-model","text":"PPO training with HuggingFace reward model: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , cliprange = 0.2 ) trainer . train ()","title":"2. PPO with Pre-trained Reward Model"},{"location":"examples/rl/#3-ppo-with-custom-reward-model","text":"Train custom reward model during PPO: def load_training_texts (): return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"3. PPO with Custom Reward Model"},{"location":"examples/rl/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"examples/rl/#4-grpo-training","text":"Group Relative Policy Optimization: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' , ) trainer . train ()","title":"4. GRPO Training"},{"location":"examples/rl/#5-gspo-training","text":"Group Sequential Policy Optimization (TRL only): trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , ) trainer . train ()","title":"5. GSPO Training"},{"location":"examples/rl/#6-multi-stage-pipeline","text":"SFT \u2192 DPO \u2192 PPO pipeline: # Stage 1: SFT from aligntune.core.backend_factory import create_sft_trainer sft_trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) sft_trainer . train () sft_path = sft_trainer . save_model () # Stage 2: DPO dpo_trainer = create_rl_trainer ( model_name = sft_path , # Start from SFT model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) dpo_trainer . train () dpo_path = dpo_trainer . save_model () # Stage 3: PPO ppo_trainer = create_rl_trainer ( model_name = dpo_path , # Start from DPO model dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 ) ppo_trainer . train ()","title":"6. Multi-Stage Pipeline"},{"location":"examples/rl/#complete-workflow-example","text":"from aligntune.core.backend_factory import create_rl_trainer from datasets import load_dataset # Create trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ] # or set use_peft = False eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Complete Workflow Example"},{"location":"examples/rl/#running-examples","text":"","title":"Running Examples"},{"location":"examples/rl/#from-command-line","text":"# DPO training python examples/dpo_intel_orca_trl/train_dpo_intel_orca_trl.py # PPO training python examples/ppo_ultrachat_unsloth/train_ppo_ultrachat_unsloth.py # GRPO training python examples/grpo_gsm8k_trl/train_grpo_direct_api.py","title":"From Command Line"},{"location":"examples/rl/#tips","text":"Start with DPO : Simpler setup, no reward model needed Model Family Consistency : For PPO, ensure all models same family Reward Models : Use pre-trained when available, train custom for domains Backend Selection : Unsloth for speed, TRL for compatibility/GSPO Memory Management : Use smaller batch sizes for PPO","title":"Tips"},{"location":"examples/rl/#next-steps","text":"SFT Examples - SFT training examples Advanced Examples - Advanced use cases RL Guide - Complete RL guide","title":"Next Steps"},{"location":"examples/sft/","text":"SFT Examples \u00b6 Comprehensive examples for Supervised Fine-Tuning with AlignTune. Basic Examples \u00b6 1. Basic SFT Training \u00b6 Simple SFT training example: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 based models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/3: 100%|| 250/250 [05:23<00:00, loss=2.34] Epoch 2/3: 100%|| 250/250 [05:18<00:00, loss=1.89] Epoch 3/3: 100%|| 250/250 [05:21<00:00, loss=1.67] Model saved to: ./output/model 2. Instruction Following \u00b6 Train model to follow instructions: trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , # Use Qwen/Llama instead of GPT2 dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , peft_enabled = True , instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () 3. Text Classification \u00b6 Note: For GPT2 models, use backend=\"trl\" and lora_target_modules=[\"c_attn\", \"c_proj\"] . GPT2 is not supported by Unsloth. Train classification model: trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) trainer . train () metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) Advanced Examples \u00b6 4. LoRA Fine-Tuning \u00b6 Efficient fine-tuning with LoRA: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , quantization = { \"load_in_4bit\" : True }, num_epochs = 3 , batch_size = 2 , learning_rate = 2e-4 ) trainer . train () 5. Sequence Packing \u00b6 Efficient training with sequence packing: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models packing = True , packing_strategy = \"bfd\" , padding_free = True , max_seq_length = 2048 , num_epochs = 3 , batch_size = 4 ) trainer . train () 6. Chat Completion \u00b6 Train conversational model: trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"train_sft\" , # Dataset split backend = \"unsloth\" , task_type = \"chat_completion\" , num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" ) trainer . train () Complete Workflow Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) # Push to Hub (optional) # url = trainer.push_to_hub(\"username/my-model\") # print(f\"Model available at: {url}\") Running Examples \u00b6 From Command Line \u00b6 # Basic SFT python examples/sft_customer_support_trl/train_sft_direct_api.py # Instruction following python examples/sft_financial_summarization_trl/train_sft_direct_api.py Tips \u00b6 Start Small : Use max_samples=1000 for testing Monitor Training : Set eval_interval to track progress Save Regularly : Use save_interval for checkpoints Choose Backend : Use Unsloth for speed, TRL for compatibility Memory Management : Use LoRA and quantization for large models Next Steps \u00b6 RL Examples - RL training examples Advanced Examples - Advanced use cases SFT Guide - Complete SFT guide","title":"SFT Examples"},{"location":"examples/sft/#sft-examples","text":"Comprehensive examples for Supervised Fine-Tuning with AlignTune.","title":"SFT Examples"},{"location":"examples/sft/#basic-examples","text":"","title":"Basic Examples"},{"location":"examples/sft/#1-basic-sft-training","text":"Simple SFT training example: from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 based models ) trainer . train () model_path = trainer . save_model () Expected Output: Training started... Epoch 1/3: 100%|| 250/250 [05:23<00:00, loss=2.34] Epoch 2/3: 100%|| 250/250 [05:18<00:00, loss=1.89] Epoch 3/3: 100%|| 250/250 [05:21<00:00, loss=1.67] Model saved to: ./output/model","title":"1. Basic SFT Training"},{"location":"examples/sft/#2-instruction-following","text":"Train model to follow instructions: trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , # Use Qwen/Llama instead of GPT2 dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , peft_enabled = True , instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train ()","title":"2. Instruction Following"},{"location":"examples/sft/#3-text-classification","text":"Note: For GPT2 models, use backend=\"trl\" and lora_target_modules=[\"c_attn\", \"c_proj\"] . GPT2 is not supported by Unsloth. Train classification model: trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) trainer . train () metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" )","title":"3. Text Classification"},{"location":"examples/sft/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"examples/sft/#4-lora-fine-tuning","text":"Efficient fine-tuning with LoRA: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , quantization = { \"load_in_4bit\" : True }, num_epochs = 3 , batch_size = 2 , learning_rate = 2e-4 ) trainer . train ()","title":"4. LoRA Fine-Tuning"},{"location":"examples/sft/#5-sequence-packing","text":"Efficient training with sequence packing: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models packing = True , packing_strategy = \"bfd\" , padding_free = True , max_seq_length = 2048 , num_epochs = 3 , batch_size = 4 ) trainer . train ()","title":"5. Sequence Packing"},{"location":"examples/sft/#6-chat-completion","text":"Train conversational model: trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"train_sft\" , # Dataset split backend = \"unsloth\" , task_type = \"chat_completion\" , num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" ) trainer . train ()","title":"6. Chat Completion"},{"location":"examples/sft/#complete-workflow-example","text":"from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 , eval_interval = 100 , save_interval = 500 ) # Train print ( \"Starting training...\" ) results = trainer . train () print ( f \"Training completed: { results } \" ) # Evaluate print ( \"Evaluating...\" ) test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Evaluation metrics: { metrics } \" ) # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test predictions print ( \"Testing predictions...\" ) prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) # Push to Hub (optional) # url = trainer.push_to_hub(\"username/my-model\") # print(f\"Model available at: {url}\")","title":"Complete Workflow Example"},{"location":"examples/sft/#running-examples","text":"","title":"Running Examples"},{"location":"examples/sft/#from-command-line","text":"# Basic SFT python examples/sft_customer_support_trl/train_sft_direct_api.py # Instruction following python examples/sft_financial_summarization_trl/train_sft_direct_api.py","title":"From Command Line"},{"location":"examples/sft/#tips","text":"Start Small : Use max_samples=1000 for testing Monitor Training : Set eval_interval to track progress Save Regularly : Use save_interval for checkpoints Choose Backend : Use Unsloth for speed, TRL for compatibility Memory Management : Use LoRA and quantization for large models","title":"Tips"},{"location":"examples/sft/#next-steps","text":"RL Examples - RL training examples Advanced Examples - Advanced use cases SFT Guide - Complete SFT guide","title":"Next Steps"},{"location":"getting-started/backend-selection/","text":"Backend Selection \u00b6 AlignTune supports multiple backends for training. Choose the one that best fits your needs. Available Backends \u00b6 TRL Backend \u00b6 Reliability : Battle-tested, stable Compatibility : Works everywhere Performance : Standard speed Use Case : Production, reliability-focused Unsloth Backend \u00b6 Speed : faster training Memory : Optimized memory usage Compatibility : Requires GPU, specific setup Use Case : Fast iteration, research Backend Support Matrix \u00b6 Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes Automatic Backend Selection \u00b6 AlignTune can automatically select the best backend: from aligntune.core.backend_factory import create_sft_trainer # Auto-select backend (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically chooses best backend ) Manual Backend Selection \u00b6 TRL Backend \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Explicitly use TRL ) Unsloth Backend \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Explicitly use Unsloth ) Backend Selection Logic \u00b6 When using backend=\"auto\" , AlignTune: Checks if Unsloth is available Checks if model is compatible with Unsloth Falls back to TRL if Unsloth unavailable or incompatible Logs the selected backend Checking Backend Status \u00b6 from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl' ][ 'available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" ) When to Use Each Backend \u00b6 Use TRL Backend When: \u00b6 You need maximum reliability You're in production You don't have GPU or Unsloth setup You need GSPO algorithm You want battle-tested code Use Unsloth Backend When: \u00b6 You need faster training (faster training) You have GPU available You're doing research/experimentation You want memory optimization You're fine-tuning large models Backend-Specific Configuration \u00b6 TRL Backend \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Standard configuration works num_epochs = 3 , batch_size = 4 ) Unsloth Backend \u00b6 trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth-specific optimizations enabled num_epochs = 3 , batch_size = 4 ) Troubleshooting \u00b6 Unsloth Not Available \u00b6 If Unsloth is not available, AlignTune automatically falls back to TRL: # This will use TRL if Unsloth unavailable trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Falls back to TRL if Unsloth unavailable ) Force TRL Backend \u00b6 # Explicitly force TRL trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Always uses TRL ) Next Steps \u00b6 SFT Guide - SFT training details RL Guide - RL training details Unsloth Compatibility - Unsloth setup and troubleshooting","title":"Backend Selection"},{"location":"getting-started/backend-selection/#backend-selection","text":"AlignTune supports multiple backends for training. Choose the one that best fits your needs.","title":"Backend Selection"},{"location":"getting-started/backend-selection/#available-backends","text":"","title":"Available Backends"},{"location":"getting-started/backend-selection/#trl-backend","text":"Reliability : Battle-tested, stable Compatibility : Works everywhere Performance : Standard speed Use Case : Production, reliability-focused","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend","text":"Speed : faster training Memory : Optimized memory usage Compatibility : Requires GPU, specific setup Use Case : Fast iteration, research","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#backend-support-matrix","text":"Algorithm TRL Backend Unsloth Backend SFT Yes Yes DPO Yes Yes PPO Yes Yes GRPO Yes Yes GSPO Yes No DAPO Yes Yes Dr. GRPO Yes Yes","title":"Backend Support Matrix"},{"location":"getting-started/backend-selection/#automatic-backend-selection","text":"AlignTune can automatically select the best backend: from aligntune.core.backend_factory import create_sft_trainer # Auto-select backend (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically chooses best backend )","title":"Automatic Backend Selection"},{"location":"getting-started/backend-selection/#manual-backend-selection","text":"","title":"Manual Backend Selection"},{"location":"getting-started/backend-selection/#trl-backend_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Explicitly use TRL )","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend_1","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Explicitly use Unsloth )","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#backend-selection-logic","text":"When using backend=\"auto\" , AlignTune: Checks if Unsloth is available Checks if model is compatible with Unsloth Falls back to TRL if Unsloth unavailable or incompatible Logs the selected backend","title":"Backend Selection Logic"},{"location":"getting-started/backend-selection/#checking-backend-status","text":"from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"TRL available: { status [ 'trl' ][ 'available' ] } \" ) print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" )","title":"Checking Backend Status"},{"location":"getting-started/backend-selection/#when-to-use-each-backend","text":"","title":"When to Use Each Backend"},{"location":"getting-started/backend-selection/#use-trl-backend-when","text":"You need maximum reliability You're in production You don't have GPU or Unsloth setup You need GSPO algorithm You want battle-tested code","title":"Use TRL Backend When:"},{"location":"getting-started/backend-selection/#use-unsloth-backend-when","text":"You need faster training (faster training) You have GPU available You're doing research/experimentation You want memory optimization You're fine-tuning large models","title":"Use Unsloth Backend When:"},{"location":"getting-started/backend-selection/#backend-specific-configuration","text":"","title":"Backend-Specific Configuration"},{"location":"getting-started/backend-selection/#trl-backend_2","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Standard configuration works num_epochs = 3 , batch_size = 4 )","title":"TRL Backend"},{"location":"getting-started/backend-selection/#unsloth-backend_2","text":"trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth-specific optimizations enabled num_epochs = 3 , batch_size = 4 )","title":"Unsloth Backend"},{"location":"getting-started/backend-selection/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/backend-selection/#unsloth-not-available","text":"If Unsloth is not available, AlignTune automatically falls back to TRL: # This will use TRL if Unsloth unavailable trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" # Falls back to TRL if Unsloth unavailable )","title":"Unsloth Not Available"},{"location":"getting-started/backend-selection/#force-trl-backend","text":"# Explicitly force TRL trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Always uses TRL )","title":"Force TRL Backend"},{"location":"getting-started/backend-selection/#next-steps","text":"SFT Guide - SFT training details RL Guide - RL training details Unsloth Compatibility - Unsloth setup and troubleshooting","title":"Next Steps"},{"location":"getting-started/basic-concepts/","text":"Basic Concepts \u00b6 This guide introduces the core concepts you need to understand to effectively use AlignTune. Training Types \u00b6 AlignTune supports two main training paradigms: Supervised Fine-Tuning (SFT) \u00b6 SFT is the process of fine-tuning a pre-trained language model on a task-specific dataset. It's used for: Instruction following Text classification Chat completion Domain adaptation from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) trainer . train () Reinforcement Learning (RL) \u00b6 RL training uses reinforcement learning algorithms to align models with human preferences or optimize for specific objectives. Supported algorithms include: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Requires reward model GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) trainer . train () Backends \u00b6 AlignTune provides two backend implementations: TRL Backend \u00b6 TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation: Reliable : Production-ready, extensively tested Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models Slower : Standard training speed Use TRL when: - You need maximum reliability - You're using algorithms not supported by Unsloth (GSPO) - You're on CPU or have compatibility issues with Unsloth Unsloth Backend \u00b6 Unsloth backend provides optimized training with significant speed improvements: Fast : faster training Memory Efficient : Optimized memory usage with LoRA/QLoRA GPU Optimized : CUDA-specific optimizations Limited : Some algorithms not supported (GSPO) GPU Required : Needs CUDA-capable GPU Use Unsloth when: - You have a CUDA-capable GPU - You want maximum training speed - You're using supported algorithms (DPO, PPO, GRPO, etc.) Backend Selection \u00b6 You can specify the backend explicitly or use auto-selection: # Explicit backend selection trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # or \"unsloth\" ) # Auto-selection (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically selects best available backend ) Configuration \u00b6 AlignTune supports three configuration methods: 1. Python API \u00b6 Direct function calls with keyword arguments: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 ) 2. YAML Configuration \u00b6 Declarative configuration files: # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : num_epochs : 3 per_device_batch_size : 4 learning_rate : 5e-5 from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) trainer = create_sft_trainer ( ** config ) 3. CLI \u00b6 Command-line interface: aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 Reward Functions \u00b6 Reward functions are used in RL training to evaluate and guide model behavior. AlignTune provides 27+ built-in reward functions: Quality Metrics \u00b6 helpfulness - Measures how helpful the response is coherence - Evaluates text coherence and flow relevance - Measures relevance to the prompt fluency - Evaluates language fluency Safety Metrics \u00b6 toxicity - Detects toxic content bias - Measures bias in responses privacy - Detects privacy violations harmful_content - Identifies harmful content Style Metrics \u00b6 formality - Measures formality level tone - Evaluates tone appropriateness length - Measures response length structure - Evaluates text structure Task-Specific Metrics \u00b6 code_quality - Evaluates code quality math_accuracy - Measures mathematical accuracy factual_correctness - Evaluates factual accuracy Using Reward Functions \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.4 , 0.3 , 0.3 ] ) Evaluation \u00b6 AlignTune provides comprehensive evaluation capabilities: Basic Metrics \u00b6 Loss : Training loss Perplexity : Language modeling quality Runtime : Training time Quality Metrics \u00b6 BLEU : Bilingual Evaluation Understudy score ROUGE : Recall-Oriented Understudy for Gisting Evaluation Task-Specific : Code syntax, math accuracy, etc. Safety Metrics \u00b6 Toxicity : Toxicity detection Bias : Bias measurement Factual Accuracy : Factual correctness # Evaluate trained model metrics = trainer . evaluate () print ( metrics ) Model Management \u00b6 Saving Models \u00b6 # Save model to local directory path = trainer . save_model ( \"./output/my_model\" ) # Push to HuggingFace Hub trainer . push_to_hub ( \"username/my-model\" , token = \"hf_...\" ) Loading Models \u00b6 from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) Next Steps \u00b6 Configuration Guide - Learn about all configuration options Backend Selection - Detailed backend comparison SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#basic-concepts","text":"This guide introduces the core concepts you need to understand to effectively use AlignTune.","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#training-types","text":"AlignTune supports two main training paradigms:","title":"Training Types"},{"location":"getting-started/basic-concepts/#supervised-fine-tuning-sft","text":"SFT is the process of fine-tuning a pre-trained language model on a task-specific dataset. It's used for: Instruction following Text classification Chat completion Domain adaptation from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) trainer . train ()","title":"Supervised Fine-Tuning (SFT)"},{"location":"getting-started/basic-concepts/#reinforcement-learning-rl","text":"RL training uses reinforcement learning algorithms to align models with human preferences or optimize for specific objectives. Supported algorithms include: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Requires reward model GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) trainer . train ()","title":"Reinforcement Learning (RL)"},{"location":"getting-started/basic-concepts/#backends","text":"AlignTune provides two backend implementations:","title":"Backends"},{"location":"getting-started/basic-concepts/#trl-backend","text":"TRL (Transformers Reinforcement Learning) backend is the standard, battle-tested implementation: Reliable : Production-ready, extensively tested Complete : Supports all algorithms (SFT, DPO, PPO, GRPO, GSPO, etc.) Compatible : Works with all HuggingFace models Slower : Standard training speed Use TRL when: - You need maximum reliability - You're using algorithms not supported by Unsloth (GSPO) - You're on CPU or have compatibility issues with Unsloth","title":"TRL Backend"},{"location":"getting-started/basic-concepts/#unsloth-backend","text":"Unsloth backend provides optimized training with significant speed improvements: Fast : faster training Memory Efficient : Optimized memory usage with LoRA/QLoRA GPU Optimized : CUDA-specific optimizations Limited : Some algorithms not supported (GSPO) GPU Required : Needs CUDA-capable GPU Use Unsloth when: - You have a CUDA-capable GPU - You want maximum training speed - You're using supported algorithms (DPO, PPO, GRPO, etc.)","title":"Unsloth Backend"},{"location":"getting-started/basic-concepts/#backend-selection","text":"You can specify the backend explicitly or use auto-selection: # Explicit backend selection trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # or \"unsloth\" ) # Auto-selection (recommended) trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" # Automatically selects best available backend )","title":"Backend Selection"},{"location":"getting-started/basic-concepts/#configuration","text":"AlignTune supports three configuration methods:","title":"Configuration"},{"location":"getting-started/basic-concepts/#1-python-api","text":"Direct function calls with keyword arguments: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 )","title":"1. Python API"},{"location":"getting-started/basic-concepts/#2-yaml-configuration","text":"Declarative configuration files: # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" datasets : - name : \"tatsu-lab/alpaca\" max_samples : 1000 train : num_epochs : 3 per_device_batch_size : 4 learning_rate : 5e-5 from aligntune.core.rl.config_loader import ConfigLoader config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) trainer = create_sft_trainer ( ** config )","title":"2. YAML Configuration"},{"location":"getting-started/basic-concepts/#3-cli","text":"Command-line interface: aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4","title":"3. CLI"},{"location":"getting-started/basic-concepts/#reward-functions","text":"Reward functions are used in RL training to evaluate and guide model behavior. AlignTune provides 27+ built-in reward functions:","title":"Reward Functions"},{"location":"getting-started/basic-concepts/#quality-metrics","text":"helpfulness - Measures how helpful the response is coherence - Evaluates text coherence and flow relevance - Measures relevance to the prompt fluency - Evaluates language fluency","title":"Quality Metrics"},{"location":"getting-started/basic-concepts/#safety-metrics","text":"toxicity - Detects toxic content bias - Measures bias in responses privacy - Detects privacy violations harmful_content - Identifies harmful content","title":"Safety Metrics"},{"location":"getting-started/basic-concepts/#style-metrics","text":"formality - Measures formality level tone - Evaluates tone appropriateness length - Measures response length structure - Evaluates text structure","title":"Style Metrics"},{"location":"getting-started/basic-concepts/#task-specific-metrics","text":"code_quality - Evaluates code quality math_accuracy - Measures mathematical accuracy factual_correctness - Evaluates factual accuracy","title":"Task-Specific Metrics"},{"location":"getting-started/basic-concepts/#using-reward-functions","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_functions = [ \"helpfulness\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.4 , 0.3 , 0.3 ] )","title":"Using Reward Functions"},{"location":"getting-started/basic-concepts/#evaluation","text":"AlignTune provides comprehensive evaluation capabilities:","title":"Evaluation"},{"location":"getting-started/basic-concepts/#basic-metrics","text":"Loss : Training loss Perplexity : Language modeling quality Runtime : Training time","title":"Basic Metrics"},{"location":"getting-started/basic-concepts/#quality-metrics_1","text":"BLEU : Bilingual Evaluation Understudy score ROUGE : Recall-Oriented Understudy for Gisting Evaluation Task-Specific : Code syntax, math accuracy, etc.","title":"Quality Metrics"},{"location":"getting-started/basic-concepts/#safety-metrics_1","text":"Toxicity : Toxicity detection Bias : Bias measurement Factual Accuracy : Factual correctness # Evaluate trained model metrics = trainer . evaluate () print ( metrics )","title":"Safety Metrics"},{"location":"getting-started/basic-concepts/#model-management","text":"","title":"Model Management"},{"location":"getting-started/basic-concepts/#saving-models","text":"# Save model to local directory path = trainer . save_model ( \"./output/my_model\" ) # Push to HuggingFace Hub trainer . push_to_hub ( \"username/my-model\" , token = \"hf_...\" )","title":"Saving Models"},{"location":"getting-started/basic-concepts/#loading-models","text":"from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" )","title":"Loading Models"},{"location":"getting-started/basic-concepts/#next-steps","text":"Configuration Guide - Learn about all configuration options Backend Selection - Detailed backend comparison SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Next Steps"},{"location":"getting-started/configuration/","text":"Configuration \u00b6 AlignTune supports flexible configuration through Python code, YAML files, or CLI arguments. Configuration Methods \u00b6 1. Python Code (Recommended for Development) \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , output_dir = \"./output\" ) 2. YAML Files (Recommended for Production) \u00b6 # config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 logging : output_dir : \"./output\" run_name : \"my_experiment\" 3. CLI Arguments \u00b6 aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --epochs 3 \\ --batch-size 4 \\ --lr 5e-5 Configuration Structure \u00b6 Model Configuration \u00b6 model_config = { \"name_or_path\" : \"microsoft/DialoGPT-small\" , # Required \"precision\" : \"fp32\" , # fp32, fp16, bf16 \"max_seq_length\" : 2048 , \"gradient_checkpointing\" : True , \"use_unsloth\" : False } Dataset Configuration \u00b6 dataset_config = { \"name\" : \"tatsu-lab/alpaca\" , # Required \"split\" : \"train\" , \"max_samples\" : 1000 , \"percent\" : 100 } Training Configuration \u00b6 training_config = { \"per_device_batch_size\" : 4 , # Required \"num_epochs\" : 3 , \"learning_rate\" : 5e-5 , \"gradient_accumulation_steps\" : 1 , \"max_steps\" : None , \"warmup_steps\" : 100 , \"weight_decay\" : 0.01 } Logging Configuration \u00b6 logging_config = { \"output_dir\" : \"./output\" , # Required \"run_name\" : \"my_experiment\" , \"loggers\" : [ \"tensorboard\" ], \"save_steps\" : 500 , \"eval_steps\" : 100 } Complete Example \u00b6 YAML Configuration \u00b6 # Complete SFT configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 max_seq_length : 2048 gradient_checkpointing : true datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 gradient_accumulation_steps : 1 num_epochs : 3 learning_rate : 5e-5 warmup_steps : 100 weight_decay : 0.01 max_steps : null eval_interval : 100 save_interval : 500 logging : output_dir : \"./output\" run_name : \"sft_experiment\" loggers : [ \"tensorboard\" ] save_steps : 500 eval_steps : 100 Python Code \u00b6 from aligntune.core.config_loader import ConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer trainer = create_sft_trainer ( config ) # Train trainer . train () Configuration Validation \u00b6 from aligntune.core.config_loader import ConfigLoader # Load and validate config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader . validate_config ( config ) Environment Variables \u00b6 You can override configuration with environment variables: export ALIGNTUNE_OUTPUT_DIR = \"./custom_output\" export ALIGNTUNE_LEARNING_RATE = 1e-4 export ALIGNTUNE_BATCH_SIZE = 8 Next Steps \u00b6 Backend Selection - Choose the right backend SFT Guide - SFT-specific configuration RL Guide - RL-specific configuration","title":"Configuration"},{"location":"getting-started/configuration/#configuration","text":"AlignTune supports flexible configuration through Python code, YAML files, or CLI arguments.","title":"Configuration"},{"location":"getting-started/configuration/#configuration-methods","text":"","title":"Configuration Methods"},{"location":"getting-started/configuration/#1-python-code-recommended-for-development","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , output_dir = \"./output\" )","title":"1. Python Code (Recommended for Development)"},{"location":"getting-started/configuration/#2-yaml-files-recommended-for-production","text":"# config.yaml model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 logging : output_dir : \"./output\" run_name : \"my_experiment\"","title":"2. YAML Files (Recommended for Production)"},{"location":"getting-started/configuration/#3-cli-arguments","text":"aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --epochs 3 \\ --batch-size 4 \\ --lr 5e-5","title":"3. CLI Arguments"},{"location":"getting-started/configuration/#configuration-structure","text":"","title":"Configuration Structure"},{"location":"getting-started/configuration/#model-configuration","text":"model_config = { \"name_or_path\" : \"microsoft/DialoGPT-small\" , # Required \"precision\" : \"fp32\" , # fp32, fp16, bf16 \"max_seq_length\" : 2048 , \"gradient_checkpointing\" : True , \"use_unsloth\" : False }","title":"Model Configuration"},{"location":"getting-started/configuration/#dataset-configuration","text":"dataset_config = { \"name\" : \"tatsu-lab/alpaca\" , # Required \"split\" : \"train\" , \"max_samples\" : 1000 , \"percent\" : 100 }","title":"Dataset Configuration"},{"location":"getting-started/configuration/#training-configuration","text":"training_config = { \"per_device_batch_size\" : 4 , # Required \"num_epochs\" : 3 , \"learning_rate\" : 5e-5 , \"gradient_accumulation_steps\" : 1 , \"max_steps\" : None , \"warmup_steps\" : 100 , \"weight_decay\" : 0.01 }","title":"Training Configuration"},{"location":"getting-started/configuration/#logging-configuration","text":"logging_config = { \"output_dir\" : \"./output\" , # Required \"run_name\" : \"my_experiment\" , \"loggers\" : [ \"tensorboard\" ], \"save_steps\" : 500 , \"eval_steps\" : 100 }","title":"Logging Configuration"},{"location":"getting-started/configuration/#complete-example","text":"","title":"Complete Example"},{"location":"getting-started/configuration/#yaml-configuration","text":"# Complete SFT configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 max_seq_length : 2048 gradient_checkpointing : true datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 gradient_accumulation_steps : 1 num_epochs : 3 learning_rate : 5e-5 warmup_steps : 100 weight_decay : 0.01 max_steps : null eval_interval : 100 save_interval : 500 logging : output_dir : \"./output\" run_name : \"sft_experiment\" loggers : [ \"tensorboard\" ] save_steps : 500 eval_steps : 100","title":"YAML Configuration"},{"location":"getting-started/configuration/#python-code","text":"from aligntune.core.config_loader import ConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load from YAML config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer trainer = create_sft_trainer ( config ) # Train trainer . train ()","title":"Python Code"},{"location":"getting-started/configuration/#configuration-validation","text":"from aligntune.core.config_loader import ConfigLoader # Load and validate config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) ConfigLoader . validate_config ( config )","title":"Configuration Validation"},{"location":"getting-started/configuration/#environment-variables","text":"You can override configuration with environment variables: export ALIGNTUNE_OUTPUT_DIR = \"./custom_output\" export ALIGNTUNE_LEARNING_RATE = 1e-4 export ALIGNTUNE_BATCH_SIZE = 8","title":"Environment Variables"},{"location":"getting-started/configuration/#next-steps","text":"Backend Selection - Choose the right backend SFT Guide - SFT-specific configuration RL Guide - RL-specific configuration","title":"Next Steps"},{"location":"getting-started/installation/","text":"Installation \u00b6 This guide will help you install AlignTune and set up your environment. Requirements \u00b6 Python 3.8 or higher PyTorch 2.0 or higher CUDA-capable GPU (recommended for training, optional for inference) Installation Methods \u00b6 Install from PyPI (Recommended) \u00b6 pip install aligntune Install from Source \u00b6 # Clone the repository git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune # Install in development mode pip install -e . # Or install with all optional dependencies pip install -e \".[all]\" Install with Optional Dependencies \u00b6 # Install with Unsloth support (for faster training) pip install aligntune [ unsloth ] # Install with evaluation tools pip install aligntune [ eval ] # Install with all optional dependencies pip install aligntune [ all ] Verify Installation \u00b6 # Check installation python -c \"import aligntune; print(aligntune.__version__)\" # Check CLI aligntune --help # Check system information aligntune info Dependencies \u00b6 Core Dependencies \u00b6 transformers - HuggingFace Transformers library trl - Transformer Reinforcement Learning datasets - HuggingFace Datasets torch - PyTorch numpy , pandas - Data processing pyyaml - Configuration management tqdm - Progress bars Optional Dependencies \u00b6 unsloth - Fast training acceleration (requires GPU) wandb - Weights & Biases logging tensorboard - TensorBoard logging lm-eval - Language model evaluation scikit-learn - Evaluation metrics seqeval - Sequence evaluation GPU Setup \u00b6 CUDA Installation \u00b6 AlignTune works with CUDA-enabled GPUs. Install PyTorch with CUDA support: # For CUDA 11.8 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 Verify GPU Access \u00b6 import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"CUDA device: { torch . cuda . get_device_name ( 0 ) if torch . cuda . is_available () else 'N/A' } \" ) Unsloth Setup (Optional) \u00b6 Unsloth provides faster training but requires specific setup: # Install Unsloth pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" # Or for local installation pip install unsloth Verify Unsloth \u00b6 from aligntune.core.backend_factory import BackendType from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" ) Environment Variables \u00b6 Set these environment variables for optimal performance: # HuggingFace cache directory export HF_HOME = /path/to/cache # CUDA device selection export CUDA_VISIBLE_DEVICES = 0 ,1 # Disable tokenizers parallelism (if you see warnings) export TOKENIZERS_PARALLELISM = false # HuggingFace offline mode (if needed) export HF_DATASETS_OFFLINE = 1 Troubleshooting \u00b6 Import Errors \u00b6 # Ensure proper Python path export PYTHONPATH = \" ${ PYTHONPATH } : $( pwd ) /src\" # Reinstall in development mode pip install -e . CUDA Issues \u00b6 # Check CUDA version nvidia-smi # Verify PyTorch CUDA python -c \"import torch; print(torch.cuda.is_available())\" Unsloth Compatibility \u00b6 If Unsloth has compatibility issues, AlignTune automatically falls back to TRL backends. See Unsloth Compatibility for details. Next Steps \u00b6 Quick Start Guide - Get started with your first training Configuration Guide - Learn about configuration options Backend Selection - Choose the right backend","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This guide will help you install AlignTune and set up your environment.","title":"Installation"},{"location":"getting-started/installation/#requirements","text":"Python 3.8 or higher PyTorch 2.0 or higher CUDA-capable GPU (recommended for training, optional for inference)","title":"Requirements"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#install-from-pypi-recommended","text":"pip install aligntune","title":"Install from PyPI (Recommended)"},{"location":"getting-started/installation/#install-from-source","text":"# Clone the repository git clone https://github.com/Lexsi-Labs/aligntune.git cd aligntune # Install in development mode pip install -e . # Or install with all optional dependencies pip install -e \".[all]\"","title":"Install from Source"},{"location":"getting-started/installation/#install-with-optional-dependencies","text":"# Install with Unsloth support (for faster training) pip install aligntune [ unsloth ] # Install with evaluation tools pip install aligntune [ eval ] # Install with all optional dependencies pip install aligntune [ all ]","title":"Install with Optional Dependencies"},{"location":"getting-started/installation/#verify-installation","text":"# Check installation python -c \"import aligntune; print(aligntune.__version__)\" # Check CLI aligntune --help # Check system information aligntune info","title":"Verify Installation"},{"location":"getting-started/installation/#dependencies","text":"","title":"Dependencies"},{"location":"getting-started/installation/#core-dependencies","text":"transformers - HuggingFace Transformers library trl - Transformer Reinforcement Learning datasets - HuggingFace Datasets torch - PyTorch numpy , pandas - Data processing pyyaml - Configuration management tqdm - Progress bars","title":"Core Dependencies"},{"location":"getting-started/installation/#optional-dependencies","text":"unsloth - Fast training acceleration (requires GPU) wandb - Weights & Biases logging tensorboard - TensorBoard logging lm-eval - Language model evaluation scikit-learn - Evaluation metrics seqeval - Sequence evaluation","title":"Optional Dependencies"},{"location":"getting-started/installation/#gpu-setup","text":"","title":"GPU Setup"},{"location":"getting-started/installation/#cuda-installation","text":"AlignTune works with CUDA-enabled GPUs. Install PyTorch with CUDA support: # For CUDA 11.8 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","title":"CUDA Installation"},{"location":"getting-started/installation/#verify-gpu-access","text":"import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"CUDA device: { torch . cuda . get_device_name ( 0 ) if torch . cuda . is_available () else 'N/A' } \" )","title":"Verify GPU Access"},{"location":"getting-started/installation/#unsloth-setup-optional","text":"Unsloth provides faster training but requires specific setup: # Install Unsloth pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" # Or for local installation pip install unsloth","title":"Unsloth Setup (Optional)"},{"location":"getting-started/installation/#verify-unsloth","text":"from aligntune.core.backend_factory import BackendType from aligntune.core.backend_factory import get_backend_status status = get_backend_status () print ( f \"Unsloth available: { status [ 'unsloth' ][ 'available' ] } \" )","title":"Verify Unsloth"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # HuggingFace cache directory export HF_HOME = /path/to/cache # CUDA device selection export CUDA_VISIBLE_DEVICES = 0 ,1 # Disable tokenizers parallelism (if you see warnings) export TOKENIZERS_PARALLELISM = false # HuggingFace offline mode (if needed) export HF_DATASETS_OFFLINE = 1","title":"Environment Variables"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#import-errors","text":"# Ensure proper Python path export PYTHONPATH = \" ${ PYTHONPATH } : $( pwd ) /src\" # Reinstall in development mode pip install -e .","title":"Import Errors"},{"location":"getting-started/installation/#cuda-issues","text":"# Check CUDA version nvidia-smi # Verify PyTorch CUDA python -c \"import torch; print(torch.cuda.is_available())\"","title":"CUDA Issues"},{"location":"getting-started/installation/#unsloth-compatibility","text":"If Unsloth has compatibility issues, AlignTune automatically falls back to TRL backends. See Unsloth Compatibility for details.","title":"Unsloth Compatibility"},{"location":"getting-started/installation/#next-steps","text":"Quick Start Guide - Get started with your first training Configuration Guide - Learn about configuration options Backend Selection - Choose the right backend","title":"Next Steps"},{"location":"getting-started/quickstart/","text":"Quick Start \u00b6 This quick start guide demonstrates how to run a complete end-to-end workflow with AlignTune in just a few steps. 1. Prepare Your Environment \u00b6 Ensure you have installed AlignTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source aligntune-env/bin/activate # If using conda conda activate aligntune Check installation: python -c \"import aligntune; print('AlignTune installed successfully')\" 2. Load a Dataset \u00b6 We'll use the Alpaca dataset for this example. from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Preview the data print ( dataset [ 0 ]) # Output: {'instruction': '...', 'input': '...', 'output': '...'} 3. Initialize and Configure the Trainer \u00b6 Use the create_sft_trainer function to define your model, dataset, and training configuration. Supervised Fine-Tuning (SFT) \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_samples = 1000 # Limit for quick start ) 4. Train the Model \u00b6 Train your pipeline on the training data. # Train the model trainer . train () During training, AlignTune will automatically handle data preprocessing and apply the chosen training strategy. You'll see progress bars and training metrics. 5. Evaluate and Predict \u00b6 After training, evaluate performance and generate predictions: # Evaluate on test set (if available) metrics = trainer . evaluate () print ( \"Evaluation metrics:\" , metrics ) # Output: {'eval_loss': 2.34, 'eval_perplexity': 10.4, ...} # Make predictions result = trainer . predict ( \"What is machine learning?\" ) print ( \"Prediction:\" , result ) Supported metrics include: Loss , Perplexity , BLEU , and ROUGE scores. 6. Save and Load the Model \u00b6 Persist your trained model for later use: # Save to disk model_path = trainer . save_model ( \"./output/my_model\" ) print ( f \"Model saved to: { model_path } \" ) # Load from disk (in a new session) from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) # Push to HuggingFace Hub (optional) # trainer.push_to_hub(\"username/my-model\", token=\"hf_...\") 7. Try Reinforcement Learning (DPO) \u00b6 Switch to reinforcement learning with minimal code changes: Reinforcement Learning (DPO) \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Create DPO trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Direct Preference Optimization backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , max_samples = 500 # Limit for quick start ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( \"DPO metrics:\" , metrics ) # Save model trainer . save_model () Next Steps \u00b6 Explore advanced configurations in the User Guide Learn about different algorithms in Algorithms Overview Compare backends with Backend Comparison Dive into RL training in RL Guide Using YAML Configuration \u00b6 Create a configuration file config.yaml : # SFT Configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 output_dir : \"./output\" logging : output_dir : \"./output\" run_name : \"my_first_training\" Then train: from aligntune.core.config_loader import ConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer trainer = create_sft_trainer ( config ) # Train trainer . train () CLI Quick Start \u00b6 # Train SFT model aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 # Train DPO model aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --epochs 1 \\ --batch-size 1 What's Next? \u00b6 Configuration Guide - Learn about all configuration options Backend Selection - Choose between TRL and Unsloth SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"Quick Start"},{"location":"getting-started/quickstart/#quick-start","text":"This quick start guide demonstrates how to run a complete end-to-end workflow with AlignTune in just a few steps.","title":"Quick Start"},{"location":"getting-started/quickstart/#1-prepare-your-environment","text":"Ensure you have installed AlignTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source aligntune-env/bin/activate # If using conda conda activate aligntune Check installation: python -c \"import aligntune; print('AlignTune installed successfully')\"","title":"1. Prepare Your Environment"},{"location":"getting-started/quickstart/#2-load-a-dataset","text":"We'll use the Alpaca dataset for this example. from datasets import load_dataset # Load dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"train\" ) # Preview the data print ( dataset [ 0 ]) # Output: {'instruction': '...', 'input': '...', 'output': '...'}","title":"2. Load a Dataset"},{"location":"getting-started/quickstart/#3-initialize-and-configure-the-trainer","text":"Use the create_sft_trainer function to define your model, dataset, and training configuration.","title":"3. Initialize and Configure the Trainer"},{"location":"getting-started/quickstart/#supervised-fine-tuning-sft","text":"from aligntune.core.backend_factory import create_sft_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-3.2-3B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 5e-5 , max_samples = 1000 # Limit for quick start )","title":"Supervised Fine-Tuning (SFT)"},{"location":"getting-started/quickstart/#4-train-the-model","text":"Train your pipeline on the training data. # Train the model trainer . train () During training, AlignTune will automatically handle data preprocessing and apply the chosen training strategy. You'll see progress bars and training metrics.","title":"4. Train the Model"},{"location":"getting-started/quickstart/#5-evaluate-and-predict","text":"After training, evaluate performance and generate predictions: # Evaluate on test set (if available) metrics = trainer . evaluate () print ( \"Evaluation metrics:\" , metrics ) # Output: {'eval_loss': 2.34, 'eval_perplexity': 10.4, ...} # Make predictions result = trainer . predict ( \"What is machine learning?\" ) print ( \"Prediction:\" , result ) Supported metrics include: Loss , Perplexity , BLEU , and ROUGE scores.","title":"5. Evaluate and Predict"},{"location":"getting-started/quickstart/#6-save-and-load-the-model","text":"Persist your trained model for later use: # Save to disk model_path = trainer . save_model ( \"./output/my_model\" ) print ( f \"Model saved to: { model_path } \" ) # Load from disk (in a new session) from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) # Push to HuggingFace Hub (optional) # trainer.push_to_hub(\"username/my-model\", token=\"hf_...\")","title":"6. Save and Load the Model"},{"location":"getting-started/quickstart/#7-try-reinforcement-learning-dpo","text":"Switch to reinforcement learning with minimal code changes:","title":"7. Try Reinforcement Learning (DPO)"},{"location":"getting-started/quickstart/#reinforcement-learning-dpo","text":"from aligntune.core.backend_factory import create_rl_trainer # Create DPO trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Direct Preference Optimization backend = \"trl\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , max_samples = 500 # Limit for quick start ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( \"DPO metrics:\" , metrics ) # Save model trainer . save_model ()","title":"Reinforcement Learning (DPO)"},{"location":"getting-started/quickstart/#next-steps","text":"Explore advanced configurations in the User Guide Learn about different algorithms in Algorithms Overview Compare backends with Backend Comparison Dive into RL training in RL Guide","title":"Next Steps"},{"location":"getting-started/quickstart/#using-yaml-configuration","text":"Create a configuration file config.yaml : # SFT Configuration model : name_or_path : \"microsoft/DialoGPT-small\" precision : fp32 datasets : - name : \"tatsu-lab/alpaca\" split : \"train\" max_samples : 1000 train : per_device_batch_size : 4 num_epochs : 3 learning_rate : 5e-5 output_dir : \"./output\" logging : output_dir : \"./output\" run_name : \"my_first_training\" Then train: from aligntune.core.config_loader import ConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = ConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer trainer = create_sft_trainer ( config ) # Train trainer . train ()","title":"Using YAML Configuration"},{"location":"getting-started/quickstart/#cli-quick-start","text":"# Train SFT model aligntune train \\ --model microsoft/DialoGPT-small \\ --dataset tatsu-lab/alpaca \\ --type sft \\ --epochs 3 \\ --batch-size 4 # Train DPO model aligntune train \\ --model microsoft/DialoGPT-medium \\ --dataset Anthropic/hh-rlhf \\ --type dpo \\ --epochs 1 \\ --batch-size 1","title":"CLI Quick Start"},{"location":"getting-started/quickstart/#whats-next","text":"Configuration Guide - Learn about all configuration options Backend Selection - Choose between TRL and Unsloth SFT Guide - Deep dive into SFT training RL Guide - Deep dive into RL training","title":"What's Next?"},{"location":"notebooks/","text":"AlignTune Notebooks \u00b6 Two notebooks demonstrate the core AlignTune workflows. - __01 - Basic SFT Training__ --- End-to-end Supervised Fine-Tuning on a small dataset for a fast demo. :octicons-arrow-right-24: [Open notebook](./01_basic_sft_training.ipynb) - __02 - DPO Preference Training__ --- Direct Preference Optimization training using a preference dataset. :octicons-arrow-right-24: [Open notebook](./02_dpo_preference_training.ipynb) Running the Notebooks \u00b6 Local Setup \u00b6 Clone the repository: git clone https://github.com/Lexsi-Labs/aligntune.git cd AlignTune Install dependencies: pip install -e . pip install jupyter notebook Launch Jupyter: jupyter notebook docs/notebooks/ Requirements \u00b6 Python 3.8+ PyTorch 2.0+ CUDA-compatible GPU (recommended for faster training) Demo Notebooks \u00b6 Interactive Colab notebooks demonstrating various AlignTune workflows: Supervised Fine-Tuning (SFT) \u00b6 Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab Reinforcement Learning (RL) \u00b6 Backend Algorithm Dataset Link TRL DPO - Open in Colab TRL GRPO GSM8K Open in Colab Contributing \u00b6 Want to improve a notebook? See our Contributing Guide .","title":"Notebooks"},{"location":"notebooks/#aligntune-notebooks","text":"Two notebooks demonstrate the core AlignTune workflows. - __01 - Basic SFT Training__ --- End-to-end Supervised Fine-Tuning on a small dataset for a fast demo. :octicons-arrow-right-24: [Open notebook](./01_basic_sft_training.ipynb) - __02 - DPO Preference Training__ --- Direct Preference Optimization training using a preference dataset. :octicons-arrow-right-24: [Open notebook](./02_dpo_preference_training.ipynb)","title":"AlignTune Notebooks"},{"location":"notebooks/#running-the-notebooks","text":"","title":"Running the Notebooks"},{"location":"notebooks/#local-setup","text":"Clone the repository: git clone https://github.com/Lexsi-Labs/aligntune.git cd AlignTune Install dependencies: pip install -e . pip install jupyter notebook Launch Jupyter: jupyter notebook docs/notebooks/","title":"Local Setup"},{"location":"notebooks/#requirements","text":"Python 3.8+ PyTorch 2.0+ CUDA-compatible GPU (recommended for faster training)","title":"Requirements"},{"location":"notebooks/#demo-notebooks","text":"Interactive Colab notebooks demonstrating various AlignTune workflows:","title":"Demo Notebooks"},{"location":"notebooks/#supervised-fine-tuning-sft","text":"Backend Model Dataset Link TRL Gemma-7b philschmid/dolly-15k-oai-style (messages format) Open in Colab TRL GemmaTX TrialBench adverse event prediction Open in Drive Unsloth Qwen-2.5-0.5-I gaming Open in Colab","title":"Supervised Fine-Tuning (SFT)"},{"location":"notebooks/#reinforcement-learning-rl","text":"Backend Algorithm Dataset Link TRL DPO - Open in Colab TRL GRPO GSM8K Open in Colab","title":"Reinforcement Learning (RL)"},{"location":"notebooks/#contributing","text":"Want to improve a notebook? See our Contributing Guide .","title":"Contributing"},{"location":"user-guide/evaluation/","text":"Evaluation Guide \u00b6 Complete guide to evaluating fine-tuned models with AlignTune, covering training evaluation, standalone evaluation, benchmarks, and custom evaluation tasks. Overview \u00b6 AlignTune provides comprehensive evaluation capabilities: Training Evaluation - Evaluate during training Standalone Evaluation - Evaluate saved models Benchmark Evaluation - Standard benchmarks (lm-eval) Custom Evaluation - Task-specific evaluation RL Evaluation - DPO, PPO, GRPO evaluation Quick Start \u00b6 Basic Training Evaluation \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) # Train with automatic evaluation trainer . train () # Evaluate on validation set metrics = trainer . evaluate () print ( metrics ) Standalone Model Evaluation \u00b6 from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" ) RL Model Evaluation \u00b6 from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , use_lora = True , base_model = \"microsoft/phi-2\" ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" ) Evaluation Types \u00b6 1. Training Evaluation \u00b6 Automatic evaluation during training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 , # Evaluate every 100 steps save_interval = 500 # Save every 500 steps ) trainer . train () # Evaluation runs automatically 2. Standalone Evaluation \u00b6 Evaluate after training: # After training metrics = trainer . evaluate () # With custom dataset from datasets import load_dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # With custom metric prefix metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) 3. Zero-Shot Evaluation \u00b6 Generate predictions for test prompts: # Single prediction result = trainer . predict ( \"What is machine learning?\" ) print ( result ) # Batch predictions prompts = [ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) 4. Benchmark Evaluation \u00b6 Run standard benchmarks with lm-eval: from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run standard benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , \"arc\" , \"mmlu\" ], batch_size = 1 ) print ( results ) 5. Custom Evaluation \u00b6 Create custom evaluation tasks: from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom task task = EvalTask ( name = \"custom_sentiment\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"Custom sentiment analysis task\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ) # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , [ task ]) 6. RL Evaluation \u00b6 Evaluate RL-trained models (DPO, PPO, GRPO): DPO Evaluation \u00b6 from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results/dpo\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , data_task_type = \"dpo\" , # DPO metrics metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , # For KL divergence # Dataset dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , # Generation max_length = 1536 , temperature = 0.0 , batch_size = 2 , # Model loading use_lora = True , base_model = \"microsoft/phi-2\" , use_unsloth = True ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" ) PPO Evaluation \u00b6 config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) print ( f \"Perplexity: { results . get ( 'perplexity' , 0.0 ) : .4f } \" ) Before/After Comparison \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Train DPO model trainer = create_rl_trainer ( model_name = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = \"microsoft/phi-2\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = False , output_dir = \"./eval_results/base\" ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = True , base_model = \"microsoft/phi-2\" , output_dir = \"./eval_results/trained\" ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Compare print ( f \"Base: { base_results [ 'win_rate' ] : .2% } \" ) print ( f \"Trained: { trained_results [ 'win_rate' ] : .2% } \" ) print ( f \"Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" ) Evaluation Metrics \u00b6 Classification Metrics \u00b6 # For text classification metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: accuracy, f1, precision, recall Generation Metrics \u00b6 # For text generation metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: perplexity, loss, BLEU, ROUGE (if applicable) RL Metrics \u00b6 DPO Metrics \u00b6 metrics = [ \"reward_margin\" , # Difference between chosen/rejected rewards \"preference_accuracy\" , # How often model prefers chosen \"win_rate\" , # % matching human preferences \"kl_divergence\" , # KL from reference (needs reference_model_path) \"log_ratio\" , # Log probability ratio \"implicit_reward\" , # Implicit reward signal \"calibration\" # Calibration score ] PPO Metrics \u00b6 metrics = [ \"perplexity\" , # Language modeling quality \"reward_accuracy\" , # Alignment with reward model \"policy_entropy\" # Policy output diversity ] RL Metrics \u00b6 metrics = [ \"kl_divergence\" , # KL divergence from reference \"reward_accuracy\" , # Reward model alignment \"policy_entropy\" # Output diversity ] Custom Metrics \u00b6 from aligntune.eval.core import EvalTask , TaskCategory task = EvalTask ( name = \"custom_task\" , category = TaskCategory . TEXT_GENERATION , metrics = [ \"accuracy\" , \"f1\" , \"custom_metric\" ], custom_metrics = [ your_custom_metric_function ] ) Evaluation Configuration \u00b6 Training Evaluation Config \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Evaluation settings eval_interval = 100 , # Evaluate every N steps save_interval = 500 , # Save every N steps load_best_model_at_end = True , # Load best model metric_for_best_model = \"eval_loss\" , # Metric to track greater_is_better = False # Lower is better for loss ) Standalone Evaluation Config \u00b6 from aligntune.eval.core import EvalConfig , EvalType , TaskCategory config = EvalConfig ( eval_type = EvalType . STANDALONE , task_categories = [ TaskCategory . TEXT_GENERATION ], metrics = [ \"accuracy\" , \"f1\" , \"bleu\" , \"rouge\" ], batch_size = 32 , max_samples = 1000 , device = \"auto\" , precision = \"bf16\" , output_dir = \"./eval_results\" , save_predictions = True , save_metrics = True ) RL Evaluation Config \u00b6 from aligntune.eval.core import EvalConfig , run_eval config = EvalConfig ( # Model configuration model_path = \"./model\" , output_dir = \"./eval_results\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , # Evaluation task: dpo, ppo, grpo, text, math, code data_task_type = \"dpo\" , # Data schema: dpo, sft, grpo # Metrics metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = None , # Reference model (for KL) reward_model = None , # Reward model (optional) # Dataset dataset_name = \"dataset_name\" , dataset_config = None , split = \"test\" , max_samples = 100 , # Generation max_length = 2048 , # Context window max_new_tokens = 512 , # Generation length temperature = 0.0 , # Sampling temperature batch_size = 8 , # Model loading use_lora = False , base_model = None , use_unsloth = False , use_vllm = False , precision = \"bf16\" , # Advanced column_mapping = None , trust_remote_code = True ) results = run_eval ( config ) Task Type Reference \u00b6 Supported task_type values for evaluation: Task Type Description Default Metrics Data Schema \"text\" Text generation perplexity, accuracy sft \"math\" Math problems math_accuracy, pass_at_k sft \"code\" Code generation pass_at_k code \"dpo\" DPO models reward_margin, preference_accuracy, win_rate dpo \"ppo\" PPO models perplexity, reward_accuracy sft \"generic\" Generic evaluation perplexity, accuracy sft Note : For RL evaluation, task_type and data_task_type must match (e.g., both \"dpo\" for DPO models). Complete Examples \u00b6 SFT Evaluation \u00b6 from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 ) # Train trainer . train () # Evaluate on test set test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Test metrics: { metrics } \" ) # Zero-shot evaluation prompts = [ \"What is AI?\" , \"Explain machine learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \\n A: { result } \\n \" ) RL Evaluation \u00b6 from aligntune.core.backend_factory import create_rl_trainer from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" # Train DPO model trainer = create_rl_trainer ( model_name = MODEL_NAME , dataset_name = DATASET , algorithm = \"dpo\" , backend = \"trl\" , output_dir = OUTPUT_DIR , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = MODEL_NAME , output_dir = f \" { OUTPUT_DIR } /eval_base\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = False , use_unsloth = True ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Print comparison print ( f \" \\n Base Model:\" ) print ( f \" Win Rate: { base_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Trained Model:\" ) print ( f \" Win Rate: { trained_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" ) Benchmark Evaluation \u00b6 from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run multiple benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , # Commonsense reasoning \"arc\" , # Science questions \"mmlu\" , # Multitask language understanding \"truthfulqa\" # Truthful question answering ], batch_size = 1 , limit = 100 # Limit samples per task ) # Print results for task , metrics in results . items (): print ( f \" { task } : { metrics } \" ) Custom Task Evaluation \u00b6 from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom tasks tasks = [ EvalTask ( name = \"sentiment_analysis\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"IMDB sentiment analysis\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ), EvalTask ( name = \"text_generation\" , category = TaskCategory . TEXT_GENERATION , description = \"Alpaca instruction following\" , dataset_name = \"tatsu-lab/alpaca\" , input_column = \"instruction\" , target_column = \"output\" , metrics = [ \"bleu\" , \"rouge\" ] ) ] # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , tasks ) # Print results for task_name , metrics in results . items (): print ( f \" { task_name } : { metrics } \" ) Best Practices \u00b6 1. Evaluation During Training \u00b6 Set appropriate eval_interval (100-500 steps) Use validation split for evaluation Track best model with load_best_model_at_end 2. Comprehensive Evaluation \u00b6 Evaluate on multiple metrics Use both automatic and manual evaluation Test on diverse datasets 3. Zero-Shot Evaluation \u00b6 Test on unseen prompts Evaluate qualitative samples Check for common failure modes 4. Benchmark Evaluation \u00b6 Use standard benchmarks for comparison Run multiple benchmarks Document results for reproducibility 5. Custom Evaluation \u00b6 Create task-specific evaluation Use domain-specific metrics Validate on real-world data 6. RL Evaluation \u00b6 Always compare base and trained models Use consistent evaluation settings Use greedy decoding (temperature=0.0) for deterministic results Reuse dataset_dict when possible Match task_type and data_task_type for RL models Troubleshooting \u00b6 Evaluation Too Slow \u00b6 # Reduce batch size or max samples metrics = trainer . evaluate ( eval_dataset = eval_dataset , batch_size = 16 , # Reduce from default max_samples = 500 # Limit samples ) Out of Memory \u00b6 # Use smaller batch size config = EvalConfig ( batch_size = 8 , # Smaller batch max_samples = 100 , # Limit samples precision = \"fp16\" # Lower precision ) Missing Metrics \u00b6 # Specify metrics explicitly task = EvalTask ( name = \"my_task\" , category = TaskCategory . TEXT_CLASSIFICATION , metrics = [ \"accuracy\" , \"f1\" , \"precision\" , \"recall\" ] ) RL Evaluation Issues \u00b6 # Missing required columns for DPO # Fix: Ensure task types match config = EvalConfig ( task_type = \"dpo\" , data_task_type = \"dpo\" # Must match ) # KL divergence requires reference model config = EvalConfig ( metrics = [ \"kl_divergence\" ], reference_model_path = \"microsoft/phi-2\" ) # LoRA adapter not loading config = EvalConfig ( model_path = \"./lora_adapter\" , use_lora = True , base_model = \"microsoft/phi-2\" ) Next Steps \u00b6 SFT Guide - SFT training and evaluation RL Guide - RL training and evaluation Model Management - Model saving and loading Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Evaluation System - Detailed system docs","title":"Evaluation"},{"location":"user-guide/evaluation/#evaluation-guide","text":"Complete guide to evaluating fine-tuned models with AlignTune, covering training evaluation, standalone evaluation, benchmarks, and custom evaluation tasks.","title":"Evaluation Guide"},{"location":"user-guide/evaluation/#overview","text":"AlignTune provides comprehensive evaluation capabilities: Training Evaluation - Evaluate during training Standalone Evaluation - Evaluate saved models Benchmark Evaluation - Standard benchmarks (lm-eval) Custom Evaluation - Task-specific evaluation RL Evaluation - DPO, PPO, GRPO evaluation","title":"Overview"},{"location":"user-guide/evaluation/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/evaluation/#basic-training-evaluation","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) # Train with automatic evaluation trainer . train () # Evaluate on validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Training Evaluation"},{"location":"user-guide/evaluation/#standalone-model-evaluation","text":"from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" )","title":"Standalone Model Evaluation"},{"location":"user-guide/evaluation/#rl-model-evaluation","text":"from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , use_lora = True , base_model = \"microsoft/phi-2\" ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" )","title":"RL Model Evaluation"},{"location":"user-guide/evaluation/#evaluation-types","text":"","title":"Evaluation Types"},{"location":"user-guide/evaluation/#1-training-evaluation","text":"Automatic evaluation during training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 , # Evaluate every 100 steps save_interval = 500 # Save every 500 steps ) trainer . train () # Evaluation runs automatically","title":"1. Training Evaluation"},{"location":"user-guide/evaluation/#2-standalone-evaluation","text":"Evaluate after training: # After training metrics = trainer . evaluate () # With custom dataset from datasets import load_dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # With custom metric prefix metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" )","title":"2. Standalone Evaluation"},{"location":"user-guide/evaluation/#3-zero-shot-evaluation","text":"Generate predictions for test prompts: # Single prediction result = trainer . predict ( \"What is machine learning?\" ) print ( result ) # Batch predictions prompts = [ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"3. Zero-Shot Evaluation"},{"location":"user-guide/evaluation/#4-benchmark-evaluation","text":"Run standard benchmarks with lm-eval: from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run standard benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , \"arc\" , \"mmlu\" ], batch_size = 1 ) print ( results )","title":"4. Benchmark Evaluation"},{"location":"user-guide/evaluation/#5-custom-evaluation","text":"Create custom evaluation tasks: from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom task task = EvalTask ( name = \"custom_sentiment\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"Custom sentiment analysis task\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ) # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , [ task ])","title":"5. Custom Evaluation"},{"location":"user-guide/evaluation/#6-rl-evaluation","text":"Evaluate RL-trained models (DPO, PPO, GRPO):","title":"6. RL Evaluation"},{"location":"user-guide/evaluation/#dpo-evaluation","text":"from aligntune.eval.core import EvalConfig , run_eval # Evaluate DPO model config = EvalConfig ( model_path = \"./output/dpo_model\" , output_dir = \"./eval_results/dpo\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , data_task_type = \"dpo\" , # DPO metrics metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , # For KL divergence # Dataset dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , max_samples = 100 , # Generation max_length = 1536 , temperature = 0.0 , batch_size = 2 , # Model loading use_lora = True , base_model = \"microsoft/phi-2\" , use_unsloth = True ) results = run_eval ( config ) print ( f \"Win Rate: { results . get ( 'win_rate' , 0.0 ) : .2% } \" )","title":"DPO Evaluation"},{"location":"user-guide/evaluation/#ppo-evaluation","text":"config = EvalConfig ( model_path = \"./output/ppo_model\" , output_dir = \"./eval_results/ppo\" , # Task configuration task_type = \"text\" , data_task_type = \"sft\" , # Metrics metrics = [ \"perplexity\" , \"reward_accuracy\" ], # Dataset dataset_name = \"HuggingFaceH4/ultrachat_200k\" , split = \"test_sft\" , max_samples = 100 , # Generation max_length = 512 , temperature = 0.7 , batch_size = 8 ) results = run_eval ( config ) print ( f \"Perplexity: { results . get ( 'perplexity' , 0.0 ) : .4f } \" )","title":"PPO Evaluation"},{"location":"user-guide/evaluation/#beforeafter-comparison","text":"from aligntune.core.backend_factory import create_rl_trainer # Train DPO model trainer = create_rl_trainer ( model_name = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = \"microsoft/phi-2\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = False , output_dir = \"./eval_results/base\" ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = \"microsoft/phi-2\" , dataset_name = \"Anthropic/hh-rlhf\" , split = \"test\" , use_lora = True , base_model = \"microsoft/phi-2\" , output_dir = \"./eval_results/trained\" ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Compare print ( f \"Base: { base_results [ 'win_rate' ] : .2% } \" ) print ( f \"Trained: { trained_results [ 'win_rate' ] : .2% } \" ) print ( f \"Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" )","title":"Before/After Comparison"},{"location":"user-guide/evaluation/#evaluation-metrics","text":"","title":"Evaluation Metrics"},{"location":"user-guide/evaluation/#classification-metrics","text":"# For text classification metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: accuracy, f1, precision, recall","title":"Classification Metrics"},{"location":"user-guide/evaluation/#generation-metrics","text":"# For text generation metrics = trainer . evaluate ( eval_dataset = eval_dataset ) # Returns: perplexity, loss, BLEU, ROUGE (if applicable)","title":"Generation Metrics"},{"location":"user-guide/evaluation/#rl-metrics","text":"","title":"RL Metrics"},{"location":"user-guide/evaluation/#dpo-metrics","text":"metrics = [ \"reward_margin\" , # Difference between chosen/rejected rewards \"preference_accuracy\" , # How often model prefers chosen \"win_rate\" , # % matching human preferences \"kl_divergence\" , # KL from reference (needs reference_model_path) \"log_ratio\" , # Log probability ratio \"implicit_reward\" , # Implicit reward signal \"calibration\" # Calibration score ]","title":"DPO Metrics"},{"location":"user-guide/evaluation/#ppo-metrics","text":"metrics = [ \"perplexity\" , # Language modeling quality \"reward_accuracy\" , # Alignment with reward model \"policy_entropy\" # Policy output diversity ]","title":"PPO Metrics"},{"location":"user-guide/evaluation/#rl-metrics_1","text":"metrics = [ \"kl_divergence\" , # KL divergence from reference \"reward_accuracy\" , # Reward model alignment \"policy_entropy\" # Output diversity ]","title":"RL Metrics"},{"location":"user-guide/evaluation/#custom-metrics","text":"from aligntune.eval.core import EvalTask , TaskCategory task = EvalTask ( name = \"custom_task\" , category = TaskCategory . TEXT_GENERATION , metrics = [ \"accuracy\" , \"f1\" , \"custom_metric\" ], custom_metrics = [ your_custom_metric_function ] )","title":"Custom Metrics"},{"location":"user-guide/evaluation/#evaluation-configuration","text":"","title":"Evaluation Configuration"},{"location":"user-guide/evaluation/#training-evaluation-config","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Evaluation settings eval_interval = 100 , # Evaluate every N steps save_interval = 500 , # Save every N steps load_best_model_at_end = True , # Load best model metric_for_best_model = \"eval_loss\" , # Metric to track greater_is_better = False # Lower is better for loss )","title":"Training Evaluation Config"},{"location":"user-guide/evaluation/#standalone-evaluation-config","text":"from aligntune.eval.core import EvalConfig , EvalType , TaskCategory config = EvalConfig ( eval_type = EvalType . STANDALONE , task_categories = [ TaskCategory . TEXT_GENERATION ], metrics = [ \"accuracy\" , \"f1\" , \"bleu\" , \"rouge\" ], batch_size = 32 , max_samples = 1000 , device = \"auto\" , precision = \"bf16\" , output_dir = \"./eval_results\" , save_predictions = True , save_metrics = True )","title":"Standalone Evaluation Config"},{"location":"user-guide/evaluation/#rl-evaluation-config","text":"from aligntune.eval.core import EvalConfig , run_eval config = EvalConfig ( # Model configuration model_path = \"./model\" , output_dir = \"./eval_results\" , device = \"cuda\" , # Task configuration task_type = \"dpo\" , # Evaluation task: dpo, ppo, grpo, text, math, code data_task_type = \"dpo\" , # Data schema: dpo, sft, grpo # Metrics metrics = [ \"reward_margin\" , \"win_rate\" ], reference_model_path = None , # Reference model (for KL) reward_model = None , # Reward model (optional) # Dataset dataset_name = \"dataset_name\" , dataset_config = None , split = \"test\" , max_samples = 100 , # Generation max_length = 2048 , # Context window max_new_tokens = 512 , # Generation length temperature = 0.0 , # Sampling temperature batch_size = 8 , # Model loading use_lora = False , base_model = None , use_unsloth = False , use_vllm = False , precision = \"bf16\" , # Advanced column_mapping = None , trust_remote_code = True ) results = run_eval ( config )","title":"RL Evaluation Config"},{"location":"user-guide/evaluation/#task-type-reference","text":"Supported task_type values for evaluation: Task Type Description Default Metrics Data Schema \"text\" Text generation perplexity, accuracy sft \"math\" Math problems math_accuracy, pass_at_k sft \"code\" Code generation pass_at_k code \"dpo\" DPO models reward_margin, preference_accuracy, win_rate dpo \"ppo\" PPO models perplexity, reward_accuracy sft \"generic\" Generic evaluation perplexity, accuracy sft Note : For RL evaluation, task_type and data_task_type must match (e.g., both \"dpo\" for DPO models).","title":"Task Type Reference"},{"location":"user-guide/evaluation/#complete-examples","text":"","title":"Complete Examples"},{"location":"user-guide/evaluation/#sft-evaluation","text":"from aligntune.core.backend_factory import create_sft_trainer from datasets import load_dataset # Create trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 , eval_interval = 100 ) # Train trainer . train () # Evaluate on test set test_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = test_dataset ) print ( f \"Test metrics: { metrics } \" ) # Zero-shot evaluation prompts = [ \"What is AI?\" , \"Explain machine learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \\n A: { result } \\n \" )","title":"SFT Evaluation"},{"location":"user-guide/evaluation/#rl-evaluation","text":"from aligntune.core.backend_factory import create_rl_trainer from aligntune.eval.core import EvalConfig , run_eval MODEL_NAME = \"microsoft/phi-2\" DATASET = \"Anthropic/hh-rlhf\" OUTPUT_DIR = \"./output/dpo_phi2\" # Train DPO model trainer = create_rl_trainer ( model_name = MODEL_NAME , dataset_name = DATASET , algorithm = \"dpo\" , backend = \"trl\" , output_dir = OUTPUT_DIR , num_epochs = 1 , use_lora = True ) trainer . train () trained_path = trainer . save_model () # Evaluate base model base_config = EvalConfig ( model_path = MODEL_NAME , output_dir = f \" { OUTPUT_DIR } /eval_base\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = False , use_unsloth = True ) base_results = run_eval ( base_config , trainer . dataset_dict ) # Evaluate trained model trained_config = EvalConfig ( model_path = trained_path , output_dir = f \" { OUTPUT_DIR } /eval_trained\" , task_type = \"dpo\" , data_task_type = \"dpo\" , metrics = [ \"reward_margin\" , \"preference_accuracy\" , \"win_rate\" ], reference_model_path = MODEL_NAME , dataset_name = DATASET , split = \"test\" , max_samples = 100 , use_lora = True , base_model = MODEL_NAME , use_unsloth = True ) trained_results = run_eval ( trained_config , trainer . dataset_dict ) # Print comparison print ( f \" \\n Base Model:\" ) print ( f \" Win Rate: { base_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Trained Model:\" ) print ( f \" Win Rate: { trained_results . get ( 'win_rate' , 0.0 ) : .2% } \" ) print ( f \" \\n Improvement: { trained_results [ 'win_rate' ] - base_results [ 'win_rate' ] : .2% } \" )","title":"RL Evaluation"},{"location":"user-guide/evaluation/#benchmark-evaluation","text":"from aligntune.eval.lm_eval_integration import LMEvalRunner runner = LMEvalRunner () # Run multiple benchmarks results = runner . run_benchmark ( model_name = \"your-model-name\" , tasks = [ \"hellaswag\" , # Commonsense reasoning \"arc\" , # Science questions \"mmlu\" , # Multitask language understanding \"truthfulqa\" # Truthful question answering ], batch_size = 1 , limit = 100 # Limit samples per task ) # Print results for task , metrics in results . items (): print ( f \" { task } : { metrics } \" )","title":"Benchmark Evaluation"},{"location":"user-guide/evaluation/#custom-task-evaluation","text":"from aligntune.eval.core import EvalRunner , EvalTask , TaskCategory # Create custom tasks tasks = [ EvalTask ( name = \"sentiment_analysis\" , category = TaskCategory . SENTIMENT_ANALYSIS , description = \"IMDB sentiment analysis\" , dataset_name = \"imdb\" , input_column = \"text\" , target_column = \"label\" , metrics = [ \"accuracy\" , \"f1\" ] ), EvalTask ( name = \"text_generation\" , category = TaskCategory . TEXT_GENERATION , description = \"Alpaca instruction following\" , dataset_name = \"tatsu-lab/alpaca\" , input_column = \"instruction\" , target_column = \"output\" , metrics = [ \"bleu\" , \"rouge\" ] ) ] # Run evaluation runner = EvalRunner () results = runner . run_evaluation ( model , tasks ) # Print results for task_name , metrics in results . items (): print ( f \" { task_name } : { metrics } \" )","title":"Custom Task Evaluation"},{"location":"user-guide/evaluation/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/evaluation/#1-evaluation-during-training","text":"Set appropriate eval_interval (100-500 steps) Use validation split for evaluation Track best model with load_best_model_at_end","title":"1. Evaluation During Training"},{"location":"user-guide/evaluation/#2-comprehensive-evaluation","text":"Evaluate on multiple metrics Use both automatic and manual evaluation Test on diverse datasets","title":"2. Comprehensive Evaluation"},{"location":"user-guide/evaluation/#3-zero-shot-evaluation_1","text":"Test on unseen prompts Evaluate qualitative samples Check for common failure modes","title":"3. Zero-Shot Evaluation"},{"location":"user-guide/evaluation/#4-benchmark-evaluation_1","text":"Use standard benchmarks for comparison Run multiple benchmarks Document results for reproducibility","title":"4. Benchmark Evaluation"},{"location":"user-guide/evaluation/#5-custom-evaluation_1","text":"Create task-specific evaluation Use domain-specific metrics Validate on real-world data","title":"5. Custom Evaluation"},{"location":"user-guide/evaluation/#6-rl-evaluation_1","text":"Always compare base and trained models Use consistent evaluation settings Use greedy decoding (temperature=0.0) for deterministic results Reuse dataset_dict when possible Match task_type and data_task_type for RL models","title":"6. RL Evaluation"},{"location":"user-guide/evaluation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/evaluation/#evaluation-too-slow","text":"# Reduce batch size or max samples metrics = trainer . evaluate ( eval_dataset = eval_dataset , batch_size = 16 , # Reduce from default max_samples = 500 # Limit samples )","title":"Evaluation Too Slow"},{"location":"user-guide/evaluation/#out-of-memory","text":"# Use smaller batch size config = EvalConfig ( batch_size = 8 , # Smaller batch max_samples = 100 , # Limit samples precision = \"fp16\" # Lower precision )","title":"Out of Memory"},{"location":"user-guide/evaluation/#missing-metrics","text":"# Specify metrics explicitly task = EvalTask ( name = \"my_task\" , category = TaskCategory . TEXT_CLASSIFICATION , metrics = [ \"accuracy\" , \"f1\" , \"precision\" , \"recall\" ] )","title":"Missing Metrics"},{"location":"user-guide/evaluation/#rl-evaluation-issues","text":"# Missing required columns for DPO # Fix: Ensure task types match config = EvalConfig ( task_type = \"dpo\" , data_task_type = \"dpo\" # Must match ) # KL divergence requires reference model config = EvalConfig ( metrics = [ \"kl_divergence\" ], reference_model_path = \"microsoft/phi-2\" ) # LoRA adapter not loading config = EvalConfig ( model_path = \"./lora_adapter\" , use_lora = True , base_model = \"microsoft/phi-2\" )","title":"RL Evaluation Issues"},{"location":"user-guide/evaluation/#next-steps","text":"SFT Guide - SFT training and evaluation RL Guide - RL training and evaluation Model Management - Model saving and loading","title":"Next Steps"},{"location":"user-guide/evaluation/#additional-resources","text":"API Reference - API documentation Examples - Code examples Evaluation System - Detailed system docs","title":"Additional Resources"},{"location":"user-guide/model-management/","text":"Model Management \u00b6 Learn how to save, load, and share your trained models with AlignTune. Saving Models \u00b6 Basic Save \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( ... ) trainer . train () # Save model path = trainer . save_model () print ( f \"Model saved to: { path } \" ) Custom Save Path \u00b6 # Save to custom directory path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" ) What Gets Saved \u00b6 When you call save_model() , AlignTune saves: Model weights ( pytorch_model.bin or model.safetensors ) Tokenizer files ( tokenizer.json , tokenizer_config.json , etc.) Model configuration ( config.json ) Training configuration ( training_config.yaml ) Task metadata (if applicable) Loading Models \u00b6 Load Saved Model \u00b6 from transformers import AutoModelForCausalLM , AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" ) Load with AlignTune \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer and load model trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Model is automatically loaded trainer . predict ( \"Hello!\" ) Pushing to HuggingFace Hub \u00b6 Basic Push \u00b6 trainer = create_sft_trainer ( ... ) trainer . train () trainer . save_model () # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-model\" , private = False ) print ( f \"Model available at: { url } \" ) Private Repository \u00b6 url = trainer . push_to_hub ( repo_id = \"username/my-private-model\" , private = True , token = \"hf_...\" # Your HuggingFace token ) Custom Commit Message \u00b6 url = trainer . push_to_hub ( repo_id = \"username/my-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" ) Generating Predictions \u00b6 Single Prediction \u00b6 # Generate from single input result = trainer . predict ( \"What is machine learning?\" ) print ( result ) Batch Predictions \u00b6 # Generate from multiple inputs results = trainer . predict ([ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ]) for result in results : print ( result ) Custom Generation Parameters \u00b6 result = trainer . predict ( \"What is machine learning?\" , max_new_tokens = 200 , temperature = 0.7 , top_p = 0.9 , do_sample = True ) Model Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on default validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation Dataset \u00b6 from datasets import load_dataset # Load custom evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( metrics ) Custom Metric Prefix \u00b6 metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) # Metrics will have \"test/\" prefix Checkpointing \u00b6 Save Checkpoints \u00b6 Checkpoints are automatically saved during training: trainer = create_sft_trainer ( ... , save_interval = 500 # Save every 500 steps ) trainer . train () Load Checkpoint \u00b6 # Resume from checkpoint trainer . load_checkpoint ( \"./output/checkpoint-1000\" ) Model Information \u00b6 Get Model Path \u00b6 # After saving path = trainer . save_model () print ( f \"Model path: { path } \" ) Check Model Status \u00b6 # Check if model is loaded if trainer . model is not None : print ( \"Model is loaded\" ) print ( f \"Device: { next ( trainer . model . parameters ()) . device } \" ) Best Practices \u00b6 1. Save Regularly \u00b6 # Save after training trainer . train () trainer . save_model () # Also save checkpoints during training # (automatic with save_interval) 2. Version Your Models \u00b6 import datetime timestamp = datetime . datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) path = trainer . save_model ( output_dir = f \"./models/experiment_ { timestamp } \" ) 3. Push Important Models \u00b6 # Push production models to Hub if is_production_model : trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False ) 4. Document Your Models \u00b6 # Save with metadata trainer . save_model () # Edit training_config.yaml to add notes Troubleshooting \u00b6 Model Not Found \u00b6 # Check if model exists from pathlib import Path model_path = Path ( \"./output/my_model\" ) if not model_path . exists (): print ( \"Model not found!\" ) Push to Hub Fails \u00b6 # Ensure you're logged in from huggingface_hub import login login ( token = \"hf_...\" ) # Then push trainer . push_to_hub ( \"username/my-model\" ) Memory Issues \u00b6 # Use smaller batch size for prediction result = trainer . predict ( \"Your input\" , max_new_tokens = 50 # Reduce if needed ) Next Steps \u00b6 Evaluation Guide - Learn about evaluation metrics SFT Guide - SFT-specific model management RL Guide - RL-specific model management","title":"Model Management"},{"location":"user-guide/model-management/#model-management","text":"Learn how to save, load, and share your trained models with AlignTune.","title":"Model Management"},{"location":"user-guide/model-management/#saving-models","text":"","title":"Saving Models"},{"location":"user-guide/model-management/#basic-save","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( ... ) trainer . train () # Save model path = trainer . save_model () print ( f \"Model saved to: { path } \" )","title":"Basic Save"},{"location":"user-guide/model-management/#custom-save-path","text":"# Save to custom directory path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" )","title":"Custom Save Path"},{"location":"user-guide/model-management/#what-gets-saved","text":"When you call save_model() , AlignTune saves: Model weights ( pytorch_model.bin or model.safetensors ) Tokenizer files ( tokenizer.json , tokenizer_config.json , etc.) Model configuration ( config.json ) Training configuration ( training_config.yaml ) Task metadata (if applicable)","title":"What Gets Saved"},{"location":"user-guide/model-management/#loading-models","text":"","title":"Loading Models"},{"location":"user-guide/model-management/#load-saved-model","text":"from transformers import AutoModelForCausalLM , AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM . from_pretrained ( \"./output/my_model\" ) tokenizer = AutoTokenizer . from_pretrained ( \"./output/my_model\" )","title":"Load Saved Model"},{"location":"user-guide/model-management/#load-with-aligntune","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer and load model trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Model is automatically loaded trainer . predict ( \"Hello!\" )","title":"Load with AlignTune"},{"location":"user-guide/model-management/#pushing-to-huggingface-hub","text":"","title":"Pushing to HuggingFace Hub"},{"location":"user-guide/model-management/#basic-push","text":"trainer = create_sft_trainer ( ... ) trainer . train () trainer . save_model () # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-model\" , private = False ) print ( f \"Model available at: { url } \" )","title":"Basic Push"},{"location":"user-guide/model-management/#private-repository","text":"url = trainer . push_to_hub ( repo_id = \"username/my-private-model\" , private = True , token = \"hf_...\" # Your HuggingFace token )","title":"Private Repository"},{"location":"user-guide/model-management/#custom-commit-message","text":"url = trainer . push_to_hub ( repo_id = \"username/my-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" )","title":"Custom Commit Message"},{"location":"user-guide/model-management/#generating-predictions","text":"","title":"Generating Predictions"},{"location":"user-guide/model-management/#single-prediction","text":"# Generate from single input result = trainer . predict ( \"What is machine learning?\" ) print ( result )","title":"Single Prediction"},{"location":"user-guide/model-management/#batch-predictions","text":"# Generate from multiple inputs results = trainer . predict ([ \"What is AI?\" , \"Explain deep learning\" , \"What is NLP?\" ]) for result in results : print ( result )","title":"Batch Predictions"},{"location":"user-guide/model-management/#custom-generation-parameters","text":"result = trainer . predict ( \"What is machine learning?\" , max_new_tokens = 200 , temperature = 0.7 , top_p = 0.9 , do_sample = True )","title":"Custom Generation Parameters"},{"location":"user-guide/model-management/#model-evaluation","text":"","title":"Model Evaluation"},{"location":"user-guide/model-management/#basic-evaluation","text":"# Evaluate on default validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/model-management/#custom-evaluation-dataset","text":"from datasets import load_dataset # Load custom evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( metrics )","title":"Custom Evaluation Dataset"},{"location":"user-guide/model-management/#custom-metric-prefix","text":"metrics = trainer . evaluate ( eval_dataset = eval_dataset , metric_key_prefix = \"test\" ) # Metrics will have \"test/\" prefix","title":"Custom Metric Prefix"},{"location":"user-guide/model-management/#checkpointing","text":"","title":"Checkpointing"},{"location":"user-guide/model-management/#save-checkpoints","text":"Checkpoints are automatically saved during training: trainer = create_sft_trainer ( ... , save_interval = 500 # Save every 500 steps ) trainer . train ()","title":"Save Checkpoints"},{"location":"user-guide/model-management/#load-checkpoint","text":"# Resume from checkpoint trainer . load_checkpoint ( \"./output/checkpoint-1000\" )","title":"Load Checkpoint"},{"location":"user-guide/model-management/#model-information","text":"","title":"Model Information"},{"location":"user-guide/model-management/#get-model-path","text":"# After saving path = trainer . save_model () print ( f \"Model path: { path } \" )","title":"Get Model Path"},{"location":"user-guide/model-management/#check-model-status","text":"# Check if model is loaded if trainer . model is not None : print ( \"Model is loaded\" ) print ( f \"Device: { next ( trainer . model . parameters ()) . device } \" )","title":"Check Model Status"},{"location":"user-guide/model-management/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/model-management/#1-save-regularly","text":"# Save after training trainer . train () trainer . save_model () # Also save checkpoints during training # (automatic with save_interval)","title":"1. Save Regularly"},{"location":"user-guide/model-management/#2-version-your-models","text":"import datetime timestamp = datetime . datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) path = trainer . save_model ( output_dir = f \"./models/experiment_ { timestamp } \" )","title":"2. Version Your Models"},{"location":"user-guide/model-management/#3-push-important-models","text":"# Push production models to Hub if is_production_model : trainer . push_to_hub ( repo_id = \"username/production-model\" , private = False )","title":"3. Push Important Models"},{"location":"user-guide/model-management/#4-document-your-models","text":"# Save with metadata trainer . save_model () # Edit training_config.yaml to add notes","title":"4. Document Your Models"},{"location":"user-guide/model-management/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/model-management/#model-not-found","text":"# Check if model exists from pathlib import Path model_path = Path ( \"./output/my_model\" ) if not model_path . exists (): print ( \"Model not found!\" )","title":"Model Not Found"},{"location":"user-guide/model-management/#push-to-hub-fails","text":"# Ensure you're logged in from huggingface_hub import login login ( token = \"hf_...\" ) # Then push trainer . push_to_hub ( \"username/my-model\" )","title":"Push to Hub Fails"},{"location":"user-guide/model-management/#memory-issues","text":"# Use smaller batch size for prediction result = trainer . predict ( \"Your input\" , max_new_tokens = 50 # Reduce if needed )","title":"Memory Issues"},{"location":"user-guide/model-management/#next-steps","text":"Evaluation Guide - Learn about evaluation metrics SFT Guide - SFT-specific model management RL Guide - RL-specific model management","title":"Next Steps"},{"location":"user-guide/overview/","text":"User Guide Overview \u00b6 Welcome to the AlignTune User Guide! This section provides comprehensive tutorials and guides for using AlignTune effectively. What is AlignTune? \u00b6 AlignTune is a production-ready fine-tuning library for Large Language Models (LLMs) that supports: Supervised Fine-Tuning (SFT) : Adapt pre-trained models to specific tasks Reinforcement Learning (RL) : Align models with human preferences using RLHF algorithms Multi-Backend Support : Choose between TRL (reliable) and Unsloth (faster) backends RL Algorithms : DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO Core Components \u00b6 1. Backend Factory \u00b6 The BackendFactory is the main entry point for creating trainers: from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Create RL trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) 2. Configuration System \u00b6 AlignTune supports three configuration methods: Python API : Direct function calls with keyword arguments YAML Files : Declarative configuration files CLI : Command-line interface 3. Reward System \u00b6 27+ built-in reward functions for quality, safety, style, and task-specific metrics. 4. Evaluation System \u00b6 Comprehensive evaluation with basic metrics, quality metrics, and safety metrics. Getting Started \u00b6 For SFT Training \u00b6 Start with SFT Guide : Complete guide to Supervised Fine-Tuning Learn Configuration : Understand configuration options Explore Examples : See real-world examples For RL Training \u00b6 Start with RL Guide : Complete guide to Reinforcement Learning Learn Reward Functions : Understand reward functions Explore RL Examples : See RL training examples Guide Structure \u00b6 Supervised Fine-Tuning (SFT) \u00b6 SFT Guide : Complete SFT training guide Task types (instruction following, classification, chat) Configuration options Best practices Examples Reinforcement Learning (RL) \u00b6 RL Guide : Complete RL training guide Algorithm overview (DPO, PPO, GRPO, etc.) Configuration options Best practices Examples Reward Functions \u00b6 Reward Functions : Using reward functions Built-in reward functions Custom reward functions Composite rewards Reward Model Training \u00b6 Reward Model Training : Training custom reward models Training from rule-based functions Integration with PPO Best practices Evaluation \u00b6 Evaluation : Model evaluation Basic metrics Quality metrics Safety metrics Task-specific metrics Model Management \u00b6 Model Management : Saving, loading, and sharing models Local model saving HuggingFace Hub integration Checkpoint management Sample Logging \u00b6 Sample Logging : Qualitative sample generation Configuration Best practices Examples Troubleshooting \u00b6 Troubleshooting : Common issues and solutions Backend issues CUDA/GPU issues Configuration issues Training issues Quick Reference \u00b6 Common Patterns \u00b6 SFT Training Pattern \u00b6 from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () trainer . evaluate () trainer . save_model () DPO Training Pattern \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model () PPO Training Pattern \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model () Next Steps \u00b6 Getting Started : Installation and setup Basic Concepts : Core concepts SFT Guide : Start with SFT training RL Guide : Explore RL training Examples : See real-world examples Additional Resources \u00b6 API Reference : Complete API documentation CLI Overview : Command-line interface API Reference : Complete API documentation Examples : Code examples and tutorials Advanced Topics : Architecture and advanced usage Contributing : Contribute to AlignTune Ready to start? Begin with SFT Guide or RL Guide !","title":"Overview"},{"location":"user-guide/overview/#user-guide-overview","text":"Welcome to the AlignTune User Guide! This section provides comprehensive tutorials and guides for using AlignTune effectively.","title":"User Guide Overview"},{"location":"user-guide/overview/#what-is-aligntune","text":"AlignTune is a production-ready fine-tuning library for Large Language Models (LLMs) that supports: Supervised Fine-Tuning (SFT) : Adapt pre-trained models to specific tasks Reinforcement Learning (RL) : Align models with human preferences using RLHF algorithms Multi-Backend Support : Choose between TRL (reliable) and Unsloth (faster) backends RL Algorithms : DPO, PPO, GRPO, GSPO, DAPO, Dr. GRPO","title":"What is AlignTune?"},{"location":"user-guide/overview/#core-components","text":"","title":"Core Components"},{"location":"user-guide/overview/#1-backend-factory","text":"The BackendFactory is the main entry point for creating trainers: from aligntune.core.backend_factory import create_sft_trainer , create_rl_trainer # Create SFT trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" ) # Create RL trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"1. Backend Factory"},{"location":"user-guide/overview/#2-configuration-system","text":"AlignTune supports three configuration methods: Python API : Direct function calls with keyword arguments YAML Files : Declarative configuration files CLI : Command-line interface","title":"2. Configuration System"},{"location":"user-guide/overview/#3-reward-system","text":"27+ built-in reward functions for quality, safety, style, and task-specific metrics.","title":"3. Reward System"},{"location":"user-guide/overview/#4-evaluation-system","text":"Comprehensive evaluation with basic metrics, quality metrics, and safety metrics.","title":"4. Evaluation System"},{"location":"user-guide/overview/#getting-started","text":"","title":"Getting Started"},{"location":"user-guide/overview/#for-sft-training","text":"Start with SFT Guide : Complete guide to Supervised Fine-Tuning Learn Configuration : Understand configuration options Explore Examples : See real-world examples","title":"For SFT Training"},{"location":"user-guide/overview/#for-rl-training","text":"Start with RL Guide : Complete guide to Reinforcement Learning Learn Reward Functions : Understand reward functions Explore RL Examples : See RL training examples","title":"For RL Training"},{"location":"user-guide/overview/#guide-structure","text":"","title":"Guide Structure"},{"location":"user-guide/overview/#supervised-fine-tuning-sft","text":"SFT Guide : Complete SFT training guide Task types (instruction following, classification, chat) Configuration options Best practices Examples","title":"Supervised Fine-Tuning (SFT)"},{"location":"user-guide/overview/#reinforcement-learning-rl","text":"RL Guide : Complete RL training guide Algorithm overview (DPO, PPO, GRPO, etc.) Configuration options Best practices Examples","title":"Reinforcement Learning (RL)"},{"location":"user-guide/overview/#reward-functions","text":"Reward Functions : Using reward functions Built-in reward functions Custom reward functions Composite rewards","title":"Reward Functions"},{"location":"user-guide/overview/#reward-model-training","text":"Reward Model Training : Training custom reward models Training from rule-based functions Integration with PPO Best practices","title":"Reward Model Training"},{"location":"user-guide/overview/#evaluation","text":"Evaluation : Model evaluation Basic metrics Quality metrics Safety metrics Task-specific metrics","title":"Evaluation"},{"location":"user-guide/overview/#model-management","text":"Model Management : Saving, loading, and sharing models Local model saving HuggingFace Hub integration Checkpoint management","title":"Model Management"},{"location":"user-guide/overview/#sample-logging","text":"Sample Logging : Qualitative sample generation Configuration Best practices Examples","title":"Sample Logging"},{"location":"user-guide/overview/#troubleshooting","text":"Troubleshooting : Common issues and solutions Backend issues CUDA/GPU issues Configuration issues Training issues","title":"Troubleshooting"},{"location":"user-guide/overview/#quick-reference","text":"","title":"Quick Reference"},{"location":"user-guide/overview/#common-patterns","text":"","title":"Common Patterns"},{"location":"user-guide/overview/#sft-training-pattern","text":"from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , num_epochs = 3 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"SFT Training Pattern"},{"location":"user-guide/overview/#dpo-training-pattern","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"DPO Training Pattern"},{"location":"user-guide/overview/#ppo-training-pattern","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"your-reward-model\" , num_epochs = 1 ) trainer . train () trainer . evaluate () trainer . save_model ()","title":"PPO Training Pattern"},{"location":"user-guide/overview/#next-steps","text":"Getting Started : Installation and setup Basic Concepts : Core concepts SFT Guide : Start with SFT training RL Guide : Explore RL training Examples : See real-world examples","title":"Next Steps"},{"location":"user-guide/overview/#additional-resources","text":"API Reference : Complete API documentation CLI Overview : Command-line interface API Reference : Complete API documentation Examples : Code examples and tutorials Advanced Topics : Architecture and advanced usage Contributing : Contribute to AlignTune Ready to start? Begin with SFT Guide or RL Guide !","title":"Additional Resources"},{"location":"user-guide/reward-functions/","text":"Reward Functions Guide \u00b6 Complete guide to using reward functions in AlignTune for RLHF training. Overview \u00b6 Reward functions measure the quality of model outputs and guide reinforcement learning training. AlignTune provides 27+ prebuilt reward functions covering quality, safety, task-specific metrics, and more. Quick Start \u00b6 Using Reward Functions in PPO \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # Reward functions for custom reward model training reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], num_epochs = 1 , batch_size = 1 ) trainer . train () Using Reward Functions in Reward Model Training \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions from registry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer with reward functions trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 ) Reward Function Categories \u00b6 Basic Quality Rewards \u00b6 Length Reward \u00b6 Encourages appropriate response length. reward_functions = [ \"length\" ] # With parameters reward_config = { \"type\" : \"length\" , \"params\" : { \"min_length\" : 10 , \"max_length\" : 500 }, \"weight\" : 1.0 } Coherence Reward \u00b6 Measures logical flow and coherence. reward_functions = [ \"coherence\" ] Fluency Reward \u00b6 Measures grammatical correctness. reward_functions = [ \"fluency\" ] Task-Specific Rewards \u00b6 Sentiment Reward \u00b6 Encourages specific sentiment. reward_functions = [ \"sentiment\" ] # With target sentiment reward_config = { \"type\" : \"sentiment\" , \"params\" : { \"target_sentiment\" : \"positive\" }, \"weight\" : 1.0 } Safety Reward \u00b6 Detects and penalizes harmful content. reward_functions = [ \"safety\" ] # With strict mode reward_config = { \"type\" : \"safety\" , \"params\" : { \"strict\" : True }, \"weight\" : 2.0 } Factuality Reward \u00b6 Measures factual accuracy. reward_functions = [ \"factuality\" ] Generation Quality Metrics \u00b6 BLEU Reward \u00b6 Measures n-gram overlap with reference. reward_functions = [ \"bleu\" ] reward_config = { \"type\" : \"bleu\" , \"params\" : { \"n_gram\" : 4 }, \"weight\" : 0.8 } ROUGE Reward \u00b6 Measures recall-oriented evaluation. reward_functions = [ \"rouge\" ] reward_config = { \"type\" : \"rouge\" , \"params\" : { \"rouge_type\" : \"rouge-l\" }, \"weight\" : 1.0 } Code Quality Rewards \u00b6 Code Syntax Reward \u00b6 Validates code syntax correctness. reward_functions = [ \"code_syntax\" ] reward_config = { \"type\" : \"code_syntax\" , \"params\" : { \"language\" : \"python\" }, \"weight\" : 1.0 } Code Execution Reward \u00b6 Tests if code runs without errors. reward_functions = [ \"code_execution\" ] reward_config = { \"type\" : \"code_execution\" , \"params\" : { \"timeout\" : 5 , \"safe_mode\" : True }, \"weight\" : 1.5 } Math and Reasoning Rewards \u00b6 Math Correctness Reward \u00b6 Validates mathematical correctness. reward_functions = [ \"math_correctness\" ] reward_config = { \"type\" : \"math_correctness\" , \"params\" : { \"tolerance\" : 1e-6 }, \"weight\" : 1.0 } Logical Consistency Reward \u00b6 Measures logical consistency. reward_functions = [ \"logical_consistency\" ] Composite Rewards \u00b6 Combine multiple reward functions with weights: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Multiple reward functions reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" , \"helpfulness\" ], reward_function_weights = [ 0.2 , # length 0.2 , # sentiment 0.3 , # safety (higher weight) 0.15 , # coherence 0.15 # helpfulness ], train_custom_reward_model = True , reward_training_texts = training_texts , reward_training_base_model = \"microsoft/DialoGPT-medium\" ) Using Reward Registry \u00b6 Get Available Functions \u00b6 from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () # List all available functions available = registry . list_rewards () print ( available ) # Get specific function length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) Create Custom Reward Function \u00b6 from aligntune.rewards import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry from aligntune.core.backend_factory import create_rl_trainer class CustomReward ( RewardFunction ): def __init__ ( self ): super () . __init__ ( RewardType . CUSTOM ) def compute ( self , text : str , ** kwargs ) -> float : # Your custom reward logic score = 0.0 # ... compute score based on text return score # Register custom function RewardRegistry . register_custom_reward ( \"custom\" , CustomReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_functions = [ \"custom\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] ) Best Practices \u00b6 1. Choose Relevant Rewards \u00b6 Select rewards that match your task: Customer Service : sentiment, politeness, helpfulness Code Generation : code_syntax, code_execution, code_completeness Math Problems : math_correctness, logical_consistency Safety-Critical : safety, toxicity, bias 2. Weight Balancing \u00b6 Balance reward weights based on importance: # Safety is critical - higher weight reward_function_weights = [ 0.1 , 0.1 , 0.5 , 0.2 , 0.1 ] # length, sentiment, safety, coherence, helpfulness 3. Start Simple \u00b6 Begin with a few key rewards, then expand: # Start with basics reward_functions = [ \"length\" , \"safety\" ] # Add more as needed reward_functions = [ \"length\" , \"safety\" , \"sentiment\" , \"coherence\" ] 4. Test Reward Functions \u00b6 Test rewards independently before combining: from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) # Test on sample text test_text = \"This is a test response.\" score = length_func . compute ( test_text ) print ( f \"Length reward: { score } \" ) Complete Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain.\" , # ... more texts ] # Create trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model with multiple functions train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest weight) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train trainer . train () Available Reward Functions \u00b6 For a complete list of all 27+ reward functions, see the Reward Functions Reference . Categories: - Basic Quality (length, coherence, fluency) - Task-Specific (sentiment, safety, factuality, bias) - Generation Metrics (BLEU, ROUGE, METEOR, BERTScore) - Code Quality (syntax, execution, completeness) - Math & Reasoning (correctness, logical consistency, commonsense) - Specialized (hallucination, toxicity, politeness, helpfulness, honesty) Next Steps \u00b6 Reward Model Training - Train custom reward models RL Training Guide - Complete RL training guide Reward Functions Reference - Complete function list Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Reward Model Training System - Detailed system docs","title":"Reward Functions"},{"location":"user-guide/reward-functions/#reward-functions-guide","text":"Complete guide to using reward functions in AlignTune for RLHF training.","title":"Reward Functions Guide"},{"location":"user-guide/reward-functions/#overview","text":"Reward functions measure the quality of model outputs and guide reinforcement learning training. AlignTune provides 27+ prebuilt reward functions covering quality, safety, task-specific metrics, and more.","title":"Overview"},{"location":"user-guide/reward-functions/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/reward-functions/#using-reward-functions-in-ppo","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # Reward functions for custom reward model training reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], num_epochs = 1 , batch_size = 1 ) trainer . train ()","title":"Using Reward Functions in PPO"},{"location":"user-guide/reward-functions/#using-reward-functions-in-reward-model-training","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions from registry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer with reward functions trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = [ \"This is a good response.\" , \"This is a bad response.\" ], batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 )","title":"Using Reward Functions in Reward Model Training"},{"location":"user-guide/reward-functions/#reward-function-categories","text":"","title":"Reward Function Categories"},{"location":"user-guide/reward-functions/#basic-quality-rewards","text":"","title":"Basic Quality Rewards"},{"location":"user-guide/reward-functions/#length-reward","text":"Encourages appropriate response length. reward_functions = [ \"length\" ] # With parameters reward_config = { \"type\" : \"length\" , \"params\" : { \"min_length\" : 10 , \"max_length\" : 500 }, \"weight\" : 1.0 }","title":"Length Reward"},{"location":"user-guide/reward-functions/#coherence-reward","text":"Measures logical flow and coherence. reward_functions = [ \"coherence\" ]","title":"Coherence Reward"},{"location":"user-guide/reward-functions/#fluency-reward","text":"Measures grammatical correctness. reward_functions = [ \"fluency\" ]","title":"Fluency Reward"},{"location":"user-guide/reward-functions/#task-specific-rewards","text":"","title":"Task-Specific Rewards"},{"location":"user-guide/reward-functions/#sentiment-reward","text":"Encourages specific sentiment. reward_functions = [ \"sentiment\" ] # With target sentiment reward_config = { \"type\" : \"sentiment\" , \"params\" : { \"target_sentiment\" : \"positive\" }, \"weight\" : 1.0 }","title":"Sentiment Reward"},{"location":"user-guide/reward-functions/#safety-reward","text":"Detects and penalizes harmful content. reward_functions = [ \"safety\" ] # With strict mode reward_config = { \"type\" : \"safety\" , \"params\" : { \"strict\" : True }, \"weight\" : 2.0 }","title":"Safety Reward"},{"location":"user-guide/reward-functions/#factuality-reward","text":"Measures factual accuracy. reward_functions = [ \"factuality\" ]","title":"Factuality Reward"},{"location":"user-guide/reward-functions/#generation-quality-metrics","text":"","title":"Generation Quality Metrics"},{"location":"user-guide/reward-functions/#bleu-reward","text":"Measures n-gram overlap with reference. reward_functions = [ \"bleu\" ] reward_config = { \"type\" : \"bleu\" , \"params\" : { \"n_gram\" : 4 }, \"weight\" : 0.8 }","title":"BLEU Reward"},{"location":"user-guide/reward-functions/#rouge-reward","text":"Measures recall-oriented evaluation. reward_functions = [ \"rouge\" ] reward_config = { \"type\" : \"rouge\" , \"params\" : { \"rouge_type\" : \"rouge-l\" }, \"weight\" : 1.0 }","title":"ROUGE Reward"},{"location":"user-guide/reward-functions/#code-quality-rewards","text":"","title":"Code Quality Rewards"},{"location":"user-guide/reward-functions/#code-syntax-reward","text":"Validates code syntax correctness. reward_functions = [ \"code_syntax\" ] reward_config = { \"type\" : \"code_syntax\" , \"params\" : { \"language\" : \"python\" }, \"weight\" : 1.0 }","title":"Code Syntax Reward"},{"location":"user-guide/reward-functions/#code-execution-reward","text":"Tests if code runs without errors. reward_functions = [ \"code_execution\" ] reward_config = { \"type\" : \"code_execution\" , \"params\" : { \"timeout\" : 5 , \"safe_mode\" : True }, \"weight\" : 1.5 }","title":"Code Execution Reward"},{"location":"user-guide/reward-functions/#math-and-reasoning-rewards","text":"","title":"Math and Reasoning Rewards"},{"location":"user-guide/reward-functions/#math-correctness-reward","text":"Validates mathematical correctness. reward_functions = [ \"math_correctness\" ] reward_config = { \"type\" : \"math_correctness\" , \"params\" : { \"tolerance\" : 1e-6 }, \"weight\" : 1.0 }","title":"Math Correctness Reward"},{"location":"user-guide/reward-functions/#logical-consistency-reward","text":"Measures logical consistency. reward_functions = [ \"logical_consistency\" ]","title":"Logical Consistency Reward"},{"location":"user-guide/reward-functions/#composite-rewards","text":"Combine multiple reward functions with weights: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Multiple reward functions reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" , \"helpfulness\" ], reward_function_weights = [ 0.2 , # length 0.2 , # sentiment 0.3 , # safety (higher weight) 0.15 , # coherence 0.15 # helpfulness ], train_custom_reward_model = True , reward_training_texts = training_texts , reward_training_base_model = \"microsoft/DialoGPT-medium\" )","title":"Composite Rewards"},{"location":"user-guide/reward-functions/#using-reward-registry","text":"","title":"Using Reward Registry"},{"location":"user-guide/reward-functions/#get-available-functions","text":"from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () # List all available functions available = registry . list_rewards () print ( available ) # Get specific function length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" )","title":"Get Available Functions"},{"location":"user-guide/reward-functions/#create-custom-reward-function","text":"from aligntune.rewards import RewardFunction , RewardType from aligntune.rewards.registry import RewardRegistry from aligntune.core.backend_factory import create_rl_trainer class CustomReward ( RewardFunction ): def __init__ ( self ): super () . __init__ ( RewardType . CUSTOM ) def compute ( self , text : str , ** kwargs ) -> float : # Your custom reward logic score = 0.0 # ... compute score based on text return score # Register custom function RewardRegistry . register_custom_reward ( \"custom\" , CustomReward ) # Use in training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , reward_functions = [ \"custom\" , \"length\" , \"safety\" ], reward_function_weights = [ 0.5 , 0.3 , 0.2 ] )","title":"Create Custom Reward Function"},{"location":"user-guide/reward-functions/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/reward-functions/#1-choose-relevant-rewards","text":"Select rewards that match your task: Customer Service : sentiment, politeness, helpfulness Code Generation : code_syntax, code_execution, code_completeness Math Problems : math_correctness, logical_consistency Safety-Critical : safety, toxicity, bias","title":"1. Choose Relevant Rewards"},{"location":"user-guide/reward-functions/#2-weight-balancing","text":"Balance reward weights based on importance: # Safety is critical - higher weight reward_function_weights = [ 0.1 , 0.1 , 0.5 , 0.2 , 0.1 ] # length, sentiment, safety, coherence, helpfulness","title":"2. Weight Balancing"},{"location":"user-guide/reward-functions/#3-start-simple","text":"Begin with a few key rewards, then expand: # Start with basics reward_functions = [ \"length\" , \"safety\" ] # Add more as needed reward_functions = [ \"length\" , \"safety\" , \"sentiment\" , \"coherence\" ]","title":"3. Start Simple"},{"location":"user-guide/reward-functions/#4-test-reward-functions","text":"Test rewards independently before combining: from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) # Test on sample text test_text = \"This is a test response.\" score = length_func . compute ( test_text ) print ( f \"Length reward: { score } \" )","title":"4. Test Reward Functions"},{"location":"user-guide/reward-functions/#complete-example","text":"from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain.\" , # ... more texts ] # Create trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model with multiple functions train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest weight) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train trainer . train ()","title":"Complete Example"},{"location":"user-guide/reward-functions/#available-reward-functions","text":"For a complete list of all 27+ reward functions, see the Reward Functions Reference . Categories: - Basic Quality (length, coherence, fluency) - Task-Specific (sentiment, safety, factuality, bias) - Generation Metrics (BLEU, ROUGE, METEOR, BERTScore) - Code Quality (syntax, execution, completeness) - Math & Reasoning (correctness, logical consistency, commonsense) - Specialized (hallucination, toxicity, politeness, helpfulness, honesty)","title":"Available Reward Functions"},{"location":"user-guide/reward-functions/#next-steps","text":"Reward Model Training - Train custom reward models RL Training Guide - Complete RL training guide Reward Functions Reference - Complete function list","title":"Next Steps"},{"location":"user-guide/reward-functions/#additional-resources","text":"API Reference - API documentation Examples - Code examples Reward Model Training System - Detailed system docs","title":"Additional Resources"},{"location":"user-guide/reward-model-training/","text":"Reward Model Training Guide \u00b6 Complete guide to training custom reward models from rule-based reward functions and integrating them into PPO training. Overview \u00b6 Reward models learn to score text quality based on training data generated from rule-based reward functions. AlignTune provides a complete system for training reward models and seamlessly integrating them into PPO training pipelines. Quick Start \u00b6 Standalone Reward Model Training \u00b6 from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_texts = [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" ] training_data = trainer . generate_training_data ( texts = training_texts , batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Reward model saved to: { model_path } \" ) PPO with Custom Reward Model Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load your training texts.\"\"\" return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train (reward model is trained first, then PPO) trainer . train () Training Modes \u00b6 1. Standalone Training \u00b6 Train reward models independently for later use: from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_training_texts , batch_size = 32 ) # Train model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/my_model\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) 2. Integrated PPO Training \u00b6 Train reward model as part of PPO training: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" ) Configuration \u00b6 Reward Model Training Config \u00b6 from aligntune.core.rl.config import RewardModelTrainingConfig config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , # Required training_texts = your_texts , # Required reward_functions = [ \"length\" , \"sentiment\" ], # Required output_dir = \"./reward_models/custom\" , # Required # Optional parameters reward_weights = [ 0.5 , 0.5 ], num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 , gradient_accumulation_steps = 4 , max_length = 512 ) Reward Model Source Config \u00b6 from aligntune.core.rl.config import RewardModelSourceConfig , RewardModelTrainingConfig # For custom training training_config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , training_texts = texts , reward_functions = [ \"length\" , \"safety\" ], output_dir = \"./reward_models/custom\" ) source_config = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) # For HuggingFace Hub source_config = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # For local model source_config = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = \"./reward_models/my_model\" ) Using Pre-trained Reward Models \u00b6 From HuggingFace Hub \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) From Local Path \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = \"./reward_models/my_custom_model\" ) Training Data Generation \u00b6 Basic Generation \u00b6 from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_texts , batch_size = 32 ) With Reference Texts \u00b6 training_data = trainer . generate_training_data ( texts = your_texts , reference_texts = reference_texts , # Optional reference texts batch_size = 32 ) Best Practices \u00b6 1. Training Data Quality \u00b6 Use diverse, representative texts Include both good and bad examples Ensure sufficient quantity (100+ texts recommended) 2. Reward Function Selection \u00b6 Choose functions relevant to your task Balance weights appropriately Start with a few key functions, expand as needed 3. Model Selection \u00b6 Use base models compatible with your policy model Consider model family consistency (for PPO) Smaller models work well for reward models 4. Training Parameters \u00b6 # Recommended settings num_epochs = 3 # 3-5 epochs usually sufficient learning_rate = 1e-5 # Lower learning rate for reward models batch_size = 8 # Adjust based on model size gradient_accumulation_steps = 4 # Effective batch size = 32 5. Validation \u00b6 Validate reward model on held-out data Test reward scores on sample texts Ensure rewards align with expectations Complete Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer from aligntune.rewards.registry import RewardRegistry def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response that addresses the question.\" , \"I'm not entirely sure, but I think the answer might be related to this.\" , \"That's a great question! Let me break it down step by step for you.\" , \"I don't have enough information to provide a complete answer.\" , \"Here's a comprehensive explanation of the topic you asked about.\" , # ... more diverse texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # Training parameters reward_training_epochs = 3 , reward_training_lr = 1e-5 , reward_training_batch_size = 8 , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 , max_samples = 100 ) # Train (reward model trained first, then PPO) print ( \"Starting training...\" ) trainer . train () # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) if __name__ == \"__main__\" : main () Validation \u00b6 Strict Validation Rules \u00b6 AlignTune enforces strict validation: Exactly One Source : Must specify exactly one reward source No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Positive Weights : All reward weights must be positive Error Handling \u00b6 No Fallbacks : System fails fast with clear errors Clear Messages : Errors include actionable information Comprehensive Validation : All inputs validated before processing Troubleshooting \u00b6 Insufficient Training Data \u00b6 # Ensure at least 10 texts if len ( training_texts ) < 10 : raise ValueError ( \"Need at least 10 training texts\" ) Invalid Reward Functions \u00b6 from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () available = registry . list_reward_functions () # Check if function exists if \"my_function\" not in available : raise ValueError ( f \"Reward function not found. Available: { available } \" ) Model Family Mismatch (PPO) \u00b6 # Ensure reward model matches policy model family # Correct: Both Qwen model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"Qwen/Qwen3-0.6B\" # Wrong: Different families model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"meta-llama/Llama-2-7b-hf\" Next Steps \u00b6 Reward Functions Guide - Explore reward functions RL Training Guide - Complete RL training guide Reward Model Training System - Detailed system docs Additional Resources \u00b6 API Reference - API documentation Examples - Code examples Reward Functions Reference - Complete function list","title":"Reward Model Training"},{"location":"user-guide/reward-model-training/#reward-model-training-guide","text":"Complete guide to training custom reward models from rule-based reward functions and integrating them into PPO training.","title":"Reward Model Training Guide"},{"location":"user-guide/reward-model-training/#overview","text":"Reward models learn to score text quality based on training data generated from rule-based reward functions. AlignTune provides a complete system for training reward models and seamlessly integrating them into PPO training pipelines.","title":"Overview"},{"location":"user-guide/reward-model-training/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/reward-model-training/#standalone-reward-model-training","text":"from aligntune.rewards.training import RewardModelTrainer from aligntune.rewards.registry import RewardRegistry # Get reward functions registry = RewardRegistry () length_func = registry . get_reward_function ( \"length\" ) sentiment_func = registry . get_reward_function ( \"sentiment\" ) safety_func = registry . get_reward_function ( \"safety\" ) # Create trainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ length_func , sentiment_func , safety_func ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_texts = [ \"This is a helpful and informative response.\" , \"I'm not sure, but here's what I think.\" , \"That's a great question! Let me explain step by step.\" ] training_data = trainer . generate_training_data ( texts = training_texts , batch_size = 32 ) # Train reward model model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/custom\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 ) print ( f \"Reward model saved to: { model_path } \" )","title":"Standalone Reward Model Training"},{"location":"user-guide/reward-model-training/#ppo-with-custom-reward-model-training","text":"from aligntune.core.backend_factory import create_rl_trainer def load_training_texts (): \"\"\"Load your training texts.\"\"\" return [ \"This is a helpful response.\" , \"I'm not sure about this.\" , \"That's a great question!\" , # ... more texts ] # Create PPO trainer with custom reward model training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) # Train (reward model is trained first, then PPO) trainer . train ()","title":"PPO with Custom Reward Model Training"},{"location":"user-guide/reward-model-training/#training-modes","text":"","title":"Training Modes"},{"location":"user-guide/reward-model-training/#1-standalone-training","text":"Train reward models independently for later use: from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_training_texts , batch_size = 32 ) # Train model_path = trainer . train_reward_model ( training_data = training_data , output_dir = \"./reward_models/my_model\" , num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 )","title":"1. Standalone Training"},{"location":"user-guide/reward-model-training/#2-integrated-ppo-training","text":"Train reward model as part of PPO training: trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" )","title":"2. Integrated PPO Training"},{"location":"user-guide/reward-model-training/#configuration","text":"","title":"Configuration"},{"location":"user-guide/reward-model-training/#reward-model-training-config","text":"from aligntune.core.rl.config import RewardModelTrainingConfig config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , # Required training_texts = your_texts , # Required reward_functions = [ \"length\" , \"sentiment\" ], # Required output_dir = \"./reward_models/custom\" , # Required # Optional parameters reward_weights = [ 0.5 , 0.5 ], num_epochs = 3 , learning_rate = 1e-5 , batch_size = 8 , gradient_accumulation_steps = 4 , max_length = 512 )","title":"Reward Model Training Config"},{"location":"user-guide/reward-model-training/#reward-model-source-config","text":"from aligntune.core.rl.config import RewardModelSourceConfig , RewardModelTrainingConfig # For custom training training_config = RewardModelTrainingConfig ( base_model_name = \"microsoft/DialoGPT-medium\" , training_texts = texts , reward_functions = [ \"length\" , \"safety\" ], output_dir = \"./reward_models/custom\" ) source_config = RewardModelSourceConfig ( source_type = \"custom_trained\" , training_config = training_config ) # For HuggingFace Hub source_config = RewardModelSourceConfig ( source_type = \"pretrained_hf\" , model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # For local model source_config = RewardModelSourceConfig ( source_type = \"pretrained_local\" , model_path = \"./reward_models/my_model\" )","title":"Reward Model Source Config"},{"location":"user-guide/reward-model-training/#using-pre-trained-reward-models","text":"","title":"Using Pre-trained Reward Models"},{"location":"user-guide/reward-model-training/#from-huggingface-hub","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" )","title":"From HuggingFace Hub"},{"location":"user-guide/reward-model-training/#from-local-path","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_path = \"./reward_models/my_custom_model\" )","title":"From Local Path"},{"location":"user-guide/reward-model-training/#training-data-generation","text":"","title":"Training Data Generation"},{"location":"user-guide/reward-model-training/#basic-generation","text":"from aligntune.rewards.training import RewardModelTrainer trainer = RewardModelTrainer ( base_model_name = \"microsoft/DialoGPT-medium\" , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], composite_weights = [ 0.3 , 0.4 , 0.3 ] ) # Generate training data training_data = trainer . generate_training_data ( texts = your_texts , batch_size = 32 )","title":"Basic Generation"},{"location":"user-guide/reward-model-training/#with-reference-texts","text":"training_data = trainer . generate_training_data ( texts = your_texts , reference_texts = reference_texts , # Optional reference texts batch_size = 32 )","title":"With Reference Texts"},{"location":"user-guide/reward-model-training/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/reward-model-training/#1-training-data-quality","text":"Use diverse, representative texts Include both good and bad examples Ensure sufficient quantity (100+ texts recommended)","title":"1. Training Data Quality"},{"location":"user-guide/reward-model-training/#2-reward-function-selection","text":"Choose functions relevant to your task Balance weights appropriately Start with a few key functions, expand as needed","title":"2. Reward Function Selection"},{"location":"user-guide/reward-model-training/#3-model-selection","text":"Use base models compatible with your policy model Consider model family consistency (for PPO) Smaller models work well for reward models","title":"3. Model Selection"},{"location":"user-guide/reward-model-training/#4-training-parameters","text":"# Recommended settings num_epochs = 3 # 3-5 epochs usually sufficient learning_rate = 1e-5 # Lower learning rate for reward models batch_size = 8 # Adjust based on model size gradient_accumulation_steps = 4 # Effective batch size = 32","title":"4. Training Parameters"},{"location":"user-guide/reward-model-training/#5-validation","text":"Validate reward model on held-out data Test reward scores on sample texts Ensure rewards align with expectations","title":"5. Validation"},{"location":"user-guide/reward-model-training/#complete-example","text":"from aligntune.core.backend_factory import create_rl_trainer from aligntune.rewards.registry import RewardRegistry def load_training_texts (): \"\"\"Load training texts for reward model.\"\"\" return [ \"This is a helpful and informative response that addresses the question.\" , \"I'm not entirely sure, but I think the answer might be related to this.\" , \"That's a great question! Let me break it down step by step for you.\" , \"I don't have enough information to provide a complete answer.\" , \"Here's a comprehensive explanation of the topic you asked about.\" , # ... more diverse texts ] def main (): # Load training data training_texts = load_training_texts () # Create PPO trainer with custom reward model trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , # Appropriate length \"sentiment\" , # Positive sentiment \"safety\" , # Safety (high priority) \"coherence\" , # Logical flow \"helpfulness\" # Helpful responses ], reward_function_weights = [ 0.15 , # length 0.15 , # sentiment 0.35 , # safety (highest) 0.20 , # coherence 0.15 # helpfulness ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom_ppo\" , # Training parameters reward_training_epochs = 3 , reward_training_lr = 1e-5 , reward_training_batch_size = 8 , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 , max_samples = 100 ) # Train (reward model trained first, then PPO) print ( \"Starting training...\" ) trainer . train () # Save model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) if __name__ == \"__main__\" : main ()","title":"Complete Example"},{"location":"user-guide/reward-model-training/#validation","text":"","title":"Validation"},{"location":"user-guide/reward-model-training/#strict-validation-rules","text":"AlignTune enforces strict validation: Exactly One Source : Must specify exactly one reward source No Empty Fields : All required fields must be non-empty Minimum Data : At least 10 training texts required Valid Functions : All reward functions must exist in registry Positive Weights : All reward weights must be positive","title":"Strict Validation Rules"},{"location":"user-guide/reward-model-training/#error-handling","text":"No Fallbacks : System fails fast with clear errors Clear Messages : Errors include actionable information Comprehensive Validation : All inputs validated before processing","title":"Error Handling"},{"location":"user-guide/reward-model-training/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/reward-model-training/#insufficient-training-data","text":"# Ensure at least 10 texts if len ( training_texts ) < 10 : raise ValueError ( \"Need at least 10 training texts\" )","title":"Insufficient Training Data"},{"location":"user-guide/reward-model-training/#invalid-reward-functions","text":"from aligntune.rewards.registry import RewardRegistry registry = RewardRegistry () available = registry . list_reward_functions () # Check if function exists if \"my_function\" not in available : raise ValueError ( f \"Reward function not found. Available: { available } \" )","title":"Invalid Reward Functions"},{"location":"user-guide/reward-model-training/#model-family-mismatch-ppo","text":"# Ensure reward model matches policy model family # Correct: Both Qwen model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"Qwen/Qwen3-0.6B\" # Wrong: Different families model_name = \"Qwen/Qwen3-0.6B\" reward_training_base_model = \"meta-llama/Llama-2-7b-hf\"","title":"Model Family Mismatch (PPO)"},{"location":"user-guide/reward-model-training/#next-steps","text":"Reward Functions Guide - Explore reward functions RL Training Guide - Complete RL training guide Reward Model Training System - Detailed system docs","title":"Next Steps"},{"location":"user-guide/reward-model-training/#additional-resources","text":"API Reference - API documentation Examples - Code examples Reward Functions Reference - Complete function list","title":"Additional Resources"},{"location":"user-guide/rl/","text":"Reinforcement Learning (RL) Training Guide \u00b6 Complete guide to Reinforcement Learning from Human Feedback (RLHF) training with AlignTune, covering all supported RLHF algorithms. Overview \u00b6 Reinforcement Learning training aligns language models with human preferences using various algorithms. AlignTune supports multiple RLHF algorithms: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Flexible policy optimization with reward models GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) - Scaled RL for LLMs while addressing key limitations in GRPO Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant that corrects optimization biases Quick Start \u00b6 Basic DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer # Create and train DPO model trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-3.2-3B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) # Train the model trainer . train () Basic PPO Training \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , ) trainer . train () Algorithms \u00b6 1. DPO (Direct Preference Optimization) \u00b6 DPO trains models directly on preference pairs without requiring a separate reward model. Advantages: - No reward model needed - Simpler training pipeline - Direct preference learning Use Cases: - Preference alignment - Human feedback integration - General RLHF training Example \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B # DPO-specific parameters beta = 0.1 , # KL penalty coefficient reference_free = False , # Use reference model loss_type = \"sigmoid\" # Loss function type ) trainer . train () Dataset Format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know.\" } 2. PPO (Proximal Policy Optimization) \u00b6 PPO optimizes policies using reward models and value functions. Advantages: - Flexible reward shaping - Can use custom reward models - Supports complex reward landscapes Use Cases: - Custom reward model training - Complex reward functions - Production RLHF systems Example \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Reward model configuration reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # PPO-specific parameters num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , # KL penalty coefficient cliprange = 0.2 , # PPO clip range vf_coef = 0.1 , # Value function coefficient gamma = 1.0 , # Discount factor lam = 0.95 # GAE lambda ) trainer . train () Custom Reward Model Training \u00b6 def load_training_texts (): \"\"\"Load training texts for reward model training.\"\"\" return [ \"This is a helpful and informative response that addresses the question clearly.\" , \"I'm not sure about this answer.\" , \"This response is clear, concise, and well-structured.\" , ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () 3. GRPO (Group Relative Policy Optimization) \u00b6 GRPO optimizes policies using group-based relative comparisons. Advantages: - Multi-criteria optimization - Group-based learning - Flexible reward combinations Use Cases: - Multi-objective optimization - Complex reward landscapes - Group-based preference learning Example \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' ) trainer . train () 4. GSPO (Group Sequential Policy Optimization) \u00b6 GSPO uses sequential group learning for policy optimization. Note: GSPO is only supported by TRL backend, not Unsloth. Advantages: - Sequential learning - Group-based optimization - Structured policy updates Use Cases: - Sequential preference learning - Structured policy optimization - Group-based RLHF Example \u00b6 trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , # Note: group_size and sequential_steps are not in standard GRPO config # These parameters are specific to GSPO implementation ) trainer . train () Backend Selection \u00b6 TRL Backend \u00b6 Use TRL when: - Need maximum compatibility - Using GSPO (TRL only) - Working with standard models - Need reliable, battle-tested training trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" ) Unsloth Backend \u00b6 Use Unsloth when: - Need faster training - Working with large models - Need memory efficiency - Training PPO, DPO, or GRPO trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" ) Configuration \u00b6 Model Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Model settings max_seq_length = 2048 , quantization = { \"load_in_4bit\" : True }, use_peft = True , # Enable LoRA lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , use_gradient_checkpointing = True , # Reward model settings (for PPO) reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , reward_model_quantization = { \"load_in_4bit\" : True } ) Training Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Training settings num_epochs = 1 , max_steps = None , # Use epochs batch_size = 4 , gradient_accumulation_steps = 4 , learning_rate = 5e-5 , weight_decay = 0.01 , warmup_steps = 100 , max_grad_norm = 1.0 , # Algorithm-specific beta = 0.1 , # DPO: KL coefficient kl_coef = 0.1 , # PPO: KL coefficient cliprange = 0.2 , # PPO: clip range temperature = 0.7 # Generation temperature ) Dataset Configuration \u00b6 trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Dataset settings max_samples = 1000 , percent = 10.0 , split = \"train\" , # Field mappings column_mapping = { \"prompt\" : \"prompt\" , \"chosen\" : \"chosen\" , \"rejected\" : \"rejected\" }, truncation_mode = \"keep_end\" , padding_free = False ) Reward Models \u00b6 Using Pre-trained Reward Models \u00b6 # From HuggingFace Hub trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # From local path trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_path = \"./reward_models/my_reward_model\" ) Training Custom Reward Models \u00b6 See Reward Model Training Guide for detailed instructions. trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" ) Advanced Features \u00b6 Model Family Consistency (PPO) \u00b6 AlignTune automatically checks that all models in PPO training belong to the same family: # Correct: All Qwen models trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # Error: Mixed families trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! ) LoRA/QLoRA Fine-Tuning \u00b6 trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # LoRA configuration use_peft = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True } ) Distributed Training \u00b6 # Distributed training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Distributed settings (configure via accelerate) # See distributed training guide ) Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation \u00b6 from datasets import load_dataset eval_dataset = load_dataset ( \"Anthropic/hh-rlhf\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset ) Zero-Shot Evaluation \u00b6 # Generate predictions prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Best Practices \u00b6 1. Algorithm Selection \u00b6 DPO : Start here for preference alignment, simpler setup PPO : Use for custom rewards, complex scenarios GRPO : Multi-criteria optimization GSPO : Sequential learning (TRL only) 2. Backend Selection \u00b6 TRL : Maximum compatibility, GSPO support Unsloth : Faster training (Faster), memory efficient 3. Reward Models \u00b6 Use pre-trained models when available Train custom models for domain-specific tasks Ensure model family consistency for PPO 4. Hyperparameters \u00b6 # DPO recommended settings beta = 0.1 # KL coefficient learning_rate = 5e-5 , batch_size = 4 # PPO recommended settings kl_coef = 0.1 cliprange = 0.2 learning_rate = 1e-6 , batch_size = 1 # Smaller for PPO 5. Memory Optimization \u00b6 # For large models trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , quantization = { \"load_in_4bit\" : True }, use_peft = True , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 ) Troubleshooting \u00b6 Out of Memory \u00b6 # Reduce batch size, use quantization trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , batch_size = 1 , # Reduce gradient_accumulation_steps = 8 , # Compensate quantization = { \"load_in_4bit\" : True }, use_gradient_checkpointing = True ) Model Family Mismatch (PPO) \u00b6 # Ensure all models are same family # Correct model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" # Wrong model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! Slow Training \u00b6 # Use Unsloth backend trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" # faster ) Poor Convergence \u00b6 # Adjust learning rate and KL coefficient trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , algorithm = \"dpo\" , learning_rate = 1e-5 , # Lower learning rate beta = 0.05 , # Lower KL penalty num_epochs = 2 # More epochs ) Complete Examples \u00b6 DPO Training \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 ) trainer . train () metrics = trainer . evaluate () model_path = trainer . save_model () PPO with Custom Reward Model \u00b6 trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train () Next Steps \u00b6 Reward Functions Guide - Explore reward functions Reward Model Training - Train custom reward models Evaluation Guide - Comprehensive evaluation SFT Guide - Supervised fine-tuning Additional Resources \u00b6 API Reference - Complete API documentation Examples - More RL examples Backend Selection - Backend guide Unsloth Compatibility - Unsloth setup","title":"Reinforcement Learning (RL)"},{"location":"user-guide/rl/#reinforcement-learning-rl-training-guide","text":"Complete guide to Reinforcement Learning from Human Feedback (RLHF) training with AlignTune, covering all supported RLHF algorithms.","title":"Reinforcement Learning (RL) Training Guide"},{"location":"user-guide/rl/#overview","text":"Reinforcement Learning training aligns language models with human preferences using various algorithms. AlignTune supports multiple RLHF algorithms: DPO (Direct Preference Optimization) - No reward model needed PPO (Proximal Policy Optimization) - Flexible policy optimization with reward models GRPO (Group Relative Policy Optimization) - Multi-criteria optimization GSPO (Group Sequential Policy Optimization) - Sequential group learning DAPO (Decouple Clip and Dynamic sAmpling Policy Optimization) - Scaled RL for LLMs while addressing key limitations in GRPO Dr. GRPO (GRPO Done Right) - Unbiased GRPO variant that corrects optimization biases","title":"Overview"},{"location":"user-guide/rl/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/rl/#basic-dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer # Create and train DPO model trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-3.2-3B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B or Llama models ) # Train the model trainer . train ()","title":"Basic DPO Training"},{"location":"user-guide/rl/#basic-ppo-training","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , ) trainer . train ()","title":"Basic PPO Training"},{"location":"user-guide/rl/#algorithms","text":"","title":"Algorithms"},{"location":"user-guide/rl/#1-dpo-direct-preference-optimization","text":"DPO trains models directly on preference pairs without requiring a separate reward model. Advantages: - No reward model needed - Simpler training pipeline - Direct preference learning Use Cases: - Preference alignment - Human feedback integration - General RLHF training","title":"1. DPO (Direct Preference Optimization)"},{"location":"user-guide/rl/#example","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , # Use TRL for GPT2 models num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , # For GPT2 models, add: lora_target_modules=[\"c_attn\", \"c_proj\"] # Or use a different model like Qwen/Qwen3-0.6B # DPO-specific parameters beta = 0.1 , # KL penalty coefficient reference_free = False , # Use reference model loss_type = \"sigmoid\" # Loss function type ) trainer . train () Dataset Format: { \"prompt\" : \"What is machine learning?\" , \"chosen\" : \"Machine learning is a subset of AI...\" , \"rejected\" : \"I don't know.\" }","title":"Example"},{"location":"user-guide/rl/#2-ppo-proximal-policy-optimization","text":"PPO optimizes policies using reward models and value functions. Advantages: - Flexible reward shaping - Can use custom reward models - Supports complex reward landscapes Use Cases: - Custom reward model training - Complex reward functions - Production RLHF systems","title":"2. PPO (Proximal Policy Optimization)"},{"location":"user-guide/rl/#example_1","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Reward model configuration reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , # PPO-specific parameters num_epochs = 1 , batch_size = 1 , learning_rate = 1e-6 , kl_coef = 0.1 , # KL penalty coefficient cliprange = 0.2 , # PPO clip range vf_coef = 0.1 , # Value function coefficient gamma = 1.0 , # Discount factor lam = 0.95 # GAE lambda ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#custom-reward-model-training","text":"def load_training_texts (): \"\"\"Load training texts for reward model training.\"\"\" return [ \"This is a helpful and informative response that addresses the question clearly.\" , \"I'm not sure about this answer.\" , \"This response is clear, concise, and well-structured.\" , ] trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" , \"coherence\" ], reward_function_weights = [ 0.2 , 0.3 , 0.3 , 0.2 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , # PPO configuration num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"Custom Reward Model Training"},{"location":"user-guide/rl/#3-grpo-group-relative-policy-optimization","text":"GRPO optimizes policies using group-based relative comparisons. Advantages: - Multi-criteria optimization - Group-based learning - Flexible reward combinations Use Cases: - Multi-objective optimization - Complex reward landscapes - Group-based preference learning","title":"3. GRPO (Group Relative Policy Optimization)"},{"location":"user-guide/rl/#example_2","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"grpo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 2 , # GRPO requires minimum batch_size=2 for generations learning_rate = 1e-6 , loss_type = 'grpo' ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#4-gspo-group-sequential-policy-optimization","text":"GSPO uses sequential group learning for policy optimization. Note: GSPO is only supported by TRL backend, not Unsloth. Advantages: - Sequential learning - Group-based optimization - Structured policy updates Use Cases: - Sequential preference learning - Structured policy optimization - Group-based RLHF","title":"4. GSPO (Group Sequential Policy Optimization)"},{"location":"user-guide/rl/#example_3","text":"trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"gspo\" , backend = \"trl\" , # GSPO only works with TRL num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , # Note: group_size and sequential_steps are not in standard GRPO config # These parameters are specific to GSPO implementation ) trainer . train ()","title":"Example"},{"location":"user-guide/rl/#backend-selection","text":"","title":"Backend Selection"},{"location":"user-guide/rl/#trl-backend","text":"Use TRL when: - Need maximum compatibility - Using GSPO (TRL only) - Working with standard models - Need reliable, battle-tested training trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" )","title":"TRL Backend"},{"location":"user-guide/rl/#unsloth-backend","text":"Use Unsloth when: - Need faster training - Working with large models - Need memory efficiency - Training PPO, DPO, or GRPO trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" )","title":"Unsloth Backend"},{"location":"user-guide/rl/#configuration","text":"","title":"Configuration"},{"location":"user-guide/rl/#model-configuration","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Model settings max_seq_length = 2048 , quantization = { \"load_in_4bit\" : True }, use_peft = True , # Enable LoRA lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , use_gradient_checkpointing = True , # Reward model settings (for PPO) reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" , reward_model_quantization = { \"load_in_4bit\" : True } )","title":"Model Configuration"},{"location":"user-guide/rl/#training-configuration","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Training settings num_epochs = 1 , max_steps = None , # Use epochs batch_size = 4 , gradient_accumulation_steps = 4 , learning_rate = 5e-5 , weight_decay = 0.01 , warmup_steps = 100 , max_grad_norm = 1.0 , # Algorithm-specific beta = 0.1 , # DPO: KL coefficient kl_coef = 0.1 , # PPO: KL coefficient cliprange = 0.2 , # PPO: clip range temperature = 0.7 # Generation temperature )","title":"Training Configuration"},{"location":"user-guide/rl/#dataset-configuration","text":"trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , # Dataset settings max_samples = 1000 , percent = 10.0 , split = \"train\" , # Field mappings column_mapping = { \"prompt\" : \"prompt\" , \"chosen\" : \"chosen\" , \"rejected\" : \"rejected\" }, truncation_mode = \"keep_end\" , padding_free = False )","title":"Dataset Configuration"},{"location":"user-guide/rl/#reward-models","text":"","title":"Reward Models"},{"location":"user-guide/rl/#using-pre-trained-reward-models","text":"# From HuggingFace Hub trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # From local path trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_path = \"./reward_models/my_reward_model\" )","title":"Using Pre-trained Reward Models"},{"location":"user-guide/rl/#training-custom-reward-models","text":"See Reward Model Training Guide for detailed instructions. trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , # Custom reward model training train_custom_reward_model = True , reward_training_texts = training_texts , reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" )","title":"Training Custom Reward Models"},{"location":"user-guide/rl/#advanced-features","text":"","title":"Advanced Features"},{"location":"user-guide/rl/#model-family-consistency-ppo","text":"AlignTune automatically checks that all models in PPO training belong to the same family: # Correct: All Qwen models trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" ) # Error: Mixed families trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , algorithm = \"ppo\" , reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family! )","title":"Model Family Consistency (PPO)"},{"location":"user-guide/rl/#loraqlora-fine-tuning","text":"trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # LoRA configuration use_peft = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.05 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True } )","title":"LoRA/QLoRA Fine-Tuning"},{"location":"user-guide/rl/#distributed-training","text":"# Distributed training trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , # Distributed settings (configure via accelerate) # See distributed training guide )","title":"Distributed Training"},{"location":"user-guide/rl/#evaluation","text":"","title":"Evaluation"},{"location":"user-guide/rl/#basic-evaluation","text":"# Evaluate on validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/rl/#custom-evaluation","text":"from datasets import load_dataset eval_dataset = load_dataset ( \"Anthropic/hh-rlhf\" , split = \"test\" ) metrics = trainer . evaluate ( eval_dataset = eval_dataset )","title":"Custom Evaluation"},{"location":"user-guide/rl/#zero-shot-evaluation","text":"# Generate predictions prompts = [ \"What is machine learning?\" , \"Explain deep learning\" ] results = trainer . predict ( prompts ) for prompt , result in zip ( prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Zero-Shot Evaluation"},{"location":"user-guide/rl/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/rl/#1-algorithm-selection","text":"DPO : Start here for preference alignment, simpler setup PPO : Use for custom rewards, complex scenarios GRPO : Multi-criteria optimization GSPO : Sequential learning (TRL only)","title":"1. Algorithm Selection"},{"location":"user-guide/rl/#2-backend-selection","text":"TRL : Maximum compatibility, GSPO support Unsloth : Faster training (Faster), memory efficient","title":"2. Backend Selection"},{"location":"user-guide/rl/#3-reward-models","text":"Use pre-trained models when available Train custom models for domain-specific tasks Ensure model family consistency for PPO","title":"3. Reward Models"},{"location":"user-guide/rl/#4-hyperparameters","text":"# DPO recommended settings beta = 0.1 # KL coefficient learning_rate = 5e-5 , batch_size = 4 # PPO recommended settings kl_coef = 0.1 cliprange = 0.2 learning_rate = 1e-6 , batch_size = 1 # Smaller for PPO","title":"4. Hyperparameters"},{"location":"user-guide/rl/#5-memory-optimization","text":"# For large models trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , quantization = { \"load_in_4bit\" : True }, use_peft = True , use_gradient_checkpointing = True , batch_size = 1 , gradient_accumulation_steps = 8 )","title":"5. Memory Optimization"},{"location":"user-guide/rl/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/rl/#out-of-memory","text":"# Reduce batch size, use quantization trainer = create_rl_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , algorithm = \"ppo\" , batch_size = 1 , # Reduce gradient_accumulation_steps = 8 , # Compensate quantization = { \"load_in_4bit\" : True }, use_gradient_checkpointing = True )","title":"Out of Memory"},{"location":"user-guide/rl/#model-family-mismatch-ppo","text":"# Ensure all models are same family # Correct model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"Skywork/Skywork-Reward-V2-Qwen3-0.6B\" # Wrong model_name = \"Qwen/Qwen3-0.6B\" reward_model_name = \"meta-llama/Llama-2-7b-hf\" # Different family!","title":"Model Family Mismatch (PPO)"},{"location":"user-guide/rl/#slow-training","text":"# Use Unsloth backend trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , algorithm = \"ppo\" , backend = \"unsloth\" # faster )","title":"Slow Training"},{"location":"user-guide/rl/#poor-convergence","text":"# Adjust learning rate and KL coefficient trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , algorithm = \"dpo\" , learning_rate = 1e-5 , # Lower learning rate beta = 0.05 , # Lower KL penalty num_epochs = 2 # More epochs )","title":"Poor Convergence"},{"location":"user-guide/rl/#complete-examples","text":"","title":"Complete Examples"},{"location":"user-guide/rl/#dpo-training","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" , num_epochs = 1 , batch_size = 4 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 1000 , beta = 0.1 ) trainer . train () metrics = trainer . evaluate () model_path = trainer . save_model ()","title":"DPO Training"},{"location":"user-guide/rl/#ppo-with-custom-reward-model","text":"trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"HuggingFaceH4/ultrafeedback_binarized\" , algorithm = \"ppo\" , backend = \"unsloth\" , train_custom_reward_model = True , reward_training_texts = load_training_texts (), reward_functions = [ \"length\" , \"sentiment\" , \"safety\" ], reward_function_weights = [ 0.3 , 0.4 , 0.3 ], reward_training_base_model = \"microsoft/DialoGPT-medium\" , reward_training_output_dir = \"./reward_models/custom\" , num_epochs = 1 , batch_size = 1 , learning_rate = 2e-4 ) trainer . train ()","title":"PPO with Custom Reward Model"},{"location":"user-guide/rl/#next-steps","text":"Reward Functions Guide - Explore reward functions Reward Model Training - Train custom reward models Evaluation Guide - Comprehensive evaluation SFT Guide - Supervised fine-tuning","title":"Next Steps"},{"location":"user-guide/rl/#additional-resources","text":"API Reference - Complete API documentation Examples - More RL examples Backend Selection - Backend guide Unsloth Compatibility - Unsloth setup","title":"Additional Resources"},{"location":"user-guide/sample-logging/","text":"Qualitative Sample Logging \u00b6 AlignTune can periodically generate qualitative samples during RL training to spot regressions long before metrics change. The feature is powered by aligntune.core.rl.sample_logger and governed by SampleLoggingConfig . When Samples Are Logged \u00b6 Available on TRL RL trainers ( ppo , grpo , gspo ) and Unsloth PPO via their calls to generate_and_log_samples . Samples run after key checkpoints (for PPO this happens at the end of training, and additional hooks log mid-training when interval_steps or percent_of_max_steps triggers fire). Responses and optional reward scores are streamed to both the configured logger and stdout so they appear in notebooks and terminal logs. Configuration Fields \u00b6 SampleLoggingConfig lives under the logging section of UnifiedConfig : Field Type Default Description enabled bool False Master switch for sample generation prompts list[str] built-in prompt trio Prompts to feed into model.generate ; falls back to defaults when empty interval_steps int? None Log every N optimizer steps percent_of_max_steps float? None Log at percentages of train.max_steps (e.g., 0.25 ) num_samples int 3 How many prompts to evaluate (capped at the number of prompts) max_new_tokens int 80 Max tokens per response temperature float 0.7 Sampling temperature top_p float 0.9 Nucleus sampling parameter Validation in __post_init__ ensures all numeric values are positive and that percentages fall inside (0, 1] . YAML Example \u00b6 logging : output_dir : ./output/ppo_run loggers : [ tensorboard ] sample_logging : enabled : true prompts : - \"Summarize the core idea of reinforcement learning in two sentences.\" - \"Write a concise plan for debugging PPO training instabilities.\" interval_steps : 200 num_samples : 2 max_new_tokens : 96 temperature : 0.6 top_p : 0.95 Python API Example \u00b6 from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , sample_logging = { \"enabled\" : True , \"percent_of_max_steps\" : 0.5 , \"num_samples\" : 2 , \"max_new_tokens\" : 72 , \"temperature\" : 0.8 , }, ) trainer . train () Behind the scenes, the backend factory merges the provided dictionary into a SampleLoggingConfig instance before passing it to the trainer. Reward-Aware Samples \u00b6 When custom reward functions are registered on the trainer (e.g., PPO with multiple heuristic rewards), each generated response is scored and the per-reward outputs are printed alongside the prompt/response pair: ================================================================================ Qualitative samples (post-train) - prompts: 2 [Sample 1/2] Prompt: Summarize the core idea... [Sample 1/2] Response: ... [Sample 1/2] Rewards: {'reward_0': 0.81, 'reward_1': 0.42} ================================================================================ Best Practices \u00b6 Keep prompts short and deterministic so diffs are easier to read in CI logs. Use percent_of_max_steps for long-running jobs where max_steps is known, otherwise rely on step intervals. On distributed training setups the samples run on the trainer's primary device; make sure enough GPU memory remains to run generation with the configured max_new_tokens . Disable qualitative samples on highly resource-constrained runs to avoid extra generation overhead. For more context on logging knobs, revisit the logging.sample_logging section of the README or the API docs for SampleLoggingConfig .","title":"Sample Logging"},{"location":"user-guide/sample-logging/#qualitative-sample-logging","text":"AlignTune can periodically generate qualitative samples during RL training to spot regressions long before metrics change. The feature is powered by aligntune.core.rl.sample_logger and governed by SampleLoggingConfig .","title":"Qualitative Sample Logging"},{"location":"user-guide/sample-logging/#when-samples-are-logged","text":"Available on TRL RL trainers ( ppo , grpo , gspo ) and Unsloth PPO via their calls to generate_and_log_samples . Samples run after key checkpoints (for PPO this happens at the end of training, and additional hooks log mid-training when interval_steps or percent_of_max_steps triggers fire). Responses and optional reward scores are streamed to both the configured logger and stdout so they appear in notebooks and terminal logs.","title":"When Samples Are Logged"},{"location":"user-guide/sample-logging/#configuration-fields","text":"SampleLoggingConfig lives under the logging section of UnifiedConfig : Field Type Default Description enabled bool False Master switch for sample generation prompts list[str] built-in prompt trio Prompts to feed into model.generate ; falls back to defaults when empty interval_steps int? None Log every N optimizer steps percent_of_max_steps float? None Log at percentages of train.max_steps (e.g., 0.25 ) num_samples int 3 How many prompts to evaluate (capped at the number of prompts) max_new_tokens int 80 Max tokens per response temperature float 0.7 Sampling temperature top_p float 0.9 Nucleus sampling parameter Validation in __post_init__ ensures all numeric values are positive and that percentages fall inside (0, 1] .","title":"Configuration Fields"},{"location":"user-guide/sample-logging/#yaml-example","text":"logging : output_dir : ./output/ppo_run loggers : [ tensorboard ] sample_logging : enabled : true prompts : - \"Summarize the core idea of reinforcement learning in two sentences.\" - \"Write a concise plan for debugging PPO training instabilities.\" interval_steps : 200 num_samples : 2 max_new_tokens : 96 temperature : 0.6 top_p : 0.95","title":"YAML Example"},{"location":"user-guide/sample-logging/#python-api-example","text":"from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , backend = \"unsloth\" , num_epochs = 1 , batch_size = 1 , sample_logging = { \"enabled\" : True , \"percent_of_max_steps\" : 0.5 , \"num_samples\" : 2 , \"max_new_tokens\" : 72 , \"temperature\" : 0.8 , }, ) trainer . train () Behind the scenes, the backend factory merges the provided dictionary into a SampleLoggingConfig instance before passing it to the trainer.","title":"Python API Example"},{"location":"user-guide/sample-logging/#reward-aware-samples","text":"When custom reward functions are registered on the trainer (e.g., PPO with multiple heuristic rewards), each generated response is scored and the per-reward outputs are printed alongside the prompt/response pair: ================================================================================ Qualitative samples (post-train) - prompts: 2 [Sample 1/2] Prompt: Summarize the core idea... [Sample 1/2] Response: ... [Sample 1/2] Rewards: {'reward_0': 0.81, 'reward_1': 0.42} ================================================================================","title":"Reward-Aware Samples"},{"location":"user-guide/sample-logging/#best-practices","text":"Keep prompts short and deterministic so diffs are easier to read in CI logs. Use percent_of_max_steps for long-running jobs where max_steps is known, otherwise rely on step intervals. On distributed training setups the samples run on the trainer's primary device; make sure enough GPU memory remains to run generation with the configured max_new_tokens . Disable qualitative samples on highly resource-constrained runs to avoid extra generation overhead. For more context on logging knobs, revisit the logging.sample_logging section of the README or the API docs for SampleLoggingConfig .","title":"Best Practices"},{"location":"user-guide/sft/","text":"Supervised Fine-Tuning (SFT) Guide \u00b6 Complete guide to Supervised Fine-Tuning with AlignTune, covering all task types, configurations, and best practices. Overview \u00b6 Supervised Fine-Tuning (SFT) is the process of training a pre-trained language model on a labeled dataset to adapt it for specific tasks. AlignTune provides a unified interface for SFT across multiple task types with support for both TRL and Unsloth backends. Supported Task Types \u00b6 AlignTune supports six main SFT task types: Instruction Following - Teach models to follow instructions Supervised Fine-Tuning - General-purpose fine-tuning Text Classification - Classify text into categories Token Classification - Named Entity Recognition (NER), POS tagging Text Generation - Generate coherent text Chat Completion - Conversational AI training Quick Start \u00b6 Basic SFT Training \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) # Train the model trainer . train () # Save the model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) Using YAML Configuration \u00b6 # config.yaml model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 dataset : name : \"tatsu-lab/alpaca\" max_samples : 1000 task_type : \"supervised_fine_tuning\" train : epochs : 3 per_device_batch_size : 4 learning_rate : 2e-4 logging : output_dir : \"./output\" from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = \"trl\" , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , max_samples = config . dataset . max_samples , task_type = config . dataset . task_type . value ) trainer . train () Task Types \u00b6 1. Instruction Following \u00b6 Train models to follow instructions and generate appropriate responses. from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth recommended for generation tasks task_type = \"instruction_following\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , # Longer sequences for instructions max_samples = 1000 , # Column mappings for instruction datasets instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () Dataset Format: { \"instruction\" : \"Explain machine learning\" , \"input\" : \"\" , \"output\" : \"Machine learning is a subset of AI...\" } 2. Supervised Fine-Tuning \u00b6 General-purpose fine-tuning for any text-to-text task. trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"databricks/databricks-dolly-15k\" , backend = \"trl\" , task_type = \"supervised_fine_tuning\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , auto_detect_fields = True # Automatically detect dataset fields ) trainer . train () 3. Text Classification \u00b6 Classify text into predefined categories (sentiment, topic, etc.). trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL recommended for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , # Classification-specific text_column = \"text\" , label_column = \"label\" , num_labels = 2 # Binary classification ) trainer . train () Dataset Format: { \"text\" : \"This movie is fantastic!\" , \"label\" : 1 } 4. Token Classification \u00b6 Named Entity Recognition (NER), Part-of-Speech tagging, etc. trainer = create_sft_trainer ( model_name = \"bert-base-uncased\" , dataset_name = \"lhoestq/conll2003\" , backend = \"trl\" , # TRL recommended for token classification task_type = \"token_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , split = \"train\" , # Dataset split tokens_column = \"tokens\" , tags_column = \"ner_tags\" , num_labels = 9 # Number of NER tag classes ) trainer . train () Dataset Format: { \"tokens\" : [ \"Apple\" , \"is\" , \"a\" , \"company\" ], \"ner_tags\" : [ 1 , 0 , 0 , 0 ] # B - ORG , O , O , O } 5. Text Generation \u00b6 Generate coherent, context-aware text. trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Salesforce/wikitext\" , backend = \"trl\" , # Unsloth recommended for generation task_type = \"text_generation\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , text_column = \"text\" ) trainer . train () 6. Chat Completion \u00b6 Train conversational AI models with chat templates. trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , backend = \"unsloth\" , task_type = \"chat_completion\" , split = \"train_sft\" , # Dataset split num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , # Longer for conversations gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" # Auto-detect chat template ) trainer . train () Dataset Format: { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Hello!\" }, { \"role\" : \"assistant\" , \"content\" : \"Hi! How can I help?\" } ] } Backend Selection \u00b6 TRL Backend (Recommended for Classification) \u00b6 Use TRL when: - Training classification models - Training token classification (NER) - Need maximum compatibility - Working with smaller models trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # Explicit TRL backend task_type = \"text_classification\" ) Unsloth Backend (Recommended for Generation) \u00b6 Use Unsloth when: - Training generation models (faster) - Working with large models - Need memory efficiency - Training instruction following or chat models trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth for speed task_type = \"instruction_following\" ) Auto Backend Selection \u00b6 Let AlignTune choose the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , # Automatic selection task_type = \"instruction_following\" ) Configuration Options \u00b6 Model Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Model settings max_seq_length = 512 , quantization = { \"load_in_4bit\" : True }, # 4-bit quantization peft_enabled = True , # Enable LoRA lora_r = 16 , # LoRA rank lora_alpha = 32 , # LoRA alpha lora_dropout = 0.1 , # For GPT2 models, use: lora_target_modules=[\"c_attn\", \"c_proj\"] use_gradient_checkpointing = True , # Memory optimization use_flash_attention_2 = True # Flash Attention (if available) ) Training Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Training settings num_epochs = 3 , max_steps = None , # Use epochs instead batch_size = 4 , gradient_accumulation_steps = 4 , # Effective batch size = 16 learning_rate = 2e-4 , weight_decay = 0.01 , warmup_steps = 100 , warmup_ratio = 0.1 , max_grad_norm = 1.0 , # Advanced settings packing = True , # Sequence packing for efficiency # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth packing_strategy = \"bfd\" , # \"bfd\" or \"wrapped\" loss_type = \"nll\" , # \"nll\" or \"dft\" completion_only_loss = True , # Only compute loss on completion activation_offloading = False # Offload activations to CPU ) Dataset Configuration \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Dataset settings max_samples = 1000 , # Limit dataset size percent = 10.0 , # Use 10% of dataset split = \"train\" , # Dataset split # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth # Column mappings column_mapping = { \"instruction\" : \"instruction\" , \"output\" : \"response\" }, # Field detection auto_detect_fields = True , # Auto-detect dataset fields dataset_num_proc = 4 # Parallel processing ) Advanced Features \u00b6 LoRA/QLoRA Fine-Tuning \u00b6 Efficient fine-tuning with parameter-efficient methods: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # LoRA configuration peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True , \"bnb_4bit_compute_dtype\" : \"float16\" , \"bnb_4bit_quant_type\" : \"nf4\" } ) Sequence Packing \u00b6 Pack multiple sequences into a single batch for efficiency: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models or models not supported by Unsloth packing = True , # Enable packing packing_strategy = \"bfd\" , # \"bfd\" (best-fit-decreasing) or \"wrapped\" padding_free = True , # No padding needed with packing max_seq_length = 2048 # Longer sequences with packing ) Mixed Precision Training \u00b6 Use FP16 or BF16 for faster training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Precision settings (handled automatically by backend) # Unsloth uses optimal precision automatically # TRL: set fp16=True or bf16=True in training config ) Gradient Checkpointing \u00b6 Reduce memory usage during training: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , use_gradient_checkpointing = True , # Enable checkpointing activation_offloading = True # Offload activations to CPU ) Evaluation \u00b6 Basic Evaluation \u00b6 # Evaluate on default validation set metrics = trainer . evaluate () print ( metrics ) Custom Evaluation Dataset \u00b6 from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" ) Zero-Shot Evaluation \u00b6 # Generate predictions for test prompts # Note: Make sure to call trainer.train() or trainer.setup_model() before using predict() test_prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( test_prompts ) for prompt , result in zip ( test_prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" ) Model Management \u00b6 Saving Models \u00b6 # Save after training model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Save to custom directory model_path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" ) Pushing to HuggingFace Hub \u00b6 # Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , private = False ) print ( f \"Model available at: { url } \" ) # Push with custom commit message url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" ) Loading Saved Models \u00b6 # Load saved model from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Use for inference result = trainer . predict ( \"What is AI?\" ) Best Practices \u00b6 1. Choose the Right Task Type \u00b6 Instruction Following : For teaching models to follow instructions Text Classification : For categorization tasks Token Classification : For NER, POS tagging Chat Completion : For conversational AI Text Generation : For general text generation 2. Backend Selection \u00b6 TRL : Use for classification, token classification, maximum compatibility Unsloth : Use for generation tasks (faster), large models 3. Memory Optimization \u00b6 # For large models, use: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , # LoRA quantization = { \"load_in_4bit\" : True }, # 4-bit quantization use_gradient_checkpointing = True , # Gradient checkpointing batch_size = 1 , # Smaller batch size gradient_accumulation_steps = 8 # Compensate with accumulation ) 4. Learning Rate Scheduling \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , learning_rate = 2e-4 , warmup_steps = 100 , # Warmup for stable training warmup_ratio = 0.1 # Or use ratio instead ) 5. Dataset Size \u00b6 Start with a subset ( max_samples=1000 ) for testing Gradually increase dataset size Use percent parameter for quick experiments 6. Sequence Length \u00b6 Classification : 512 tokens is usually sufficient Instruction Following : 1024-2048 tokens Chat Completion : 2048+ tokens for conversations Text Generation : 1024-2048 tokens 7. Regular Checkpointing \u00b6 trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , save_interval = 500 , # Save every 500 steps eval_interval = 100 # Evaluate every 100 steps ) Troubleshooting \u00b6 Out of Memory Errors \u00b6 # Reduce batch size and use gradient accumulation trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 , # Reduce batch size gradient_accumulation_steps = 8 , # Increase accumulation use_gradient_checkpointing = True , # Enable checkpointing quantization = { \"load_in_4bit\" : True } # Use quantization ) Slow Training \u00b6 # Use Unsloth backend for generation tasks trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # faster packing = True , # Enable sequence packing dataset_num_proc = 4 # Parallel data processing ) Poor Model Performance \u00b6 # Increase training duration and adjust learning rate trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 5 , # More epochs learning_rate = 1e-4 , # Lower learning rate warmup_steps = 200 , # More warmup max_samples = None # Use full dataset ) Dataset Format Issues \u00b6 # Use column mapping for custom datasets trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"my-custom-dataset\" , column_mapping = { \"instruction\" : \"prompt\" , # Map custom column names \"output\" : \"completion\" }, auto_detect_fields = False # Disable auto-detection ) Examples \u00b6 Complete Instruction Following Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"gpt2\" , # Use Qwen/Llama instead of GPT2 for Unsloth dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , max_samples = 1000 , peft_enabled = True , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 models quantization = { \"load_in_4bit\" : True } ) # Train print ( \"Starting training...\" ) trainer . train () # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test result = trainer . predict ( \"What is machine learning?\" ) print ( f \"Prediction: { result } \" ) Complete Classification Example \u00b6 from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) # Save model_path = trainer . save_model () Next Steps \u00b6 RL Training Guide - Learn about reinforcement learning training Reward Functions Guide - Explore reward functions for RL Evaluation Guide - Comprehensive evaluation guide Model Management - Advanced model management Additional Resources \u00b6 API Reference - Complete API documentation Examples - More SFT examples Backend Selection - Detailed backend guide Configuration Guide - Configuration reference","title":"Supervised Fine-Tuning (SFT)"},{"location":"user-guide/sft/#supervised-fine-tuning-sft-guide","text":"Complete guide to Supervised Fine-Tuning with AlignTune, covering all task types, configurations, and best practices.","title":"Supervised Fine-Tuning (SFT) Guide"},{"location":"user-guide/sft/#overview","text":"Supervised Fine-Tuning (SFT) is the process of training a pre-trained language model on a labeled dataset to adapt it for specific tasks. AlignTune provides a unified interface for SFT across multiple task types with support for both TRL and Unsloth backends.","title":"Overview"},{"location":"user-guide/sft/#supported-task-types","text":"AlignTune supports six main SFT task types: Instruction Following - Teach models to follow instructions Supervised Fine-Tuning - General-purpose fine-tuning Text Classification - Classify text into categories Token Classification - Named Entity Recognition (NER), POS tagging Text Generation - Generate coherent text Chat Completion - Conversational AI training","title":"Supported Task Types"},{"location":"user-guide/sft/#quick-start","text":"","title":"Quick Start"},{"location":"user-guide/sft/#basic-sft-training","text":"from aligntune.core.backend_factory import create_sft_trainer # Create and train SFT model trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # or \"unsloth\" for faster training num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , max_samples = 1000 ) # Train the model trainer . train () # Save the model model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" )","title":"Basic SFT Training"},{"location":"user-guide/sft/#using-yaml-configuration","text":"# config.yaml model : name_or_path : \"microsoft/DialoGPT-medium\" max_seq_length : 512 dataset : name : \"tatsu-lab/alpaca\" max_samples : 1000 task_type : \"supervised_fine_tuning\" train : epochs : 3 per_device_batch_size : 4 learning_rate : 2e-4 logging : output_dir : \"./output\" from aligntune.core.sft.config_loader import SFTConfigLoader from aligntune.core.backend_factory import create_sft_trainer # Load configuration config = SFTConfigLoader . load_from_yaml ( \"config.yaml\" ) # Create trainer from config trainer = create_sft_trainer ( model_name = config . model . name_or_path , dataset_name = config . dataset . name , backend = \"trl\" , num_epochs = config . train . epochs , batch_size = config . train . per_device_batch_size , learning_rate = config . train . learning_rate , max_seq_length = config . model . max_seq_length , max_samples = config . dataset . max_samples , task_type = config . dataset . task_type . value ) trainer . train ()","title":"Using YAML Configuration"},{"location":"user-guide/sft/#task-types","text":"","title":"Task Types"},{"location":"user-guide/sft/#1-instruction-following","text":"Train models to follow instructions and generate appropriate responses. from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth recommended for generation tasks task_type = \"instruction_following\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , # Longer sequences for instructions max_samples = 1000 , # Column mappings for instruction datasets instruction_column = \"instruction\" , response_column = \"output\" , input_column = \"input\" ) trainer . train () Dataset Format: { \"instruction\" : \"Explain machine learning\" , \"input\" : \"\" , \"output\" : \"Machine learning is a subset of AI...\" }","title":"1. Instruction Following"},{"location":"user-guide/sft/#2-supervised-fine-tuning","text":"General-purpose fine-tuning for any text-to-text task. trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"databricks/databricks-dolly-15k\" , backend = \"trl\" , task_type = \"supervised_fine_tuning\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 512 , auto_detect_fields = True # Automatically detect dataset fields ) trainer . train ()","title":"2. Supervised Fine-Tuning"},{"location":"user-guide/sft/#3-text-classification","text":"Classify text into predefined categories (sentiment, topic, etc.). trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL recommended for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , # Classification-specific text_column = \"text\" , label_column = \"label\" , num_labels = 2 # Binary classification ) trainer . train () Dataset Format: { \"text\" : \"This movie is fantastic!\" , \"label\" : 1 }","title":"3. Text Classification"},{"location":"user-guide/sft/#4-token-classification","text":"Named Entity Recognition (NER), Part-of-Speech tagging, etc. trainer = create_sft_trainer ( model_name = \"bert-base-uncased\" , dataset_name = \"lhoestq/conll2003\" , backend = \"trl\" , # TRL recommended for token classification task_type = \"token_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , split = \"train\" , # Dataset split tokens_column = \"tokens\" , tags_column = \"ner_tags\" , num_labels = 9 # Number of NER tag classes ) trainer . train () Dataset Format: { \"tokens\" : [ \"Apple\" , \"is\" , \"a\" , \"company\" ], \"ner_tags\" : [ 1 , 0 , 0 , 0 ] # B - ORG , O , O , O }","title":"4. Token Classification"},{"location":"user-guide/sft/#5-text-generation","text":"Generate coherent, context-aware text. trainer = create_sft_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Salesforce/wikitext\" , backend = \"trl\" , # Unsloth recommended for generation task_type = \"text_generation\" , num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , text_column = \"text\" ) trainer . train ()","title":"5. Text Generation"},{"location":"user-guide/sft/#6-chat-completion","text":"Train conversational AI models with chat templates. trainer = create_sft_trainer ( model_name = \"mistralai/Mistral-7B-v0.1\" , dataset_name = \"HuggingFaceH4/ultrachat_200k\" , backend = \"unsloth\" , task_type = \"chat_completion\" , split = \"train_sft\" , # Dataset split num_epochs = 2 , batch_size = 2 , learning_rate = 2e-4 , max_seq_length = 2048 , # Longer for conversations gradient_accumulation_steps = 4 , messages_column = \"messages\" , chat_template = \"auto\" # Auto-detect chat template ) trainer . train () Dataset Format: { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Hello!\" }, { \"role\" : \"assistant\" , \"content\" : \"Hi! How can I help?\" } ] }","title":"6. Chat Completion"},{"location":"user-guide/sft/#backend-selection","text":"","title":"Backend Selection"},{"location":"user-guide/sft/#trl-backend-recommended-for-classification","text":"Use TRL when: - Training classification models - Training token classification (NER) - Need maximum compatibility - Working with smaller models trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # Explicit TRL backend task_type = \"text_classification\" )","title":"TRL Backend (Recommended for Classification)"},{"location":"user-guide/sft/#unsloth-backend-recommended-for-generation","text":"Use Unsloth when: - Training generation models (faster) - Working with large models - Need memory efficiency - Training instruction following or chat models trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # Unsloth for speed task_type = \"instruction_following\" )","title":"Unsloth Backend (Recommended for Generation)"},{"location":"user-guide/sft/#auto-backend-selection","text":"Let AlignTune choose the best backend: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"auto\" , # Automatic selection task_type = \"instruction_following\" )","title":"Auto Backend Selection"},{"location":"user-guide/sft/#configuration-options","text":"","title":"Configuration Options"},{"location":"user-guide/sft/#model-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Model settings max_seq_length = 512 , quantization = { \"load_in_4bit\" : True }, # 4-bit quantization peft_enabled = True , # Enable LoRA lora_r = 16 , # LoRA rank lora_alpha = 32 , # LoRA alpha lora_dropout = 0.1 , # For GPT2 models, use: lora_target_modules=[\"c_attn\", \"c_proj\"] use_gradient_checkpointing = True , # Memory optimization use_flash_attention_2 = True # Flash Attention (if available) )","title":"Model Configuration"},{"location":"user-guide/sft/#training-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Training settings num_epochs = 3 , max_steps = None , # Use epochs instead batch_size = 4 , gradient_accumulation_steps = 4 , # Effective batch size = 16 learning_rate = 2e-4 , weight_decay = 0.01 , warmup_steps = 100 , warmup_ratio = 0.1 , max_grad_norm = 1.0 , # Advanced settings packing = True , # Sequence packing for efficiency # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth packing_strategy = \"bfd\" , # \"bfd\" or \"wrapped\" loss_type = \"nll\" , # \"nll\" or \"dft\" completion_only_loss = True , # Only compute loss on completion activation_offloading = False # Offload activations to CPU )","title":"Training Configuration"},{"location":"user-guide/sft/#dataset-configuration","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Dataset settings max_samples = 1000 , # Limit dataset size percent = 10.0 , # Use 10% of dataset split = \"train\" , # Dataset split # Note: For GPT2 models, use backend=\"trl\" as GPT2 is not supported by Unsloth # Column mappings column_mapping = { \"instruction\" : \"instruction\" , \"output\" : \"response\" }, # Field detection auto_detect_fields = True , # Auto-detect dataset fields dataset_num_proc = 4 # Parallel processing )","title":"Dataset Configuration"},{"location":"user-guide/sft/#advanced-features","text":"","title":"Advanced Features"},{"location":"user-guide/sft/#loraqlora-fine-tuning","text":"Efficient fine-tuning with parameter-efficient methods: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # LoRA configuration peft_enabled = True , lora_r = 16 , lora_alpha = 32 , lora_dropout = 0.1 , lora_target_modules = [ \"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\" ], # Quantization quantization = { \"load_in_4bit\" : True , \"bnb_4bit_compute_dtype\" : \"float16\" , \"bnb_4bit_quant_type\" : \"nf4\" } )","title":"LoRA/QLoRA Fine-Tuning"},{"location":"user-guide/sft/#sequence-packing","text":"Pack multiple sequences into a single batch for efficiency: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models or models not supported by Unsloth packing = True , # Enable packing packing_strategy = \"bfd\" , # \"bfd\" (best-fit-decreasing) or \"wrapped\" padding_free = True , # No padding needed with packing max_seq_length = 2048 # Longer sequences with packing )","title":"Sequence Packing"},{"location":"user-guide/sft/#mixed-precision-training","text":"Use FP16 or BF16 for faster training: trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , # Precision settings (handled automatically by backend) # Unsloth uses optimal precision automatically # TRL: set fp16=True or bf16=True in training config )","title":"Mixed Precision Training"},{"location":"user-guide/sft/#gradient-checkpointing","text":"Reduce memory usage during training: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , use_gradient_checkpointing = True , # Enable checkpointing activation_offloading = True # Offload activations to CPU )","title":"Gradient Checkpointing"},{"location":"user-guide/sft/#evaluation","text":"","title":"Evaluation"},{"location":"user-guide/sft/#basic-evaluation","text":"# Evaluate on default validation set metrics = trainer . evaluate () print ( metrics )","title":"Basic Evaluation"},{"location":"user-guide/sft/#custom-evaluation-dataset","text":"from datasets import load_dataset # Load evaluation dataset eval_dataset = load_dataset ( \"tatsu-lab/alpaca\" , split = \"test\" ) # Evaluate metrics = trainer . evaluate ( eval_dataset = eval_dataset ) print ( f \"Loss: { metrics [ 'eval_loss' ] } \" ) print ( f \"Perplexity: { metrics . get ( 'eval_perplexity' , 'N/A' ) } \" )","title":"Custom Evaluation Dataset"},{"location":"user-guide/sft/#zero-shot-evaluation","text":"# Generate predictions for test prompts # Note: Make sure to call trainer.train() or trainer.setup_model() before using predict() test_prompts = [ \"What is machine learning?\" , \"Explain deep learning\" , \"What is NLP?\" ] results = trainer . predict ( test_prompts ) for prompt , result in zip ( test_prompts , results ): print ( f \"Q: { prompt } \" ) print ( f \"A: { result } \\n \" )","title":"Zero-Shot Evaluation"},{"location":"user-guide/sft/#model-management","text":"","title":"Model Management"},{"location":"user-guide/sft/#saving-models","text":"# Save after training model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Save to custom directory model_path = trainer . save_model ( output_dir = \"./my_models/experiment_1\" )","title":"Saving Models"},{"location":"user-guide/sft/#pushing-to-huggingface-hub","text":"# Push to Hub url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , private = False ) print ( f \"Model available at: { url } \" ) # Push with custom commit message url = trainer . push_to_hub ( repo_id = \"username/my-sft-model\" , commit_message = \"Trained on Alpaca dataset with 3 epochs\" )","title":"Pushing to HuggingFace Hub"},{"location":"user-guide/sft/#loading-saved-models","text":"# Load saved model from aligntune.core.backend_factory import create_sft_trainer trainer = create_sft_trainer ( model_name = \"./output/my_model\" , # Path to saved model dataset_name = \"tatsu-lab/alpaca\" ) # Use for inference result = trainer . predict ( \"What is AI?\" )","title":"Loading Saved Models"},{"location":"user-guide/sft/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/sft/#1-choose-the-right-task-type","text":"Instruction Following : For teaching models to follow instructions Text Classification : For categorization tasks Token Classification : For NER, POS tagging Chat Completion : For conversational AI Text Generation : For general text generation","title":"1. Choose the Right Task Type"},{"location":"user-guide/sft/#2-backend-selection","text":"TRL : Use for classification, token classification, maximum compatibility Unsloth : Use for generation tasks (faster), large models","title":"2. Backend Selection"},{"location":"user-guide/sft/#3-memory-optimization","text":"# For large models, use: trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , peft_enabled = True , # LoRA quantization = { \"load_in_4bit\" : True }, # 4-bit quantization use_gradient_checkpointing = True , # Gradient checkpointing batch_size = 1 , # Smaller batch size gradient_accumulation_steps = 8 # Compensate with accumulation )","title":"3. Memory Optimization"},{"location":"user-guide/sft/#4-learning-rate-scheduling","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , learning_rate = 2e-4 , warmup_steps = 100 , # Warmup for stable training warmup_ratio = 0.1 # Or use ratio instead )","title":"4. Learning Rate Scheduling"},{"location":"user-guide/sft/#5-dataset-size","text":"Start with a subset ( max_samples=1000 ) for testing Gradually increase dataset size Use percent parameter for quick experiments","title":"5. Dataset Size"},{"location":"user-guide/sft/#6-sequence-length","text":"Classification : 512 tokens is usually sufficient Instruction Following : 1024-2048 tokens Chat Completion : 2048+ tokens for conversations Text Generation : 1024-2048 tokens","title":"6. Sequence Length"},{"location":"user-guide/sft/#7-regular-checkpointing","text":"trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , save_interval = 500 , # Save every 500 steps eval_interval = 100 # Evaluate every 100 steps )","title":"7. Regular Checkpointing"},{"location":"user-guide/sft/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/sft/#out-of-memory-errors","text":"# Reduce batch size and use gradient accumulation trainer = create_sft_trainer ( model_name = \"meta-llama/Llama-2-7b-hf\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 , # Reduce batch size gradient_accumulation_steps = 8 , # Increase accumulation use_gradient_checkpointing = True , # Enable checkpointing quantization = { \"load_in_4bit\" : True } # Use quantization )","title":"Out of Memory Errors"},{"location":"user-guide/sft/#slow-training","text":"# Use Unsloth backend for generation tasks trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" , # faster packing = True , # Enable sequence packing dataset_num_proc = 4 # Parallel data processing )","title":"Slow Training"},{"location":"user-guide/sft/#poor-model-performance","text":"# Increase training duration and adjust learning rate trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"tatsu-lab/alpaca\" , num_epochs = 5 , # More epochs learning_rate = 1e-4 , # Lower learning rate warmup_steps = 200 , # More warmup max_samples = None # Use full dataset )","title":"Poor Model Performance"},{"location":"user-guide/sft/#dataset-format-issues","text":"# Use column mapping for custom datasets trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"my-custom-dataset\" , column_mapping = { \"instruction\" : \"prompt\" , # Map custom column names \"output\" : \"completion\" }, auto_detect_fields = False # Disable auto-detection )","title":"Dataset Format Issues"},{"location":"user-guide/sft/#examples","text":"","title":"Examples"},{"location":"user-guide/sft/#complete-instruction-following-example","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"gpt2\" , # Use Qwen/Llama instead of GPT2 for Unsloth dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" , # Use TRL for GPT2 models task_type = \"instruction_following\" , column_mapping = { \"output\" : \"response\" }, # Map output column to response num_epochs = 3 , batch_size = 4 , learning_rate = 2e-4 , max_seq_length = 1024 , max_samples = 1000 , peft_enabled = True , lora_target_modules = [ \"c_attn\" , \"c_proj\" ], # For GPT2 models quantization = { \"load_in_4bit\" : True } ) # Train print ( \"Starting training...\" ) trainer . train () # Evaluate print ( \"Evaluating...\" ) metrics = trainer . evaluate () print ( f \"Evaluation metrics: { metrics } \" ) # Save model_path = trainer . save_model () print ( f \"Model saved to: { model_path } \" ) # Test result = trainer . predict ( \"What is machine learning?\" ) print ( f \"Prediction: { result } \" )","title":"Complete Instruction Following Example"},{"location":"user-guide/sft/#complete-classification-example","text":"from aligntune.core.backend_factory import create_sft_trainer # Create trainer trainer = create_sft_trainer ( model_name = \"distilbert-base-uncased\" , dataset_name = \"imdb\" , backend = \"trl\" , # TRL for classification task_type = \"text_classification\" , num_epochs = 3 , batch_size = 8 , learning_rate = 5e-5 , max_seq_length = 512 , max_samples = 5000 , text_column = \"text\" , label_column = \"label\" , num_labels = 2 ) # Train trainer . train () # Evaluate metrics = trainer . evaluate () print ( f \"Accuracy: { metrics . get ( 'eval_accuracy' , 'N/A' ) } \" ) # Save model_path = trainer . save_model ()","title":"Complete Classification Example"},{"location":"user-guide/sft/#next-steps","text":"RL Training Guide - Learn about reinforcement learning training Reward Functions Guide - Explore reward functions for RL Evaluation Guide - Comprehensive evaluation guide Model Management - Advanced model management","title":"Next Steps"},{"location":"user-guide/sft/#additional-resources","text":"API Reference - Complete API documentation Examples - More SFT examples Backend Selection - Detailed backend guide Configuration Guide - Configuration reference","title":"Additional Resources"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting \u00b6 This guide helps you resolve common issues when using AlignTune. Backend Issues \u00b6 Unsloth Not Available \u00b6 Error : Unsloth not available or ImportError: Unsloth backend not available Causes : - Unsloth not installed - CUDA not available - PyTorch version mismatch - CUDA version incompatibility Solutions : Install Unsloth : pip install unsloth Check CUDA Availability : import torch print ( torch . cuda . is_available ()) print ( torch . version . cuda ) Use TRL Backend Instead : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Use TRL instead ) Run Diagnostics : aligntune diagnose Backend Interference \u00b6 Error : Unsloth patches interfering with TRL backend Cause : Importing BackendType enum triggers Unsloth loading Solution : Use string-based backend selection # CORRECT - String-based selection from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" # Use string, not BackendType enum ) # AVOID - Importing BackendType causes Unsloth interference from aligntune.core.backend_factory import create_rl_trainer , BackendType # This import triggers Unsloth loading even when using TRL backend CUDA and GPU Issues \u00b6 CUDA Out of Memory (OOM) \u00b6 Error : RuntimeError: CUDA out of memory Solutions : Reduce Batch Size : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 # Reduce from default ) Enable Gradient Checkpointing : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , gradient_checkpointing = True ) Use LoRA/QLoRA : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , use_peft = True , lora_r = 8 , # Lower rank = less memory lora_alpha = 16 ) Use 4-bit Quantization (Unsloth): trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Use CPU : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , device_map = \"cpu\" ) CUDA Symbol Errors \u00b6 Error : CUDA symbol errors or undefined symbol: cublasLtMatmul Cause : PyTorch and CUDA version mismatch Solutions : Use TRL Backend : backend = \"trl\" # TRL is more compatible Reinstall PyTorch with Matching CUDA : pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Check Compatibility : aligntune diagnose Configuration Issues \u00b6 Invalid Configuration \u00b6 Error : ConfigurationError or validation errors Solutions : Validate Configuration : aligntune validate config.yaml Check Required Parameters : model_name : Must be a valid HuggingFace model ID dataset_name : Must be a valid HuggingFace dataset ID algorithm : Must be one of: dpo , ppo , grpo , gspo , dapo , drgrpo , gbmpo , counterfact_grpo , bolt Check Parameter Types : # CORRECT num_epochs = 3 # int learning_rate = 5e-5 # float batch_size = 4 # int # INCORRECT num_epochs = \"3\" # string learning_rate = \"5e-5\" # string Missing Required Parameters \u00b6 Error : ValueError: Missing required parameter Common Missing Parameters : RL Training without Algorithm : # MISSING algorithm trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" ) # CORRECT trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" ) PPO without Reward Model : # PPO requires reward model trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"your-reward-model\" # Required for PPO ) Dataset Issues \u00b6 Dataset Not Found \u00b6 Error : DatasetNotFoundError or FileNotFoundError Solutions : Check Dataset Name : # Use correct HuggingFace dataset ID dataset_name = \"tatsu-lab/alpaca\" # dataset_name = \"alpaca\" # May not work Check Dataset Access : Some datasets require authentication Use huggingface-cli login to authenticate Use Local Dataset : from datasets import load_dataset dataset = load_dataset ( \"path/to/local/dataset\" ) Dataset Format Issues \u00b6 Error : KeyError or missing columns Solutions : Specify Column Mapping : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , column_mapping = { \"instruction\" : \"prompt\" , \"output\" : \"response\" } ) Check Dataset Structure : from datasets import load_dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" ) print ( dataset [ \"train\" ][ 0 ]) # Check structure Training Issues \u00b6 Loss Not Decreasing \u00b6 Symptoms : Loss stays constant or increases Solutions : Reduce Learning Rate : learning_rate = 1e-6 # Try lower learning rate Increase Warmup Steps : warmup_steps = 100 warmup_ratio = 0.1 Check Data Quality : Verify dataset is correct Check for data preprocessing issues Use Different Optimizer : optimizer = \"adamw_torch\" # or \"lion\", \"adafactor\" Training Too Slow \u00b6 Solutions : Use Unsloth Backend : backend = \"unsloth\" # faster Increase Batch Size : batch_size = 8 # If memory allows gradient_accumulation_steps = 4 Use Mixed Precision : precision = \"bf16\" # or \"fp16\" Enable Flash Attention : use_flash_attention_2 = True Training Crashes \u00b6 Error : Training process crashes or hangs Solutions : Check Logs : # Check training logs tail -f output/training.log Reduce Batch Size : batch_size = 1 # Minimal batch size Disable Gradient Checkpointing (if causing issues): gradient_checkpointing = False Use TRL Backend (more stable): backend = \"trl\" Model Issues \u00b6 Model Not Found \u00b6 Error : ModelNotFoundError or OSError: Can't load model Solutions : Check Model Name : # Use correct HuggingFace model ID model_name = \"microsoft/DialoGPT-small\" # model_name = \"DialoGPT-small\" # May not work Check Model Access : Some models require authentication Use huggingface-cli login to authenticate Use Local Model : model_name = \"./path/to/local/model\" Model Loading Errors \u00b6 Error : RuntimeError during model loading Solutions : Check Model Compatibility : Verify model architecture is supported Check model size vs. available memory Use Device Map : device_map = \"auto\" # Automatic device placement Use Quantization : quantization = { \"load_in_4bit\" : True } # 4-bit quantization Evaluation Issues \u00b6 Evaluation Fails \u00b6 Error : EvaluationError or evaluation crashes Solutions : Check Evaluation Dataset : # Ensure eval dataset exists if trainer . eval_dataset is None : print ( \"No evaluation dataset available\" ) Reduce Evaluation Samples : max_samples_for_quality_metrics = 50 # Limit samples Skip Quality Metrics : compute_rouge = False compute_bleu = False Memory Issues \u00b6 Out of Memory (OOM) \u00b6 Solutions : Reduce Model Size : Use smaller models Use quantized models (4-bit, 8-bit) Optimize Training : gradient_checkpointing = True activation_offloading = True use_peft = True # LoRA reduces memory Reduce Sequence Length : max_seq_length = 256 # Reduce from 512 Use Gradient Accumulation : batch_size = 1 gradient_accumulation_steps = 8 # Simulates batch_size=8 Getting Help \u00b6 Diagnostic Commands \u00b6 # Run comprehensive diagnostics aligntune diagnose # Validate configuration aligntune validate config.yaml # Check backend availability aligntune list-backends-cmd Useful Resources \u00b6 Unsloth Compatibility Guide - Unsloth-specific issues Backend Selection Guide - Backend comparison Configuration Guide - Configuration options GitHub Issues - Report bugs Reporting Issues \u00b6 When reporting issues, please include: Error Message : Full error traceback Configuration : Your configuration file or code Environment : Python version, PyTorch version, CUDA version Diagnostics : Output of aligntune diagnose Common Error Messages \u00b6 Backend not available \u00b6 Solution : Use a different backend or install missing dependencies Algorithm not supported \u00b6 Solution : Check algorithm name and backend compatibility Dataset format error \u00b6 Solution : Check dataset structure and column mappings Model loading failed \u00b6 Solution : Verify model name and access permissions CUDA out of memory \u00b6 Solution : Reduce batch size, enable gradient checkpointing, or use quantization Prevention Tips \u00b6 Always Validate Configuration : Use aligntune validate before training Start Small : Test with small datasets and models first Monitor Resources : Watch GPU memory and CPU usage Use Auto Backend : Let AlignTune select the best backend Check Compatibility : Run aligntune diagnose regularly Next Steps \u00b6 Backend Selection - Choose the right backend Configuration Guide - Configure training properly Performance Optimization - Optimize training speed and memory","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#troubleshooting","text":"This guide helps you resolve common issues when using AlignTune.","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#backend-issues","text":"","title":"Backend Issues"},{"location":"user-guide/troubleshooting/#unsloth-not-available","text":"Error : Unsloth not available or ImportError: Unsloth backend not available Causes : - Unsloth not installed - CUDA not available - PyTorch version mismatch - CUDA version incompatibility Solutions : Install Unsloth : pip install unsloth Check CUDA Availability : import torch print ( torch . cuda . is_available ()) print ( torch . version . cuda ) Use TRL Backend Instead : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"trl\" # Use TRL instead ) Run Diagnostics : aligntune diagnose","title":"Unsloth Not Available"},{"location":"user-guide/troubleshooting/#backend-interference","text":"Error : Unsloth patches interfering with TRL backend Cause : Importing BackendType enum triggers Unsloth loading Solution : Use string-based backend selection # CORRECT - String-based selection from aligntune.core.backend_factory import create_rl_trainer trainer = create_rl_trainer ( model_name = \"Qwen/Qwen3-0.6B\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" , backend = \"trl\" # Use string, not BackendType enum ) # AVOID - Importing BackendType causes Unsloth interference from aligntune.core.backend_factory import create_rl_trainer , BackendType # This import triggers Unsloth loading even when using TRL backend","title":"Backend Interference"},{"location":"user-guide/troubleshooting/#cuda-and-gpu-issues","text":"","title":"CUDA and GPU Issues"},{"location":"user-guide/troubleshooting/#cuda-out-of-memory-oom","text":"Error : RuntimeError: CUDA out of memory Solutions : Reduce Batch Size : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , batch_size = 1 # Reduce from default ) Enable Gradient Checkpointing : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , gradient_checkpointing = True ) Use LoRA/QLoRA : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , use_peft = True , lora_r = 8 , # Lower rank = less memory lora_alpha = 16 ) Use 4-bit Quantization (Unsloth): trainer = create_sft_trainer ( model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" , dataset_name = \"tatsu-lab/alpaca\" , backend = \"unsloth\" ) Use CPU : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , device_map = \"cpu\" )","title":"CUDA Out of Memory (OOM)"},{"location":"user-guide/troubleshooting/#cuda-symbol-errors","text":"Error : CUDA symbol errors or undefined symbol: cublasLtMatmul Cause : PyTorch and CUDA version mismatch Solutions : Use TRL Backend : backend = \"trl\" # TRL is more compatible Reinstall PyTorch with Matching CUDA : pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Check Compatibility : aligntune diagnose","title":"CUDA Symbol Errors"},{"location":"user-guide/troubleshooting/#configuration-issues","text":"","title":"Configuration Issues"},{"location":"user-guide/troubleshooting/#invalid-configuration","text":"Error : ConfigurationError or validation errors Solutions : Validate Configuration : aligntune validate config.yaml Check Required Parameters : model_name : Must be a valid HuggingFace model ID dataset_name : Must be a valid HuggingFace dataset ID algorithm : Must be one of: dpo , ppo , grpo , gspo , dapo , drgrpo , gbmpo , counterfact_grpo , bolt Check Parameter Types : # CORRECT num_epochs = 3 # int learning_rate = 5e-5 # float batch_size = 4 # int # INCORRECT num_epochs = \"3\" # string learning_rate = \"5e-5\" # string","title":"Invalid Configuration"},{"location":"user-guide/troubleshooting/#missing-required-parameters","text":"Error : ValueError: Missing required parameter Common Missing Parameters : RL Training without Algorithm : # MISSING algorithm trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" ) # CORRECT trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"dpo\" ) PPO without Reward Model : # PPO requires reward model trainer = create_rl_trainer ( model_name = \"microsoft/DialoGPT-medium\" , dataset_name = \"Anthropic/hh-rlhf\" , algorithm = \"ppo\" , reward_model_name = \"your-reward-model\" # Required for PPO )","title":"Missing Required Parameters"},{"location":"user-guide/troubleshooting/#dataset-issues","text":"","title":"Dataset Issues"},{"location":"user-guide/troubleshooting/#dataset-not-found","text":"Error : DatasetNotFoundError or FileNotFoundError Solutions : Check Dataset Name : # Use correct HuggingFace dataset ID dataset_name = \"tatsu-lab/alpaca\" # dataset_name = \"alpaca\" # May not work Check Dataset Access : Some datasets require authentication Use huggingface-cli login to authenticate Use Local Dataset : from datasets import load_dataset dataset = load_dataset ( \"path/to/local/dataset\" )","title":"Dataset Not Found"},{"location":"user-guide/troubleshooting/#dataset-format-issues","text":"Error : KeyError or missing columns Solutions : Specify Column Mapping : trainer = create_sft_trainer ( model_name = \"microsoft/DialoGPT-small\" , dataset_name = \"tatsu-lab/alpaca\" , column_mapping = { \"instruction\" : \"prompt\" , \"output\" : \"response\" } ) Check Dataset Structure : from datasets import load_dataset dataset = load_dataset ( \"tatsu-lab/alpaca\" ) print ( dataset [ \"train\" ][ 0 ]) # Check structure","title":"Dataset Format Issues"},{"location":"user-guide/troubleshooting/#training-issues","text":"","title":"Training Issues"},{"location":"user-guide/troubleshooting/#loss-not-decreasing","text":"Symptoms : Loss stays constant or increases Solutions : Reduce Learning Rate : learning_rate = 1e-6 # Try lower learning rate Increase Warmup Steps : warmup_steps = 100 warmup_ratio = 0.1 Check Data Quality : Verify dataset is correct Check for data preprocessing issues Use Different Optimizer : optimizer = \"adamw_torch\" # or \"lion\", \"adafactor\"","title":"Loss Not Decreasing"},{"location":"user-guide/troubleshooting/#training-too-slow","text":"Solutions : Use Unsloth Backend : backend = \"unsloth\" # faster Increase Batch Size : batch_size = 8 # If memory allows gradient_accumulation_steps = 4 Use Mixed Precision : precision = \"bf16\" # or \"fp16\" Enable Flash Attention : use_flash_attention_2 = True","title":"Training Too Slow"},{"location":"user-guide/troubleshooting/#training-crashes","text":"Error : Training process crashes or hangs Solutions : Check Logs : # Check training logs tail -f output/training.log Reduce Batch Size : batch_size = 1 # Minimal batch size Disable Gradient Checkpointing (if causing issues): gradient_checkpointing = False Use TRL Backend (more stable): backend = \"trl\"","title":"Training Crashes"},{"location":"user-guide/troubleshooting/#model-issues","text":"","title":"Model Issues"},{"location":"user-guide/troubleshooting/#model-not-found","text":"Error : ModelNotFoundError or OSError: Can't load model Solutions : Check Model Name : # Use correct HuggingFace model ID model_name = \"microsoft/DialoGPT-small\" # model_name = \"DialoGPT-small\" # May not work Check Model Access : Some models require authentication Use huggingface-cli login to authenticate Use Local Model : model_name = \"./path/to/local/model\"","title":"Model Not Found"},{"location":"user-guide/troubleshooting/#model-loading-errors","text":"Error : RuntimeError during model loading Solutions : Check Model Compatibility : Verify model architecture is supported Check model size vs. available memory Use Device Map : device_map = \"auto\" # Automatic device placement Use Quantization : quantization = { \"load_in_4bit\" : True } # 4-bit quantization","title":"Model Loading Errors"},{"location":"user-guide/troubleshooting/#evaluation-issues","text":"","title":"Evaluation Issues"},{"location":"user-guide/troubleshooting/#evaluation-fails","text":"Error : EvaluationError or evaluation crashes Solutions : Check Evaluation Dataset : # Ensure eval dataset exists if trainer . eval_dataset is None : print ( \"No evaluation dataset available\" ) Reduce Evaluation Samples : max_samples_for_quality_metrics = 50 # Limit samples Skip Quality Metrics : compute_rouge = False compute_bleu = False","title":"Evaluation Fails"},{"location":"user-guide/troubleshooting/#memory-issues","text":"","title":"Memory Issues"},{"location":"user-guide/troubleshooting/#out-of-memory-oom","text":"Solutions : Reduce Model Size : Use smaller models Use quantized models (4-bit, 8-bit) Optimize Training : gradient_checkpointing = True activation_offloading = True use_peft = True # LoRA reduces memory Reduce Sequence Length : max_seq_length = 256 # Reduce from 512 Use Gradient Accumulation : batch_size = 1 gradient_accumulation_steps = 8 # Simulates batch_size=8","title":"Out of Memory (OOM)"},{"location":"user-guide/troubleshooting/#getting-help","text":"","title":"Getting Help"},{"location":"user-guide/troubleshooting/#diagnostic-commands","text":"# Run comprehensive diagnostics aligntune diagnose # Validate configuration aligntune validate config.yaml # Check backend availability aligntune list-backends-cmd","title":"Diagnostic Commands"},{"location":"user-guide/troubleshooting/#useful-resources","text":"Unsloth Compatibility Guide - Unsloth-specific issues Backend Selection Guide - Backend comparison Configuration Guide - Configuration options GitHub Issues - Report bugs","title":"Useful Resources"},{"location":"user-guide/troubleshooting/#reporting-issues","text":"When reporting issues, please include: Error Message : Full error traceback Configuration : Your configuration file or code Environment : Python version, PyTorch version, CUDA version Diagnostics : Output of aligntune diagnose","title":"Reporting Issues"},{"location":"user-guide/troubleshooting/#common-error-messages","text":"","title":"Common Error Messages"},{"location":"user-guide/troubleshooting/#backend-not-available","text":"Solution : Use a different backend or install missing dependencies","title":"Backend not available"},{"location":"user-guide/troubleshooting/#algorithm-not-supported","text":"Solution : Check algorithm name and backend compatibility","title":"Algorithm not supported"},{"location":"user-guide/troubleshooting/#dataset-format-error","text":"Solution : Check dataset structure and column mappings","title":"Dataset format error"},{"location":"user-guide/troubleshooting/#model-loading-failed","text":"Solution : Verify model name and access permissions","title":"Model loading failed"},{"location":"user-guide/troubleshooting/#cuda-out-of-memory","text":"Solution : Reduce batch size, enable gradient checkpointing, or use quantization","title":"CUDA out of memory"},{"location":"user-guide/troubleshooting/#prevention-tips","text":"Always Validate Configuration : Use aligntune validate before training Start Small : Test with small datasets and models first Monitor Resources : Watch GPU memory and CPU usage Use Auto Backend : Let AlignTune select the best backend Check Compatibility : Run aligntune diagnose regularly","title":"Prevention Tips"},{"location":"user-guide/troubleshooting/#next-steps","text":"Backend Selection - Choose the right backend Configuration Guide - Configure training properly Performance Optimization - Optimize training speed and memory","title":"Next Steps"}]}