<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Comprehensive fine-tuning library for SFT and RL training with multi-backend support">
    <meta name="author" content="AlignTune Contributors">
    <link rel="canonical" href="https://aligntune.github.io/algorithms/ppo/">
    <link rel="shortcut icon" href="../../img/favicon.ico">

    
<title>PPO - AlignTune</title>


    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css">
    <link href='//rsms.me/inter/inter.css' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="../../css/base.min.css" rel="stylesheet">
    <link href="../../css/cinder.min.css" rel="stylesheet">

    
        
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css">
        
    
    <link href="../../assets/_mkdocstrings.css" rel="stylesheet">
    <link href="../../assets/overrides.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    

    
<!-- Favicons -->
<link rel="apple-touch-icon" sizes="180x180" href="../../assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../assets/favicon-16x16.png">
<link rel="manifest" href="../../assets/site.webmanifest">
<link rel="shortcut icon" href="../../assets/favicon.ico">

<!-- Android Chrome Icons -->
<link rel="icon" type="image/png" sizes="192x192" href="../../assets/android-chrome-192x192.png">
<link rel="icon" type="image/png" sizes="512x512" href="../../assets/android-chrome-512x512.png">

 

</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
 <div class="container">

 <!-- Collapsed navigation -->
 <div class="navbar-header">
 <!-- Expander button -->
 <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
 <span class="sr-only">Toggle navigation</span>
 <span class="icon-bar"></span>
 <span class="icon-bar"></span>
 <span class="icon-bar"></span>
 </button>
 

 <!-- Main title -->

 
 <a class="navbar-brand" href="../..">AlignTune</a>
 
 </div>

 <!-- Expanded navigation -->
 <div class="navbar-collapse collapse">
 <!-- Main navigation -->
 <ul class="nav navbar-nav">
 
 
 <li class="dropdown">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">Getting Started <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../..">Home</a>
</li>

 
 
<li >
    <a href="../../getting-started/installation/">Installation</a>
</li>

 
 
<li >
    <a href="../../getting-started/quickstart/">Quick Start</a>
</li>

 
 
<li >
    <a href="../../getting-started/basic-concepts/">Basic Concepts</a>
</li>

 
 
<li >
    <a href="../../getting-started/configuration/">Configuration</a>
</li>

 
 
<li >
    <a href="../../getting-started/backend-selection/">Backend Selection</a>
</li>

 
 </ul>
 </li>
 
 
 
 <li class="dropdown active">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../../user-guide/overview/">Overview</a>
</li>

 
 
<li >
    <a href="../../user-guide/sft/">Supervised Fine-Tuning (SFT)</a>
</li>

 
 
<li >
    <a href="../../user-guide/rl/">Reinforcement Learning (RL)</a>
</li>

 
 
<li >
    <a href="../../user-guide/reward-functions/">Reward Functions</a>
</li>

 
 
<li >
    <a href="../../user-guide/reward-model-training/">Reward Model Training</a>
</li>

 
 
<li >
    <a href="../../user-guide/evaluation/">Evaluation</a>
</li>

 
 
<li >
    <a href="../../user-guide/model-management/">Model Management</a>
</li>

 
 
<li >
    <a href="../../user-guide/sample-logging/">Sample Logging</a>
</li>

 
 
<li >
    <a href="../../user-guide/troubleshooting/">Troubleshooting</a>
</li>

 
 
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">Algorithms</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../overview/">Overview</a>
</li>

        
            
<li >
    <a href="../dpo/">DPO</a>
</li>

        
            
<li class="active">
    <a href="./">PPO</a>
</li>

        
            
<li >
    <a href="../grpo/">GRPO</a>
</li>

        
            
<li >
    <a href="../gspo/">GSPO</a>
</li>

        
            
<li >
    <a href="../dapo/">DAPO</a>
</li>

        
            
<li >
    <a href="../dr-grpo/">Dr. GRPO</a>
</li>

        
    </ul>
  </li>

 
 
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">Backends</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../backends/overview/">Overview</a>
</li>

        
            
<li >
    <a href="../../backends/trl/">TRL Backend</a>
</li>

        
            
<li >
    <a href="../../backends/unsloth/">Unsloth Backend</a>
</li>

        
            
<li >
    <a href="../../backends/comparison/">Comparison</a>
</li>

        
    </ul>
  </li>

 
 </ul>
 </li>
 
 
 
 <li class="dropdown">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">Advanced Topics <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../../advanced/architecture/">Architecture</a>
</li>

 
 
<li >
    <a href="../../advanced/custom-backends.md">Custom Backends</a>
</li>

 
 
<li >
    <a href="../../advanced/distributed.md">Distributed Training</a>
</li>

 
 
<li >
    <a href="../../advanced/performance.md">Performance Optimization</a>
</li>

 
 </ul>
 </li>
 
 
 
 <li class="dropdown">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Reference <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../../api-reference/overview/">Overview</a>
</li>

 
 
<li >
    <a href="../../api-reference/core/">Core API</a>
</li>

 
 
<li >
    <a href="../../api-reference/backend-factory/">Backend Factory</a>
</li>

 
 
<li >
    <a href="../../api-reference/configuration/">Configuration Classes</a>
</li>

 
 
<li >
    <a href="../../api-reference/trainers/">Trainers</a>
</li>

 
 </ul>
 </li>
 
 
 
 <li class="dropdown">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">Examples <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../../examples/overview/">Overview</a>
</li>

 
 
<li >
    <a href="../../examples/sft/">SFT Examples</a>
</li>

 
 
<li >
    <a href="../../examples/rl/">RL Examples</a>
</li>

 
 
<li >
    <a href="../../examples/advanced/">Advanced Examples</a>
</li>

 
 
<li >
    <a href="../../notebooks/">Notebooks</a>
</li>

 
 </ul>
 </li>
 
 
 
 <li class="dropdown">
 <a href="#" class="dropdown-toggle" data-toggle="dropdown">Project <b class="caret"></b></a>
 <ul class="dropdown-menu">
 
 
<li >
    <a href="../../cli-reference/">CLI Reference</a>
</li>

 
 
<li >
    <a href="../../cli/commands/">CLI Commands</a>
</li>

 
 
<li >
    <a href="../../cli/configuration/">CLI Configuration Files</a>
</li>

 
 
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">Compatibility</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../unsloth_compatibility/">Unsloth Compatibility</a>
</li>

        
            
<li >
    <a href="../../compatibility/backend-matrix/">Backend Support Matrix</a>
</li>

        
    </ul>
  </li>

 
 
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">Contributing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../contributing/guide/">Contributing Guide</a>
</li>

        
            
<li >
    <a href="../../contributing/code-style/">Code Style</a>
</li>

        
            
<li >
    <a href="../../contributing/testing/">Testing</a>
</li>

        
    </ul>
  </li>

 
 </ul>
 </li>
 
 
 </ul>

 <ul class="nav navbar-nav navbar-right">
 <li>
 <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
 <i class="fas fa-search"></i> Search
 </a>
 </li>
 <li >
 <a rel="prev" href="../dpo/">
 <i class="fas fa-arrow-left"></i> Previous
 </a>
 </li>
 <li >
 <a rel="next" href="../grpo/">
 Next <i class="fas fa-arrow-right"></i>
 </a>
 </li>
 
 </ul>
 </div>
 </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#ppo-proximal-policy-optimization">PPO (Proximal Policy Optimization)</a></li>
            <li class="second-level"><a href="#overview">Overview</a></li>
                
                <li class="third-level"><a href="#key-features">Key Features</a></li>
            <li class="second-level"><a href="#how-it-works">How It Works</a></li>
                
                <li class="third-level"><a href="#mathematical-formulation">Mathematical Formulation</a></li>
            <li class="second-level"><a href="#usage">Usage</a></li>
                
                <li class="third-level"><a href="#basic-ppo-training">Basic PPO Training</a></li>
                <li class="third-level"><a href="#ppo-with-unsloth-backend">PPO with Unsloth Backend</a></li>
                <li class="third-level"><a href="#ppo-with-custom-reward-model">PPO with Custom Reward Model</a></li>
                <li class="third-level"><a href="#ppo-with-reward-functions">PPO with Reward Functions</a></li>
            <li class="second-level"><a href="#configuration-parameters">Configuration Parameters</a></li>
                
                <li class="third-level"><a href="#ppo-specific-parameters">PPO-Specific Parameters</a></li>
                <li class="third-level"><a href="#reward-model-parameters">Reward Model Parameters</a></li>
            <li class="second-level"><a href="#dataset-format">Dataset Format</a></li>
                
                <li class="third-level"><a href="#supported-datasets">Supported Datasets</a></li>
            <li class="second-level"><a href="#best-practices">Best Practices</a></li>
                
                <li class="third-level"><a href="#1-reward-model-selection">1. Reward Model Selection</a></li>
                <li class="third-level"><a href="#2-clipping-range">2. Clipping Range</a></li>
                <li class="third-level"><a href="#3-kl-coefficient">3. KL Coefficient</a></li>
                <li class="third-level"><a href="#4-number-of-generations">4. Number of Generations</a></li>
            <li class="second-level"><a href="#evaluation">Evaluation</a></li>
                
            <li class="second-level"><a href="#common-issues">Common Issues</a></li>
                
                <li class="third-level"><a href="#reward-model-not-found">Reward Model Not Found</a></li>
                <li class="third-level"><a href="#training-instability">Training Instability</a></li>
                <li class="third-level"><a href="#low-rewards">Low Rewards</a></li>
            <li class="second-level"><a href="#comparison-with-other-algorithms">Comparison with Other Algorithms</a></li>
                
            <li class="second-level"><a href="#references">References</a></li>
                
            <li class="second-level"><a href="#next-steps">Next Steps</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="ppo-proximal-policy-optimization">PPO (Proximal Policy Optimization)<a class="headerlink" href="#ppo-proximal-policy-optimization" title="Permanent link">&para;</a></h1>
<p>Proximal Policy Optimization (PPO) is a powerful RLHF algorithm that uses a reward model to optimize language model behavior through policy gradient methods.</p>
<hr />
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>PPO optimizes the policy by generating responses, evaluating them with a reward model, and updating the policy to maximize expected reward while staying close to the reference policy.</p>
<h3 id="key-features">Key Features<a class="headerlink" href="#key-features" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Reward Model</strong>: Uses neural reward models for fine-grained control</li>
<li><strong>Online Learning</strong>: Generates and evaluates responses during training</li>
<li><strong>Both Backends</strong>: Supported by TRL and Unsloth</li>
<li><strong>Flexible</strong>: Works with custom reward models and functions</li>
</ul>
<hr />
<h2 id="how-it-works">How It Works<a class="headerlink" href="#how-it-works" title="Permanent link">&para;</a></h2>
<p>PPO alternates between:
1. <strong>Rollout Phase</strong>: Generate responses from the current policy
2. <strong>Evaluation Phase</strong>: Score responses with reward model
3. <strong>Update Phase</strong>: Update policy to maximize reward (with KL constraint)</p>
<h3 id="mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Permanent link">&para;</a></h3>
<p>The PPO objective maximizes:</p>
<div class="highlight"><pre><span></span><code>L_PPO = E[min(r_t * A_t, clip(r_t, 1-ε, 1+ε) * A_t)]
</code></pre></div>
<p>Where:
- <code>r_t</code>: Probability ratio between policy and reference
- <code>A_t</code>: Advantage estimate
- <code>ε</code>: Clipping parameter</p>
<hr />
<h2 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">&para;</a></h2>
<h3 id="basic-ppo-training">Basic PPO Training<a class="headerlink" href="#basic-ppo-training" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">aligntune.core.backend_factory</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_rl_trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">create_rl_trainer</span><span class="p">(</span>
 <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen3-0.6B&quot;</span><span class="p">,</span>
 <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;Anthropic/hh-rlhf&quot;</span><span class="p">,</span>
 <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;ppo&quot;</span><span class="p">,</span>
 <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;trl&quot;</span><span class="p">,</span>
 <span class="n">reward_model_name</span><span class="o">=</span><span class="s2">&quot;your-reward-model&quot;</span><span class="p">,</span> <span class="c1"># Required</span>
 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<h3 id="ppo-with-unsloth-backend">PPO with Unsloth Backend<a class="headerlink" href="#ppo-with-unsloth-backend" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">create_rl_trainer</span><span class="p">(</span>
 <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;</span><span class="p">,</span>
 <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;Anthropic/hh-rlhf&quot;</span><span class="p">,</span>
 <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;ppo&quot;</span><span class="p">,</span>
 <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;unsloth&quot;</span><span class="p">,</span>
 <span class="n">reward_model_name</span><span class="o">=</span><span class="s2">&quot;your-reward-model&quot;</span><span class="p">,</span>
 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<h3 id="ppo-with-custom-reward-model">PPO with Custom Reward Model<a class="headerlink" href="#ppo-with-custom-reward-model" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Train custom reward model first</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">aligntune.rewards.training</span><span class="w"> </span><span class="kn">import</span> <span class="n">RewardModelTrainer</span>

<span class="n">reward_trainer</span> <span class="o">=</span> <span class="n">RewardModelTrainer</span><span class="p">(</span>
 <span class="n">base_model_name</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen3-0.6B&quot;</span><span class="p">,</span>
 <span class="n">reward_functions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;helpfulness&quot;</span><span class="p">,</span> <span class="s2">&quot;safety&quot;</span><span class="p">,</span> <span class="s2">&quot;coherence&quot;</span><span class="p">],</span>
 <span class="n">composite_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">reward_model_path</span> <span class="o">=</span> <span class="n">reward_trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Use in PPO training</span>
<span class="n">ppo_trainer</span> <span class="o">=</span> <span class="n">create_rl_trainer</span><span class="p">(</span>
 <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;</span><span class="p">,</span>
 <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;Anthropic/hh-rlhf&quot;</span><span class="p">,</span>
 <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;ppo&quot;</span><span class="p">,</span>
 <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;unsloth&quot;</span><span class="p">,</span>
 <span class="n">reward_model_path</span><span class="o">=</span><span class="n">reward_model_path</span><span class="p">,</span>
 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="ppo-with-reward-functions">PPO with Reward Functions<a class="headerlink" href="#ppo-with-reward-functions" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">create_rl_trainer</span><span class="p">(</span>
 <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen3-0.6B&quot;</span><span class="p">,</span>
 <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;Anthropic/hh-rlhf&quot;</span><span class="p">,</span>
 <span class="n">algorithm</span><span class="o">=</span><span class="s2">&quot;ppo&quot;</span><span class="p">,</span>
 <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;trl&quot;</span><span class="p">,</span>
 <span class="n">reward_functions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;helpfulness&quot;</span><span class="p">,</span> <span class="s2">&quot;safety&quot;</span><span class="p">],</span>
 <span class="n">reward_function_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="configuration-parameters">Configuration Parameters<a class="headerlink" href="#configuration-parameters" title="Permanent link">&para;</a></h2>
<h3 id="ppo-specific-parameters">PPO-Specific Parameters<a class="headerlink" href="#ppo-specific-parameters" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cliprange</code></td>
<td><code>float</code></td>
<td><code>0.2</code></td>
<td>PPO clipping range</td>
</tr>
<tr>
<td><code>kl_coef</code></td>
<td><code>float</code></td>
<td><code>0.1</code></td>
<td>KL penalty coefficient</td>
</tr>
<tr>
<td><code>vf_coef</code></td>
<td><code>float</code></td>
<td><code>0.1</code></td>
<td>Value function coefficient</td>
</tr>
<tr>
<td><code>gamma</code></td>
<td><code>float</code></td>
<td><code>1.0</code></td>
<td>Discount factor</td>
</tr>
<tr>
<td><code>lam</code></td>
<td><code>float</code></td>
<td><code>0.95</code></td>
<td>GAE lambda parameter</td>
</tr>
<tr>
<td><code>num_generations</code></td>
<td><code>int</code></td>
<td><code>8</code></td>
<td>Number of generations per prompt</td>
</tr>
<tr>
<td><code>num_ppo_epochs</code></td>
<td><code>int</code></td>
<td><code>4</code></td>
<td>Number of PPO update epochs</td>
</tr>
</tbody>
</table>
<h3 id="reward-model-parameters">Reward Model Parameters<a class="headerlink" href="#reward-model-parameters" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>reward_model_name</code></td>
<td><code>str</code></td>
<td><code>None</code></td>
<td>HuggingFace reward model ID</td>
</tr>
<tr>
<td><code>reward_model_path</code></td>
<td><code>str</code></td>
<td><code>None</code></td>
<td>Local reward model path</td>
</tr>
<tr>
<td><code>reward_functions</code></td>
<td><code>list</code></td>
<td><code>[]</code></td>
<td>List of reward function names</td>
</tr>
<tr>
<td><code>reward_function_weights</code></td>
<td><code>list</code></td>
<td><code>[]</code></td>
<td>Weights for reward functions</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="dataset-format">Dataset Format<a class="headerlink" href="#dataset-format" title="Permanent link">&para;</a></h2>
<p>PPO requires prompts (not preference pairs):</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
 <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is machine learning?&quot;</span><span class="p">,</span>
 <span class="c1"># Responses are generated during training</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="supported-datasets">Supported Datasets<a class="headerlink" href="#supported-datasets" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Anthropic/hh-rlhf</strong>: Can extract prompts</li>
<li><strong>OpenAssistant/oasst1</strong>: Conversation prompts</li>
<li>Custom datasets with <code>prompt</code> column</li>
</ul>
<hr />
<h2 id="best-practices">Best Practices<a class="headerlink" href="#best-practices" title="Permanent link">&para;</a></h2>
<h3 id="1-reward-model-selection">1. Reward Model Selection<a class="headerlink" href="#1-reward-model-selection" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Use pre-trained models</strong>: HuggingFace reward models</li>
<li><strong>Train custom models</strong>: For domain-specific tasks</li>
<li><strong>Use reward functions</strong>: For rule-based rewards</li>
</ul>
<h3 id="2-clipping-range">2. Clipping Range<a class="headerlink" href="#2-clipping-range" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Conservative (0.1-0.2)</strong>: Stable training, slower updates</li>
<li><strong>Aggressive (0.2-0.3)</strong>: Faster updates, risk of instability</li>
</ul>
<h3 id="3-kl-coefficient">3. KL Coefficient<a class="headerlink" href="#3-kl-coefficient" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Low (0.01-0.1)</strong>: More exploration, risk of divergence</li>
<li><strong>High (0.1-0.5)</strong>: More conservative, stays close to reference</li>
</ul>
<h3 id="4-number-of-generations">4. Number of Generations<a class="headerlink" href="#4-number-of-generations" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Few (4-8)</strong>: Faster training, less exploration</li>
<li><strong>Many (8-16)</strong>: More exploration, slower training</li>
</ul>
<hr />
<h2 id="evaluation">Evaluation<a class="headerlink" href="#evaluation" title="Permanent link">&para;</a></h2>
<p>PPO evaluation includes:</p>
<div class="highlight"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="n">EvalConfig</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;./output/ppo_model&quot;</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./eval_results/ppo&quot;</span><span class="p">,</span>

    <span class="c1"># Task configuration</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="n">data_task_type</span><span class="o">=</span><span class="s2">&quot;sft&quot;</span><span class="p">,</span>

    <span class="c1"># Metrics</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;perplexity&quot;</span><span class="p">,</span> <span class="s2">&quot;reward_accuracy&quot;</span><span class="p">],</span>

    <span class="c1"># Dataset</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;HuggingFaceH4/ultrachat_200k&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test_sft&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>

    <span class="c1"># Generation</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span>
<span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">run_eval</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">Note</span><span class="p">:</span> <span class="n">For</span> <span class="n">Base</span> <span class="n">model</span> <span class="n">we</span> <span class="n">don</span><span class="s1">&#39;t need to pass model_path also if model is not trained using lora no need to pass base_model also</span>
</code></pre></div>
<hr />
<h2 id="common-issues">Common Issues<a class="headerlink" href="#common-issues" title="Permanent link">&para;</a></h2>
<h3 id="reward-model-not-found">Reward Model Not Found<a class="headerlink" href="#reward-model-not-found" title="Permanent link">&para;</a></h3>
<p><strong>Solutions</strong>:
- Verify reward model name/path
- Check HuggingFace authentication
- Train custom reward model</p>
<h3 id="training-instability">Training Instability<a class="headerlink" href="#training-instability" title="Permanent link">&para;</a></h3>
<p><strong>Solutions</strong>:
- Reduce learning rate
- Increase KL coefficient
- Reduce clipping range
- Use smaller batch size</p>
<h3 id="low-rewards">Low Rewards<a class="headerlink" href="#low-rewards" title="Permanent link">&para;</a></h3>
<p><strong>Solutions</strong>:
- Check reward model quality
- Verify reward model is appropriate for task
- Adjust reward function weights
- Increase number of generations</p>
<hr />
<h2 id="comparison-with-other-algorithms">Comparison with Other Algorithms<a class="headerlink" href="#comparison-with-other-algorithms" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>PPO</th>
<th>DPO</th>
<th>GRPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reward Model</td>
<td>Required</td>
<td>Not needed</td>
<td>Not needed</td>
</tr>
<tr>
<td>Training Speed</td>
<td>Slow</td>
<td>Fast</td>
<td>Medium</td>
</tr>
<tr>
<td>Setup Complexity</td>
<td>High</td>
<td>Low</td>
<td>Medium</td>
</tr>
<tr>
<td>Reward Control</td>
<td>Fine-grained</td>
<td>Coarse</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Paper</a></li>
<li><a href="https://huggingface.co/docs/trl/ppo_trainer">TRL PPO Documentation</a></li>
</ul>
<hr />
<h2 id="next-steps">Next Steps<a class="headerlink" href="#next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li><strong><a href="../dpo/">DPO Guide</a></strong> - Learn about Direct Preference Optimization</li>
<li><strong><a href="../grpo/">GRPO Guide</a></strong> - Learn about Group Relative Policy Optimization</li>
<li><strong><a href="../../user-guide/reward-model-training/">Reward Model Training</a></strong> - Train custom reward models</li>
</ul></div>
        
        
    </div>

    
      <footer class="col-md-12 text-center">
          
          
            <hr>
            <p>
            <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
            </p>
          

          
          
      </footer>
    
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../../js/bootstrap-3.0.3.min.js"></script>

    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
        
                <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/languages/python.min.js"></script>
                <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/languages/yaml.min.js"></script>
        
    <script>hljs.initHighlightingOnLoad();</script>
    

    <script>var base_url = "../.."</script>
    
    <script src="../../js/base.js"></script>
    <script src="../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">
                    <span aria-hidden="true">&times;</span>
                    <span class="sr-only">Close</span>
                </button>
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    </body>

</html>
