recipe:
  name: "llama-code-bolt"
  description: "Train LLaMA 3.2 3B on code generation tasks using BOLT"
  model: "meta-llama/Llama-3.2-3B-Instruct"
  dataset: "mbpp"
  task: "code_generation"
  algorithm: "bolt"
  backend: "trl"
  tags: ["llama", "bolt", "code", "curriculum"]
  requires_auth: true
  estimated_time: "2-3 hours"
  estimated_memory: "18GB VRAM"

config:
  algo: bolt
  model:
    name_or_path: "meta-llama/Llama-3.2-3B-Instruct"
    backend: "trl"
    precision: "bf16"
    gradient_checkpointing: true
    max_seq_length: 1024
    use_peft: true
    lora_r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  datasets:
    - name: "mbpp"
      split: "train"
      column_mapping:
        text: "prompt"
        code: "response"
  rewards:
    - type: "code_execution"
      weight: 0.7
      params: {}
    - type: "code_quality"
      weight: 0.3
      params: {}
  train:
    epochs: 1
    max_steps: 150
    beta: 0.005
    enable_thinking: false
    per_device_batch_size: 4
    gradient_accumulation_steps: 2
    num_generations: 8
    learning_rate: 0.000025
    max_prompt_length: 512
    max_completion_length: 512
    temperature: 0.6
    top_p: 0.95
    baseline_enabled: true
    use_baseline_advantages: true
    baseline_warm_start: null
    baseline_rho_min: 0.875
    baseline_rho_max: 0.96
    baseline_D_half: 0.5
    curriculum_enabled: true
    curriculum_epsilon: 0.05
    curriculum_update_freq: 10
    eval_steps: 50
    save_steps: 50
    logging_steps: 10
    seed: 42
  logging:
    output_dir: "./output/llama-bolt-code"
    run_name: "llama-code-bolt"
    loggers: []
  distributed:
    backend: "single"
  chat_template: "llama3"
