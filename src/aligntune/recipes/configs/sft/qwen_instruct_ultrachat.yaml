recipe:
  name: "qwen-instruct-ultrachat"
  description: "Fine-tune Qwen 2.5 3B on UltraChat for conversational AI"
  model: "Qwen/Qwen2.5-3B-Instruct"
  dataset: "HuggingFaceH4/ultrachat_200k"
  task: "conversation"
  algorithm: "sft"
  backend: "unsloth"
  tags: ["qwen", "conversation", "efficient", "popular"]
  requires_auth: false
  estimated_time: "2-3 hours"
  estimated_memory: "16GB VRAM"

config:
  algo: sft
  
  model:
    name_or_path: "Qwen/Qwen2.5-3B-Instruct"
    precision: "bf16"
    gradient_checkpointing: true
    max_seq_length: 2048
    use_peft: true
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.1
    lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  datasets:
    - name: "HuggingFaceH4/ultrachat_200k"
      split: "train_sft"
      max_samples: 50000
      column_mapping:
        text: "messages"
  
  train:
    epochs: 1
    per_device_batch_size: 4
    gradient_accumulation_steps: 4
    learning_rate: 2e-4
    warmup_steps: 50
    max_grad_norm: 1.0
    optimizer: "adamw_8bit"
    lr_scheduler: "cosine"
    logging_steps: 10
    save_steps: 500
    eval_steps: 500
  
  logging:
    output_dir: "./output/qwen-ultrachat-sft"
    run_name: "qwen-conversation-ultrachat"
    loggers: []
  
  distributed:
    backend: "single"
  
  chat_template: "auto"
